,header,contents,type,page_url,is_howto,char_count,word_count
0,What’s behind the stack?,"We want to empower a data scientist to develop applications - not just models - independently. Compare this to a modern full-stack engineer who is able to develop full-fledged websites on their own, thanks to the modern web stack.
 
 ![](/assets/stack-ds.png)
 
 A definining feature of data science applications is that they need _data_. The data may be tabular, stored in a data warehouse, or unstructured data like text or images. In any case, the question of data discovery and data access needs to be addressed.
 
 ![](/assets/stack-data.png)
 
 Another defining feature of data science is that it involves processing data or _computation_. Machine learning in particular is a compute-heavy activity, so being able to provision enough compute capacity is crucial. Even if your data is small and models lightweight, you can benefit from parallelized computation to speed up exploration and experimentation.
 
 ![](/assets/stack-compute.png)
 
 A data science application consists of multiple units of computation which can be [organized as a workflow](/docs/dags-in-data-science).
 Whereas the compute layer provides raw horsepower for executing functions, it is the job of a workflow orchestrator to ensure
 that the workflow gets executed reliably, on schedule, without any human intervention.
 
 ![](/assets/stack-dag.png)
 
 The development of a workflow happens through many iterations. Multiple variants of the project may be tested concurrently, developed by a team of data scientists working simultaneously. Tracking and organizing the work requires a robust versioning
 layer that makes sure that variants don't interfere with each other and [experiments can be tracked and analyzed consistently](/docs/versioning-code-and-models).
 
 ![](/assets/stack-version.png)
 
 To produce real business value, workflows must be connected to surrounding
 business systems and set up to run reliably without human intervention, that is, they must be _deployed to production_. There isn't a single way to deploy machine learning to production. Instead, different applications necessitate different deployment patterns.
 
 ![](/assets/stack-deploy.png)
 
 Once all these layers are in place, one can start optimizing the quality and performance of models. The infrastructure stack
 can be shared by a diverse set of applications and models, from basic regression to sophisticated deep learning. They
 all benefit from the layers below.
 
 ![](/assets/stack-model.png)",H2,https://outerbounds.com/docs/infra-stack#whats-behind-the-stack,False,2467.0,353.0
1,Using the stack,"You can start building applications with the stack today! Making this happen requires collaboration between engineers and data
 scientists: The engineers need to set up the data, compute, and orchestration layers. Fear not: Our [documentation for engineering](/engineering/welcome/) includes customizable blueprints that help you get started.
 
 Once the foundational layers are in place, [Metaflow](https://docs.metaflow.org) wraps them in a human-friendly API, allowing
 data scientists to build applications without having to get their hands dirty with the infrastructure. Metaflow comes with a built-in versioning layer, as well as patterns for robust deployments, which empower data scientists to prototype and productionize applications independently.
 
 ![](/assets/stack-triangles.png)
 
 While the stack provides a robust scaffolding for applications, it doesn't build them by itself. It is still up to you, the
 data scientist, to understand business requirements, data, modeling constraints, and patterns of deployments. This can feel
 like a daunting job!
 
 Over many years, we have helped hundreds of data scientists and engineers [on our Slack](http://slack.outerbounds.co) who
 have asked a myriad of questions, some mundane and some very advanced, touching all layers of the stack. We are collecting
 these questions with answers on this site, so they can benefit you during your journey.
 
 If you need help with anything related to the stack and you can't find an answer quickly here, [join our Slack for support](http://slack.outerbounds.co)!",H2,https://outerbounds.com/docs/infra-stack#using-the-stack,False,1562.0,222.0
2,See Also,"[Test the stack live with a Metaflow Sandbox](/sandbox)
 
 
 [Video: The Modern Stack for ML Infrastructure](https://www.youtube.com/watch?v=gZnhSHvhuFQ)
 
 
 [Book: Effective Data Science Infrastructure](https://www.manning.com/books/effective-data-science-infrastructure)",H2,https://outerbounds.com/docs/infra-stack#see-also,False,273.0,24.0
3,Data,,H3,https://outerbounds.com/docs/how-to-index#data,False,0.0,1.0
4,Local Data,,H4,https://outerbounds.com/docs/how-to-index#local-data,False,0.0,1.0
5,Cloud Data,,H4,https://outerbounds.com/docs/how-to-index#cloud-data,False,0.0,1.0
6,Core Concepts,,H4,https://outerbounds.com/docs/how-to-index#core-concepts,False,0.0,1.0
7,Compute,,H3,https://outerbounds.com/docs/how-to-index#compute,False,0.0,1.0
8,Configuring Remote Instances,,H4,https://outerbounds.com/docs/how-to-index#configuring-remote-instances,False,0.0,1.0
9,Performance Acceleration,,H4,https://outerbounds.com/docs/how-to-index#performance-acceleration,False,0.0,1.0
10,Orchestration,,H3,https://outerbounds.com/docs/how-to-index#orchestration,False,0.0,1.0
11,Flow Architecture,,H4,https://outerbounds.com/docs/how-to-index#flow-architecture,False,0.0,1.0
12,Iterative Flow Development,,H4,https://outerbounds.com/docs/how-to-index#iterative-flow-development,False,0.0,1.0
13,Core Concepts,,H4,https://outerbounds.com/docs/how-to-index#core-concepts,False,0.0,1.0
14,Versioning,,H3,https://outerbounds.com/docs/how-to-index#versioning,False,0.0,1.0
15,Versioned Flows and Artifacts,,H4,https://outerbounds.com/docs/how-to-index#versioned-flows-and-artifacts,False,0.0,1.0
16,Versioned Environments,,H4,https://outerbounds.com/docs/how-to-index#versioned-environments,False,0.0,1.0
17,Experiment Tracking,,H4,https://outerbounds.com/docs/how-to-index#experiment-tracking,False,0.0,1.0
18,Core Concepts,,H4,https://outerbounds.com/docs/how-to-index#core-concepts,False,0.0,1.0
19,Deployment,,H3,https://outerbounds.com/docs/how-to-index#deployment,False,0.0,1.0
20,Alerting,,H4,https://outerbounds.com/docs/how-to-index#alerting,False,0.0,1.0
21,Deploying Models,,H4,https://outerbounds.com/docs/how-to-index#deploying-models,False,0.0,1.0
22,Deploying Flows,,H4,https://outerbounds.com/docs/how-to-index#deploying-flows,False,0.0,1.0
23,Testing,,H4,https://outerbounds.com/docs/how-to-index#testing,False,0.0,1.0
24,Modeling,,H3,https://outerbounds.com/docs/how-to-index#modeling,False,0.0,1.0
25,Modeling Frameworks,,H4,https://outerbounds.com/docs/how-to-index#modeling-frameworks,False,0.0,1.0
26,Flow Design,,H4,https://outerbounds.com/docs/how-to-index#flow-design,False,0.0,1.0
27,Hyperparameter Tuning,,H4,https://outerbounds.com/docs/how-to-index#hyperparameter-tuning,False,0.0,1.0
28,Core Concepts,,H4,https://outerbounds.com/docs/how-to-index#core-concepts,False,0.0,1.0
29,Question,How can I do a grid search with Metaflow using scikit-learn's [ParameterGrid](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.ParameterGrid.html#sklearn.model_selection.ParameterGrid)?,H2,https://outerbounds.com/docs/grid-search-with-metaflow#question,True,210.0,12.0
30,Solution,"This flow uses scikit-learn's `ParameterGrid` object to evaluate each grid point on a different worker using Metaflow's `foreach` pattern. This parallelizes workers across processes on a single node, or you can use Metaflow's [@batch](https://docs.metaflow.org/metaflow/scaling-out-and-up) decorator to execute on different machines.
 
 
 
 
 
 
 ```py title=""foreach_param_grid.py""
 from metaflow import FlowSpec, step
 
 class ParamGridFlow(FlowSpec):
     
     @step
     def start(self):
         from sklearn.datasets import load_iris
         data = load_iris()
         self.X, self.y = data['data'], data['target']
         self.next(self.make_grid)
         
     @step
     def make_grid(self):
         from sklearn.model_selection import ParameterGrid
         param_values = {'max_depth': [2, 4, 8, 16], 
                         'criterion': ['entropy', 'gini']}
         self.grid_points = list(
             ParameterGrid(param_values)
         )
         # evaluate each in cross product of ParameterGrid.
         self.next(self.evaluate_model, 
                   foreach='grid_points')
         
     @step
     def evaluate_model(self):
         from sklearn.tree import ExtraTreeClassifier
         from sklearn.model_selection import cross_val_score
         self.clf = ExtraTreeClassifier(**self.input)
         self.scores = cross_val_score(self.clf, self.X, 
                                       self.y, cv=5)
         self.next(self.join)
         
     @step
     def join(self, inputs):
         import numpy as np
         self.mean_scores = [np.mean(model.scores) 
                             for model in inputs]
         self.next(self.end)
         
     @step
     def end(self):
         pass
     
 if __name__ == ""__main__"":
     ParamGridFlow()
 ```
 
 
 
 
 
 ```bash
 python foreach_param_grid.py run
 ```
 
 <CodeOutputBlock lang=""bash"">
 
 ```
     ...
      [1654221281136541/make_grid/2 (pid 71177)] Task is starting.
      [1654221281136541/make_grid/2 (pid 71177)] Foreach yields 8 child steps.
      [1654221281136541/make_grid/2 (pid 71177)] Task finished successfully.
     ...
      [1654221281136541/join/11 (pid 71261)] Task is starting.
      [1654221281136541/join/11 (pid 71261)] Task finished successfully.
     ...
      [1654221281136541/end/12 (pid 71284)] Task is starting.
      [1654221281136541/end/12 (pid 71284)] Task finished successfully.
     ...
 ```
 
 </CodeOutputBlock>",H2,https://outerbounds.com/docs/grid-search-with-metaflow#solution,False,2446.0,634.0
31,Further Reading,"* [Parallel cross-validation](/docs/cross-validation-in-parallel)
 * [Optimize hyperparameters in parallel on GPUs](/docs/scale-model-training-and-tuning)",H2,https://outerbounds.com/docs/grid-search-with-metaflow#further-reading,False,154.0,10.0
32,Question,"I have a scikit-learn workflow that I want to incorporate into a Metaflow flow. How can I include model fitting, prediction, feature transformations, and other capabilities enabled by scikit-learn in flow steps?",H2,https://outerbounds.com/docs/use-sklearn-estimators-with-metaflow#question,True,211.0,32.0
33,Solution,"Note that this example uses a [random forest classifier](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) but the following applies to all scikit-learn models.
 
 To turn this into a Metaflow flow, you first need to decide what your steps are going to be. In this case, there are distinct steps to: 
 
 1. Load data.
 2. Instantiate a model.
 3. Train a model with cross-validation.",H2,https://outerbounds.com/docs/use-sklearn-estimators-with-metaflow#solution,False,428.0,60.0
34,Estimators to Flows,"In general, this involves some design choices and we have some rules of thumb [here](https://docs.metaflow.org/metaflow/basics). A benefit of separating flows into Metaflow steps is that you can resume failed computation from any step without having to recompute everything prior to the failed step which makes development much faster.",H3,https://outerbounds.com/docs/use-sklearn-estimators-with-metaflow#estimators-to-flows,False,335.0,48.0
35,Run Flow,"This flow shows how to:
 * Import `FlowSpec` and `step`.
 * Include step-specific imports within each step.
 * Assign any data structures you wish to pass between steps to self.
 * Train a model and apply cross validation to evaluate it.
 
 
 
 
 
 
 ```py title=""fit_sklearn_estimator.py""
 from metaflow import FlowSpec, step
 
 class SklearnFlow(FlowSpec):
 
     @step
     def start(self):
         from sklearn import datasets
         self.iris = datasets.load_iris()
         self.X = self.iris['data']
         self.y = self.iris['target']
         self.next(self.rf_model) 
 
     @step
     def rf_model(self):
         from sklearn.ensemble import RandomForestClassifier
         self.clf = RandomForestClassifier(
             n_estimators=10, 
             max_depth=None,
             min_samples_split=2, 
             random_state=0
         )
         self.next(self.train)
     
     @step
     def train(self):
         from sklearn.model_selection import cross_val_score
         self.scores = cross_val_score(self.clf, self.X, 
                                       self.y, cv=5)
         self.next(self.end)
     
     @step
     def end(self):
         print(""SklearnFlow is all done."")
 
 
 if __name__ == ""__main__"":
     SklearnFlow()
 ```
 
 
 
 
 The example shows how to use the `--with card` CLI option to use a Metaflow `card` which produces HTML visualizations. 
 
 
 ```bash
 python fit_sklearn_estimator.py run --with card
 ```
 
 <CodeOutputBlock lang=""bash"">
 
 ```
     ...
      [1663366789156643/end/4 (pid 5065)] Task is starting.
      [1663366789156643/end/4 (pid 5065)] SklearnFlow is all done.
      [1663366789156643/end/4 (pid 5065)] Task finished successfully.
     ...
 ```
 
 </CodeOutputBlock>",H3,https://outerbounds.com/docs/use-sklearn-estimators-with-metaflow#run-flow,False,1744.0,447.0
36,View Card,"Now you can view the card for the `train` step using this command:
 
 
 ```bash
 python fit_sklearn_estimator.py card view train
 ```",H3,https://outerbounds.com/docs/use-sklearn-estimators-with-metaflow#view-card,False,133.0,22.0
37,Further Reading,"* [Grid search with Metaflow](/docs/grid-search-with-metaflow)
 * [Parallel cross-validation](/docs/cross-validation-in-parallel)",H2,https://outerbounds.com/docs/use-sklearn-estimators-with-metaflow#further-reading,False,129.0,8.0
38,Question,How do I build and fit a Keras model in a Metaflow flow?,H2,https://outerbounds.com/docs/use-keras-with-metaflow#question,True,56.0,13.0
39,Solution,"There are several ways to build Keras models. In the example you will see the [Keras Sequential API](https://keras.io/api/models/sequential/) in a Metaflow flow. 
 
 
 
 
 
 The flow shows how to:
 * Load and preprocess data.
 * Make a Keras model. 
 * Fit and save the model.
     * Since Keras models are not serializable with Pickle and Metaflow uses Pickle to serialize artifacts, the model is saved to disk in `.h5` format and then saved as bytes instead of directly saving the object to `self.model`.
 * Load the model and evaluate it on the test dataset.
 
 
 
 
 
 
 ```py title=""fit_keras.py""
 from metaflow import FlowSpec, step, Parameter, batch
 import tensorflow as tf
 import tempfile
 from keras_helpers import create_model
 
 class KerasFlow(FlowSpec):
 
     @step
     def start(self):
         import numpy as np
         self.n_class = 10
         self.input_shape = (28, 28, 1)
         mnist = tf.keras.datasets.mnist.load_data()
         (x_train, y_train), (x_test, y_test) = mnist
         x_train = x_train.astype(""float32"") / 255
         x_test = x_test.astype(""float32"") / 255
         self.x_train = np.expand_dims(x_train, -1)
         self.x_test = np.expand_dims(x_test, -1)
         self.y_train = tf.keras.utils.to_categorical(
             y_train, self.n_class)
         self.y_test = tf.keras.utils.to_categorical(
             y_test, self.n_class)
         self.next(self.fit_model) 
     
     @step
     def fit_model(self):
         model = create_model(self.x_train, self.y_train,
                              self.input_shape, 
                              self.n_class)
         with tempfile.NamedTemporaryFile() as f:
             tf.keras.models.save_model(model, f.name,
                                        save_format='h5')
             self.model = f.read()
         self.next(self.evaluate_model)
 
     @step
     def evaluate_model(self):
         with tempfile.NamedTemporaryFile() as f:
             f.write(self.model)
             f.flush()
             model =  tf.keras.models.load_model(f.name)
         self.score = model.evaluate(self.x_test,
                                     self.y_test,
                                     verbose=0)
         self.next(self.end)
 
     @step
     def end(self):
         print(""Test loss:"", self.score[0])
         print(""Test accuracy:"", self.score[1])
 
 if __name__ == ""__main__"":
     KerasFlow()
 ```
 
 
 
 
 The Keras code used in this flow comes from this [MNIST example](https://keras.io/examples/vision/mnist_convnet/). The model is built in the `create_model` function of the `keras_helpers.py` script that is used in the flow.
 
 
 ```py title=""keras_helpers.py""
 import tensorflow as tf
 
 def create_model(x_train, y_train, input_shape, 
                  n_class):
     model = tf.keras.Sequential([
         tf.keras.layers.Input(shape=input_shape),
         tf.keras.layers.Conv2D(
             32, kernel_size=(3, 3), activation=""relu""),
         tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),
         tf.keras.layers.Conv2D(
             64, kernel_size=(3, 3), activation=""relu""),
         tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),
         tf.keras.layers.Flatten(),
         tf.keras.layers.Dropout(0.5),
         tf.keras.layers.Dense(n_class, 
                               activation=""softmax""),
     ])
     model.compile(loss=""categorical_crossentropy"",
                   optimizer=""adam"", 
                   metrics=[""accuracy""])
     model.fit(x_train, y_train, 
               batch_size=32, epochs=1, 
               validation_split=0.1)
     return model
 ```",H2,https://outerbounds.com/docs/use-keras-with-metaflow#solution,False,3614.0,1004.0
40,Run Flow,"```bash
 python fit_keras.py run
 ```
 
 <CodeOutputBlock lang=""bash"">
 
 ```
     ...
      [1654221292500619/start/1 (pid 71403)] Task is starting.
      [1654221292500619/start/1 (pid 71403)] Task finished successfully.
     ...
      [1654221292500619/end/4 (pid 71613)] Task is starting.
      [1654221292500619/end/4 (pid 71613)] Test loss: 0.0586005374789238
      [1654221292500619/end/4 (pid 71613)] Test accuracy: 0.9815000295639038
      [1654221292500619/end/4 (pid 71613)] Task finished successfully.
     ...
 ```
 
 </CodeOutputBlock>",H3,https://outerbounds.com/docs/use-keras-with-metaflow#run-flow,False,549.0,94.0
41,Further Reading,"* [Uploading and downloading data in flows](https://docs.metaflow.org/metaflow/data)
 * [Scaling flows](https://docs.metaflow.org/metaflow/scaling-out-and-up)
 * [Keras examples](https://keras.io/examples/)
 * [Save and load Keras models](https://www.tensorflow.org/guide/keras/save_and_serialize)",H2,https://outerbounds.com/docs/use-keras-with-metaflow#further-reading,False,297.0,19.0
42,Question,I have an [Optuna](https://optuna.org) process for hyperparameter tuning and want to structure it in a Metaflow flow.,H2,https://outerbounds.com/docs/use-optuna-with-metaflow#question,True,117.0,17.0
43,Solution,"There are several ways to leverage Optuna's optimization features with Metaflow. When designing a Metaflow flow to structure, execute, and store results of an Optuna study it is important to understand the characteristics of the objective function you are optimizing. For example, depending on how long it takes to evaluate the objective function you may wish to execute the flow on a single process, with multiple processes, or even across multiple nodes.",H2,https://outerbounds.com/docs/use-optuna-with-metaflow#solution,False,456.0,72.0
44,Run Flow,"This flow shows how you can run an optimization loop with 10 evaluations of the objective function in a single process. 
 
 Resources to help extend the flow for multi-process and multi-node implementations are linked in the further reading section below.
 
 
 
 
 
 
 ```py title=""optuna_flow.py""
 from metaflow import FlowSpec, step
 from metaflow.cards import Image
 
 def objective(trial):
     from sklearn.datasets import load_iris
     from sklearn.tree import ExtraTreeClassifier
     from sklearn.model_selection import cross_val_score
     import numpy as np
     data = load_iris()
     X, y = data['data'], data['target']
     max_depth = trial.suggest_int('max_depth', 2, 16)
     criterion = trial.suggest_categorical(
         'criterion', 
         [""gini"", ""entropy""]
     )
     model = ExtraTreeClassifier(max_depth=max_depth, 
                                 criterion=criterion)
     return np.mean(cross_val_score(model, X, y, cv=5))
 
 class OptunaFlow(FlowSpec):
     
     @step
     def start(self):
         self.next(self.optimization_loop)
         
     @step
     def optimization_loop(self):
         import optuna
         self.study = optuna.create_study() 
         self.study.optimize(objective, n_trials=10)
         self.next(self.end)
         
     @step
     def end(self):
         self.results = self.study.trials_dataframe()
     
 if __name__ == ""__main__"":
     OptunaFlow()
 ```
 
 
 
 
 
 ```bash
 python optuna_flow.py run
 ```
 
 <CodeOutputBlock lang=""bash"">
 
 ```
     ...
      [1654221285645277/end/3 (pid 71342)] Task is starting.
      [1654221285645277/end/3 (pid 71342)] Task finished successfully.
     ...
 ```
 
 </CodeOutputBlock>",H3,https://outerbounds.com/docs/use-optuna-with-metaflow#run-flow,False,1694.0,386.0
45,Access Artifacts Outside of Flow,"The following can be run in a Python script or notebook to access the contents of the DataFrame that was stored as a flow artifact with `self.results`. 
 
 
 ```python
 from metaflow import Flow 
 run = Flow('OptunaFlow').latest_run
 run.data.results.head()
 ```
     
 <HTMLOutputBlock >
 
 
 
 
 ```html
 <div>
 <style scoped>
     .dataframe tbody tr th:only-of-type {
         vertical-align: middle;
     }
 
     .dataframe tbody tr th {
         vertical-align: top;
     }
 
     .dataframe thead th {
         text-align: right;
     }
 </style>
 <table border=""1"" class=""dataframe"">
   <thead>
     <tr style=""text-align: right;"">
       <th></th>
       <th>number</th>
       <th>value</th>
       <th>datetime_start</th>
       <th>datetime_complete</th>
       <th>duration</th>
       <th>params_criterion</th>
       <th>params_max_depth</th>
       <th>state</th>
     </tr>
   </thead>
   <tbody>
     <tr>
       <th>0</th>
       <td>0</td>
       <td>0.926667</td>
       <td>2022-06-02 20:54:48.817826</td>
       <td>2022-06-02 20:54:48.861575</td>
       <td>0 days 00:00:00.043749</td>
       <td>gini</td>
       <td>6</td>
       <td>COMPLETE</td>
     </tr>
     <tr>
       <th>1</th>
       <td>1</td>
       <td>0.733333</td>
       <td>2022-06-02 20:54:48.861829</td>
       <td>2022-06-02 20:54:48.864031</td>
       <td>0 days 00:00:00.002202</td>
       <td>entropy</td>
       <td>2</td>
       <td>COMPLETE</td>
     </tr>
     <tr>
       <th>2</th>
       <td>2</td>
       <td>0.913333</td>
       <td>2022-06-02 20:54:48.864197</td>
       <td>2022-06-02 20:54:48.866374</td>
       <td>0 days 00:00:00.002177</td>
       <td>gini</td>
       <td>10</td>
       <td>COMPLETE</td>
     </tr>
     <tr>
       <th>3</th>
       <td>3</td>
       <td>0.946667</td>
       <td>2022-06-02 20:54:48.866538</td>
       <td>2022-06-02 20:54:48.868643</td>
       <td>0 days 00:00:00.002105</td>
       <td>entropy</td>
       <td>9</td>
       <td>COMPLETE</td>
     </tr>
     <tr>
       <th>4</th>
       <td>4</td>
       <td>0.960000</td>
       <td>2022-06-02 20:54:48.868813</td>
       <td>2022-06-02 20:54:48.870919</td>
       <td>0 days 00:00:00.002106</td>
       <td>gini</td>
       <td>15</td>
       <td>COMPLETE</td>
     </tr>
   </tbody>
 </table>
 </div>
 ```
 
 
 
 </HTMLOutputBlock>",H3,https://outerbounds.com/docs/use-optuna-with-metaflow#access-artifacts-outside-of-flow,False,2338.0,612.0
46,Further Reading,"* Metaflow's [foreach](https://docs.metaflow.org/metaflow/basics#foreach) pattern
 * [Optuna parallelism documentation](https://optuna.readthedocs.io/en/stable/tutorial/10_key_features/004_distributed.html#easy-parallelization)
 * [Slack thread](https://outerbounds-community.slack.com/archives/C020U025QJK/p1633429832144100) from [@msubzero2000](https://github.com/msubzero2000/bayesian-optimisation-metaflow-optuna) on distributed tuning with Metaflow and Optuna.",H2,https://outerbounds.com/docs/use-optuna-with-metaflow#further-reading,False,465.0,20.0
47,A Reminder on Supervised Learning,"Recall that supervised learning is the subset of machine learning in which you’re predicting a label: If you’re predicting a categorical variable, such as “click” or “not” for website conversion rate, or “benign” or “malignant” for diagnostic imaging of tumors, the task is known as classification; If you’re predicting a numerical variable, such as the price of a property or the lifetime value of a customer, the task is known as regression; In both cases, you’ll need to have a robust way to gauge the performance of any model you build and cross-validation is one such method.",H2,https://outerbounds.com/docs/what-is-cross-val#a-reminder-on-supervised-learning,False,580.0,97.0
48,"Training Data, Accuracy, and Cross-validation","To build a supervised learning model, you require two things:
 
 Training data (which the algorithms learn from) and
 An evaluation metric, such as accuracy.
 After training your algorithm on your training data, you can use it to make predictions on a labeled holdout (or test) set and compare those predictions with the known labels to compute how well it performs.
 
 You can also use a technique called (k-fold) cross-validation (CV), where you train and test several times using different holdout sets and compute the relevant accuracies:
 
 ![](/assets/what-is-cross-val-01.png)
 <Caption> Cross-validation iterates through each fold with new train and test splits </Caption>
 
 The algorithm for k-fold cross-validation is as follows:
 
 * You split your labeled data into k subsets S_1, … , S_k;
 * For each i, 
     * You hold out S_i, 
     * You train your model on the rest of the data
     * You compute the accuracy of your trained model on S_i
 * This gives you k accuracies for your model, which tells you how well your model performs on unseen data and hence how it generalizes!
 
 As we discuss below in the section Practical notes on using CV, in many situations you should use both a holdout in addition to CV.
 
 As cross-validation is strictly more expensive than a simple one-fold test/train split, it’s a good question as to why we want to do it in the first place. The answer is that it is both helpful to reduce the chance of overfitting and it also allows us to retrieve statistics on model performance, such as median accuracies and percentiles.",H2,https://outerbounds.com/docs/what-is-cross-val#training-data-accuracy-and-cross-validation,False,1572.0,282.0
49,Practical Notes on Using Cross-validation,"1. The more folds you use, the more certainty you will have about your model performance BUT also: the more folds you use, the more computationally intensive your k-fold CV is as the more models you train!
 2. For classification tasks, you’ll most likely want to use stratified cross-validation, which preserves the target variable across folds so that, for example, you don’t get one fold with one target variable for all data points. Major packages for supervised learning, such as [scikit-learn](https://scikit-learn.org/stable/) in Python and [tidymodels](https://www.tidymodels.org/) in R, have APIs to access stratified CV.
 3. Cross-validation is often used with grid search, randomized grid search, and other methods of hyperparameter optimization.
 4. If you are using CV to make any decisions like model selection (for example, using hyperparameter tuning), you also need to set aside a holdout set to evaluate the final performance of your model in order to avoid overfitting. Moreover, after you have found the best model using CV, you want to retrain your models on the entire training data before moving it to production.
 5. Cross-validation for time series works differently! A common mistake in data science is applying traditional CV to time series. See Rob Hyndman’s post about CV for time series [here](https://robjhyndman.com/hyndsight/tscv/) for further details.",H2,https://outerbounds.com/docs/what-is-cross-val#practical-notes-on-using-cross-validation,False,1384.0,209.0
50,How do I?,[Use Metaflow to train a model for each cross-validation fold](/docs/cross-validation-in-parallel),H2,https://outerbounds.com/docs/what-is-cross-val#how-do-i,False,98.0,10.0
51,Question,How can I use Metaflow to train a model for each cross-validation fold?,H2,https://outerbounds.com/docs/cross-validation-in-parallel#question,True,71.0,13.0
52,Solution,"You can use Metaflow’s `foreach` pattern to run do this. A nice side effect is that structuring code this way makes it easy to run each parallel model training on [whatever resources you need](https://docs.metaflow.org/metaflow/scaling-out-and-up).
 
 This example uses the vanilla [KFold cross-validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics)#k-fold_cross-validation) implementation in scikit-learn, but using `foreach` can work with other lists such as any of the [other cross-validation](https://scikit-learn.org/stable/modules/cross_validation.html) techniques scikit-learn offers to deal with imbalanced classes, time series data, and more. 
 
 
 
 
 
 This flow shows how to 
 * Load data.
 * Generate KFold splits. See the highlighted lines in `make_splits`.
 * Fit and score a model for each fold.
 * Average the scores of the model and save for later analysis.
 
 
 
 
 
 
 ```py title=""kfold_cv.py""
 from metaflow import FlowSpec, step, Parameter
 
 class KFoldFlow(FlowSpec):
     
     k = Parameter('k', default=5)
 
     @step
     def start(self):
         from sklearn.datasets import load_wine
         data = load_wine()
         self.x = data['data']
         self.y = data['target']
         self.next(self.make_splits)
         
     @step
     def make_splits(self):
         from sklearn.model_selection import KFold
         kfold = KFold(n_splits=self.k)
         #highlight-start
         self.split = []
         for train_id, valid_id in kfold.split(self.x):
             self.split.append((train_id, valid_id))
         #highlight-end
         self.next(self.fit_and_score_model, 
                   foreach=""split"") 
 
     @step
     def fit_and_score_model(self):
         from sklearn.tree import ExtraTreeClassifier
         from sklearn.metrics import accuracy_score
         self.model = ExtraTreeClassifier()
         train_x = self.x[self.input[0]]
         valid_x = self.x[self.input[1]]
         train_y = self.y[self.input[0]]
         valid_y = self.y[self.input[1]]
         self.model.fit(train_x, train_y)
         self.score = accuracy_score(
             valid_y, 
             self.model.predict(valid_x)
         )
         self.next(self.average_scores)
         
     @step
     def average_scores(self, models):
         import numpy as np
         self.mean_score = np.mean([model.score 
                                    for model in models])
         self.next(self.end)
     
     @step
     def end(self):
         pass
 
 
 if __name__ == ""__main__"":
     KFoldFlow()
 ```
 
 
 
 
 
 ```bash
 python kfold_cv.py run
 ```
 
 <CodeOutputBlock lang=""bash"">
 
 ```
     ...
      [1654221286227325/make_splits/2 (pid 71301)] Task is starting.
      [1654221286227325/make_splits/2 (pid 71301)] Foreach yields 5 child steps.
      [1654221286227325/make_splits/2 (pid 71301)] Task finished successfully.
     ...
      [1654221286227325/fit_and_score_model/3 (pid 71327)] Task is starting.
      [1654221286227325/fit_and_score_model/4 (pid 71328)] Task is starting.
      [1654221286227325/fit_and_score_model/5 (pid 71330)] Task is starting.
      [1654221286227325/fit_and_score_model/6 (pid 71331)] Task is starting.
      [1654221286227325/fit_and_score_model/7 (pid 71332)] Task is starting.
      [1654221286227325/fit_and_score_model/5 (pid 71330)] Task finished successfully.
      [1654221286227325/fit_and_score_model/3 (pid 71327)] Task finished successfully.
      [1654221286227325/fit_and_score_model/4 (pid 71328)] Task finished successfully.
      [1654221286227325/fit_and_score_model/6 (pid 71331)] Task finished successfully.
      [1654221286227325/fit_and_score_model/7 (pid 71332)] Task finished successfully.
     ...
 ```
 
 </CodeOutputBlock>",H2,https://outerbounds.com/docs/cross-validation-in-parallel#solution,False,3747.0,809.0
53,Further Reading,* [Optimize hyperparameters in parallel on GPUs](/docs/scale-model-training-and-tuning),H2,https://outerbounds.com/docs/cross-validation-in-parallel#further-reading,False,87.0,7.0
54,Question,How do I build and fit a PyTorch model in a Metaflow flow?,H2,https://outerbounds.com/docs/use-pytorch-with-metaflow#question,True,58.0,13.0
55,Solution,"PyTorch has many use cases and model types. It is important to think about designing your flow based on which steps you want to run in the same compute environment. For example, if you want to run training with a GPU and validation with a CPU you can separate these into steps and have them use different resources using Metaflow decorators like `@batch` and `@kubernetes`.
 
 
 
 
 
 To structure the model as a flow you need to import Metaflow's `FlowSpec` object and `step` object. The flow in this example uses Metaflow's integration with conda for dependency management. In this case a single conda environment is created for all steps in. 
 
 The flow shows how to:
 * Read in hyperparameters of the training process with Metaflow's `Parameter`.
 * Fetch the MNIST dataset and store it in PyTorch `DataLoader` objects to be used in downstream flow tasks.
 * Train the CNN and save the model as a flow artifact.
 * Evaluate the model in a separate step. 
     * Separating training and evaluation steps is useful when you want to run different flow steps on different hardware.
 
 
 
 
 
 
 ```py title=""fit_torch.py""
 from metaflow import (FlowSpec, step, Parameter, 
                       conda_base)
 from torch_utilities import (train, test, Net, 
                              get_data_loaders)
 
 @conda_base(libraries={""pytorch"":""1.11.0"", 
                        ""torchvision"":""0.12.0""}, 
             python=""3.8"")
 class TorchFlow(FlowSpec):
     
     lr = Parameter('lr', default=0.01)
     epochs = Parameter('epochs', default=1)
     
     @step
     def start(self):
         self.next(self.get_data)
         
     @step
     def get_data(self):
         import torch
         train_dataset, train_args = get_data_loaders()
         test_dataset, test_args = get_data_loaders(
             ""test"")
         self.train_loader = torch.utils.data.DataLoader(
             train_dataset, **train_args)
         self.test_loader = torch.utils.data.DataLoader(
             test_dataset, **test_args)
         self.next(self.fit_model)
 
     @step
     def fit_model(self):
         import torch
         import torch.optim as optim
         from torch.optim.lr_scheduler import StepLR
         self.model = Net()
         optimizer = optim.Adadelta(
             self.model.parameters(), lr=self.lr)
         scheduler = StepLR(optimizer, step_size=1)
         for epoch in range(1, self.epochs + 1):
             train(self.model, self.train_loader, 
                   optimizer, epoch)
             _ = test(self.model, self.test_loader)
             scheduler.step()
         self.next(self.evaluate_model)
         
     @step
     def evaluate_model(self):
         self.test_score = test(self.model, 
                                self.test_loader)
         print(f""Model scored {100*self.test_score}%"")
         self.next(self.end)
         
     @step
     def end(self):
         pass
     
 if __name__ == ""__main__"":
     TorchFlow()
 ```
 
 
 
 
 In this flow you see a CNN example from the [PyTorch examples](https://github.com/pytorch/examples/blob/main/mnist/main.py) that is trained on the MNIST digit classification task. Here is the `torch_utilities.py` script used in the flow. It contains PyTorch definitions for a convolutional neural network, a training function, a testing function, and a data loading function. 
 
 
 ```py title=""torch_utilities.py""
 import torch
 import torch.nn as nn
 import torch.nn.functional as F
 import torch
 from torchvision import datasets, transforms
 
 class Net(nn.Module):
     def __init__(self):
         super(Net, self).__init__()
         self.conv1 = nn.Conv2d(1, 32, 3, 1)
         self.conv2 = nn.Conv2d(32, 64, 3, 1)
         self.dropout1 = nn.Dropout(0.25)
         self.dropout2 = nn.Dropout(0.5)
         self.fc1 = nn.Linear(9216, 128)
         self.fc2 = nn.Linear(128, 10)
 
     def forward(self, x):
         x = self.conv1(x)
         x = F.relu(x)
         x = self.conv2(x)
         x = F.relu(x)
         x = F.max_pool2d(x, 2)
         x = self.dropout1(x)
         x = torch.flatten(x, 1)
         x = self.fc1(x)
         x = F.relu(x)
         x = self.dropout2(x)
         x = self.fc2(x)
         output = F.log_softmax(x, dim=1)
         return output
     
 def train(model, train_loader, optimizer, epoch):
     model.train()
     for idx, (data, target) in enumerate(train_loader):
         optimizer.zero_grad()
         output = model(data)
         loss = F.nll_loss(output, target)
         loss.backward()
         optimizer.step()
         if idx * len(data) % 10000 == 0:
             out = 'Train Epoch: ' + \
                 '{} [{}/{} ({:.0f}%)]\tLoss: {:.6f}'.format(
                 epoch, idx * len(data), len(train_loader.dataset),
                 100. * idx / len(train_loader), loss.item())
             print(out)
 
 def test(model, test_loader):
     model.eval()
     test_loss = 0
     correct = 0
     with torch.no_grad():
         for data, target in test_loader:
             output = model(data)
             test_loss += F.nll_loss(
                 output, target, 
                 reduction='sum'
             ).item()
             pred = output.argmax(dim=1, keepdim=True)
             correct += pred.eq(
                 target.view_as(pred)
             ).sum().item()
     return correct / len(test_loader.dataset)
 
 
 def get_data_loaders(name=""train""):
     transform = transforms.Compose([
          transforms.ToTensor(),
          transforms.Normalize((0.1307,), 
                               (0.3081,))
     ])
     if name==""train"":    
         dataset = datasets.MNIST('../data', 
                                  train=True, 
                                  download=True, 
                                  transform=transform)
         train_args = {'batch_size': 32}
         return dataset, train_args
     elif name==""test"": 
         dataset = datasets.MNIST('../data', train=False,
                                   transform=transform)
         test_args = {'batch_size': 32}
         return dataset, test_args
 ```",H2,https://outerbounds.com/docs/use-pytorch-with-metaflow#solution,False,6086.0,1787.0
56,Run Flow,"With the `torch_utilities.py` dependencies and flow defined in `fit_torch.py`, you can run the script with the following command:
 
 
 ```bash
 python fit_torch.py --environment=conda run
 ```
 
 <CodeOutputBlock lang=""bash"">
 
 ```
      Workflow starting (run-id 634):
      [634/start/3294 (pid 3782)] Task is starting.
      [634/start/3294 (pid 3782)] Task finished successfully.
      [634/get_data/3295 (pid 4121)] Task is starting.
     100.0%05-25 14:54:17.557 [634/get_data/3295 (pid 4121)] 0.0%
      [634/get_data/3295 (pid 4121)] 
     102.8%05-25 14:54:18.083 [634/get_data/3295 (pid 4121)] 3.5%
      [634/get_data/3295 (pid 4121)] 
     100.0%05-25 14:54:24.052 [634/get_data/3295 (pid 4121)] 0.1%
      [634/get_data/3295 (pid 4121)] 
     112.7%05-25 14:54:24.307 [634/get_data/3295 (pid 4121)] 22.5%
      [634/get_data/3295 (pid 4121)] 
      [634/get_data/3295 (pid 4121)] Task finished successfully.
      [634/fit_model/3296 (pid 4267)] Task is starting.
      [634/fit_model/3296 (pid 4267)] Train Epoch: 1 [0/60000 (0%)]	Loss: 2.299288
      [634/fit_model/3296 (pid 4267)] Train Epoch: 1 [20000/60000 (33%)]	Loss: 0.907390
      [634/fit_model/3296 (pid 4267)] Train Epoch: 1 [40000/60000 (67%)]	Loss: 0.384034
      [634/fit_model/3296 (pid 4267)] Task finished successfully.
      [634/evaluate_model/3297 (pid 4295)] Task is starting.
      [634/evaluate_model/3297 (pid 4295)] Model scored 92.52%
      [634/evaluate_model/3297 (pid 4295)] Task finished successfully.
      [634/end/3298 (pid 4316)] Task is starting.
      [634/end/3298 (pid 4316)] Task finished successfully.
      Done!
 ```
 
 </CodeOutputBlock>",H3,https://outerbounds.com/docs/use-pytorch-with-metaflow#run-flow,False,1646.0,288.0
57,Further Reading,"* [Scaling with Metaflow](https://docs.metaflow.org/v/r/metaflow/scaling)
 * [PyTorch GPU flow](/docs/scale-model-training-and-tuning)",H2,https://outerbounds.com/docs/use-pytorch-with-metaflow#further-reading,False,134.0,8.0
58,Question,How can I build and fit an XGBoost model in a Metaflow flow?,H2,https://outerbounds.com/docs/use-xgboost-with-metaflow#question,True,60.0,13.0
59,Solution,There are two common ways to fit XGBoost models and you can use both with Metaflow. In XGBoost documentation they are referred to as the [learning API](https://xgboost.readthedocs.io/en/stable/python/python_api.html#module-xgboost.training) and the [scikit-learn API](https://xgboost.readthedocs.io/en/stable/python/python_api.html#module-xgboost.sklearn). This example uses the learning API but you can build flows with either.,H2,https://outerbounds.com/docs/use-xgboost-with-metaflow#solution,False,428.0,44.0
60,Run Flow,"The flow shows how to:
 1. Load training data.
 2. Instantiate the XGBoost model.
 3. Train the model with cross-validation.
 
 
 
 
 
 
 ```py title=""xgb_learning_api.py""
 from metaflow import FlowSpec, step, Parameter
 
 class XGBFlow(FlowSpec):
     
     @step
     def start(self):
         from sklearn import datasets
         self.iris = datasets.load_iris()
         self.X = self.iris['data']
         self.y = self.iris['target']
         self.next(self.train_model)
     
     @step
     def train_model(self):
         import xgboost as xgb
         dtrain = xgb.DMatrix(self.X, self.y)
         self.results = xgb.cv(
             params = {'num_class':3, 
                       'objective':'multi:softmax', 
                       'eval_metric':""mlogloss""}, 
             dtrain=dtrain, 
             verbose_eval=False
         )
         self.next(self.end)
     
     @step
     def end(self):
         print(""Flow is done."")
 
 if __name__ == ""__main__"":
     XGBFlow()
 ```
 
 
 
 
 
 ```bash
 python xgb_learning_api.py run
 ```
 
 <CodeOutputBlock lang=""bash"">
 
 ```
      Workflow starting (run-id 1654221281882630):
      [1654221281882630/start/1 (pid 71160)] Task is starting.
      [1654221281882630/start/1 (pid 71160)] Task finished successfully.
      [1654221281882630/train_model/2 (pid 71199)] Task is starting.
      [1654221281882630/train_model/2 (pid 71199)] Task finished successfully.
      [1654221281882630/end/3 (pid 71262)] Task is starting.
      [1654221281882630/end/3 (pid 71262)] Flow is done.
      [1654221281882630/end/3 (pid 71262)] Task finished successfully.
      Done!
 ```
 
 </CodeOutputBlock>",H3,https://outerbounds.com/docs/use-xgboost-with-metaflow#run-flow,False,1653.0,415.0
61,Access Artifacts Outside of Flow,"The following can be run in a Python script or notebook to access the contents of the dataframe that was stored as a flow artifact with `self.results`:
 
 
 ```python
 from metaflow import Flow
 run = Flow('XGBFlow').latest_run
 run.data.results.head()
 ```
     
 <HTMLOutputBlock >
 
 
 
 
 ```html
 <div>
 <style scoped>
     .dataframe tbody tr th:only-of-type {
         vertical-align: middle;
     }
 
     .dataframe tbody tr th {
         vertical-align: top;
     }
 
     .dataframe thead th {
         text-align: right;
     }
 </style>
 <table border=""1"" class=""dataframe"">
   <thead>
     <tr style=""text-align: right;"">
       <th></th>
       <th>train-mlogloss-mean</th>
       <th>train-mlogloss-std</th>
       <th>test-mlogloss-mean</th>
       <th>test-mlogloss-std</th>
     </tr>
   </thead>
   <tbody>
     <tr>
       <th>0</th>
       <td>0.741877</td>
       <td>0.001425</td>
       <td>0.750814</td>
       <td>0.002562</td>
     </tr>
     <tr>
       <th>1</th>
       <td>0.533298</td>
       <td>0.003306</td>
       <td>0.550585</td>
       <td>0.001667</td>
     </tr>
     <tr>
       <th>2</th>
       <td>0.394987</td>
       <td>0.002554</td>
       <td>0.421669</td>
       <td>0.002304</td>
     </tr>
     <tr>
       <th>3</th>
       <td>0.300281</td>
       <td>0.002392</td>
       <td>0.337402</td>
       <td>0.003478</td>
     </tr>
     <tr>
       <th>4</th>
       <td>0.231565</td>
       <td>0.001567</td>
       <td>0.280347</td>
       <td>0.004483</td>
     </tr>
   </tbody>
 </table>
 </div>
 ```
 
 
 
 </HTMLOutputBlock>",H3,https://outerbounds.com/docs/use-xgboost-with-metaflow#access-artifacts-outside-of-flow,False,1582.0,422.0
62,Further Reading,"* [Use scikit-learn estimators with Metaflow](/docs/use-sklearn-estimators-with-metaflow)
 * [Grid search with Metaflow](/docs/grid-search-with-metaflow)
 * [Parallel cross-validation](/docs/cross-validation-in-parallel)",H2,https://outerbounds.com/docs/use-xgboost-with-metaflow#further-reading,False,220.0,14.0
63,Decision Tree Classifiers,"Decision tree classifiers are algorithms that make predictions by climbing down a decision tree, such as that pictured below, for each data point, to reach a target value. Each branch is a decision, such as ""is feature x > 5?”
 
 ![](/assets/use-tree-model-01.png)
 <Caption> Decision tree visualization </Caption>
 
 Decision trees have upsides, such as being explainable machine learning algorithms, but are also not particularly accurate and prone to overfitting, as they create linear decision boundaries:
 
 ![](/assets/use-tree-model-04.png)
 <Caption> How a decision tree makes a prediction </Caption>
 
 Another way to state this is that decision trees are prone to having high variance (thinking through the bias-variance tradeoff).
 
 One way to reduce overfitting is to use ensemble learning, which uses multiple models together. Both random forests and boosted trees are ensemble methods that use decision trees (the former uses a method called bagging, the latter boosting) so let’s jump in.",H2,https://outerbounds.com/docs/use-tree-model#decision-tree-classifiers,False,1004.0,150.0
64,Bagging and Random Forests,"Bagging is an ensemble method that uses a technique called bootstrap aggregation (hence “bagging”). The basic idea is to take a bunch of subsets of the dataset, train a model on each subset, and then get your models to vote when predicting. Essentially, we’re averaging over all our models – this is the aggregation and the [bootstrap is the way we sample our data each time](https://en.wikipedia.org/wiki/Bootstrapping_(statistics)).
 
 ![](/assets/use-tree-model-02.png)
 <Caption> Bagging, a common approach to ensembling models </Caption>
 
 Random forests are essentially bagging over decision trees!",H2,https://outerbounds.com/docs/use-tree-model#bagging-and-random-forests,False,605.0,86.0
65,Boosting and Boosted Trees,"Boosting, like bagging, is an ensemble method, but it works differently:
 
 We start off with one model, commonly referred to as a weak learner (a model weakly correlated with the signal in the data);
 We make predictions for each data point and assign higher weights to the data points that were classified incorrectly;
 We build a second model on our data set with re-assigned weights so that it attempts to correct the incorrect predictions;
 We continue this process: reweighting our data points and building more models to find a strong learner (strongly correlated with the true classification).
 
 ![](/assets/use-tree-model-03.png)
 <Caption>  </Caption>",H2,https://outerbounds.com/docs/use-tree-model#boosting-and-boosted-trees,False,662.0,103.0
66,When to Use Boosting and When to Use Bagging,"There is no free lunch here but we can provide some rules of thumb for thinking through which to use
 
 * When a single model, such as a decision tree, is overfitting, using bagging (such as random forests) can improve performance;
 * When a single model has low accuracy, boosting, such as boosted trees, can often improve performance, whereas bagging may not.
 * Having provided these rules of thumb, you can also try both in parallel to find out which performs better for you!
 
 
 In a word,
 
 * With boosting: more trees eventually lead to overfitting;
 * With bagging: more trees do not lead to more overfitting.
 
 In practice, boosting seems to work better most of the time as long as you tune and evaluate properly to avoid overfitting. If you want to get started with random forests, you can do so with scikit-learn’s [RandomForestEstimator](https://scikit-learn.org/stable/modules/ensemble.html#forest). If you want to get started with boosted trees, check out [XGBoost](https://xgboost.readthedocs.io/en/stable/).",H2,https://outerbounds.com/docs/use-tree-model#when-to-use-boosting-and-when-to-use-bagging,False,1026.0,162.0
67,How do I?,"[Use scikit-learn estimators with Metaflow](/docs/use-sklearn-estimators-with-metaflow)
 
 
 [Use XGBoost with Metaflow](/docs/use-xgboost-with-metaflow)",H2,https://outerbounds.com/docs/use-tree-model#how-do-i,False,153.0,11.0
68,Question,How can I reuse model code in training and prediction flows?,H2,https://outerbounds.com/docs/reuse-model-object#question,True,60.0,11.0
69,Solution,A common pattern when using Metaflow is to move complex business logic outside of the flow. This makes the logic callable from multiple flows and more easily tested independent of the flow.,H2,https://outerbounds.com/docs/reuse-model-object#solution,False,189.0,32.0
70,Make Class Used in Multiple Flows,"Imagine you have the following model class: 
 
 
 ```py title=""model.py""
 class Model():
     
     def init_model(self, model_type = None, params:dict = {}):
         return model_type(**params)
         
     def train(self, model, features, labels): 
         return model.fit(features, labels)
     
     def score(self, model, features, true_labels):
         preds = model.predict(features)
         return {
           ""accuracy"": sum(true_labels==preds)/len(true_labels)
         }
 ```
 
 Now you can use [multiple inheritance](https://docs.python.org/3/tutorial/classes.html#multiple-inheritance) with this object when we instantiate our `FlowSpec` class.",H3,https://outerbounds.com/docs/reuse-model-object#make-class-used-in-multiple-flows,False,665.0,144.0
71,Use Model Class in Training Flow,"This flow demonstrates how the `Model` class functions can be inherited by the flow. The flow shows how to:
 * Instantiate and splits a dataset from scikit-learn.
 * Initialize a model using the previously defined class. 
 * Train the model.
 * Score the model on a validation set and prints the result. 
 
 
 
 
 <CustomCode height=""50vh"">
 
 
 
 ```py title=""train_model_flow.py""
 from metaflow import step, FlowSpec
 from model import Model
 
 class TrainingFlow(FlowSpec, Model):
     
     @step
     def start(self):
         from sklearn import datasets
         from sklearn.model_selection import train_test_split
         self.iris = datasets.load_iris()
         X, y = self.iris['data'], self.iris['target']
         self.labels = self.iris['target_names']
         split = train_test_split(X, y, test_size=0.2)
         self.X_train, self.X_test = split[0], split[1]
         self.y_train, self.y_test = split[2], split[3]
         self.next(self.make_model)
     
     @step
     def make_model(self):
         from sklearn.ensemble import RandomForestClassifier
         self.params = {""max_depth"": 8}
         self.model = self.init_model(
             model_type = RandomForestClassifier,
             params = self.params
         )
         self.next(self.train_model)
         
     @step 
     def train_model(self):
         self.model = self.train(self.model, self.X_train, self.y_train)
         self.next(self.end)
         
     @step
     def end(self):
         scores = self.score(self.model, self.X_test, self.y_test)
         print('Accuracy: ', scores['accuracy'])
     
 if __name__ == ""__main__"":
     TrainingFlow()
 ```
 
 </CustomCode>
 
 
 
 
 
 ```bash
 python train_model_flow.py run
 ```
 
 <CodeOutputBlock lang=""bash"">
 
 ```
      Workflow starting (run-id 864):
      [864/start/4336 (pid 17451)] Task is starting.
      [864/start/4336 (pid 17451)] Task finished successfully.
      [864/make_model/4337 (pid 17461)] Task is starting.
      [864/make_model/4337 (pid 17461)] Task finished successfully.
      [864/train_model/4338 (pid 17466)] Task is starting.
      [864/train_model/4338 (pid 17466)] Task finished successfully.
      [864/end/4339 (pid 17475)] Task is starting.
      [864/end/4339 (pid 17475)] Accuracy:  0.9
      [864/end/4339 (pid 17475)] Task finished successfully.
      Done!
 ```
 
 </CodeOutputBlock>",H3,https://outerbounds.com/docs/reuse-model-object#use-model-class-in-training-flow,False,2375.0,536.0
72,Use Model Class in Scoring Flow,"Now you can use multiple inheritance again to instantiate a different flow. 
 
 This flow shows how to:
 * Create a test dataset to score.
 * Instantiate a model using the trained model object from `TrainFlow`.
 * Use the common `Model` class function to score the model on the test dataset.
 
 
 
 
 
 
 
 ```py title=""scoring_model_flow.py""
 from metaflow import step, FlowSpec
 from model import Model
 
 class ScoringFlow(FlowSpec, Model):
     
     sibling_flow = 'TrainingFlow'
     
     @step
     def start(self):
         from sklearn import datasets
         iris = datasets.load_iris()
         self.X, self.y = iris['data'], iris['target']
         self.next(self.score_trained_model)
     
     @step
     def score_trained_model(self):
         from metaflow import Flow
         run = Flow(self.sibling_flow).latest_successful_run
         self.model = run['end'].task.data.model
         self.scores = self.score(self.model, self.X, self.y)
         self.next(self.end)
         
     @step
     def end(self):
         print('Accuracy: ', self.scores['accuracy'])
     
 if __name__ == ""__main__"":
     ScoringFlow()
 ```
 
 
 
 
 
 ```bash
 python scoring_model_flow.py run
 ```
 
 <CodeOutputBlock lang=""bash"">
 
 ```
      Workflow starting (run-id 865):
      [865/start/4341 (pid 17487)] Task is starting.
      [865/start/4341 (pid 17487)] Task finished successfully.
      [865/score_trained_model/4342 (pid 17499)] Task is starting.
      [865/score_trained_model/4342 (pid 17499)] Task finished successfully.
      [865/end/4343 (pid 17511)] Task is starting.
      [865/end/4343 (pid 17511)] Accuracy:  0.98
      [865/end/4343 (pid 17511)] Task finished successfully.
      Done!
 ```
 
 </CodeOutputBlock>",H3,https://outerbounds.com/docs/reuse-model-object#use-model-class-in-scoring-flow,False,1736.0,375.0
73,Further Reading,"* [How to Reuse Parameters across Flows](/docs/reuse-parameters-across-flows/)
 * [How to Store Artifacts across Metaflow Steps](/docs/pass-artifacts-between-steps)
 * [Organizing Metaflow Results](https://docs.metaflow.org/metaflow/tagging)",H2,https://outerbounds.com/docs/reuse-model-object#further-reading,False,241.0,19.0
74,Question,How do I scale model training and hyperparameter tuning to GPUs with Metaflow?,H2,https://outerbounds.com/docs/scale-model-training-and-tuning#question,True,78.0,13.0
75,Solution,"To scale horizontally, you can structure Metaflow flows with branching and looping patterns. To scale vertically you can access more processor and memory resources for a task in your flow by using Metaflow's `@batch` and `@kubernetes` decorators.",H2,https://outerbounds.com/docs/scale-model-training-and-tuning#solution,False,246.0,37.0
76,Define PyTorch Dependencies,"This example will show how to tune a PyTorch model on GPUs. The script contains functions to 
    * Load data.
    * Instantiate a neural net.
    * Train and evaluate a neural net.
 
 The [original code](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html#training-an-image-classifier) for the example comes from the PyTorch documentation.
 
 <CustomCode height=""30vh"">
 
 
 
 ```py title=""torch_steps.py""
 import torch
 import torchvision
 import torch.optim as optim
 import torchvision.transforms as transforms
 import torch.nn as nn
 import torch.nn.functional as F
 
 class Net(nn.Module):
     
     def __init__(self):
         super().__init__()
         self.conv1 = nn.Conv2d(3, 6, 5)
         self.pool = nn.MaxPool2d(2, 2)
         self.conv2 = nn.Conv2d(6, 16, 5)
         self.fc1 = nn.Linear(16 * 5 * 5, 120)
         self.fc2 = nn.Linear(120, 84)
         self.fc3 = nn.Linear(84, 10)
 
     def forward(self, x):
         x = self.pool(F.relu(self.conv1(x)))
         x = self.pool(F.relu(self.conv2(x)))
         # flatten all dims except batch
         x = torch.flatten(x, 1)
         x = F.relu(self.fc1(x))
         x = F.relu(self.fc2(x))
         x = self.fc3(x)
         return x
 
 def load_data():
     transform = transforms.Compose(
         [transforms.ToTensor(),
          transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])
     batch_size = 4
     trainset = torchvision.datasets.CIFAR10(
         root='./data', 
         train=True,
         download=True, 
         transform=transform
     )
     trainloader = torch.utils.data.DataLoader(
         trainset, 
         batch_size=batch_size,
         shuffle=True, 
         num_workers=2
     )
     testset = torchvision.datasets.CIFAR10(
         root='./data', 
         train=False,
         download=True, 
         transform=transform
     )
     testloader = torch.utils.data.DataLoader(
         testset, 
         batch_size=batch_size,
         shuffle=False, 
         num_workers=2
     )
     classes = ('plane', 'car', 'bird', 'cat','deer', 
                'dog', 'frog', 'horse', 'ship', 'truck')
     return trainloader, testloader, classes
 
 def train_model(trainloader, lr, epochs=1):
     net = Net()
     
     if torch.cuda.is_available():
         device = torch.device('cuda:0')
     else: 
         device = torch.device('cpu')
     
     net.to(device)
     criterion = nn.CrossEntropyLoss()
     optimizer = optim.SGD(net.parameters(), 
                           lr=lr, momentum=0.9)
     for epoch in range(epochs):
         for i, data in enumerate(trainloader, 0):
             inputs = data[0].to(device)
             labels = data[1].to(device)
             optimizer.zero_grad()
             outputs = net(inputs)
             loss = criterion(outputs, labels)
             loss.backward()
             optimizer.step()
     return net
 
 def run_inference_and_tests(net, testloader):
     correct = 0
     total = 0
     with torch.no_grad():
         for data in testloader:
             images, labels = data
             outputs = net(images)
             _, predicted = torch.max(outputs.data, 1)
             total += labels.size(0)
             correct += (predicted == labels).sum().item()
     return 100 * correct // total
 ```
 
 </CustomCode>",H3,https://outerbounds.com/docs/scale-model-training-and-tuning#define-pytorch-dependencies,False,3299.0,923.0
77,Run Flow,"This flow leverages the functions in `torch_steps.py` to:
 * Create dataloaders for training and testing.
 * Train multiple models in parallel on GPU instances using Metaflow's `@batch` decorator in the `train` step.
     * In this case each model will get a different learning rate assigned. This is simple for demo purposes, in practice you will want to explore more dimensions of hyperparameter space when you have the resources.
 * Evaluate each model.
 * Join the results of evaluation to select the best model.
 * Print the highest accuracy model.
     * You can improve the model by changing hyperparameters in `torch_steps.py` or by expanding the hyperparameter tuning task with more branches in the flow.
     
 The example uses Metaflow's `@conda` decorator to create a similar environment across local and remote compute.
 
 
 
 
 
 
 ```py title=""scale_tuning_gpu.py""
 from metaflow import (FlowSpec, step, Parameter, 
                       batch, JSONType, conda_base)
 import json
 import torch_steps
 
 @conda_base(libraries={""pytorch"":""1.11.0"", 
                        ""torchvision"":""0.12.0""}, 
             python=""3.8"")
 class GPUFlow(FlowSpec):
     
     learning_rates = Parameter(
         'learning-rates',
         default=json.dumps([0.01,0.001]),
         type=JSONType
     )
 
     @step
     def start(self):
         data = torch_steps.load_data()
         self.trainloader = data[0]
         self.testloader = data[1]
         self.classes = data[2]
         self.next(self.train, foreach='learning_rates')
         
         
     @batch(gpu=1)
     @step
     def train(self):
         self.model = torch_steps.train_model(
             self.trainloader,
             lr=self.input
         )
         self.next(self.evaluate_model)
 
 
     @step
     def evaluate_model(self):
         result = torch_steps.run_inference_and_tests(
             self.model,
             self.testloader
         )
         self.accuracy = result
         self.next(self.join)
 
 
     @step
     def join(self, inputs):
         best_model = None; best_score = -1
         for i in inputs:
             if i.accuracy > best_score: 
                 best_score = i.accuracy
                 best_model = i.model
         self.best_model = best_model
         self.best_score = best_score
         print(f""Best model accuracy was {best_score}%."")
         self.next(self.end)
 
 
     @step
     def end(self):
         print(""Done"")
 
 
 if __name__ == ""__main__"":
     GPUFlow()
 ```
 
 
 
 
 
 ```bash
 python scale_tuning_gpu.py --environment=conda run
 ```
 
 <CodeOutputBlock lang=""bash"">
 
 ```
     ...
      [560/start/2899 (pid 65768)] Task is starting.
      [560/start/2899 (pid 65768)] Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz
     100.0%05-24 21:25:42.845 [560/start/2899 (pid 65768)] 0.0%9%4.2%
      [560/start/2899 (pid 65768)] Extracting ./data/cifar-10-python.tar.gz to ./data
      [560/start/2899 (pid 65768)] Files already downloaded and verified
      [560/start/2899 (pid 65768)] Foreach yields 2 child steps.
      [560/start/2899 (pid 65768)] Task finished successfully.
     ...
      [560/join/2904 (pid 65886)] Task is starting.
      [560/join/2904 (pid 65886)] Best model accuracy was 46%.
      [560/join/2904 (pid 65886)] Task finished successfully.
     ...
      [560/end/2905 (pid 65898)] Task is starting.
      [560/end/2905 (pid 65898)] Done
      [560/end/2905 (pid 65898)] Task finished successfully.
     ...
 ```
 
 </CodeOutputBlock>",H3,https://outerbounds.com/docs/scale-model-training-and-tuning#run-flow,False,3555.0,867.0
78,Further Reading,"* [How to use multiple CPU cores inside of Metaflow tasks](/docs/use-multiple-cpu-cores)
 * [Scale out and up](https://docs.metaflow.org/v/r/metaflow/scaling)
 * [PyTorch distributed overview](https://pytorch.org/tutorials/beginner/dist_overview.html)",H2,https://outerbounds.com/docs/scale-model-training-and-tuning#further-reading,False,251.0,20.0
79,Question,How can I build a custom docker image to run a Metaflow step?,H2,https://outerbounds.com/docs/build-custom-image#question,True,61.0,13.0
80,Solution,"Metaflow has decorators to run steps on remote compute environments like `@batch` and `@kubernetes`. The environments these jobs run in can both be created from a Docker image. 
 
 In some circumstances you may need to create your own image for running a step or a flow. In this case there are a few things to consider when building your image.",H2,https://outerbounds.com/docs/build-custom-image#solution,False,344.0,62.0
81,Specify an Image in a Flow,"First it is important to mention how Metaflow knows which image to use. You can read more about using a custom image [here](/docs/use-custom-image).
 If you do not specify the image argument like `@batch(image=""my_image:latest"")`, Metaflow will look to see if you have [configured](/engineering/operations/configure-metaflow/) a default container image for the compute plugin you are using in the `METAFLOW_DEFAULT_CONTAINER_IMAGE` variable. 
 
 If this configuration is not specified and you do not specify the `image` argument in the decorator, the image is built from the [official Python image](https://hub.docker.com/_/python/) for the version of Python you are using in your local environment.",H4,https://outerbounds.com/docs/build-custom-image#specify-an-image-in-a-flow,False,699.0,97.0
82,Write a Dockerfile,Docker images are built using a Dockerfile. When building one to use with Metaflow there are a few considerations to keep in mind.,H3,https://outerbounds.com/docs/build-custom-image#write-a-dockerfile,False,130.0,23.0
83,Base Image,"A minimum requirement is that you will need Python in the image - we suggest starting from an official Python image. For example, you can add the following at the start of your Dockerfile:
 ```
 FROM python:3.10
 ```
 
 The image should come with standard CLI tools like tar, so we suggest avoiding starting the Dockerfile with `FROM scratch`.",H4,https://outerbounds.com/docs/build-custom-image#base-image,False,343.0,59.0
84,User Permissions,"Metaflow needs to be able to write in the working directory. In the Dockerfile this concerns the `WORKDIR` and `USER` commands. You should make sure that the user running commands can write in the working directory, especially when you do explicitly set these in your Dockerfile. Note that many images use root user by default, and Metaflow does not so you may have to explicitly specify a non-root `USER` in your Dockerfile. You can use the following to check the user for your image:
 
 ```bash
 docker run --rm -it <YOUR IMAGE> bash -c id
 ```
 
 For example, by default this Python image user id is root:
 
 
 ```bash
 docker run --rm -it python:3.10 bash -c id
 ```
 
 <CodeOutputBlock lang=""bash"">
 
 ```
     uid=0(root) gid=0(root) groups=0(root)
 ```
 
 </CodeOutputBlock>
 
 You can change the user in the Dockerfile like 
 
 ```Dockerfile
 FROM my_base_image:latest
 USER my_user
 ...
 ```",H4,https://outerbounds.com/docs/build-custom-image#user-permissions,False,900.0,154.0
85,Using ENTRYPOINT and CMD,"We suggest you do not set either of these in your Dockerfile. Metaflow [constructs a command](https://github.com/Netflix/metaflow/blob/62f5e52ebce755d9130287c1576011cb056e0e3d/metaflow/metaflow_environment.py#L83-L102) to run the container for you, so defining the ENTRYPOINT too can produce unexpected errors.",H4,https://outerbounds.com/docs/build-custom-image#using-entrypoint-and-cmd,False,310.0,31.0
86,Example,"Here is an example of a standard Dockerfile. 
 * The `WORKDIR` is changed and the `USER` has write permission. 
 * The `COPY` command moves a `requirements.txt` file into the image and installs the contents. You could follow a similar copying process to install custom modules that are not on PyPi. 
 * Also notice there is no CMD or ENTRYPOINT since Metaflow will override this for you anyways.
 
 
 
 
 
 
 ```Dockerfile title=""Dockerfile""
 FROM python:3.10
 
 RUN mkdir /logs && chown 1000 /logs
 RUN mkdir /metaflow && chown 1000 /metaflow
 ENV HOME=/metaflow
 WORKDIR /metaflow
 USER 1000
 
 COPY requirements.txt requirements.txt
 RUN pip install --no-cache-dir --upgrade pip && \
     pip install --no-cache-dir -r requirements.txt
 ```",H4,https://outerbounds.com/docs/build-custom-image#example,False,743.0,123.0
87,Build your Image,"This process is not unique to Metaflow. Once you have written a Dockerfile like the one above, you can [build it](https://docs.docker.com/engine/reference/commandline/build/) from the directory like:
 
 ```bash
 docker build .
 ```
 
 If you are building or running your image on MacOS, and plan to later deploy to a Linux machine, you will need to specify `--platform=linux/amd64` in your build and run commands. For example, when using the EC2 instances that power AWS Batch environments you will want to make sure the image is built for the right platform. You can set the platform automatically when building and running images by using an environment variable:
 
 ```
 export DOCKER_DEFAULT_PLATFORM=linux/amd64  
 ```
 
 Another alternative is to specify the platform in the beginning of your Dockerfile:
 
 ```
 FROM --platform=linux/amd64 image:tag
 ```",H3,https://outerbounds.com/docs/build-custom-image#build-your-image,False,861.0,132.0
88,Configure Metaflow to Use your Image,"Once you have built your image you need to tell Metaflow to use it. This requires pushing the image to a registry that you have permission to access. For example, in AWS you might want your image to reside in [ECR](https://docs.aws.amazon.com/AmazonECR/latest/userguide/docker-push-ecr-image.html). 
 
 
 In a flow the most direct way to tell Metaflow to use this image for a step is to use the plugin decorators like `@batch(image=<my_image>:<my_tag>)` and `@kubernetes(image=<my_image>:<my_tag>)`. You can also set default environment variables so Metaflow knows to look for a certain image in a specified container registry by default. 
 
 Some configuration variables to keep in mind for specifying a URI for a default image and container registry are `METAFLOW_DEFAULT_CONTAINER_IMAGE` and `METAFLOW_DEFAULT_CONTAINER_REGISTRY`. 
 * `METAFLOW_DEFAULT_CONTAINER_IMAGE` dictates the default container image that Metaflow should use. 
 * `METAFLOW_DEFAULT_CONTAINER_REGISTRY` controls which registry Metaflow uses to pick the image - this defaults to DockerHub.
 
 These will then be used as a default across compute plugins. [Metaflow configuration variables](/engineering/operations/configure-metaflow/) can be set in the active `METAFLOW_PROFILE` stored in `~/.metaflow-config/` or as environment variables. 
 
 For example, if your container registry is in AWS ECR you can set an environment variable like:
 ```bash
 export METAFLOW_DEFAULT_CONTAINER_REGISTRY=<aws_account_id>.dkr.ecr.<region>.amazonaws.com
 ```
 and then decorate your flow steps like:
 ```python
 @batch(image=""image-in-my-registry:latest"")
 @step
 def containerized_step(self):
     ...
 ```
 
 Alternatively, you can specify the registry, image, and tag all in the decorator:
 ```python
 @batch(image=""url-to-docker-repo/docker-image:version"")
 @step
 def containerized_step(self):
     ...
 ```
 
 
 
 
 Note that if you are manually configuring the underlying resources for remote compute plugins (as opposed to automating deployment through CloudFormation or Terraform) you will need to make ensure that the appropriate roles are available for those resources.",H3,https://outerbounds.com/docs/build-custom-image#configure-metaflow-to-use-your-image,False,2140.0,284.0
89,Further Reading,"* [Use a custom image](/docs/use-custom-image) in your flow
 * See configuration details in: [metaflow_config.py](https://github.com/Netflix/metaflow/blob/493d406347c11787d73f0cc76fb798d8526b4955/metaflow/metaflow_config.py)
 * See where in the Metaflow code image and container registry variables are used for [@batch](https://github.com/Netflix/metaflow/blob/master/metaflow/plugins/aws/batch/batch_decorator.py#L112-L138) and [@kubernetes](https://github.com/Netflix/metaflow/blob/master/metaflow/plugins/kubernetes/kubernetes_decorator.py#L101-L121)
 * [Building a Dockerfile for a Python environment](https://docs.docker.com/language/python/build-images/)
 * [Understand how CMD and ENTRYPOINT interact](https://docs.docker.com/engine/reference/builder/#understand-how-cmd-and-entrypoint-interact)
 * [Best practices for containerizing Python applications with Docker](https://snyk.io/blog/best-practices-containerizing-python-docker/)",H2,https://outerbounds.com/docs/build-custom-image#further-reading,False,940.0,56.0
90,Question,How can I use multiple CPU cores within Metaflow tasks?,H2,https://outerbounds.com/docs/use-multiple-cpu-cores#question,True,55.0,10.0
91,Answer,"You can use multiple cores like you normally would in your Python code in a Metaflow task. 
 For example, you can use Python's [concurrent.future](https://docs.python.org/3/library/concurrent.futures.html) module to launch multiple processes and threads.",H2,https://outerbounds.com/docs/use-multiple-cpu-cores#answer,False,254.0,32.0
92,Write Flow with Concurrency in a Task,,H3,https://outerbounds.com/docs/use-multiple-cpu-cores#write-flow-with-concurrency-in-a-task,False,0.0,1.0
93,Run Flow,"```bash
 python use_multiple_cpu_cores.py run
 ```
 
 <CodeOutputBlock lang=""bash"">
 
 ```
      Workflow starting (run-id 1663278003721691):
      [1663278003721691/start/1 (pid 47937)] Task is starting.
      [1663278003721691/start/1 (pid 47937)] Task finished successfully.
      [1663278003721691/end/2 (pid 47941)] Task is starting.
      [1663278003721691/end/2 (pid 47941)] All tasks completed in 3.011s
      [1663278003721691/end/2 (pid 47941)] thread_0
      [1663278003721691/end/2 (pid 47941)] thread_1
      [1663278003721691/end/2 (pid 47941)] thread_2
      [1663278003721691/end/2 (pid 47941)] thread_3
      [1663278003721691/end/2 (pid 47941)] Task finished successfully.
      Done!
 ```
 
 </CodeOutputBlock>",H3,https://outerbounds.com/docs/use-multiple-cpu-cores#run-flow,False,729.0,121.0
94,Further Reading,"* [Parallelization Over Multiple Cores](https://docs.metaflow.org/metaflow/scaling-out-and-up/effortless-scaling-with-kubernetes#parallelization-over-multiple-cores)
 * [How to Scale Model Training and Tuning to GPU](/docs/scale-model-training-and-tuning/)",H2,https://outerbounds.com/docs/use-multiple-cpu-cores#further-reading,False,256.0,15.0
95,Question,How do I package files so they are usable in remote compute steps?,H2,https://outerbounds.com/docs/package-files-remote-compute#question,True,66.0,13.0
96,Solution,"Metaflow includes all  `.py` files in your flow script's directory in the  distribution. This means all `.py` file dependencies will be available in remote compute environments. 
 
 The rest of this page shows how to include files that do not end in `.py` using the `--package-suffixes` argument.
 
 :::note
 This page describes how to use the `--package-suffixes` command line argument. You can also use the `METAFLOW_DEFAULT_PACKAGE_SUFFIXES` configuration option. 
 :::",H2,https://outerbounds.com/docs/package-files-remote-compute#solution,False,472.0,73.0
97,Define a Dependency in the Flow Directory,"Suppose you have the following two `.sql` files in your flow script's directory and you want to access them on a remote compute instance. 
 
 
 ```sql title=""query1.sql""
 SELECT * FROM DB1;
 ```
 
 
 ```sql title=""query2.sql""
 SELECT * FROM DB2;
 ```
 
 :::note
 This pattern is not unique to the `.sql` file extension.
 :::",H3,https://outerbounds.com/docs/package-files-remote-compute#define-a-dependency-in-the-flow-directory,False,324.0,56.0
98,Define a Flow that Uses the Dependencies Remotely,"This flow shows how to:
 * Run the `start` step remotely using Kubernetes.
 * Use the `query1.sql` and `query2.sql` files in the remote compute environment.
 
 
 
 
 
 
 
 ```py title=""package_suffixes_flow.py""
 from metaflow import FlowSpec, step, kubernetes
 
 class PackageSuffixesFlow(FlowSpec):
     
     query1_file = 'query1.sql'
     query2_file = 'query2.sql'
     
     def read_query(self, file):
         file_obj = open(file, 'r')
         result = file_obj.read()
         file_obj.close()
         return result
     
     @kubernetes
     @step
     def start(self):
         self.query1 = self.read_query(self.query1_file)
         self.query2 = self.read_query(self.query2_file)
         self.next(self.end)
         
     @step
     def end(self):
         print(""Query 1:"", self.query1)
         print(""Query 2:"", self.query2)
 
 if __name__ == ""__main__"":
     PackageSuffixesFlow()
 ```",H3,https://outerbounds.com/docs/package-files-remote-compute#define-a-flow-that-uses-the-dependencies-remotely,False,909.0,221.0
99,Package the Dependencies and Run the Flow,"The key to this page is Metaflow's `package-suffixes` argument.
 To run the `PackageSuffixesFlow` and copy the local `.sql` files so they are accessible in the `start` step running in a Kubernetes pod, you can run the following command: 
 
 
 ```bash
 python package_suffixes_flow.py --package-suffixes='.sql' run
 ```
 
 <CodeOutputBlock lang=""bash"">
 
 ```
      Workflow starting (run-id 186474):
      [186474/start/1009082 (pid 67155)] Task is starting.
      [186474/start/1009082 (pid 67155)] [pod t-8dvdn-8f2mw] Task is starting (Pod is pending, Container is waiting - ContainerCreating)...
      [186474/start/1009082 (pid 67155)] [pod t-8dvdn-8f2mw] Setting up task environment.
      [186474/start/1009082 (pid 67155)] [pod t-8dvdn-8f2mw] Downloading code package...
      [186474/start/1009082 (pid 67155)] [pod t-8dvdn-8f2mw] Code package downloaded.
      [186474/start/1009082 (pid 67155)] [pod t-8dvdn-8f2mw] Task is starting.
      [186474/start/1009082 (pid 67155)] [pod t-8dvdn-8f2mw] Task finished with exit code 0.
      [186474/start/1009082 (pid 67155)] Task finished successfully.
      [186474/end/1009083 (pid 67159)] Task is starting.
      [186474/end/1009083 (pid 67159)] Query 1: SELECT * FROM DB1;
      [186474/end/1009083 (pid 67159)] 
      [186474/end/1009083 (pid 67159)] Query 2: SELECT * FROM DB2;
      [186474/end/1009083 (pid 67159)] 
      [186474/end/1009083 (pid 67159)] Task finished successfully.
      Done!
 ```
 
 </CodeOutputBlock>",H3,https://outerbounds.com/docs/package-files-remote-compute#package-the-dependencies-and-run-the-flow,False,1481.0,250.0
100,Further Reading,"* [Load Local Data with IncludeFile](/docs/load-local-data-with-include)
 * [Share Local Data with S3 in Metaflow](/docs/sharing-local-data-with-s3)",H2,https://outerbounds.com/docs/package-files-remote-compute#further-reading,False,148.0,14.0
101,Question,How can I use a custom docker image to run a Metaflow step?,H2,https://outerbounds.com/docs/use-custom-image#question,True,59.0,13.0
102,Solution,Metaflow has decorators to run steps on remote compute environments like `@batch` and `@kubernetes`. The environments run jobs created from a Docker image.,H2,https://outerbounds.com/docs/use-custom-image#solution,False,155.0,23.0
103,Select an Image,"You can either [build an image](/docs/build-custom-image) or choose one. If you choose an existing image, make sure that Python can be invoked from the container. 
 
 You can tell Metaflow which image you want to use in several ways:
 * passing the `image` argument in a decorator like `@batch(image=""my_image:latest"")`
 * in [Metaflow config files](/engineering/operations/configure-metaflow/)
     * `METAFLOW_DEFAULT_CONTAINER_REGISTRY` controls which registry Metaflow uses to pick the image - this defaults to DockerHub but could also be a URL to a public or private ECR repository on AWS. 
     * `METAFLOW_DEFAULT_CONTAINER_IMAGE` dictates the default container image that Metaflow should use.
 * don't specify and let Metaflow default to the [official Python image](https://hub.docker.com/_/python/)
     * in this case, the default corresponds to the major.minor version of Python that the user used to launch the flow",H3,https://outerbounds.com/docs/use-custom-image#select-an-image,False,927.0,143.0
104,Run Flow,"For example, this flow uses the [official Python image](https://hub.docker.com/_/python/) in the `run_in_container` step. In this example only the image name and tag is specified but know that you can also pass in the full URL to the image in `@batch` or `@kubernetes` - `@batch(image=""url-to-docker-repo/docker-image:version"")`.",H3,https://outerbounds.com/docs/use-custom-image#run-flow,False,329.0,44.0
105,Note about GPU images,"In these decorators you will see resource arguments like `cpu=1`. Assuming that your Metaflow deployment allows you to access compute instances with GPU resources, you can also set `gpu=N` and Metaflow will automatically prepare your image in a way that works with GPU. In this example access means that the AWS Batch compute environment will need access to [EC2 instances with GPUs](https://aws.amazon.com/ec2/instance-types/#Accelerated_Computing).
 
 
 
 
 
 
 ```py title=""use_image_flow.py""
 from metaflow import FlowSpec, step, batch, conda
 import os
 
 class UseImageFlow(FlowSpec):
     
     @step
     def start(self):
         self.next(self.run_in_container)
     
     #highlight-next-line
     @batch(image=""python:3.10"", cpu=1)
     @step 
     def run_in_container(self):
         self.artifact_from_container = 7
         self.next(self.end)
         
     @step
     def end(self):
         pass
         
 if __name__ == ""__main__"":
     UseImageFlow()
 ```
 
 
 
 
 
 ```bash
 python use_image_flow.py run
 ```
 
 <CodeOutputBlock lang=""bash"">
 
 ```
      Workflow starting (run-id 685):
      [685/start/3463 (pid 33227)] Task is starting.
      [685/start/3463 (pid 33227)] Task finished successfully.
      [685/run_in_container/3464 (pid 33231)] Task is starting.
      [685/run_in_container/3464 (pid 33231)] [98f8111b-76f1-4e04-97d4-7d7bc2c58abd] Task is starting (status SUBMITTED)...
      [685/run_in_container/3464 (pid 33231)] [98f8111b-76f1-4e04-97d4-7d7bc2c58abd] Task is starting (status STARTING)...
      [685/run_in_container/3464 (pid 33231)] [98f8111b-76f1-4e04-97d4-7d7bc2c58abd] Task is starting (status RUNNING)...
      [685/run_in_container/3464 (pid 33231)] [98f8111b-76f1-4e04-97d4-7d7bc2c58abd] Setting up task environment.
      [685/run_in_container/3464 (pid 33231)] [98f8111b-76f1-4e04-97d4-7d7bc2c58abd] Downloading code package...
      [685/run_in_container/3464 (pid 33231)] [98f8111b-76f1-4e04-97d4-7d7bc2c58abd] Code package downloaded.
      [685/run_in_container/3464 (pid 33231)] [98f8111b-76f1-4e04-97d4-7d7bc2c58abd] Task is starting.
      [685/run_in_container/3464 (pid 33231)] [98f8111b-76f1-4e04-97d4-7d7bc2c58abd] Task finished with exit code 0.
      [685/run_in_container/3464 (pid 33231)] Task finished successfully.
      [685/end/3465 (pid 33245)] Task is starting.
      [685/end/3465 (pid 33245)] Task finished successfully.
      Done!
 ```
 
 </CodeOutputBlock>",H4,https://outerbounds.com/docs/use-custom-image#note-about-gpu-images,False,2440.0,408.0
106,Access Artifacts Outside of Flow,"The following can be run in a Python script or notebook to access the artifact produced in the container step:
 
 
 ```python
 from metaflow import Flow
 run = Flow(""UseImageFlow"").latest_run
 assert run.successful",H3,https://outerbounds.com/docs/use-custom-image#access-artifacts-outside-of-flow,False,214.0,32.0
107,Further Reading,"* [Build a custom image](/docs/build-custom-image)
 * Set environment variables in a container using [Metaflow's @environment decorator](/docs/set-env-vars-with-decorator) or [.env file](/docs/set-env-vars-with-file)
 * See where in the Metaflow code image and container registry variables are used for [@batch](https://github.com/Netflix/metaflow/blob/master/metaflow/plugins/aws/batch/batch_decorator.py#L112-L138) and [@kubernetes](https://github.com/Netflix/metaflow/blob/master/metaflow/plugins/kubernetes/kubernetes_decorator.py#L101-L121)",H2,https://outerbounds.com/docs/use-custom-image#further-reading,False,545.0,37.0
108,Question,How can I schedule flows to run at a specific time on AWS?,H2,https://outerbounds.com/docs/schedule-flow-on-step-functions#question,True,58.0,13.0
109,Solution,There is a Metaflow decorator for that!,H2,https://outerbounds.com/docs/schedule-flow-on-step-functions#solution,False,39.0,7.0
110,Scheduling Flows,"You can use Metaflow's `@schedule` flow-level decorator to run on [AWS Step Functions](https://docs.metaflow.org/going-to-production-with-metaflow/scheduling-metaflow-flows/scheduling-with-aws-step-functions) where Metaflow automatically maps a `FlowSpec` onto an AWS Step Functions state machine. Alternatively, you can schedule flows with [Argo Workflows](https://docs.metaflow.org/going-to-production-with-metaflow/scheduling-metaflow-flows/scheduling-with-argo-workflows). 
 
 After deploying the script containing your flow to AWS Step Functions the execution of a `FlowSpec` can happen on any event-based trigger you setup or a time-based trigger defined with Metaflow's `@schedule` decorator.",H3,https://outerbounds.com/docs/schedule-flow-on-step-functions#scheduling-flows,False,699.0,69.0
111,Run Flow,"This flow is scheduled to run daily. Notice Metaflow's `@schedule` decorator has arguments that determine when the flow is run.  Time based triggers you can use include:
 * `@schedule(weekly=True)` runs the workflow on Sundays at midnight.
 * `@schedule(daily=True)` runs the workflow every day at midnight.
 * `@schedule(hourly=True)` runs the workflow every hour.
 * `@schedule(cron='0 10 * * ? *')` runs the workflow at the given Cron schedule, in this case at 10am UTC every day. You can use the rules defined [here](https://docs.aws.amazon.com/eventbridge/latest/userguide/eb-create-rule-schedule.html) to define the schedule for the cron option.
 
 
 
 
 
 
 ```py title=""schedule_flow_aws.py""
 from metaflow import FlowSpec, schedule, step
 from datetime import datetime
 
 @schedule(daily=True)
 class DailyFlowAWS(FlowSpec):
 
     @step
     def start(self):
         now = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
         print('time is %s' % now)
         self.next(self.end)
 
     @step
     def end(self):
         pass
 
 if __name__ == '__main__':
     DailyFlowAWS()
 ```
 
 
 
 
 
 ```bash
 python daily_flow_aws.py --with retry step-functions create
 ```
 
 <CodeOutputBlock lang=""bash"">
 
 ```
     Deploying DailyFlowAWS to AWS Step Functions...
     It seems this is the first time you are deploying DailyFlowAWS to AWS Step Functions.
     
     A new production token generated.
     
     The namespace of this production flow is
         production:dailyflowaws-0-liek
     To analyze results of this production flow add this line in your notebooks:
         namespace(""production:dailyflowaws-0-liek"")
     If you want to authorize other people to deploy new versions of this flow to AWS Step Functions, they need to call
         step-functions create --authorize dailyflowaws-0-liek
     when deploying this flow to AWS Step Functions for the first time.
     See ""Organizing Results"" at https://docs.metaflow.org/ for more information about production tokens.
     
     State Machine DailyFlowAWS for flow DailyFlowAWS pushed to AWS Step Functions successfully.
     
     What will trigger execution of the workflow:
         This workflow triggers automatically via a cron schedule DailyFlowAWS defined in AWS EventBridge.
 ```
 
 </CodeOutputBlock>
 
 After running the above command your flow will be triggered daily!",H3,https://outerbounds.com/docs/schedule-flow-on-step-functions#run-flow,False,2358.0,445.0
112,Manually Trigger Flow,"You can manually trigger the flow at any time:
 
 
 ```bash
 python daily_flow_aws.py step-functions trigger
 ```
 
 <CodeOutputBlock lang=""bash"">
 
 ```
     Workflow DailyFlowAWS triggered on AWS Step Functions (run-id sfn-ef136a09-7082-4826-ac41-4704e145ebe2).
 ```
 
 </CodeOutputBlock>
 
 You can also interact with the flow through your AWS Step Functions console:
 
 ![](/assets/aws-step-func.png)",H3,https://outerbounds.com/docs/schedule-flow-on-step-functions#manually-trigger-flow,False,404.0,54.0
113,Further Reading,"* [Human-centric data science on Kubernetes with Metaflow](https://outerbounds.com/blog/human-centric-data-science-on-kubernetes-with-metaflow/)
 * [Machine learning pipelines: from prototype to production](https://outerbounds.com/blog/machine-learning-pipelines-from-prototype-to-production/)
 * [Preventing retries](https://docs.metaflow.org/metaflow/failures#how-to-prevent-retries) for Metaflow steps
 * [Scheduling with AWS EventBridge](https://docs.aws.amazon.com/eventbridge/latest/userguide/eb-create-rule-schedule.html)",H2,https://outerbounds.com/docs/schedule-flow-on-step-functions#further-reading,False,528.0,27.0
114,Question,How can I use PyTest with a flow?,H2,https://outerbounds.com/docs/use-pytest#question,True,33.0,8.0
115,Solution,"There are two related cases to consider:
 * Test the logic within steps.
 * Test the flow itself.",H2,https://outerbounds.com/docs/use-pytest#solution,False,97.0,18.0
116,Testing Logic in Steps,"It is a helpful design pattern to move non-orchestration logic out of the actual flows and write unit tests for the component functions. In other words, if you have logic in the step of a flow like the highlight flow you can refactor it in the following way.
 
  
 
 
 
 Here is a pseudo-code example of a flow you may want to refactor in this way.
 
 
 
 
 
 
 ```py title=""flow_with_logic_in_steps.py""
 class MyFlow(FlowSpec):
 
     @step
     def start(self):
         # logic A
         # logic B 
         # logic C
         self.next(self.next_step)
         
     # rest of flow
 ```
 
 
 
 
 
 To refactor you can first make a separate file to contain the logic that can be tested independent of the flow:
 
 
 ```py title=""my_module.py""
 def do_logic():
     # logic A
     # logic B 
     # logic C
 ```
 
 
 
 
 
 This is the suggested design pattern because now you can unit test this logic in the way you normally would, and then import it in the flow.
 
 
 
 
 
 
 ```py title=""flow_with_logic_imported.py""
 class MyFlow(FlowSpec):
 
     @step
     def start(self):
         from my_module import do_logic
         do_logic()
         self.next(self.next_step)
     
     # rest of flow
 ```
 
 
 
 
 
 In general, separating the implementation of the logic from the flow makes code bases leveraging Metaflow easier to maintain and test. It is a particularly useful design pattern when you have multiple flows and/or steps that import the same logic.",H3,https://outerbounds.com/docs/use-pytest#testing-logic-in-steps,False,1466.0,351.0
117,Testing a Flow,"In the second case, suppose you have a flow you would like to write a unit test for.
 
 
 
 
 
 In this example there is a data artifact `x` which is stored in `self.x`. 
 
 
 
 
 
 
 ```py title=""simple_flow.py""
 from metaflow import FlowSpec, step
 
 class FlowToTest(FlowSpec):
 
     @step
     def start(self):
         self.x = 0
         self.next(self.end)
 
     @step
     def end(self):
         self.x += 1
 
 if __name__ == '__main__':
     FlowToTest()
 ```
 
 
 
 
 Suppose you want to test that after running the flow the artifact value is what you expect. 
 
 ```python
 assert x == 1 # goal: check this is true using PyTest
 ```
 
 
 
 
 To do this you can:
 * Switch your Metaflow profile to ensure tests use a separate (local) metadata and datastore.
 * Define a test file and use PyTest to test the flow.",H3,https://outerbounds.com/docs/use-pytest#testing-a-flow,False,825.0,193.0
118,Switch Metaflow Profiles,"By default, Metaflow creates a profile for you at `~/.metaflow_config/config.json`. You can make and activate a custom profile that tells Metaflow to use different metadata and datastores. For example, you can define to `~/.metaflowconfig/config_test.json` like:
 
 ```json
 {
     ""METAFLOW_DEFAULT_DATASTORE"": ""local""
 }
 ```
 
 to separate data from test runs from your actual runs.",H4,https://outerbounds.com/docs/use-pytest#switch-metaflow-profiles,False,385.0,57.0
119,Run PyTest Script,"Now you can define a PyTest script that will:
 * Run the flow.
 * Use Metaflow's client API to access the artifact of interest.
 * Test the artifact value is as expected.
 
 
 
 
 
 
 ```py title=""test_simple_flow.py""
 import os
 os.environ['METAFLOW_PROFILE'] = 'test'
 from metaflow import Flow
 import subprocess
 
 def test_flow():
     cmd = ['python', 'simple_flow.py', 'run', '--run-id-file', 'test_id']
     subprocess.check_call(cmd)
     with open('test_id') as f:
         run = Flow('FlowToTest')[f.read()]
         assert run.data.x == 1
 ```
 
 
 
 
 
 ```bash
 pytest
 ```
 
 <CodeOutputBlock lang=""bash"">
 
 ```
     ============================= test session starts ==============================
     platform darwin -- Python 3.9.7, pytest-6.2.4, py-1.10.0, pluggy-0.13.1
     rootdir: /Users/eddie/Dev/outerbounds-docs/docs/docs/data-science/deployment
     plugins: anyio-3.5.0
     collected 1 item                                                               
     
     test_simple_flow.py .                                                    [100%]
     
     ============================== 1 passed in 1.65s ===============================
 ```
 
 </CodeOutputBlock>",H4,https://outerbounds.com/docs/use-pytest#run-pytest-script,False,1193.0,299.0
120,Question,How can I make a Sendgrid email alert from a step in my Metaflow flow?,H2,https://outerbounds.com/docs/email-flow-results#question,True,70.0,15.0
121,Solution,"You can format a human-readable report as a [Metaflow Card](https://docs.metaflow.org/metaflow/visualizing-results). You can then send the card, which is a simple HTML file, as an email attachment.",H2,https://outerbounds.com/docs/email-flow-results#solution,False,197.0,26.0
122,Get Sendgrid API Key,"Before using the code in this example to send emails from your Python code, you need to get a [Sendgrid API key](https://docs.sendgrid.com/api-reference/api-keys/create-api-keys). You can set this as an environment variable: 
 
 ```bash
 export SENDGRID_API_KEY=<YOUR KEY>
 ```",H3,https://outerbounds.com/docs/email-flow-results#get-sendgrid-api-key,False,277.0,37.0
123,Use Sendgrid's Python Library to Send Email,"The following code snippet will be called from the flow. It uses Sendgrid's Python library to send an email with HTML attached. The HTML will contain the contents of a [Metaflow card](https://docs.metaflow.org/metaflow/visualizing-results#what-are-cards), a quick way to visualize flow artifacts in HTML.
 
 You can replace this `send_email` function with another one that uses an email provider of your choice. The example flow works the same way with any email provider.
 
 
 ```py title=""emailer.py""
 import sendgrid
 import base64
 
 def send_email(from_email, to_email, subject, html, key):
     
     message = sendgrid.Mail(
         from_email=from_email,
         to_emails=to_email,
         subject=subject,
         plain_text_content='Find a Metaflow card attached'
     )
 
     encoded_content = base64.b64encode(
         html.encode('utf-8')).decode('utf-8')
     
     att = sendgrid.Attachment(
         file_content=encoded_content,
         file_type='text/html',
         file_name='card.html',
         disposition='attachment')
     
     message.add_attachment(att)
     sendgrid.SendGridAPIClient(key).send(message)
 ```",H3,https://outerbounds.com/docs/email-flow-results#use-sendgrid-s-python-library-to-send-email,False,1146.0,223.0
124,Run Flow,"This flow shows how to:
 * Pull an image with an internet request.
 * Store the image in a Metaflow card.
 * Access the card and attach it to an email sent with Sendgrid.
 * Pull an image with an HTTP request to show how you can include images in the card. The `@card` decorator packages them in the HTML file, so the recipient of the email will be able to see them too.
 
 
 
 
 
 
 ```py title=""send_email_from_flow.py""
 import os
 from metaflow import FlowSpec, step, current, card, Flow
 from metaflow.cards import Markdown, Image, get_cards
 import requests",H3,https://outerbounds.com/docs/email-flow-results#run-flow,False,562.0,100.0
125,Visualize Artifact,"You can use the following command to look at your card and verify the email sent the same in an HTML attachment.
 
 
 ```bash
 python send_email_from_flow.py card view start
 ```",H3,https://outerbounds.com/docs/email-flow-results#visualize-artifact,False,178.0,31.0
126,Further Reading,"* [Scheduling Metaflow Flows](https://docs.metaflow.org/going-to-production-with-metaflow/scheduling-metaflow-flows)
 * [Cards Documentation](https://docs.metaflow.org/metaflow/visualizing-results)",H2,https://outerbounds.com/docs/email-flow-results#further-reading,False,197.0,7.0
127,Question,How can I train a model in my flow and deploy it with Sagemaker?,H2,https://outerbounds.com/docs/deploy-with-sagemaker#question,True,64.0,14.0
128,Solution,"Sagemaker allows you to host a model on an EC2 instance and gives you an endpoint you can make requests to. 
 
 First there are some configuration details you will need to address to deploy a model from your flow.",H2,https://outerbounds.com/docs/deploy-with-sagemaker#solution,False,213.0,41.0
129,Set Sagemaker IAM Role,"To use Sagemaker as shown on this page you will need an [appropriate](https://docs.aws.amazon.com/sagemaker/latest/dg/api-permissions-reference.html) IAM Role configured for Sagemaker execution. 
 
 
 AWS has [managed policies](https://docs.aws.amazon.com/sagemaker/latest/dg/security-iam-awsmanpol.html) you can use for this like `AmazonSageMakerFullAccess` or the `DataScientist` role.",H3,https://outerbounds.com/docs/deploy-with-sagemaker#set-sagemaker-iam-role,False,387.0,37.0
130,Configuration,"Next we will configure important environment variables. In this example a `.env` file is used to manage environment variables for:
 * An IAM role defined for Sagemaker Execution.
 * An S3 prefix URI where Sagemaker can upload your custom code.
 
 
 ```env title=""my.env""
 ROLE=<YOUR IAM ROLE>
 CODE_LOCATION=<S3 URI TO PUSH YOUR CUSTOM CODE TO>
 ```",H3,https://outerbounds.com/docs/deploy-with-sagemaker#configuration,False,349.0,56.0
131,Define Sagemaker Entry Point,"Here is the file defining the entry point that will be passed to the Sagemaker deployment. 
 This file includes the `model_fn` function which will load the model when a prediction is requested. 
 There are other [functions related to model serving](https://sagemaker.readthedocs.io/en/v1.72.1/frameworks/sklearn/using_sklearn.html#serve-a-model) that are not included in this file because the SageMaker scikit-learn model server has a default implementation for those functions.
 
 
 ```py title=""sagemaker_entry_point.py""
 import joblib
 import os
 
 def model_fn(model_dir):
     return joblib.load(os.path.join(model_dir, ""model/model.joblib""))
 ```",H3,https://outerbounds.com/docs/deploy-with-sagemaker#define-sagemaker-entry-point,False,652.0,81.0
132,Define Deployment Logic,"The following script contains a light wrapper around the SageMaker SDK that will be called from the flow:
 
 <CustomCode height=""30vh"">
 
 
 
 ```py title=""deployer.py""
 import os
 import joblib
 import shutil
 import tarfile
 import numpy as np
 from sklearn.base import BaseEstimator
 from sagemaker.sklearn import SKLearnModel
 from metaflow import S3
 from dotenv import load_dotenv
 load_dotenv('my.env')
 
 def to_sagemaker(
     model:BaseEstimator = None,
     sagemaker_model_name:str = ""model"",
     model_save_name:str = ""model"", 
     endpoint_name:str = ""sklearn endpoint"", 
     instance_type:str = ""ml.c5.2xlarge"", 
     entry_point:str = ""sagemaker_entry_point.py"",
     sklearn_version:str = ""1.0-1"",
     role:str = os.getenv('ROLE'), 
     code_location:str = os.getenv('CODE_LOCATION'),
     run = None,
 ):
 
     # save model to local folder
     # this should match what is in sagemaker_entry_point
     model_save_name = ""model""
     os.makedirs(model_save_name, exist_ok=True)
     out_path = ""{}/{}.joblib"".format(model_save_name, model_save_name)
     joblib.dump(model, out_path)
     
     # save model as tar.gz
     local_tar_name = ""{}.tar.gz"".format(model_save_name)
     with tarfile.open(local_tar_name, 
                       mode=""w:gz"") as _tar:
         _tar.add(model_save_name, recursive=True)
 
     # save model onto S3
     with S3(run=run) as s3:
         with open(local_tar_name, ""rb"") as in_file:
             data = in_file.read()
             model_s3_path = s3.put(local_tar_name, data)
 
     # remove local model folder and tar
     shutil.rmtree(model_save_name)
     os.remove(local_tar_name)
     
     print(""Creating and deploying Sagemaker model..."")
     sklearn_model = SKLearnModel(
         name=sagemaker_model_name,
         model_data=model_s3_path, 
         role=role,
         entry_point=entry_point,
         framework_version=sklearn_version,
         code_location=code_location
     )
     
     predictor = sklearn_model.deploy(
         instance_type=instance_type,
         initial_instance_count=1,
         endpoint_name=endpoint_name
     )
     
     return model_s3_path
 ```
 
 </CustomCode>",H3,https://outerbounds.com/docs/deploy-with-sagemaker#define-deployment-logic,False,2175.0,470.0
133,Run Flow,"Here is a flow that shows how to:
 * Train a scikit-learn model.
 * Save the model on S3. 
 * Deploy the model with Sagemaker. 
 * Verify the model is predicting the same locally and in deployment.
 * (optionally) Clean up the Sagemaker deployment resources. 
 
 
 
 
 
 
 ```py title=""deploy_to_sagemaker.py""
 from metaflow import FlowSpec, step, S3, conda_base
 import os
 import json
 
 class DeployToSagemakerFlow(FlowSpec):
     
     @step
     def start(self):
         from sklearn import datasets
         from sklearn.model_selection import train_test_split
         self.iris = datasets.load_iris()
         X, y = self.iris['data'], self.iris['target']
         self.labels = self.iris['target_names']
         split = train_test_split(X, y, test_size=0.2)
         self.X_train, self.X_test = split[0], split[1]
         self.y_train, self.y_test = split[2], split[3]
         self.next(self.train_rf_model)
         
     @step
     def train_rf_model(self):
         from sklearn.ensemble import RandomForestClassifier
         from sklearn.model_selection import cross_val_score
         self.clf = RandomForestClassifier(random_state=0)
         self.clf.fit(self.X_train, self.y_train)
         # will use local preds as test of `deploy` step.
         self.local_y_pred = self.clf.predict(self.X_test)
         self.next(self.deploy)
 
     @step
     def deploy(self):
         import time
         import deployer 
         t = int(round(time.time() * 1000))
         self.sagemaker_model_name = 'rf-model-{}'.format(t)
         self.model_save_name = 'model'
         self.endpoint_name = 'rf-endpoint-{}'.format(t)
         self.instance_type = 'ml.c5.2xlarge'
         self.entry_point = 'sagemaker_entry_point.py'
         self.sklearn_sage_version = '1.0-1'
         self.model_s3_path = deployer.to_sagemaker(
             model = self.clf,
             sagemaker_model_name = self.sagemaker_model_name,
             model_save_name = self.model_save_name,
             endpoint_name = self.endpoint_name, 
             instance_type = self.instance_type, 
             entry_point = self.entry_point,
             sklearn_version = self.sklearn_sage_version,
             role = os.getenv('ROLE'), 
             code_location=os.getenv('CODE_LOCATION'),
             run = self
         )
         self.next(self.end)
 
     @step
     def end(self):
         print(""\nModel name is: {}"".format(
             self.sagemaker_model_name))
         print(""Endpoint name is: {}\n"".format(
             self.endpoint_name))
 
 if __name__ == ""__main__"":
     DeployToSagemakerFlow()
 ```
 
 
 
 
 
 
 ```bash
 python deploy_to_sagemaker.py run
 ```
 
 <CodeOutputBlock lang=""bash"">
 
 ```
      Workflow starting (run-id 847):
      [847/start/4208 (pid 8886)] Task is starting.
      [847/start/4208 (pid 8886)] Task finished successfully.
      [847/train_rf_model/4209 (pid 8900)] Task is starting.
      [847/train_rf_model/4209 (pid 8900)] Task finished successfully.
      [847/deploy/4210 (pid 8923)] Task is starting.
      [847/deploy/4210 (pid 8923)] Creating and deploying Sagemaker model...
      [847/deploy/4210 (pid 8923)] -----!
      [847/deploy/4210 (pid 8923)] Task finished successfully.
      [847/end/4211 (pid 8960)] Task is starting.
      [847/end/4211 (pid 8960)] 
      [847/end/4211 (pid 8960)] Model name is: rf-model-1657218689272
      [847/end/4211 (pid 8960)] Endpoint name is: rf-endpoint-1657218689272
      [847/end/4211 (pid 8960)] 
      [847/end/4211 (pid 8960)] Task finished successfully.
      Done!
 ```
 
 </CodeOutputBlock>
 
 You can see your endpoint by going to the Sagemaker section of AWS console and clicking `> inference > endpoints`. It will take a few minutes for the model and endpoint to create.",H3,https://outerbounds.com/docs/deploy-with-sagemaker#run-flow,False,3774.0,881.0
134,Make Prediction,"Now you can make a prediction with your deployed model! 
 You can run the following code in a notebook cell or Python script after executing the flow. 
 The snippet uses data that Metaflow has stored to:
 * Load `X_test` data from the `start` step.
 * Send the data to the endpoint to request predictions.
 * Parse the response.
 * Ensure the deployed model predictions are the same as those of the local model version.
 
 
 ```python
 from metaflow import Flow
 import boto3
 import pandas as pd
 import numpy as np
 import re
 
 sagemaker_runtime = boto3.client(""sagemaker-runtime"", 
                                  region_name='us-east-2')",H3,https://outerbounds.com/docs/deploy-with-sagemaker#make-prediction,False,644.0,134.0
135,Clean up Sagemaker Resources,"```python
 from sagemaker import Session
 from metaflow import Flow
 
 run_data = Flow('DeployToSagemakerFlow').latest_run.data
 model_name=run_data.sagemaker_model_name
 endpoint_name=run_data.endpoint_name
 
 sagemaker_session = Session()
 sm_client = sagemaker_session.boto_session.client(""sagemaker"")
 sm_client.delete_endpoint(EndpointName=endpoint_name)
 sm_client.delete_model(ModelName=model_name)
 ```
 
 <CodeOutputBlock lang=""python"">
 
 ```
     {'ResponseMetadata': {'RequestId': '694abcde-f0e3-4c9d-a199-8946096bdb20',
       'HTTPStatusCode': 200,
       'HTTPHeaders': {'x-amzn-requestid': '694abcde-f0e3-4c9d-a199-8946096bdb20',
        'content-type': 'application/x-amz-json-1.1',
        'content-length': '0',
        'date': 'Thu, 07 Jul 2022 18:34:30 GMT'},
       'RetryAttempts': 0}}
 ```
 
 </CodeOutputBlock>",H3,https://outerbounds.com/docs/deploy-with-sagemaker#clean-up-sagemaker-resources,False,835.0,97.0
136,Further Reading,"* [Machine learning pipelines: from prototype to production](https://outerbounds.com/blog/machine-learning-pipelines-from-prototype-to-production)
 * [Scheduling Metaflow Flows with AWS Step Functions](https://docs.metaflow.org/going-to-production-with-metaflow/scheduling-metaflow-flows/scheduling-with-aws-step-functions)",H2,https://outerbounds.com/docs/deploy-with-sagemaker#further-reading,False,323.0,16.0
137,Question,How do I define Python list objects as parameters of a flow?,H2,https://outerbounds.com/docs/define-lists-as-params#question,True,60.0,12.0
138,Solution,"The [Parameter documentation](https://docs.metaflow.org/api/flowspec#parameters) says you can ""specify one of `str`, `float`, `int`, `bool`, or `JSONType`"". This page will show how you can define lists using the `str` parameter type or with the `multiple` option in the parameter definition.",H2,https://outerbounds.com/docs/define-lists-as-params#solution,False,291.0,38.0
139,String List Parameter,,H3,https://outerbounds.com/docs/define-lists-as-params#string-list-parameter,False,0.0,1.0
140,Write a Flow,"This flow defines a `Parameter` called `my_values` in the flow code and `vals` as a command line argument. The default is set to the string `'1,2,3'`. Notice the `separator` argument to the `Parameter` constructor. The separator is eventually passed as an argument to [Python's str.split function](https://docs.python.org/3/library/stdtypes.html?highlight=split#str.split) when the `my_values` constant is assigned a value. In this case the value of `my_values` is `['1', '2', '3']`. The elements of the list can be mapped into any type you want as the `start` step shows. 
 
 
 
 
 
 
 ```py title=""define_list_as_str_param.py""
 from metaflow import FlowSpec, step, Parameter
 
 class ListStringParamFlow(FlowSpec):
     
     # highlight-next-line
     my_values = Parameter(""vals"", default = '1,2,3', separator = ',')
     
     @step
     def start(self):
         self.int_data = list(map(int, self.my_values))
         self.next(self.end)
         
     @step
     def end(self):
         print(self.int_data)
 
 if __name__ == ""__main__"":
     ListStringParamFlow()
 ```",H4,https://outerbounds.com/docs/define-lists-as-params#write-a-flow,False,1077.0,202.0
141,Run Flow with Default Parameters,"```bash
 python define_list_as_str_param.py run
 ```
 
 <CodeOutputBlock lang=""bash"">
 
 ```
      Workflow starting (run-id 1666829547854560):
      [1666829547854560/start/1 (pid 76010)] Task is starting.
      [1666829547854560/start/1 (pid 76010)] Task finished successfully.
      [1666829547854560/end/2 (pid 76013)] Task is starting.
      [1666829547854560/end/2 (pid 76013)] [1, 2, 3]
      [1666829547854560/end/2 (pid 76013)] Task finished successfully.
      Done!
 ```
 
 </CodeOutputBlock>",H4,https://outerbounds.com/docs/define-lists-as-params#run-flow-with-default-parameters,False,503.0,83.0
142,Pass Parameter Values to the Flow,"```bash
 python define_list_as_str_param.py run --vals '4,5,6'
 ```
 
 <CodeOutputBlock lang=""bash"">
 
 ```
      Workflow starting (run-id 1666829549149463):
      [1666829549149463/start/1 (pid 76030)] Task is starting.
      [1666829549149463/start/1 (pid 76030)] Task finished successfully.
      [1666829549149463/end/2 (pid 76033)] Task is starting.
      [1666829549149463/end/2 (pid 76033)] [4, 5, 6]
      [1666829549149463/end/2 (pid 76033)] Task finished successfully.
      Done!
 ```
 
 </CodeOutputBlock>",H4,https://outerbounds.com/docs/define-lists-as-params#pass-parameter-values-to-the-flow,False,518.0,85.0
143,List Multiple Parameter Values,"In the previous section, you saw how to pass many values into a flow as single parameter values using strings. 
 In this section, you will see how to reuse the same parameter name while passing many parameter values in their native type.
 This lets you use the values in flows without extra typecasting, and can make it easier to read flow run commands.
 <br></br>",H3,https://outerbounds.com/docs/define-lists-as-params#list-multiple-parameter-values,False,364.0,65.0
144,Write a Flow,"This flow defines a `Parameter` called `my_values` in the flow code and `vals` as a command line argument. 
 The parameter definition includes setting `multiple=True`, which means we can pass multiple values to this parameter at flow run time.
 In this case, the value of `my_values` is `[2]` by default.  
 
 
 
 
 
 
 
 ```py title=""define_multiple_params.py""
 from metaflow import FlowSpec, step, Parameter
 
 class ListMultipleParamFlow(FlowSpec):
     
     # highlight-next-line
     my_values = Parameter(""val"", default = 2, multiple = True)
 
     @step
     def start(self):
         self.next(self.end)
         
     @step
     def end(self):
         print(list(self.my_values))
 
 if __name__ == ""__main__"":
     ListMultipleParamFlow()
 ```",H4,https://outerbounds.com/docs/define-lists-as-params#write-a-flow,False,754.0,155.0
145,Run Flow with List of Parameter Values,"To run the flow and access a list of values in `self.my_values`, you can then pass as many values as you want to the parameter name in the run.
 
 
 ```bash
 python define_multiple_params.py run --val 1 --val 2 --val 3
 ```
 
 <CodeOutputBlock lang=""bash"">
 
 ```
      Workflow starting (run-id 1666829550543713):
      [1666829550543713/start/1 (pid 76050)] Task is starting.
      [1666829550543713/start/1 (pid 76050)] Task finished successfully.
      [1666829550543713/end/2 (pid 76053)] Task is starting.
      [1666829550543713/end/2 (pid 76053)] [1, 2, 3]
      [1666829550543713/end/2 (pid 76053)] Task finished successfully.
      Done!
 ```
 
 </CodeOutputBlock>",H4,https://outerbounds.com/docs/define-lists-as-params#run-flow-with-list-of-parameter-values,False,674.0,120.0
146,Further Reading,"* [How to reuse parameters in multiple flows](/docs/reuse-parameters-across-flows/)
 * [How to load data in flows using IncludeFile](/docs/load-local-data-with-include/)",H2,https://outerbounds.com/docs/define-lists-as-params#further-reading,False,169.0,17.0
147,Question,"How can I use Metaflow to save and version data artifacts such as numpy arrays, pandas dataframes, or other Python objects with Metaflow. How can I access and update artifacts throughout the steps of a flow?",H2,https://outerbounds.com/docs/pass-artifacts-between-steps#question,True,207.0,36.0
148,Solution,"In this example you will see how you can save any Python object that can be [pickled](https://docs.python.org/3/library/pickle.html#:~:text=%E2%80%9CPickling%E2%80%9D%20is%20the%20process%20whereby,back%20into%20an%20object%20hierarchy.) as an artifact - called `some_data` in this example - by storing it in `self`. You can then later access and update the artifact with `self` to propagate changes.",H2,https://outerbounds.com/docs/pass-artifacts-between-steps#solution,False,400.0,46.0
149,Run Flow,"This flow shows how to
 * Store a flow artifact.
 * Update the artifact in a downstream step.
 * Watch how the artifacts change during the flow.
 
 
 
 
 
 
 ```py title=""pass_artifacts_between_steps.py""
 from metaflow import FlowSpec, step
 
 class ArtFlow(FlowSpec):
 
     @step
     def start(self):
         #highlight-next-line
         self.some_data = [1,2,3] # define artifact state
         self.next(self.middle)
         
     @step 
     def middle(self):
         #highlight-next-line
         print(f'the data artifact is: {self.some_data}')
         self.some_data = [1,2,4] # update artifact state
         self.next(self.end)
 
     @step
     def end(self):
         #highlight-next-line
         print(f'the data artifact is: {self.some_data}')
 
 if __name__ == '__main__':
     ArtFlow()
 ```
 
 
 
 
 When you run the flow, the artifact is correctly accessed across steps.  Note that this functionality works regardless if you are running your flows locally or remotely (for example with `@batch`).
 
 
 ```bash
 python pass_artifacts_between_steps.py run --run-id-file artifacts-run.txt
 ```
 
 <CodeOutputBlock lang=""bash"">
 
 ```
     ...
      [1654221288112057/middle/2 (pid 71321)] Task is starting.
      [1654221288112057/middle/2 (pid 71321)] the data artifact is: [1, 2, 3]
      [1654221288112057/middle/2 (pid 71321)] Task finished successfully.
     ...
      [1654221288112057/end/3 (pid 71343)] Task is starting.
      [1654221288112057/end/3 (pid 71343)] the data artifact is: [1, 2, 4]
      [1654221288112057/end/3 (pid 71343)] Task finished successfully.
     ...
 ```
 
 </CodeOutputBlock>",H3,https://outerbounds.com/docs/pass-artifacts-between-steps#run-flow,False,1632.0,342.0
150,Access Artifacts Outside of Flow,"You can use [the client API](https://docs.metaflow.org/metaflow/client) to access data artifacts after a run is complete. There are many ways to access this data, but we show you several examples below.
 
 You can reference `Run(<FlowName>/<Run ID>)` to access artifacts:
 
 
 ```python
 from metaflow import Run",H3,https://outerbounds.com/docs/pass-artifacts-between-steps#access-artifacts-outside-of-flow,False,312.0,46.0
151,Further Reading,"* [Metaflow client API documentation](https://docs.metaflow.org/metaflow/client)
 * [How to use artifacts in Metaflow join step](/docs/use-artifacts-in-join-step)",H2,https://outerbounds.com/docs/pass-artifacts-between-steps#further-reading,False,162.0,14.0
152,Question,How do I import common packages from a parent directory in a flow?,H2,https://outerbounds.com/docs/access-parent-directories#question,True,66.0,13.0
153,Solution,"From your Metaflow code, you can access higher-level directories by adding a [symlink](https://en.wikipedia.org/wiki/Symbolic_link). The symlinks in this example are represented by arrows pointing to the common package defined in the parent directory `../common_utils`. 
 
 
 
 
 
 ```
 flow_type_1
     flow1.py
     common_utils -> ../common_utils
 flow_type_2
     flow2.py
     common_utils -> ../common_utils
 common_utils
     __init__.py
     some_module.py
 ```
 
 
 
 
 
 
 Since Metaflow version 2.5.2, you can add symlinks in the directories containing your flow script, and Metaflow will dereference the symlinks and include the contents (to `../common_utils` in this case) with your code. This allows you to import from `common_utils` inside flow steps like those in the `flow1.py` script whether they are run locally or remotely.
 
 The rest of this page goes through an example that uses symlinks in this way.",H2,https://outerbounds.com/docs/access-parent-directories#solution,False,924.0,156.0
154,Define Common Functionality,"You can define generic functionality in a custom Python package and reuse it across flows in the directory structure shown above. Here is a definition for a function that will be used in `flow_type_1/flow1.py`:
 
 
 ```py title=""common_utils/some_module.py""
 import os
 
 def general_function():
     return os.getcwd()
 ```
 
 Then you can import the function in the initialization module for the `common_utils` package:
 
 
 ```py title=""common_utils/__init__.py""
 from .some_module import general_function
 ```",H3,https://outerbounds.com/docs/access-parent-directories#define-common-functionality,False,513.0,74.0
155,Create Symlink,"In order to import `general_function` from the `common_utils` package in a flow, you need to add a symlink from the directory containing your flow script to the higher-level package you want to import in the flow. 
 
 The `ln` command is a standard Unix way to do this. It has an argument `-s` that creates a symbolic link between the file at `../common_utils` and `flow_type_1/common_utils`:
 
 
 ```bash
 ln -s ../common_utils flow_type_1/common_utils
 ```",H3,https://outerbounds.com/docs/access-parent-directories#create-symlink,False,458.0,74.0
156,Write a Flow that Imports the Function,"This flow imports the `common_utils` package in the start step. This works because of the symlink to the `common_utils` package from the last section.
 
 
 
 
 
 
 ```py title=""flow_type_1/flow1.py""
 from metaflow import FlowSpec, step
 import os
 
 class Flow1(FlowSpec):
 
     @step
     def start(self):
         from common_utils import general_function
         self.result = general_function()
         self.next(self.end)
     
     @step
     def end(self):
         pass
     
 if __name__ == ""__main__"":
     Flow1()
 ```",H3,https://outerbounds.com/docs/access-parent-directories#write-a-flow-that-imports-the-function,False,532.0,126.0
157,Run the Flow,"```bash
 python 'flow_type_1/flow1.py' run
 ```
 
 <CodeOutputBlock lang=""bash"">
 
 ```
      Workflow starting (run-id 1666836481231665):
      [1666836481231665/start/1 (pid 81716)] Task is starting.
      [1666836481231665/start/1 (pid 81716)] Task finished successfully.
      [1666836481231665/end/2 (pid 81719)] Task is starting.
      [1666836481231665/end/2 (pid 81719)] Task finished successfully.
      Done!
 ```
 
 </CodeOutputBlock>",H3,https://outerbounds.com/docs/access-parent-directories#run-the-flow,False,445.0,72.0
158,Further Reading,"* [How to Reuse Model Code](/docs/reuse-model-object/)
 * [How to Load Local Data on Remote Compute](/docs/load-local-data-with-include/)",H2,https://outerbounds.com/docs/access-parent-directories#further-reading,False,137.0,15.0
159,Question,How can I pass data artifacts of a Metaflow flow through a join step? What are my options for merging artifacts?,H2,https://outerbounds.com/docs/use-artifacts-in-join-step#question,True,112.0,21.0
160,Solution,"You can `merge_artifacts` in the join step. There are additional Metaflow features that allow you to `exclude` upstream artifacts during the merge. You will also want to be aware of any potential collisions with upstream artifact names.
 
 
 
 
 
 This flow shows how to:
 * Access upstream values after branches are joined.
 * Select a value from a specific branch because there is a naming collision.
 * Exclude an upstream value from the merge.
 
 
 
 
 
 
 ```py title=""join_step_artifacts.py""
 from metaflow import FlowSpec, step
 
 class JoinArtifacts(FlowSpec):
 
     @step
     def start(self):
         self.pre_branch_data = 0
         self.next(self.branch_a, self.branch_b)
         
     @step
     def branch_a(self):
         self.x = 1 # define x 
         self.a = ""a""
         self.next(self.join)
         
     @step
     def branch_b(self):
         self.x = 2 # define another x! 
         self.b = ""b""
         self.next(self.join)
     
     @step
     def join(self, inputs):
         # pick which x to propagate
         self.x = inputs.branch_a.x 
         self.merge_artifacts(inputs, exclude=[""a""])
         self.next(self.end)
         
     @step 
     def end(self):
         print(""`pre_branch_data` "" + \
               f""value is: {self.pre_branch_data}."")
         print(f""`x` value is: {self.x}."")
         print(f""`b` value is: {self.b}."")
         try: 
             print(f""`a` value is: {self.a}."")
         except AttributeError as e:
             print(""`a` was excluded! \U0001F632"")
         
 
 if __name__ == ""__main__"":
     JoinArtifacts()
 ```
 
 
 
 
 
 ```bash
 python join_step_artifacts.py run
 ```
 
 <CodeOutputBlock lang=""bash"">
 
 ```
      Workflow starting (run-id 1654221288038724):
      [1654221288038724/start/1 (pid 71304)] Task is starting.
      [1654221288038724/start/1 (pid 71304)] Task finished successfully.
      [1654221288038724/branch_a/2 (pid 71314)] Task is starting.
      [1654221288038724/branch_b/3 (pid 71315)] Task is starting.
      [1654221288038724/branch_a/2 (pid 71314)] Task finished successfully.
      [1654221288038724/branch_b/3 (pid 71315)] Task finished successfully.
      [1654221288038724/join/4 (pid 71337)] Task is starting.
      [1654221288038724/join/4 (pid 71337)] Task finished successfully.
      [1654221288038724/end/5 (pid 71375)] Task is starting.
      [1654221288038724/end/5 (pid 71375)] `pre_branch_data` value is: 0.
      [1654221288038724/end/5 (pid 71375)] `x` value is: 1.
      [1654221288038724/end/5 (pid 71375)] `b` value is: b.
      [1654221288038724/end/5 (pid 71375)] `a` was excluded! 😲
      [1654221288038724/end/5 (pid 71375)] Task finished successfully.
      Done!
 ```
 
 </CodeOutputBlock>",H2,https://outerbounds.com/docs/use-artifacts-in-join-step#solution,False,2726.0,638.0
161,Further Reading,"* [Inspecting flows and results](https://docs.metaflow.org/metaflow/client)
 * More examples using `@retry` and `@catch` in the [Effective Data Science Infrastructure](https://livebook.manning.com/book/effective-data-science-infrastructure/chapter-4/v-8/1) book",H2,https://outerbounds.com/docs/use-artifacts-in-join-step#further-reading,False,261.0,19.0
162,Question,How can I create a nested foreach with a Metaflow DAG?,H2,https://outerbounds.com/docs/nested-foreach#question,True,54.0,11.0
163,Solution,"You can nest [linear](https://docs.metaflow.org/metaflow/basics#linear), [branching](https://docs.metaflow.org/metaflow/basics#branch), and [foreach](https://docs.metaflow.org/metaflow/basics#foreach) steps, or any of their combinations between, a foreach and its corresponding join step. [Here](https://outerboundsco.slack.com/archives/C02116BBNTU/p1666304143387609) is a Slack conversation you can read to learn the fundamentals of nested foreach. The rest of this post demonstrates a machine learning use case.",H2,https://outerbounds.com/docs/nested-foreach#solution,False,513.0,47.0
164,Write a Flow with a Nested Foreach,"For example, flow trains two hyperparameter configurations for each of two model types. In this case the models are `sklearn.linear_model.LogisticRegression` and `sklearn.ensemble.RandomForestClassifier`. However, Metaflow is agnostic to the modeling frameworks you use so you can extend this template to many more models and hyperparameter combinations. 
 
 The flow contains the following steps:
 * `start` configures the first for each split using the `self.model_param_combination` variable. The variable name passed to foreach must be an iterable. 
 * foreach model and list of parameter combinations, the `tune` step calls another foreach over the parameter combinations unique to this model.
 * foreach parameter combination foreach model, the `train_and_score` step fits a classifier and tracks cross validation accuracies. 
 * `join_train` collects results foreach `train_and_score` step of one branch of the `tune` step. 
 * `join_tune` collects results foreach `tune` step. 
 
 ![](/assets/nested-foreach.png)
 
 
 
 
 
 
 
 ```py title=""nested_foreach.py""
 from metaflow import FlowSpec, step
 
 class NestedForeachFlow(FlowSpec):
 
     @step
     def start(self):
         from sklearn.datasets import load_iris
         from sklearn.linear_model import LogisticRegression
         from sklearn.ensemble import RandomForestClassifier
         iris = load_iris()
         self.X = iris['data']
         self.y = iris['target']
         self.model_param_combination = [
             (
                 LogisticRegression,
                 [{""C"": 0.5}, {""C"": 1.0}]
             ),
             (
                 RandomForestClassifier, 
                 [{""max_depth"": 2}, {""max_depth"": 6}]
             )
         ]
         #highlight-next-line
         self.next(self.tune, foreach=""model_param_combination"")
 
     @step
     def tune(self):
         self.model, self.param_dict = self.input
         #highlight-next-line
         self.next(self.train_and_score, foreach=""param_dict"")
 
     @step
     def train_and_score(self):
         from sklearn.model_selection import cross_val_score
         self.params = self.input
         self.clf = self.model(**self.params)
         self.scores = cross_val_score(
             self.clf, self.X, self.y, cv=5)
         self.next(self.join_train)
 
     @step
     def join_train(self, inputs):
         import json
         import numpy as np
         self.scores = {
             'model': [],
             'params': [],
             'mean accuracy': [],
             'std accuracy': [],
         }
         for i in inputs:
             self.scores['model'].append(i.clf.__class__)
             self.scores['params'].append(json.dumps(i.params))
             self.scores['mean accuracy'].append(np.mean(i.scores))
             self.scores['std accuracy'].append(np.std(i.scores))
         self.next(self.join_tune)
 
     @step
     def join_tune(self, inputs):
         self.all_scores = {} 
         for model in inputs:
             if self.all_scores == {}:
                 self.all_scores = model.scores
             else:
                 for k,v in model.scores.items():
                     self.all_scores[k] += v
         self.next(self.end)
 
     @step
     def end(self):
         pass
 
 if __name__ == ""__main__"":
     NestedForeachFlow()
 ```",H3,https://outerbounds.com/docs/nested-foreach#write-a-flow-with-a-nested-foreach,False,3322.0,887.0
165,Run the Flow,"```bash
 python nested_foreach.py run
 ```
 
 <CodeOutputBlock lang=""bash"">
 
 ```
      Workflow starting (run-id 1663634177889364):
      [1663634177889364/start/1 (pid 40412)] Task is starting.
      [1663634177889364/start/1 (pid 40412)] Foreach yields 2 child steps.
      [1663634177889364/start/1 (pid 40412)] Task finished successfully.
      [1663634177889364/tune/2 (pid 40416)] Task is starting.
      [1663634177889364/tune/3 (pid 40417)] Task is starting.
      [1663634177889364/tune/3 (pid 40417)] Foreach yields 2 child steps.
      [1663634177889364/tune/3 (pid 40417)] Task finished successfully.
      [1663634177889364/train_and_score/4 (pid 40424)] Task is starting.
      [1663634177889364/train_and_score/5 (pid 40425)] Task is starting.
      [1663634177889364/tune/2 (pid 40416)] Foreach yields 2 child steps.
      [1663634177889364/tune/2 (pid 40416)] Task finished successfully.
      [1663634177889364/train_and_score/6 (pid 40426)] Task is starting.
      [1663634177889364/train_and_score/7 (pid 40427)] Task is starting.
      [1663634177889364/train_and_score/6 (pid 40426)] Task finished successfully.
      [1663634177889364/train_and_score/7 (pid 40427)] Task finished successfully.
      [1663634177889364/join_train/8 (pid 40440)] Task is starting.
      [1663634177889364/train_and_score/4 (pid 40424)] Task finished successfully.
      [1663634177889364/train_and_score/5 (pid 40425)] Task finished successfully.
      [1663634177889364/join_train/9 (pid 40441)] Task is starting.
      [1663634177889364/join_train/8 (pid 40440)] Task finished successfully.
      [1663634177889364/join_train/9 (pid 40441)] Task finished successfully.
      [1663634177889364/join_tune/10 (pid 40448)] Task is starting.
      [1663634177889364/join_tune/10 (pid 40448)] Task finished successfully.
      [1663634177889364/end/11 (pid 40452)] Task is starting.
      [1663634177889364/end/11 (pid 40452)] Task finished successfully.
      Done!
 ```
 
 </CodeOutputBlock>",H3,https://outerbounds.com/docs/nested-foreach#run-the-flow,False,1995.0,309.0
166,Analyze Results,"After running the flow you can use this code in any Python script or notebook to access the results of each model training run:
 
 
 ```python
 from metaflow import Flow
 import pandas as pd
 results = pd.DataFrame(Flow('NestedForeachFlow').latest_run.data.all_scores)
 results
 ```
     
 <HTMLOutputBlock >
 
 
 
 
 ```html
 <div>
 <style scoped>
     .dataframe tbody tr th:only-of-type {
         vertical-align: middle;
     }
 
     .dataframe tbody tr th {
         vertical-align: top;
     }
 
     .dataframe thead th {
         text-align: right;
     }
 </style>
 <table border=""1"" class=""dataframe"">
   <thead>
     <tr style=""text-align: right;"">
       <th></th>
       <th>model</th>
       <th>params</th>
       <th>mean accuracy</th>
       <th>std accuracy</th>
     </tr>
   </thead>
   <tbody>
     <tr>
       <th>0</th>
       <td>&lt;class 'sklearn.linear_model._logistic.Logisti...</td>
       <td>{""C"": 0.5}</td>
       <td>0.966667</td>
       <td>0.021082</td>
     </tr>
     <tr>
       <th>1</th>
       <td>&lt;class 'sklearn.linear_model._logistic.Logisti...</td>
       <td>{""C"": 1.0}</td>
       <td>0.973333</td>
       <td>0.024944</td>
     </tr>
     <tr>
       <th>2</th>
       <td>&lt;class 'sklearn.ensemble._forest.RandomForestC...</td>
       <td>{""max_depth"": 2}</td>
       <td>0.960000</td>
       <td>0.024944</td>
     </tr>
     <tr>
       <th>3</th>
       <td>&lt;class 'sklearn.ensemble._forest.RandomForestC...</td>
       <td>{""max_depth"": 6}</td>
       <td>0.960000</td>
       <td>0.024944</td>
     </tr>
   </tbody>
 </table>
 </div>
 ```
 
 
 
 </HTMLOutputBlock>",H3,https://outerbounds.com/docs/nested-foreach#analyze-results,False,1628.0,388.0
167,Further Reading,"* [How to use Optuna with Metaflow](/docs/use-optuna-with-metaflow/)
 * [How to use artifacts in a Metaflow join step](/docs/use-artifacts-in-join-step/)",H2,https://outerbounds.com/docs/nested-foreach#further-reading,False,153.0,17.0
168,Question,How can I use environment variables in a step of my Metaflow flow?,H2,https://outerbounds.com/docs/set-env-vars-with-decorator#question,True,66.0,13.0
169,Solution,"There are several ways this page will show you how to do this. The most straightforward way is to pass a dictionary to the `vars` argument of Metaflow's `@environment` decorator. This will work locally and with remote compute instances running your steps. 
 
 These methods demonstrate how to set arbitrary environment variables in the execution environment of steps in your flow. If you are looking to understand the environment variables specific to your Metaflow configuration - which by default lives at `$HOME/.metaflowconfig/config.json` - you can see them [here](https://github.com/Netflix/metaflow/blob/master/metaflow/metaflow_config.py).
 
 You can pass a dictionary containing variables you want to use in a step to the `vars` argument of Metaflow's `@environment` step decorator.",H2,https://outerbounds.com/docs/set-env-vars-with-decorator#solution,False,791.0,113.0
170,Set Environment Variable,"You can pass a dictionary containing variables you want to use in a step to the `vars` argument of Metaflow's `@environment` step decorator. 
     
 Before running the flow, let's define an environment variable to read: 
 
 ```
 export MSG=""hi""
 ```",H3,https://outerbounds.com/docs/set-env-vars-with-decorator#set-environment-variable,False,249.0,46.0
171,Run Flow,"This flow shows how to:
 * Use `@environment` to pass and then print the contents of `STEP_ENV_VAR` in `read_locally`. 
 * Use `@environment` to pass and then print the contents of `STEP_ENV_VAR` in `read_in_container`.
     * This step demonstrates reading the environment variable into a customizable Docker image. 
     * This step is run on AWS batch. You could replace the `@batch` decorator with `@kubernetes`.
 
 
 
 
 
 
 ```py title=""set_environment_variables_decorator.py""
 from metaflow import FlowSpec, step, batch, environment
 import os
 
 IMAGE = 'public.ecr.aws/docker/library/' + \
         'python:3.9.12-buster'
 
 class EnvVarFlow(FlowSpec):
     
     @step
     def start(self):
         self.next(self.read_locally, 
                   self.read_in_container)
 
     #highlight-start
     @environment(vars={
         ""STEP_VAR"": f""{os.getenv('MSG')} from local""})
     #highlight-end
     @step
     def read_locally(self):
         print(f""secret message: {os.getenv('STEP_VAR')}"")
         self.next(self.join)
         
     #highlight-start
     @environment(vars = 
         {""STEP_VAR"": f""{os.getenv('MSG')} from container""})
     #highlight-end
     @batch(image=IMAGE, 
            cpu=1)
     @step 
     def read_in_container(self):
         print(f""secret message: {os.getenv('STEP_VAR')}"")
         self.next(self.join)
         
     @step
     def join(self, inputs):
         self.next(self.end)
         
     @step
     def end(self):
         pass
         
 if __name__ == ""__main__"":
     EnvVarFlow()
 ```
 
 
 
 
 
 ```bash
 python set_environment_variables_decorator.py run
 ```
 
 <CodeOutputBlock lang=""bash"">
 
 ```
     ...
      [414/read_locally/2469 (pid 88407)] Task is starting.
      [414/read_locally/2469 (pid 88407)] secret message: hi from local environment
      [414/read_locally/2469 (pid 88407)] Task finished successfully.
     ...
      [414/read_in_container/2470 (pid 88410)] Task is starting.
      [414/read_in_container/2470 (pid 88410)] [241ee31e-ca72-4f3e-9553-3d0791b16f0b] Task is starting (status SUBMITTED)...
      [414/read_in_container/2470 (pid 88410)] [241ee31e-ca72-4f3e-9553-3d0791b16f0b] Task is starting (status RUNNABLE)...
      [414/read_in_container/2470 (pid 88410)] [241ee31e-ca72-4f3e-9553-3d0791b16f0b] Task is starting (status STARTING)...
      [414/read_in_container/2470 (pid 88410)] [241ee31e-ca72-4f3e-9553-3d0791b16f0b] Task is starting (status RUNNING)...
      [414/read_in_container/2470 (pid 88410)] [241ee31e-ca72-4f3e-9553-3d0791b16f0b] Setting up task environment.
      [414/read_in_container/2470 (pid 88410)] [241ee31e-ca72-4f3e-9553-3d0791b16f0b] Downloading code package...
      [414/read_in_container/2470 (pid 88410)] [241ee31e-ca72-4f3e-9553-3d0791b16f0b] Code package downloaded.
      [414/read_in_container/2470 (pid 88410)] [241ee31e-ca72-4f3e-9553-3d0791b16f0b] Task is starting.
      [414/read_in_container/2470 (pid 88410)] [241ee31e-ca72-4f3e-9553-3d0791b16f0b] secret message: hi from a container
      [414/read_in_container/2470 (pid 88410)] [241ee31e-ca72-4f3e-9553-3d0791b16f0b] Task finished with exit code 0.
      [414/read_in_container/2470 (pid 88410)] Task finished successfully.
     ...
 ```
 
 </CodeOutputBlock>",H3,https://outerbounds.com/docs/set-env-vars-with-decorator#run-flow,False,3253.0,603.0
172,Further Reading,"* [Use `.env` file with Metaflow](/docs/set-env-vars-with-file)
 * [metaflow_config.py](https://github.com/Netflix/metaflow/blob/493d406347c11787d73f0cc76fb798d8526b4955/metaflow/metaflow_config.py)",H2,https://outerbounds.com/docs/set-env-vars-with-decorator#further-reading,False,198.0,8.0
173,Question,How can I use environment variables in a step of my Metaflow flow with `.env` file?,H2,https://outerbounds.com/docs/set-env-vars-with-file#question,True,83.0,16.0
174,Solution,"One way you can read environment variables in Python is using the PyPi module `python_dotenv` to load a `.env` file. You can customize your environment variables as desired by modifying your `.env` file. If you don't need to use a `.env` file to manage environment variables, you may consider using [Metaflow's environment decorator](/docs/set-env-vars-with-decorator).",H2,https://outerbounds.com/docs/set-env-vars-with-file#solution,False,369.0,53.0
175,Create .env File,"This `.env` file will be used in this example:
 
 
 ```env title="".env""
 SUPER_DUPER_SECRET=hi
 ```",H3,https://outerbounds.com/docs/set-env-vars-with-file#create-env-file,False,99.0,15.0
176,Run Flow,"The flow shows how to: 
 * Use `python_dotenv` to load contents of a `.env` file in a Python script. 
 * Use a secret from the `.env` file in the `read_locally` step.
 * Use a secret from the `.env` file in the `read_in_container` step that runs with Metaflow's `@batch` decorator.
     * Note you can also use remote compute using the `@kubernetes` decorator.
     * The Dockerfile used to build the image used in the batch step is shown below the flow.
 
 
 
 
 
 
 ```py title=""set_environment_variables_file.py""
 from metaflow import FlowSpec, step, batch",H3,https://outerbounds.com/docs/set-env-vars-with-file#run-flow,False,559.0,103.0
177,Further Reading,"* [Using `@environment` decorator](/docs/set-env-vars-with-decorator)
 * [metaflow_config.py](https://github.com/Netflix/metaflow/blob/493d406347c11787d73f0cc76fb798d8526b4955/metaflow/metaflow_config.py)
 * [Building a Dockerfile for a Python environment](https://docs.docker.com/language/python/build-images/)",H2,https://outerbounds.com/docs/set-env-vars-with-file#further-reading,False,311.0,14.0
178,Question,How do I design steps to handle potential task failures at runtime?,H2,https://outerbounds.com/docs/handle-tasks-that-fail/#question,True,67.0,12.0
179,Solution,Metaflow has two decorators that address this.,H2,https://outerbounds.com/docs/handle-tasks-that-fail/#solution,False,46.0,7.0
180,Using @retry and @catch,"You can use Metaflow's `@retry` decorator before step definitions. The `@retry` decorator takes an argument called `times` which takes a number in [0,4]. This is intended to handle transient failures and is particularly useful when running tasks on the cloud where machine failures are more common.
 
 You can also use this in the command line like `python flow.py run --with retry`. By default this will retry failed steps with no `@retry` decorator defined three times. 
 
 Similarly, the `@catch` decorator will catch exceptions raised in the task. However `@catch` is intended for use cases where you want to continue the flow after any exception. Catch contains an optional argument `var` which you can save as a flow artifact if you want to later access the exception. 
 
 :::caution 
 when using `@catch` you should design the steps in your flow after the `@catch` to tolerate exceptions in that step.
 :::",H3,https://outerbounds.com/docs/handle-tasks-that-fail/#using-retry-and-catch,False,913.0,153.0
181,Run Flow,"This flow shows how to:
 * Create a `foreach` branch in `start` that creates three `divide` tasks.
 * Using `@retry` to rerun `divide` when the step code produces an exception.
 * Saving the exception using `@catch`.
     * In the `join` task, use the saved exception to only store results if the `divide` parent task succeeded.
 
 
 ```py title=""handle_failed_task.py""
 from metaflow import FlowSpec, step, retry, catch
 
 class CatchRetryFlow(FlowSpec):
 
     @step
     def start(self):
         self.divisors = [0, 1, 2]
         self.next(self.divide, foreach='divisors')
 
     #highlight-start
     @catch(var='divide_fail')
     @retry(times=1)
     #highlight-end
     @step
     def divide(self):
         #highlight-next-line
         self.res = 10 / self.input
         self.next(self.join)
 
     @step
     def join(self, inputs):
         #highlight-next-line
         self.results = [i.res 
                         for i in inputs 
                         if not i.divide_fail]
         print('results', self.results)
         self.next(self.end)
 
     @step
     def end(self):
         print('done!')
 
 if __name__ == '__main__':
     CatchRetryFlow()
 ```
 
 
 ```bash
 python handle_failed_task.py run
 ```
 
 <CodeOutputBlock lang=""bash"">
 
 ```
      Workflow starting (run-id 1654221294647384):
      [1654221294647384/start/1 (pid 71451)] Task is starting.
      [1654221294647384/start/1 (pid 71451)] Task finished successfully.
      [1654221294647384/divide/2 (pid 71461)] Task is starting.
      [1654221294647384/divide/3 (pid 71462)] Task is starting.
      [1654221294647384/divide/4 (pid 71463)] Task is starting.
      [1654221294647384/divide/2 (pid 71461)] Traceback (most recent call last):
      [1654221294647384/divide/2 (pid 71461)] ZeroDivisionError: division by zero
      [1654221294647384/divide/2 (pid 71461)] 
      [1654221294647384/divide/2 (pid 71480)] Task is starting (retry).
      [1654221294647384/divide/3 (pid 71462)] Task finished successfully.
      [1654221294647384/divide/4 (pid 71463)] Task finished successfully.
      [1654221294647384/divide/2 (pid 71480)] >  Traceback (most recent call last):
      [1654221294647384/divide/2 (pid 71480)] >  ZeroDivisionError: division by zero
      [1654221294647384/divide/2 (pid 71480)] Task finished successfully.
      [1654221294647384/join/5 (pid 71492)] Task is starting.
      [1654221294647384/join/5 (pid 71492)] results [10.0, 5.0]
      [1654221294647384/join/5 (pid 71492)] Task finished successfully.
      [1654221294647384/end/6 (pid 71504)] Task is starting.
      [1654221294647384/end/6 (pid 71504)] done!
      [1654221294647384/end/6 (pid 71504)] Task finished successfully.
      Done!
 ```
 
 </CodeOutputBlock>",H3,https://outerbounds.com/docs/handle-tasks-that-fail/#run-flow,False,2741.0,569.0
182,Further Reading,"* [Debugging flows with `resume`](/docs/debug-errors-with-resume)
 * [Dealing with failures](https://docs.metaflow.org/metaflow/failures) in Metaflow
 * More examples in the [Effective Data Science Infrastructure](https://www.manning.com/books/effective-data-science-infrastructure) book",H2,https://outerbounds.com/docs/handle-tasks-that-fail/#further-reading,False,287.0,21.0
183,DAGs and Data Science Workflows,"The complexity of machine learning and data science workflows can become complicated very quickly. To avoid a jungle of data pipelines and models, we need organizational principles for our workflows, and directed acyclic graphs aka DAGs have become a standard for this:
 
 ![](/assets/dags-in-data-science-01.png)
 <Caption> Common data science DAG </Caption>
 
 DAGs don’t only exist to tame complexity in the abstract, but are practically useful for several reasons:
 
 They provide a common vocabulary of steps (the nodes) and transitions between them (the edges), which make it easier to write and understand nontrivial applications that are structured as DAGs.
 They both encourage and force us to be explicit about the order in which our workflows are executed. This is particularly useful when the order is anything more complicated than a linear flow, such as you would see in a notebook. Being explicit about the order of operations helps us to manage what could be a jungle of models and data pipelines.
 Using DAGs, we can signal when the order of operations doesn’t matter, such as when training independent models. We can parallelize these operations automatically, which is essential for performant code.
 
 In summary, it is helpful to view DAGs as a language, not a programming language per se, but rather a formal construct for human-to-human communication. With DAGs, we can speak about complex sequences of operations in a concise and understandable manner.",H2,https://outerbounds.com/docs/dags-in-data-science#dags-and-data-science-workflows,False,1476.0,233.0
184,What exactly is DAG?,"It is worth defining explicitly what a DAG is: Directed Acyclic Graph may sound like a mouthful but you can think of DAGs as follows:
 
 * A graph is a set of nodes connected by edges
 * A graph is directed if each edge is an arrow (that is, pointing from one node to another)
 * Acyclic means there are no cycles
 
 The nodes of the DAG of a data science or machine learning workflow are the steps in your workflow, such as (but not limited to) getting your data, training a model, or choosing the best model, as in the DAG above. In the case of Metaflow, a step can contain any Python code - like a cell in a notebook. The edges tell you which step(s) follow from other steps and this is why they need to be directed. The reason we don’t want cycles may now be clear: we don’t want our pipelines to end up in infinite loops!
 
 In an industrial setting, DAGs can be and often are far more complicated than the toy case above:
 
 ![](/assets/dags-in-data-science-02.png)
 <Caption> DAGs expand quickly in data science workflows </Caption>
 
 To be explicit here, DAGs are important in order to tame the complexity of machine learning workflows. There is an important corollary: not all data science workflows are DAGs, rather, DAGs are a particular type of workflow.",H2,https://outerbounds.com/docs/dags-in-data-science#what-exactly-is-dag,False,1267.0,232.0
185,Orchestrating Workflows,"Once we have a DAG, in order to perform our computation, we require a workflow orchestrator. The workflow orchestrator needs to perform a seemingly simple task: given a workflow or DAG definition, execute the steps defined by the graph in order. The orchestrator walks through the graph, sending each step to a separate compute layer that is responsible for executing the code contained in a step. For data-intense, distributed workflows a compute layer like Spark or Kubernetes can be used, whereas local processes suffice for prototyping.
 In contrast to prototyping, production workflows need to run automatically and reliably without human intervention. Workflow orchestrators play key roles in ensuring that production workflows, such as those training models and producing predictions regularly, execute in a scalable and highly available manner. Hence it makes sense to use a battle-hardened, proven system in production such as:
 * [Argo](https://argoproj.github.io/), a modern orchestrator that runs natively on Kubernetes
 * [Airflow](https://airflow.apache.org/), a popular open-source workflow orchestrator 
 * or a low-maintenance managed solution such as [Google Cloud Composer](https://cloud.google.com/composer) or [AWS Step Functions](https://aws.amazon.com/step-functions/?step-functions.sort-by=item.additionalFields.postDateTime&step-functions.sort-order=desc)",H2,https://outerbounds.com/docs/dags-in-data-science#orchestrating-workflows,False,1380.0,175.0
186,Developing Workflows,"Earlier, data science applications were often developed as independent modules which were glued together as a DAG afterwards, sometimes even by a separate team. As a result, it was hard to ensure that the whole application worked correctly end to end. Debugging workflow failures was often a painful experience as issues couldn’t be easily reproduced and fixed locally.
 A better approach is to consider the whole workflow as a key deliverable of a data science project, which is developed, tested, and deployed as a functional unit. When using a local orchestrator like the one provided by Metaflow, you can rapidly prototype end-to-end workflows almost as easily as how you develop code in a notebook. After the workflow works locally, it can be deployed to a production orchestrator without any changes in the code. After all, a data science workflow is just a DAG of steps that can be orchestrated by many different systems.",H2,https://outerbounds.com/docs/dags-in-data-science#developing-workflows,False,928.0,153.0
187,How do I?,"[Pass artifacts between steps in Metaflow flows](/docs/pass-artifacts-between-steps)
 
 
 [Schedule Metaflow flows on AWS Step Functions](/docs/schedule-flow-on-step-functions)",H2,https://outerbounds.com/docs/dags-in-data-science#how-do-i,False,176.0,16.0
188,Question,"I have a prototype flow that failed and I want to identify why it failed, where it failed, and debug it.",H2,https://outerbounds.com/docs/debug-errors-with-resume#question,True,104.0,21.0
189,Solution,,H2,https://outerbounds.com/docs/debug-errors-with-resume#solution,False,0.0,1.0
190,Run Flow with Error,"When running `debug_error_with_resume.py` a `ZeroDivisionError` is produced in the `join` step.
 
 This flow shows how to: 
 * Pass artifacts into a join step.
 * Start a process to deal with Python errors in a task.
 
 
 
 
 
 
 ```py title=""debug_error_with_resume.py""
 from metaflow import FlowSpec, step
 
 class DebugFlow(FlowSpec):
 
     @step
     def start(self):
         self.next(self.a, self.b)
 
     @step
     def a(self):
         self.x = 1
         self.next(self.join)
 
     @step
     def b(self):
         self.x = 0
         self.next(self.join)
 
     @step
     def join(self, inputs):
         # divisor is next line is 0!
         #highlight-next-line
         self.result =  inputs.a.x / inputs.b.x 
         self.next(self.end)
 
     @step
     def end(self):
         pass
 
 if __name__ == '__main__':
     DebugFlow()
 ```
 
 
 
 
 
 ```bash
 python debug_error_with_resume.py run
 ```
 
 <CodeOutputBlock lang=""bash"">
 
 ```
     ...
      [1654221288262697/a/2 (pid 71324)] Task is starting.
      [1654221288262697/a/2 (pid 71324)] Task finished successfully.
     ...
      [1654221288262697/b/3 (pid 71325)] Task is starting.
      [1654221288262697/b/3 (pid 71325)] Task finished successfully.
     ...
 ```
 
 </CodeOutputBlock>",H3,https://outerbounds.com/docs/debug-errors-with-resume#run-flow-with-error,False,1269.0,309.0
191,Debug Flow,"Having seen that the code failed at the `join` step, you can fix whatever may have caused this and `resume` the flow from the faulty `step`. There is a highlighted line in the in `join` step of this script containing the `ZeroDivisionError`. You can replace this line with 
 
 &ensp;&ensp;```self.result = inputs.a.x / (inputs.b.x + 1e-12)```
 
 to fix the error.",H3,https://outerbounds.com/docs/debug-errors-with-resume#debug-flow,False,363.0,62.0
192,Resume Flow from Failed Task,"Now you can `resume` from `join` without re-running the `start`,  `a`, and `b` steps. Note that by default the `resume` feature will enter the flow at the step that produced the error in the last run. In this example none of the steps are time intensive, but you can imagine scenarios such as model training where steps may take a long time to compute and you wouldn't want to re-run `a` and `b` if those tasks did expensive model training and the error was in the downstream `join` task.  
 
 
 ```bash
 python debug_error_with_resume.py resume
 ```
 
 <CodeOutputBlock lang=""bash"">
 
 ```
     Metaflow 2.6.0 executing DebugFlow for user:eddie
     Validating your flow...
         The graph looks good!
     Running pylint...
         Pylint is happy!
     2022-06-02 20:54:52.800 Gathering required information to resume run (this may take a bit of time)...
     2022-06-02 20:54:52.808 Workflow starting (run-id 1654221292799841):
     2022-06-02 20:54:52.809 [1654221292799841/start/1] Cloning results of a previously run task 1654221288262697/start/1
     2022-06-02 20:54:53.413 [1654221292799841/a/2] Cloning results of a previously run task 1654221288262697/a/2
     2022-06-02 20:54:53.419 [1654221292799841/b/3] Cloning results of a previously run task 1654221288262697/b/3
     2022-06-02 20:54:54.069 [1654221292799841/join/4 (pid 71446)] Task is starting.
     2022-06-02 20:54:54.709 [1654221292799841/join/4 (pid 71446)] Task finished successfully.
     2022-06-02 20:54:54.713 [1654221292799841/end/5 (pid 71454)] Task is starting.
     2022-06-02 20:54:55.102 [1654221292799841/end/5 (pid 71454)] Task finished successfully.
     2022-06-02 20:54:55.103 Done!
 ```
 
 </CodeOutputBlock>",H3,https://outerbounds.com/docs/debug-errors-with-resume#resume-flow-from-failed-task,False,1705.0,282.0
193,Further Reading,"* [Reproduce production issues locally](https://docs.metaflow.org/metaflow/debugging#reproducing-production-issues-locally)
 * [Handle potential task failures](/docs/handle-tasks-that-fail)",H2,https://outerbounds.com/docs/debug-errors-with-resume#further-reading,False,189.0,10.0
194,Question,How can I use multiple Metaflow configuration files?,H2,https://outerbounds.com/docs/use-multiple-metaflow-configs#question,True,52.0,8.0
195,Solution,"Metaflow has many configuration options. You can manage them with environment variables or through a configuration file that by default lives at `~/.metaflowconfig/config_default.json`. When you run a flow, Metaflow will first try to pull these values from environment variables, and then from the Metaflow config JSON file.
 
 This page describes the more portable config files approach through the simplest possible example. If you want to go through the variables common to both settings you can see their definitions in the Metaflow [config](https://github.com/Netflix/metaflow/blob/master/metaflow/metaflow_config.py) file.",H2,https://outerbounds.com/docs/use-multiple-metaflow-configs#solution,False,628.0,84.0
196,The Default Mode,"After you install Metaflow you can run 
 ```metaflow configure```
 to see a list of configuration options. 
 
 Your default configuration file will reside in `~/.metaflowconfig/config_default.json`. 
 The `METAFLOW_PROFILE` variable determines which configuration file is looked at. For example, if you set 
 ```
 export METAFLOW_PROFILE=local
 ```
 and try to run a flow, it will produce an error saying `Unable to locate METAFLOW_PROFILE 'local' in '~/.metaflowconfig')`.",H3,https://outerbounds.com/docs/use-multiple-metaflow-configs#the-default-mode,False,473.0,67.0
197,Question,How to list all steps of a flow using the Client API?,H2,https://outerbounds.com/docs/list-flow-steps#question,True,53.0,12.0
198,Solution,"You can use the Client API to access by:
 * Flows and Runs (instances of Flows)
 * Steps and Tasks (instances of Steps) 
 
 You can also find [more about the Client API](https://docs.metaflow.org/api/client) in Metaflow's documentation.",H2,https://outerbounds.com/docs/list-flow-steps#solution,False,236.0,37.0
199,Run Flow,"This flow shows five steps. 
 There is one data artifact that has its state changed in each of the first four steps. 
 After running the flow you will see how to use the Client API to access the results of the tasks. 
 
 
 
 
 
 
 ```py title=""list_steps_flow.py""
 from metaflow import FlowSpec, step
 
 class ListStepsFlow(FlowSpec):
     
     @step
     def start(self):
         self.art = 1
         self.next(self.a)
     
     @step
     def a(self):
         self.art = 2
         self.next(self.b)
         
     @step
     def b(self):
         self.art = 3
         self.next(self.c)
         
     @step
     def c(self):
         self.art = 5
         self.next(self.end)
         
     @step
     def end(self):
         pass
     
 if __name__ == ""__main__"":
     ListStepsFlow()
 ```
 
 
 
 
 
 ```bash
 python list_steps_flow.py run
 ```
 
 <CodeOutputBlock lang=""bash"">
 
 ```
      Workflow starting (run-id 1659644592744103):
      [1659644592744103/start/1 (pid 20429)] Task is starting.
      [1659644592744103/start/1 (pid 20429)] Task finished successfully.
      [1659644592744103/a/2 (pid 20432)] Task is starting.
      [1659644592744103/a/2 (pid 20432)] Task finished successfully.
      [1659644592744103/b/3 (pid 20435)] Task is starting.
      [1659644592744103/b/3 (pid 20435)] Task finished successfully.
      [1659644592744103/c/4 (pid 20438)] Task is starting.
      [1659644592744103/c/4 (pid 20438)] Task finished successfully.
      [1659644592744103/end/5 (pid 20441)] Task is starting.
      [1659644592744103/end/5 (pid 20441)] Task finished successfully.
      Done!
 ```
 
 </CodeOutputBlock>",H3,https://outerbounds.com/docs/list-flow-steps#run-flow,False,1636.0,400.0
200,Access Step Data,"This code snippet shows how to use the Client API to:
 * Gather all steps from the latest run of `ListStepsFlow`.
     * Note that `Flow(flow_name).latest_run` will return a [generator](https://docs.python.org/3/glossary.html#term-generator) that can be converted to a list. 
 * Print the step name and artifact state at that step. 
 
 
 ```python
 from metaflow import Flow
 msg = ""Step {:5s} has data artifact `art` with value = {}""
 steps = list(Flow('ListStepsFlow').latest_run)
 for step in steps[::-1]:
     step_name = step.pathspec.split('/')[-1]
     artifact_value = step.task.data.art
     print(msg.format(step_name, artifact_value))
 ```
 
 <CodeOutputBlock lang=""python"">
 
 ```
     Step start has data artifact `art` with value = 1
     Step a     has data artifact `art` with value = 2
     Step b     has data artifact `art` with value = 3
     Step c     has data artifact `art` with value = 5
     Step end   has data artifact `art` with value = 5
 ```
 
 </CodeOutputBlock>",H3,https://outerbounds.com/docs/list-flow-steps#access-step-data,False,994.0,192.0
201,Further Reading,"* [Organizing Results](https://docs.metaflow.org/metaflow/tagging)
 * [Scheduling Metaflow Flows](https://docs.metaflow.org/going-to-production-with-metaflow/scheduling-metaflow-flows)",H2,https://outerbounds.com/docs/list-flow-steps#further-reading,False,184.0,7.0
202,Question,How can I track artifacts of my flows with Weights and Biases?,H2,https://outerbounds.com/docs/track-wandb#question,True,62.0,12.0
203,Solution,You can track flow artifacts using any Weights and Biases calls you already use. This can be especially useful if you want to track artifacts during the lifecycle of long-running tasks.,H2,https://outerbounds.com/docs/track-wandb#solution,False,185.0,31.0
204,Login to Weights and Biases,"To run this code first go [sign up](https://wandb.ai) for a Weights and Biases account and make sure you have logged in with your API key. It is recommended that you store the key as an environment variable. In the example shown later, the Weights and Biases ""entity"" and ""project"" are also stored as environment variables:
 ```
 export WANDB_API_KEY=<YOUR KEY>
 export WANDB_ENTITY=<YOUR USERNAME>
 export WANDB_PROJECT=<YOUR PROJECT>
 ```
 
 Then you can install and log in to the Weights and Biases Python client:
 ```
 pip install wandb
 ```
 
 If you don't set the `WANDB_API_KEY` environment variable, you will need to paste your key after running:
 ```
 wandb login
 ```",H3,https://outerbounds.com/docs/track-wandb#login-to-weights-and-biases,False,677.0,108.0
205,Define Logging Function,"Here is a function that takes in arguments from a dataset and classification model and logs with Weights and Biases. It uses Weights and Biases Scikit-learn integration, but you can replace it with arbitrary logging functions relevant to your workflow.
 
 <CustomCode height=""20vh"">
 
 
 
 ```py title=""wandb_helpers.py""
 import os
 import wandb
 def plot_results(X_train, y_train, X_test, y_test, 
                  y_pred, y_probs, clf, labels):
     wandb.init(entity=os.getenv(""WANDB_ENTITY""), 
                project=os.getenv(""WANDB_PROJECT""), 
                mode=""offline"")
     wandb.sklearn.plot_class_proportions(y_train,
                                          y_test, 
                                          labels)
     wandb.sklearn.plot_learning_curve(clf, 
                                       X_train, 
                                       y_train)
     wandb.sklearn.plot_roc(y_test, y_probs, labels)
     wandb.sklearn.plot_precision_recall(y_test, 
                                         y_probs, 
                                         labels)
     wandb.sklearn.plot_feature_importances(clf)
     wandb.sklearn.plot_classifier(
         clf, X_train, X_test, y_train, y_test, y_pred, 
         y_probs, labels, is_binary=True, 
         model_name='RandomForest'
     )
     wandb.finish()
 ```
 
 </CustomCode>",H3,https://outerbounds.com/docs/track-wandb#define-logging-function,False,1349.0,448.0
206,Run Flow,"The flow shows how to:
 * Load data in the `start` step. 
 * Build a model and call a custom logging function in the `model` step. 
     * Call the custom logging function `plot_results` to 
     * This step uses Metaflow's `@environment` decorator to pass environment variables relevant to Weights and Biases into the step. This is useful when you want to track a step run on a remote machine using a Metaflow decorator like `@batch` or `@kubernetes`.
 
 
 
 
 
 
 ```py title=""track_with_wandb_custom.py""
 from metaflow import FlowSpec, step, environment, batch, conda_base
 import os
 import wandb
 from wandb_helpers import plot_results
     
 @conda_base(libraries={""wandb"": ""0.12.15"", ""scikit-learn"": ""1.0.2"", 
                        ""pandas"": ""1.4.2""})
 class TrackPlotsFlow(FlowSpec):
     
     @step
     def start(self):
         from sklearn import datasets
         from sklearn.model_selection import train_test_split
         self.iris = datasets.load_iris()
         self.X = self.iris['data']
         self.y = self.iris['target']
         self.labels = self.iris['target_names']
         split = train_test_split(self.X, self.y, 
                                  test_size=0.2)
         self.X_train = split[0] 
         self.X_test = split[1]
         self.y_train = split[2]
         self.y_test = split[3]
         self.next(self.model)
 
     # Copy env vars to tasks on a different machine.
     @environment(vars={
         ""WANDB_API_KEY"": os.getenv(""WANDB_API_KEY""), 
         ""WANDB_NAME"": ""Plot RandomForestClassifier"",
         ""WANDB_ENTITY"": os.getenv(""WANDB_ENTITY""),
         ""WANDB_PROJECT"": os.getenv(""WANDB_PROJECT"")
     })
     @batch(cpu=2)
     @step
     def model(self):
         from sklearn.ensemble import RandomForestClassifier
         self.clf = RandomForestClassifier(
             n_estimators=10, max_depth=None,
             min_samples_split=2, random_state=0
         )
         from sklearn.model_selection import cross_val_score
         self.clf.fit(self.X_train, self.y_train)
         self.y_pred = self.clf.predict(self.X_test)
         self.y_probs = self.clf.predict_proba(
             self.X_test
         )
         #highlight-start
         plot_results(self.X_train, self.y_train, 
                      self.X_test, self.y_test, 
                      self.y_pred, self.y_probs, 
                      self.clf, self.labels)
         #highlight-end
         self.next(self.end)
     
     @step
     def end(self):
         print(""Flow is all done."")
 
 if __name__ == ""__main__"":
     TrackPlotsFlow()
 ```
 
 
 
 
 
 ```bash
 python track_with_wandb_custom.py --environment=conda run
 ```
 
 <CodeOutputBlock lang=""bash"">
 
 ```
      Workflow starting (run-id 110):
      [110/start/494 (pid 11536)] Task is starting.
      [110/start/494 (pid 11536)] Task finished successfully.
      [110/model/495 (pid 11746)] Task is starting.
      [110/model/495 (pid 11746)] [6d6302c8-20ee-4212-9d09-e67fc49beead] Task is starting (status SUBMITTED)...
      [110/model/495 (pid 11746)] [6d6302c8-20ee-4212-9d09-e67fc49beead] Task is starting (status RUNNABLE)...
      [110/model/495 (pid 11746)] [6d6302c8-20ee-4212-9d09-e67fc49beead] Task is starting (status RUNNING)...
      [110/model/495 (pid 11746)] [6d6302c8-20ee-4212-9d09-e67fc49beead] Setting up task environment.
      [110/model/495 (pid 11746)] [6d6302c8-20ee-4212-9d09-e67fc49beead] Downloading code package...
      [110/model/495 (pid 11746)] [6d6302c8-20ee-4212-9d09-e67fc49beead] Code package downloaded.
      [110/model/495 (pid 11746)] [6d6302c8-20ee-4212-9d09-e67fc49beead] Task is starting.
      [110/model/495 (pid 11746)] [6d6302c8-20ee-4212-9d09-e67fc49beead] Bootstrapping environment...
      [110/model/495 (pid 11746)] [6d6302c8-20ee-4212-9d09-e67fc49beead] Environment bootstrapped.
      [110/model/495 (pid 11746)] [6d6302c8-20ee-4212-9d09-e67fc49beead] Task finished with exit code 0.
      [110/model/495 (pid 11746)] Task finished successfully.
      [110/end/496 (pid 12169)] Task is starting.
      [110/end/496 (pid 12169)] Flow is all done.
      [110/end/496 (pid 12169)] Task finished successfully.
      Done!
 ```
 
 </CodeOutputBlock>",H3,https://outerbounds.com/docs/track-wandb#run-flow,False,4196.0,930.0
207,Further Reading,"* [Inspecting flows and results](https://docs.metaflow.org/metaflow/client)
 * [Visualizing Metaflow results](https://docs.metaflow.org/metaflow/visualizing-results)
 * [Track with CometML](/docs/track-comet)",H2,https://outerbounds.com/docs/track-wandb#further-reading,False,208.0,13.0
208,Question,How can I add and remove tags to flow runs programmatically?,H2,https://outerbounds.com/docs/add-remove-tags#question,True,60.0,11.0
209,Solution,You can do this within a flow or using the Client API.,H2,https://outerbounds.com/docs/add-remove-tags#solution,False,54.0,12.0
210,Run Flow,"This flow shows how to: 
 * Load a dataset.
 * Train a Scikit-learn model.
 * Evaluate the model on a test set.
     * Tag the model as a `production_candidate` if the model score is greater than `accuracy_threshold`.
 
 
 
 
 
 
 
 ```py title=""add_remove_tags_programmatically.py""
 from metaflow import FlowSpec, step, Flow, current, Parameter
 
 class ModelTaggingFlow(FlowSpec):
 
     max_depth = Parameter('max-depth', default=2)
     tag_msg = 'Tagging run {} as a promising model'
     accuracy_threshold = 0.85
 
     @step
     def start(self):
         from sklearn import datasets
         from sklearn.model_selection import train_test_split
         data = datasets.load_wine()
         data = train_test_split(data['data'],
                                 data['target'],
                                 random_state = 42)
         self.X_train = data[0]
         self.X_test = data[1]
         self.y_train = data[2]
         self.y_test = data[3]
         self.next(self.train)
 
     @step
     def train(self):
         from sklearn.tree import DecisionTreeClassifier
         self.params = {
             'max_leaf_nodes': None,
             'max_depth': self.max_depth,
             'max_features' : 'sqrt',
             'random_state': 0
         }
         self.model = DecisionTreeClassifier(**self.params)
         self.model.fit(self.X_train, self.y_train)
         self.next(self.eval_and_tag)
 
     @step
     def eval_and_tag(self):
         from sklearn.metrics import (accuracy_score,
                                      classification_report)
         self.pred = self.model.predict(self.X_test)
         self.accuracy = float(
             accuracy_score(self.y_test, self.pred))
         print(self.accuracy)
         if self.accuracy > self.accuracy_threshold:
             print(self.tag_msg.format(current.run_id))
             run = Flow(current.flow_name)[current.run_id]
             run.add_tag('promising model')
         self.next(self.end)
 
     @step
     def end(self):
         pass
 
 if __name__ == '__main__':
     ModelTaggingFlow()
 ```
 
 
 
 
 
 ```bash
 python add_remove_tags_programmatically.py run
 ```
 
 <CodeOutputBlock lang=""bash"">
 
 ```
      Workflow starting (run-id 1659643565956220):
      [1659643565956220/start/1 (pid 20307)] Task is starting.
      [1659643565956220/start/1 (pid 20307)] Task finished successfully.
      [1659643565956220/train/2 (pid 20310)] Task is starting.
      [1659643565956220/train/2 (pid 20310)] Task finished successfully.
      [1659643565956220/eval_and_tag/3 (pid 20313)] Task is starting.
      [1659643565956220/eval_and_tag/3 (pid 20313)] 0.8666666666666667
      [1659643565956220/eval_and_tag/3 (pid 20313)] Tagging run 1659643565956220 as a promising model
      [1659643565956220/eval_and_tag/3 (pid 20313)] Task finished successfully.
      [1659643565956220/end/4 (pid 20316)] Task is starting.
      [1659643565956220/end/4 (pid 20316)] Task finished successfully.
      Done!
 ```
 
 </CodeOutputBlock>
 
 
 ```bash
 python add_remove_tags_programmatically.py run --max-depth 6
 ```
 
 <CodeOutputBlock lang=""bash"">
 
 ```
      Workflow starting (run-id 1659643570153989):
      [1659643570153989/start/1 (pid 20322)] Task is starting.
      [1659643570153989/start/1 (pid 20322)] Task finished successfully.
      [1659643570153989/train/2 (pid 20325)] Task is starting.
      [1659643570153989/train/2 (pid 20325)] Task finished successfully.
      [1659643570153989/eval_and_tag/3 (pid 20328)] Task is starting.
      [1659643570153989/eval_and_tag/3 (pid 20328)] 0.8888888888888888
      [1659643570153989/eval_and_tag/3 (pid 20328)] Tagging run 1659643570153989 as a promising model
      [1659643570153989/eval_and_tag/3 (pid 20328)] Task finished successfully.
      [1659643570153989/end/4 (pid 20331)] Task is starting.
      [1659643570153989/end/4 (pid 20331)] Task finished successfully.
      Done!
 ```
 
 </CodeOutputBlock>",H3,https://outerbounds.com/docs/add-remove-tags#run-flow,False,3960.0,892.0
211,Observe Model Scores,"You can use the client API to get the latest flow runs. Here is a way to list the `accuracy` value of each `Run`.
 
 
 ```python
 from metaflow import Flow
 flow = Flow('ModelTaggingFlow')
 tag = 'promising model'
 runs = list(flow.runs(tag))
 print(""All models tagged with `{}`:"".format(tag))
 for run in runs: 
     acc = round(100 * run.data.accuracy, 2)
     print(""\tRun {}: {}% Accuracy"".format(run.id, acc))
 ```
 
 <CodeOutputBlock lang=""python"">
 
 ```
     All models tagged with `promising model`:
     	Run 1659643570153989: 88.89% Accuracy
     	Run 1659643565956220: 86.67% Accuracy
     	Run 1659643254562464: 86.67% Accuracy
     	Run 1659643511759883: 88.89% Accuracy
     	Run 1659643507482620: 86.67% Accuracy
     	Run 1659643258636203: 88.89% Accuracy
 ```
 
 </CodeOutputBlock>",H3,https://outerbounds.com/docs/add-remove-tags#observe-model-scores,False,799.0,137.0
212,Update Tags Using the Client API,"You can use the `run.add_tag`, `run.remove_tag` or `run.replace_tag` functions to change a `Run` tag.  
 
 
 
 
 
 These lines will add the `production candidate` tag for each `promising model` with an 87% accuracy score.
 
 
 
 
 
 
 ```python
 flow = Flow('ModelTaggingFlow')
 runs = list(flow.runs('promising model'))
 for run in runs: 
     if run.data.accuracy > .87: 
         run.add_tag('production candidate')
 ```
 
 
 
 
 
 Now you can see the model accuracy only for these models. This can be a useful pattern when reviewing models or testing and promoting them to production. 
 
 
 ```python
 flow = Flow('ModelTaggingFlow')
 tag = 'production candidate'
 runs = list(flow.runs(tag))
 print(""All models tagged `{}`:"".format(tag))
 for run in runs: 
     acc = round(100 * run.data.accuracy, 2)
     print(""\tRun {}: {}% Accuracy"".format(run.id, acc))
 ```
 
 <CodeOutputBlock lang=""python"">
 
 ```
     All models tagged `production candidate`:
     	Run 1659643570153989: 88.89% Accuracy
     	Run 1659643511759883: 88.89% Accuracy
     	Run 1659643258636203: 88.89% Accuracy
 ```
 
 </CodeOutputBlock>",H3,https://outerbounds.com/docs/add-remove-tags#update-tags-using-the-client-api,False,1116.0,193.0
213,Further Reading,"* [Organizing Results](https://docs.metaflow.org/metaflow/tagging)
 * [Client API](https://docs.metaflow.org/api/client)",H2,https://outerbounds.com/docs/add-remove-tags#further-reading,False,120.0,6.0
214,Question,How can I track artifacts of my flows with Comet ML?,H2,https://outerbounds.com/docs/track-comet#question,True,52.0,11.0
215,Solution,"You can track flow artifacts using any Comet ML calls you already use because you can use any Python code in Metaflow steps. 
 In addition, [the Comet ML team developed an integration](https://www.comet.com/site/blog/comet-metaflow-the-mlops-tech-stack-that-truly-scales-with-your-engineering-team/) with Metaflow to make tracking artifacts produced in flow runs even more convenient.
 
 The remainder of this page will walk through the following topics:
 * What is Comet ML?
 * How to write a flow using the Comet integration?
 * How to run the flow that tracks experiments with Comet?",H2,https://outerbounds.com/docs/track-comet#solution,False,586.0,84.0
216,What is Comet ML?,"[Comet ML](https://www.comet.com/docs/v2/) is a platform to track, compare, explain, and optimize. 
 There is a `comet_ml` Python library that allows you to read and write data about the configuration and results of your data science experiments.
 After you [sign up](https://www.comet.com/signup), you can use your Comet API key to create an `Experiment` in Python code or with their APIs. 
 
 
 ```python
 import comet_ml
 import os
 
 experiment = comet_ml.Experiment(
     # read env var set like `export COMET_API_KEY=<>`
     api_key=os.getenv('COMET_API_KEY'),
     # read env var set like `export COMET_PROJECT_NAME=<>`
     project_name=os.getenv('COMET_PROJECT_NAME')
 )
 ```
 
 <CodeOutputBlock lang=""python"">
 
 ```
     COMET WARNING: As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.
     COMET INFO: Experiment is live on comet.com https://www.comet.com/eddie-outerbounds/comet-integration/107ad5abd1614ce3aabedefc49859d1c
     
 ```
 
 </CodeOutputBlock>
 
 Experiments are the core data structure Comet helps you organize information with.
 You can read more about Experiments [here](https://www.comet.com/docs/v2/api-and-sdk/python-sdk/experiment-overview/).
 
 The rest of this page shows how to use Comet's Metaflow integration to automate the creation and reporting of data to Comet `Experiment` objects.",H3,https://outerbounds.com/docs/track-comet#what-is-comet-ml,False,1433.0,208.0
217,Write a Flow using the Comet Integration,"The script shows how to:
 * Login to Comet before running the script.
     * The `init()` call in the main section of this script establishes a connection to Comet. This will try to read the value in the `COMET_API_KEY` environment variable if you have it set. You can read more about configuring Comet in a Python environment [here](http://3.234.183.22/docs/python-sdk/advanced/#python-configuration).
 * Create a set of Comet `Experiment` objects to track both the individual tasks and the state of the flow as a whole. 
 * Log parameters and metrics with Comet from the flow runtime.
     * Observe the `train_model` step and notice that `self.comet_experiment` is accessible automatically because of the `@comet_flow` decorator. 
 
 
 
 
 
 
 ```py title=""track_with_comet_integration.py""
 from comet_ml import init
 from comet_ml.integration.metaflow import comet_flow
 from metaflow import FlowSpec, JSONType, Parameter, card, step",H3,https://outerbounds.com/docs/track-comet#write-a-flow-using-the-comet-integration,False,937.0,143.0
218,Run the Flow,"Now that you have configured Comet to track Experiments for this flow, you can run it from the command line in the normal Metaflow way.
 
 
 ```bash
 python track_with_comet_integration.py run
 ```
 
 <CodeOutputBlock lang=""bash"">
 
 ```
      Workflow starting (run-id 1665870683555031):
      [1665870683555031/start/1 (pid 28379)] Task is starting.
      [1665870683555031/start/1 (pid 28379)] 
      [1665870683555031/start/1 (pid 28379)] 
      [1665870683555031/start/1 (pid 28379)] Task finished successfully.
      [1665870683555031/train_model/2 (pid 28385)] Task is starting.
      [1665870683555031/train_model/2 (pid 28385)] 
      [1665870683555031/train_model/2 (pid 28385)] Task finished successfully.
      [1665870683555031/end/3 (pid 28391)] Task is starting.
      [1665870683555031/end/3 (pid 28391)] 
      [1665870683555031/end/3 (pid 28391)] 
      [1665870683555031/end/3 (pid 28391)] Task finished successfully.
      Done!
 ```
 
 </CodeOutputBlock>",H3,https://outerbounds.com/docs/track-comet#run-the-flow,False,975.0,166.0
219,Further Reading,"* [Inspecting Flows and Results](https://docs.metaflow.org/metaflow/client)
 * [Visualizing Metaflow Results](https://docs.metaflow.org/metaflow/visualizing-results)
 * [Track with Weights and Biases](/docs/track-wandb)",H2,https://outerbounds.com/docs/track-comet#further-reading,False,219.0,15.0
220,Question,How can I filter the runs of my flows based on some condition?,H2,https://outerbounds.com/docs/filter-on-condition#question,True,62.0,13.0
221,Solution,You can use the Metaflow [Client API](https://metaflow-docs-git-polish-client-api-metaflow.vercel.app/metaflow/client#navigating-the-object-hierarchy) to access the results of your flows.,H2,https://outerbounds.com/docs/filter-on-condition#solution,False,187.0,14.0
222,Run Flow,"This flow and the subsequent client API calls show how to:
 * Define a flow that has a conditional value saved as a flow artifact.
 * Filter runs of this flow by successful runs.
 * Filter runs of this flow by time.
 * Filter runs of this flow by a flow artifact value. 
 
 
 
 
 
 
 ```py title=""filter_on_condition.py""
 from metaflow import FlowSpec, step
 
 class ConditionalFilterFlow(FlowSpec):
     
     @step
     def start(self):
         import random
         self.fancy_conditional = random.choice([1,2,3])
         print(self.fancy_conditional)
         self.next(self.end)
         
     @step
     def end(self):
         pass
     
 if __name__ == ""__main__"":
     ConditionalFilterFlow()
 ```
 
 
 
 
 
 ```bash
 python filter_on_condition.py run
 ```
 
 <CodeOutputBlock lang=""bash"">
 
 ```
      Workflow starting (run-id 1659645557151571):
      [1659645557151571/start/1 (pid 20584)] Task is starting.
      [1659645557151571/start/1 (pid 20584)] 3
      [1659645557151571/start/1 (pid 20584)] Task finished successfully.
      [1659645557151571/end/2 (pid 20587)] Task is starting.
      [1659645557151571/end/2 (pid 20587)] Task finished successfully.
      Done!
 ```
 
 </CodeOutputBlock>",H3,https://outerbounds.com/docs/filter-on-condition#run-flow,False,1213.0,255.0
223,Filter by Successful Runs,"Here is an example using the Client API to access the results of the run you just did. Note that `Flow(flow_name)` will return a [generator](https://docs.python.org/3/glossary.html#term-generator) that can be iterated over. You may also want to cut off the amount of runs that are fetched.
 
 
 ```python
 from metaflow import Flow
 flow_name = 'ConditionalFilterFlow'
 max_runs = 100 # limit query in case of many flow runs
 successful_runs = []
 for i, run in enumerate(Flow(flow_name)):
     if run.successful:
         successful_runs.append(run)
     if i >= max_runs:
         break
 ```
 
 
 ```python
 successful_runs
 ```
 
 <CodeOutputBlock lang=""python"">
 
 ```
     [Run('ConditionalFilterFlow/1659645557151571'),
      Run('ConditionalFilterFlow/1659645118452480'),
      Run('ConditionalFilterFlow/1658853090048097'),
      Run('ConditionalFilterFlow/1658852893601432'),
      Run('ConditionalFilterFlow/1658840520845011'),
      Run('ConditionalFilterFlow/1658731222406680'),
      Run('ConditionalFilterFlow/1658729896148442'),
      Run('ConditionalFilterFlow/1658729755179231'),
      Run('ConditionalFilterFlow/1658729474247844'),
      Run('ConditionalFilterFlow/1658729270057885'),
      Run('ConditionalFilterFlow/1658728960003690')]
 ```
 
 </CodeOutputBlock>",H3,https://outerbounds.com/docs/filter-on-condition#filter-by-successful-runs,False,1282.0,185.0
224,Filter by Time,"You can use properties of the Metaflow `Run` to filter. This snippet shows how to use the `datetime` library to filter runs from January 1st of this year to today: 
 
 
 ```python
 from metaflow import Flow
 from datetime import datetime
 now = datetime.now()
 start_year, today = datetime(now.year, 1, 1), datetime.now()
 runs_this_year = []
 max_runs = 100
 for i, run in enumerate(Flow(flow_name)): 
     if run.created_at > start_year and run.created_at < today: 
         runs_this_year.append(run)
     if i >= max_runs:
         break
 ```
 
 
 ```python
 runs_this_year
 ```
 
 <CodeOutputBlock lang=""python"">
 
 ```
     [Run('ConditionalFilterFlow/1659645557151571'),
      Run('ConditionalFilterFlow/1659645118452480'),
      Run('ConditionalFilterFlow/1658853090048097'),
      Run('ConditionalFilterFlow/1658852893601432'),
      Run('ConditionalFilterFlow/1658840520845011'),
      Run('ConditionalFilterFlow/1658731222406680'),
      Run('ConditionalFilterFlow/1658729896148442'),
      Run('ConditionalFilterFlow/1658729755179231'),
      Run('ConditionalFilterFlow/1658729474247844'),
      Run('ConditionalFilterFlow/1658729270057885'),
      Run('ConditionalFilterFlow/1658728960003690')]
 ```
 
 </CodeOutputBlock>",H3,https://outerbounds.com/docs/filter-on-condition#filter-by-time,False,1234.0,182.0
225,Filter by Flow Artifact,"This snippet shows how to use the datetime library to filter runs from January 1st of this year to today.
 
 
 ```python
 from metaflow import Flow
 runs_with_fancy_condition = []
 num_runs_without_fancy_condition = 0
 filter_value = 1
 key_errors = 0 
 max_runs = 100
 for i, run in enumerate(Flow(flow_name)):
     try:
         if run['start'].task.data.fancy_conditional == filter_value:
             runs_with_fancy_condition.append(run)
         else:
             num_runs_without_fancy_condition += 1
     except KeyError:
         key_errors += 1
 print(""This flow has {} runs that have different fancy_conditional values != 1"".format(
     num_runs_without_fancy_condition))
 print(""This flow has {} runs that do not have the fancy_conditional parameter"".format(
     key_errors))
 ```
 
 <CodeOutputBlock lang=""python"">
 
 ```
     This flow has 5 runs that have different fancy_conditional values != 1
     This flow has 4 runs that do not have the fancy_conditional parameter
 ```
 
 </CodeOutputBlock>
 
 
 ```python
 for run in runs_with_fancy_condition:
     assert run.data.fancy_conditional == filter_value
 ```",H3,https://outerbounds.com/docs/filter-on-condition#filter-by-flow-artifact,False,1129.0,210.0
226,Further Reading,"* [Organizing Results](https://docs.metaflow.org/metaflow/tagging)
 * [Client API](https://docs.metaflow.org/api/client)",H2,https://outerbounds.com/docs/filter-on-condition#further-reading,False,120.0,6.0
227,Question,How can I reuse parameters across multiple Metaflow flows?,H2,https://outerbounds.com/docs/reuse-parameters-across-flows/#question,True,58.0,9.0
228,Solution,"One way is to return `metaflow.Parameter` objects from a function. Then you can call this function at the beginning of your flows. Another way is to use a [mixin](https://realpython.com/lessons/mixin-classes/) class that includes parameter definitions. This is useful when the reused parameter relates to other functionality you want multiple flows to inherit from. For example, if you have a common dataset that requires the same transformation across several flows, then you may want to group the dataset location parameter and the function that defines transformation logic in the same mixin class.
 
 The rest of this page walks through examples of each method.",H2,https://outerbounds.com/docs/reuse-parameters-across-flows/#solution,False,665.0,102.0
229,Create a Function with Common Parameters,"First you will see a function `parameterize_flow` being defined in `shared_params.py`. This highlights the first case where we want to reuse parameters and do not care about coupling the parameter definitions with other common functionality. The function can then be imported and used to define parameters in a flow.
 
 
 ```py title=""shared_params.py""
 from metaflow import Parameter
 
 def parameterize_flow():
     
     param_a = Parameter(""a"", default = 11)
     param_b = Parameter(""b"", default = 77)
     return param_a, param_b
 ```",H3,https://outerbounds.com/docs/reuse-parameters-across-flows/#create-a-function-with-common-parameters,False,540.0,93.0
230,Import Common Parameters in a Flow,,H3,https://outerbounds.com/docs/reuse-parameters-across-flows/#import-common-parameters-in-a-flow,False,0.0,1.0
231,Run the Flow,"```bash
 python reuse_parameters_across_flows.py run
 ```
 
 <CodeOutputBlock lang=""bash"">
 
 ```
      Workflow starting (run-id 1663274891128793):
      [1663274891128793/start/1 (pid 40538)] Task is starting.
      [1663274891128793/start/1 (pid 40538)] Task finished successfully.
      [1663274891128793/end/2 (pid 40544)] Task is starting.
      [1663274891128793/end/2 (pid 40544)] FlowA.param_a is 11
      [1663274891128793/end/2 (pid 40544)] FlowA.param_b is 77
      [1663274891128793/end/2 (pid 40544)] Task finished successfully.
      Done!
 ```
 
 </CodeOutputBlock>",H3,https://outerbounds.com/docs/reuse-parameters-across-flows/#run-the-flow,False,581.0,94.0
232,Create Mixin with Common Parameters,"In this section you will see an example using a mixin class. 
 The mixin approach helps with parameter reuse when the parameters are coupled with additional functionality you want to reuse across flows.
 This code defines a minimal example of a mixin class that a flow will inherit from. To see this pattern in a more realistic use case you can view the [mixin classes](https://github.com/outerbounds/diffusion-metaflow/blob/e511ecc1597f403d7ff4f14c70e2759caf4de570/base.py#L20) and [flows](https://github.com/outerbounds/diffusion-metaflow/blob/e511ecc1597f403d7ff4f14c70e2759caf4de570/meta_dynamic_prompts.py) from a [project](/blog/parallelizing-stable-diffusion-production-use-cases/) using Metaflow with the text-to-image model Stable Diffusion from [Stability AI](https://stability.ai/).
 
 
 ```py title=""shared_params_and_functionality.py""
 from metaflow import Parameter
 
 class FlowMixin:
 
     param_a = Parameter(""a"", default = 11)
     param_b = Parameter(""b"", default = 77)
 
     @property
     def model_config(self):
         return {""a"": self.param_a, ""b"": self.param_b}
 ```",H3,https://outerbounds.com/docs/reuse-parameters-across-flows/#create-mixin-with-common-parameters,False,1095.0,140.0
233,Create a Flow that Inherits from the Mixin,"Now you can implement a flow that inherits from `metaflow.FlowSpec` and the custom `FlowMixin` you just defined. 
 In this example the parameters and functionality of `FlowMixin` will be inherited by `ReuseParametersMixin`. 
 This is visible in the `end` step where `self.model_config` is called.
 
 
 
 
 
 
 ```py title=""reuse_parameters_across_flows_mixin.py""
 from metaflow import FlowSpec, step, Parameter
 from shared_params_and_functionality import FlowMixin
 
 class ReuseParametersMixin(FlowSpec, FlowMixin):
     
     @step
     def start(self):
         self.next(self.end)
     
     @step
     def end(self):
         #highlight-next-line
         print(self.model_config)
     
 if __name__ == ""__main__"":
     ReuseParametersMixin()
 ```",H3,https://outerbounds.com/docs/reuse-parameters-across-flows/#create-a-flow-that-inherits-from-the-mixin,False,753.0,140.0
234,Run the Flow,"```bash
 python reuse_parameters_across_flows_mixin.py run
 ```
 
 <CodeOutputBlock lang=""bash"">
 
 ```
      Workflow starting (run-id 1663275411181645):
      [1663275411181645/start/1 (pid 40942)] Task is starting.
      [1663275411181645/start/1 (pid 40942)] Task finished successfully.
      [1663275411181645/end/2 (pid 40946)] Task is starting.
      [1663275411181645/end/2 (pid 40946)] {'a': 11, 'b': 77}
      [1663275411181645/end/2 (pid 40946)] Task finished successfully.
      Done!
 ```
 
 </CodeOutputBlock>",H3,https://outerbounds.com/docs/reuse-parameters-across-flows/#run-the-flow,False,523.0,84.0
235,Further Reading,"* [How to define parameters for flows](https://docs.metaflow.org/metaflow/basics#how-to-define-parameters-for-flows)
 * [How to reuse model code in training and prediction flows](/docs/reuse-model-object/)",H2,https://outerbounds.com/docs/reuse-parameters-across-flows/#further-reading,False,205.0,18.0
236,Code,"Data science workflows are (Python) code. Obviously, we can’t reproduce the system’s behavior if we don’t know the code that produced the results in the first place. Luckily, there isn’t a big difference between versioning traditional software and versioning data science codes, so we can use widely-used tools like Git to version code. Metaflow snapshots all code executed remotely automatically, so you can know exactly what code powered any past experiment.",H2,https://outerbounds.com/docs/reproducible-ml#code,False,460.0,71.0
237,Models,"Preferably, a model behaves like a mathematical function, *y = f(x, p)*: given input data *x* and parameters *p*, we always get the same result *y*. Theoretically, to make models reproducible, we need to store the model itself, *f*, which is typically expressed as code, as well as its parameters *p* which is an array of data. In practice, machine learning models are exposed to various side effects as described below, so just persisting the model structure and its parameters is not sufficient but it is a good starting point.
 
 In the case of Metaflow, you can treat models simply as any other artifacts and let Metaflow worry about versioning and storing them. You can take a step further and try to ensure that the training procedure that produced the parameters p is also reproducible. Doing this requires that you store the hyperparameters that were used to train the model, and also address elements deeper in the stack.",H2,https://outerbounds.com/docs/reproducible-ml#models,False,930.0,159.0
238,Libraries,"Most of the code that powers a data science application is not written by you but it resides in 3rd party libraries, such as PyTorch, Tensorflow, pandas, or scikit-learn. Changing libraries are a common source of abrupt breakage and unpredictable behavior: Even when running on one machine, you might install the same Python packages on the same exact machine on two different days and the results produced are different. Even if you request the same version of a package, the library may have internal dependencies of its own which change over time.
 
 Addressing unpredictable behavior caused by libraries requires that all dependencies are snapshot and frozen, similar to the code you write. How to do this exactly is a deep question of its own. Metaflow’s [@conda decorator](https://docs.metaflow.org/metaflow/dependencies) is one possible solution.",H2,https://outerbounds.com/docs/reproducible-ml#libraries,False,853.0,132.0
239,Environment,"Frustratingly, even perfectly frozen code and libraries may produce unexpected behavior when executed in a new compute environment: Machine learning processes like model training often require special hardware accelerators like GPUs with complex device drivers which evolve over time. Having common, shared, reliable infrastructure (e.g. [@batch or @kubernetes compute environments](https://docs.metaflow.org/metaflow/scaling-out-and-up)) helps to contain and control changes in the environment.
 
 
 When using a solid data science infrastructure like the one enabled by Metaflow, achieving this level of reproducibility and reliability is quite achievable at least for the most critical applications.",H2,https://outerbounds.com/docs/reproducible-ml#environment,False,702.0,91.0
240,Data,"If you want to reproduce a specific past prediction *y* using our model *y = f(x, p)*, you need to use the exact input data *x* as well. Also to retrain the exact same model, you need the exact snapshot of a past data. In the case of data science, data often goes through a series of feature transformations which may happen in an outside system, e.g. in an extract, transform, and load pipeline. Hence managing change carefully in all the systems that touch data is critical but it is not easy. Many companies find partial solutions that satisfy the needs of most critical applications which allows them to achieve partial reproducibility without a thoroughly versioned data warehouse.",H2,https://outerbounds.com/docs/reproducible-ml#data,False,686.0,118.0
241,Entropy,"Finally, even after perfectly freezing code, models, environments, and data, you may find out that results differ. Many machine learning and data science algorithms have deliberately stochastic, random attributes, which yield different results every time the code is executed. Or, there may be attributes that depend, say, on the time of day. To achieve perfect reproducibility, you must fix random seeds, clocks, and other sources of entropy across all code, libraries, and environments, which can be a daunting task.
 
 Luckily, very few applications require absolutely perfect reproducibility. If you need this, your best bet is to stick with the simplest possible models, code, and libraries which can be audited thoroughly.",H2,https://outerbounds.com/docs/reproducible-ml#entropy,False,728.0,111.0
242,How do I?,[Set environment variables with Metaflow decorator](/docs/set-env-vars-with-decorator),H2,https://outerbounds.com/docs/reproducible-ml#how-do-i,False,86.0,6.0
243,Question,How can I specify conda channels in my Metaflow flows and steps?,H2,https://outerbounds.com/docs/specify-conda-channels#question,True,64.0,12.0
244,Solution,"You can change default [conda channels](https://docs.conda.io/projects/conda/en/latest/user-guide/concepts/channels.html#what-is-a-conda-channel) by specifying the channel in the Metaflow `@conda` or `@conda_base` decorator, or by using the `CONDA_CHANNELS` environment variable.",H2,https://outerbounds.com/docs/specify-conda-channels#solution,False,279.0,24.0
245,Set the Channel Explicitly in a Decorator,"For example, this flow shows how to install a specific version of PyTorch from the `pytorch` conda channel using an argument to the `@conda` step-level decorator.
 
 
 
 
 This is specified like: 
 
 ```
 @conda(libraries={
     ""conda-channel::python-module"": ""module-version"",
     ""another-conda-channel::another-python-module"": ""module-version"",
     ...
 })
 ```
 
 
 
 
 
 ```py title=""specify_conda_channel_step.py""
 from metaflow import FlowSpec, step, conda
 
 class SpecifyChannelsStep(FlowSpec):
     
     @step
     def start(self):
         self.next(self.make_pytorch_model)
     
     #highlight-next-line
     @conda(libraries={""pytorch::pytorch"": ""1.11.0""})
     @step 
     def make_pytorch_model(self):
         import torch
         self.next(self.end)
         
     @step
     def end(self):
         pass
         
 if __name__ == ""__main__"":
     SpecifyChannelsStep()
 ```
 
 
 
 
 
 ```bash
 python specify_conda_channel_step.py --environment=conda run
 ```
 
 <CodeOutputBlock lang=""bash"">
 
 ```
      Workflow starting (run-id 671):
      [671/start/3418 (pid 88727)] Task is starting.
      [671/start/3418 (pid 88727)] Task finished successfully.
      [671/make_pytorch_model/3419 (pid 88800)] Task is starting.
      [671/make_pytorch_model/3419 (pid 88800)] Task finished successfully.
      [671/end/3420 (pid 88930)] Task is starting.
      [671/end/3420 (pid 88930)] Task finished successfully.
      Done!
 ```
 
 </CodeOutputBlock>
 
 
 
 
 The above example uses the `pytorch` conda channel, but you can use any conda channel you'd like including a private one: 
 
 ```
 @conda(libraries={""my-private-channel::pandas"": ""0.22.0""})
 ```",H3,https://outerbounds.com/docs/specify-conda-channels#set-the-channel-explicitly-in-a-decorator,False,1675.0,324.0
246,Set Global Channels with Environment Variables,"You can also set an environment variable called `CONDA_CHANNELS`. For example you can run a very similar flow to the previous one with the `pytorch` module from the `pytorch` channel and use the `CONDA_CHANNELS` environment variable to tell Metaflow to install the rest of the packages from a different channel, `conda-forge` in this case. Also notice that instead of using the `@conda` step-level decorator, this example uses the `@conda_base` decorator that locks dependencies for the entire flow.
 
 
 
 
 
 
 ```py title=""specify_conda_channel_flow.py""
 from metaflow import FlowSpec, step, conda_base",H3,https://outerbounds.com/docs/specify-conda-channels#set-global-channels-with-environment-variables,False,605.0,91.0
247,Further Reading,"* [Speed up conda](/docs/speed-up-conda)
 * [Reproducibility in machine learning](/docs/reproducible-ml)",H2,https://outerbounds.com/docs/specify-conda-channels#further-reading,False,104.0,9.0
248,Iterative Development Processes in Machine Learning,"The development process of machine learning is rooted in experimentation. Compared to traditional software engineering where it is possible to review and validate the correctness of a piece of code, data science is more empirical. It is hard to know in advance what data set versions and models yield the best results, which necessitates highly iterative development.
 It isn’t always clear in which order the steps should be done or when the data is sufficiently prepared for modeling. Observations during modeling and deployment phases may require further iteration in the data and modeling phases. In short, it takes a longer time and more iterations to convince ourselves that an ML system works correctly compared to a traditional software system.
 This has implications to how ML systems should be versioned. As a starting point, it is a good idea to use version control systems like Git for code and CI/CD systems to automate workflows but the non-linearity and interdependency of data and modeling phases of ML add new versioning considerations.",H2,https://outerbounds.com/docs/versioning-code-and-models#iterative-development-processes-in-machine-learning,False,1053.0,169.0
249,Versioning Models and Model Registries,"An ML model is a combination of code and data. The code expresses the model architecture and often the training code that is used to fit the model. The data includes the parameters of a trained model, and possibly hyperparameters needed for training the model. The code can be versioned in a version control system like Git, and models with their parameters can be serialized e.g. as Metaflow artifacts which are stored and versioned automatically. 
 Besides tracking the models themselves, we should track the state of the development and deployment process. For instance, machine learning deployment strategies lead to many model versions being deployed in production concurrently. This makes it important to catalog model versions somewhere they can be easily discovered and deployed into the production environment. 
 
 
 The term model registry describes a system where deployable model versions are recorded. The registry might also include information about how to deploy models or metadata like what lifecycle stage a model is in. When a model produces subpar or unexpected results in production, it is important to know which model exactly produced the results, and how and when the model itself was produced, which is the information that a model registry can help to provide   The model registry can be provided by the deployment tool of your choice, you can install a separate service for it, or you can rely on Metaflow’s artifacts to track the lineage of models and [keep deployments organized](https://docs.metaflow.org/going-to-production-with-metaflow/coordinating-larger-metaflow-projects).",H2,https://outerbounds.com/docs/versioning-code-and-models#versioning-models-and-model-registries,False,1608.0,247.0
250,How do I?,"[Reproduce machine learning workflows](/docs/reproducible-ml)
 
 
 [Organize results in Metaflow](https://docs.metaflow.org/metaflow/tagging)",H2,https://outerbounds.com/docs/versioning-code-and-models#how-do-i,False,141.0,10.0
251,Further Reading,"[Managing dependencies](https://docs.metaflow.org/metaflow/dependencies) with Metaflow
 
 
 [Single flow, multiple developers](https://docs.metaflow.org/going-to-production-with-metaflow/coordinating-larger-metaflow-projects#single-flow-multiple-developers)",H2,https://outerbounds.com/docs/speed-up-conda#further-reading,False,257.0,10.0
252,Question,"Why is conda ""bootstrapping environment"" for so long? Can I speed this up?",H2,https://outerbounds.com/docs/speed-up-conda#question,True,74.0,13.0
253,Solution,"When you run a flow for the first time, each conda environment you define needs to gather and install the dependencies which can take a long time. This will be faster the second time you run the flow. 
 
 To speed up conda, you can install [mamba](https://github.com/mamba-org/mamba) which is a reimplementation of conda's package manager in C++. 
 
 To install mamba:
 
 1. [Install conda](https://docs.conda.io/projects/conda/en/latest/user-guide/install/index.html). Here is the command to download on MacOS or Linux. You can find Windows installation instructions [here](https://docs.conda.io/en/latest/miniconda.html#windows-installers). 
 
 ```
 bash Miniconda3-latest-MacOSX-x86_64.sh
 bash Miniconda3-latest-Linux-x86_64.sh
 ```
 
 2. [Install mamba](https://github.com/mamba-org/mamba).
 
 ```
 conda install mamba -n base -c conda-forge
 ```
 
 3. Set Metaflow's dependency resolver environment variable.
 
 ```
 export METAFLOW_CONDA_DEPENDENCY_RESOLVER=mamba
 ```
 
 You can also set the `METAFLOW_CONDA_DEPENDENCY_RESOLVER` environment variable in your Metaflow config which by default lives at `$HOME/.metaflowconfig/config.json`.",H2,https://outerbounds.com/docs/speed-up-conda#solution,False,1144.0,138.0
254,Question,How to determine whether I should store data in a flow's `self` keyword?,H2,https://outerbounds.com/docs/whether-to-self#question,True,72.0,13.0
255,Solution,This page discusses two considerations to help you answer this question when writing Metaflow flows. The first is whether the object you want to assign to `self.variable_name` is able to be serialized with pickle and the second is about what type of data it is.,H2,https://outerbounds.com/docs/whether-to-self#solution,False,261.0,45.0
256,Why Assign Data to the self Keyword?,"In Metaflow, data can be assigned to variables with the flow object's `self` keyword like `self.variable_name`. 
 This makes the contents of `self.variable_name` accessible in downstream steps or outside of the flow's runtime environment. 
 Storing data with the `self` keyword in this way is referred to as storing flow artifacts.",H3,https://outerbounds.com/docs/whether-to-self#why-assign-data-to-the-self-keyword,False,331.0,51.0
257,The self Keyword and Serialization,"It is important to know that when you use the `self` keyword, Metaflow uses Python's built-in [pickle](https://docs.python.org/3/library/pickle.html) module to serialize artifacts. This allows Metaflow to move artifacts so they are accessible in any downstream compute environment you run tasks in. Sometimes you may observe incompatibilities with pickle and popular machine learning libraries. In this case, libraries will typically provide their own serialization mechanism that you can use. [Here](/docs/pass-xgboost-dmatrix-between-steps/) is an example with XGBoost, which uses a dataset object called the `DMatrix` that cannot be serialized with pickle.",H3,https://outerbounds.com/docs/whether-to-self#the-self-keyword-and-serialization,False,659.0,87.0
258,What Type of Data to Assign to self,"Generally, there are three types of data that flows will read, create, and write.
 1. Input data
 2. Flow internal state
 3. Output data
 
 Using the `self` keyword in a flow is meant to track flow internal state for objects that can be pickled.
 These artifacts are intended to track the state of variables that change throughout the flow lifecycle.
 
 In a machine learning context, examples of data you might consider a flow artifact include:
 * The distribution of a dataset's features.
 * Hyperparameters and corresponding performance metric values.
 * A URL to a new dataset version that was created during the flow.",H3,https://outerbounds.com/docs/whether-to-self#what-type-of-data-to-assign-to-self,False,622.0,105.0
259,How do I?,"[Pass Artifacts through a Join Step](/docs/use-artifacts-in-join-step/)
 
 
 [Save and Version State of Artifacts](/docs/pass-artifacts-between-steps/)",H2,https://outerbounds.com/docs/whether-to-self#how-do-i,False,151.0,14.0
260,What Type of Data Not to Assign to self,"In the list of three kinds of data above, you typically will not want to use `self` for input and output data. 
 
 Input datasets are typically stored in some data warehouse so they don't need to be stored by Metaflow again. 
 They are often large, and it can be costly to duplicate storage by copying into your Metaflow data store.
 Examples of input datasets include raw data and features for model training.
 
 Similarly, output datasets are meant to be consumed by systems outside Metaflow, so it is better to store them in another database or to a known location. 
 This location might be a S3 bucket or a similar solution that makes sense for the downstream data access pattern.
 Examples of output datasets include transformed versions of raw datasets.
 
 Instead of using `self` for these large datasets, you can efficiently load these kinds of data using Metaflow's built-in [cloud data integrations](https://docs.metaflow.org/metaflow/data#data-in-s3-metaflows3).",H3,https://outerbounds.com/docs/whether-to-self#what-type-of-data-not-to-assign-to-self,False,973.0,157.0
261,How do I?,"[Load CSV Data in Metaflow Steps](/docs/load-csv-in-metaflow-steps)
 
 
 [Load from S3 to pandas](/docs/load-parquet-data-to-pandas-df)
 
 
 [Chunk a DataFrame using Foreach](/docs/chunk-df)",H2,https://outerbounds.com/docs/whether-to-self#how-do-i,False,190.0,20.0
262,Question,"How can we use namespaces to keep results of our team's flow runs organized and accessible, no matter who ran the flow?",H2,https://outerbounds.com/docs/organize-with-namespaces#question,True,119.0,22.0
263,Solution,"Metaflow persists all runs and the data they produce. This data can be accessed using the Client API. 
 Namespaces are a mechanism to organize these results and the Client API data access patterns.
 By default, the active namespace will be `user:<name>` where `name` is the user name of the person who ran the flow.
 
 
 
 
 
 
 This flow will be used throughout the remainder of this post. The important part is in the `end` step, where a random `choice` is stored as an artifact resulting from a run of `TeamCollabFlow`. This data could be any artifact you store",H2,https://outerbounds.com/docs/organize-with-namespaces#solution,False,564.0,103.0
264,Run Flow,"```py title=""team_collab_flow.py""
 from metaflow import FlowSpec, step, current
 
 class TeamCollabFlow(FlowSpec):
     
     @step
     def start(self):
         print(""current.username: {}"".format(current.username))
         print(""current.namespace: {}"".format(current.namespace))
         self.next(self.end)
         
     @step
     def end(self):
         import random
         self.choice = random.choice([1,2,3,4,5])
         print(""Random choice was {}"".format(self.choice))
     
 if __name__ == ""__main__"":
     TeamCollabFlow()
 ```
 
 
 
 
 
 
 ```bash
 python team_collab_flow.py run
 ```
 
 <CodeOutputBlock lang=""bash"">
 
 ```
      Workflow starting (run-id 1658843440575774):
      [1658843440575774/start/1 (pid 67239)] Task is starting.
      [1658843440575774/start/1 (pid 67239)] current.username: eddie
      [1658843440575774/start/1 (pid 67239)] current.namespace: user:eddie
      [1658843440575774/start/1 (pid 67239)] Task finished successfully.
      [1658843440575774/end/2 (pid 67242)] Task is starting.
      [1658843440575774/end/2 (pid 67242)] Random choice was 1
      [1658843440575774/end/2 (pid 67242)] Task finished successfully.
      Done!
 ```
 
 </CodeOutputBlock>",H3,https://outerbounds.com/docs/organize-with-namespaces#run-flow,False,1209.0,234.0
265,Access Results from your Namespace,"By default, the Client API assumes it should pull data from the current user name. This means by default can use any Client API calls and you don't need to worry about others who are running this same flow and storing results in the same S3 bucket (or other storage location) as you. You will only get results from your namespace unless you explicitly set the namespace.
 
 
 ```python
 from metaflow import Flow
 run = Flow('TeamCollabFlow').latest_successful_run
 run_id, choice = run.id, run.data.choice
 print(""Run with id={} has choice={}"".format(run_id,choice))
 ```
 
 <CodeOutputBlock lang=""python"">
 
 ```
     Run with id=1658843440575774 has choice=1
 ```
 
 </CodeOutputBlock>",H3,https://outerbounds.com/docs/organize-with-namespaces#access-results-from-your-namespace,False,688.0,105.0
266,Share Results with Teammates,"If you want to your teammate access a run ID from your namespace, they will need to first switch the namespace before making the corresponding Client API call. The following example shows an error that occurs after switching to a namespace that doesn't contain the `run_id` that you ran in the previous section. This is what happens when your teammate tries to access your result before switching to your namespace, where the `run_id` exists.
 
 
 ```python
 from metaflow import namespace, get_namespace, Flow
 from metaflow.exception import MetaflowNamespaceMismatch
 
 not_my_namespace = 'user:my-teammate'
 namespace(not_my_namespace) # teammate's default namespace 
 flow_name = 'TeamCollabFlow'
 try:
     run = Flow(flow_name).latest_successful_run
 except MetaflowNamespaceMismatch as m: 
     print(m)
     print(""\tNo {} results in the {} namespace"".format(flow_name, get_namespace()))
 ```
 
 <CodeOutputBlock lang=""python"">
 
 ```
     Object not in namespace 'user:my-teammate'
     	No TeamCollabFlow results in the user:my-teammate namespace
 ```
 
 </CodeOutputBlock>
 
 Your teammate can use your namespace to access the result. The following snippet shows how you can get your namespace as using `default_namespace`. This will return a string that you or any of your colleagues can pass to `namespace` before fetching your flow results:
 
 
 ```python
 my_namespace = default_namespace()
 namespace(my_namespace) # give the my_namespace string to your colleague
 run = Flow(flow_name).latest_successful_run
 run_id, choice = run.id, run.data.choice
 print(""Run with id={} has choice={}"".format(run_id,choice))
 ```
 
 <CodeOutputBlock lang=""python"">
 
 ```
     Run with id=1658843440575774 has choice=1
 ```
 
 </CodeOutputBlock>
 
 You can use these any time to activate your default name space:
 
 
 ```python
 from metaflow import namespace, default_namespace
 _ = namespace(default_namespace())
 ```",H3,https://outerbounds.com/docs/organize-with-namespaces#share-results-with-teammates,False,1922.0,275.0
267,Use the Run ID to access in a Global Namespace,"This example shows how to access results across all namespaces represented in your flow data storage location, regardless of the user. This is done by setting `namespace(None)` and using the `run.id`.
 
 
 ```python
 namespace(None)
 run = Run('TeamCollabFlow/{}'.format(run_id))
 print(""Run with id={} has data={}"".format(run_id,data))
 ```
 
 <CodeOutputBlock lang=""python"">
 
 ```
     Run with id=1658843440575774 has data=5
 ```
 
 </CodeOutputBlock>",H3,https://outerbounds.com/docs/organize-with-namespaces#use-the-run-id-to-access-in-a-global-namespace,False,455.0,61.0
268,The Production Namespace,Metaflow also maintains a `production` namespace that is separate from any `user` namespace. This is used when you [schedule `production` flows to run automatically](https://docs.metaflow.org/going-to-production-with-metaflow/scheduling-metaflow-flows). In the case where a flow run is triggered via a production scheduler it may not make sense to associate the runs to a single user. You can read more about the production name space [here](https://docs.metaflow.org/metaflow/tagging#production-namespaces).,H3,https://outerbounds.com/docs/organize-with-namespaces#the-production-namespace,False,508.0,60.0
269,How do I?,"[Use the Client API to manage deployment auth, resume production runs in a local namespace, and more?](https://docs.metaflow.org/metaflow/tagging#production-namespaces)",H2,https://outerbounds.com/docs/organize-with-namespaces#how-do-i,False,168.0,17.0
270,Accessing Results in a Second Flow,"This flow shows how to:
 * Access data from another flow using the `get_flow_data` function
     * Use the `namespace` call to change active namespaces.
     * Access results from past runs of `other_flow_name`.  
     * Use the `default_namespace` call to return to the original namespace.
 * Print the data from the other flow during the `AccessOtherNamespace` run.
 
 
 
 
 
 
 ```py title=""access_namespace_in_flow.py""
 from metaflow import (Flow, FlowSpec, step, namespace, 
                       default_namespace, Parameter)
 
 def get_flow_data(flow, new_ns, original_ns=default_namespace()):
     try:
         namespace(new_ns)
         run = Flow(flow).latest_successful_run
     except:
         return
     namespace(original_ns)
     return run
 
 class AccessOtherNamespace(FlowSpec):
     
     other_flow_name = Parameter('other-flow-name', 
                                 default='TeamCollabFlow')
     other_namespace = Parameter('other-namespace',
                                 default=default_namespace())
     msg = ""{}.latest_successful_run.data. has value {}.""
     
     @step
     def start(self):
         
         # access other_flow_name in other_namespace
         run = get_flow_data(
             flow = self.other_flow_name,
             new_ns = self.other_namespace
         )
         if run is None:
             print(""Flow {} not found in {} namespace."".format(
                 self.other_flow_name,
                 self.other_namespace
             ))
         else:
             print(self.msg.format(
                 self.other_flow_name,
                 run.data.choice,
             ))
         self.next(self.end)
         
     @step
     def end(self):
         pass 
         
 if __name__ == ""__main__"":
     AccessOtherNamespace()
 ```",H3,https://outerbounds.com/docs/organize-with-namespaces#accessing-results-in-a-second-flow,False,1796.0,556.0
271,Run the Second Flow,"```bash
 python access_namespace_in_flow.py run
 ```
 
 <CodeOutputBlock lang=""bash"">
 
 ```
      Workflow starting (run-id 1658847270992887):
      [1658847270992887/start/1 (pid 67909)] Task is starting.
      [1658847270992887/start/1 (pid 67909)] TeamCollabFlow.latest_successful_run.data. has value 1.
      [1658847270992887/start/1 (pid 67909)] Task finished successfully.
      [1658847270992887/end/2 (pid 67912)] Task is starting.
      [1658847270992887/end/2 (pid 67912)] Task finished successfully.
      Done!
 ```
 
 </CodeOutputBlock>",H3,https://outerbounds.com/docs/organize-with-namespaces#run-the-second-flow,False,551.0,84.0
272,Further Reading,"* [Coordinating Larger Metaflow Projects](https://docs.metaflow.org/going-to-production-with-metaflow/coordinating-larger-metaflow-projects)
 * [Scheduling Metaflow Flows](https://docs.metaflow.org/going-to-production-with-metaflow/scheduling-metaflow-flows)",H2,https://outerbounds.com/docs/organize-with-namespaces#further-reading,False,258.0,9.0
273,Question,"`XGBoost` uses a data structure called a `DMatrix`, which I cannot assign to `self` because it is not pickleable. How do I pass a `DMatrix` between `steps`?",H2,https://outerbounds.com/docs/pass-xgboost-dmatrix-between-steps#question,True,156.0,27.0
274,Solution,"The easiest solution is to use `save_binary` method and use the file name you save to as a Metaflow artifact. 
 
 However, if you want to serialize the contents of a `DMatrix` so you can access the same object across steps, you have to perform the following workaround:",H2,https://outerbounds.com/docs/pass-xgboost-dmatrix-between-steps#solution,False,269.0,49.0
275,Define Helper Functions,,H3,https://outerbounds.com/docs/pass-xgboost-dmatrix-between-steps#define-helper-functions,False,0.0,1.0
276,Saving The Data,"1. First serialize the data in `DMatrix` to disk by using the `save_binary` method. 
 2. Read the data into a variable and assign it to self. This is the first highlighted section in the code below.",H3,https://outerbounds.com/docs/pass-xgboost-dmatrix-between-steps#saving-the-data,False,198.0,37.0
277,Loading the Data,"1. Save the binary data you stored in self from the previous step to disk.
 2. Load the data into `xgb.DMatrix` using the file name. This is the second highlighted section in the code below.",H3,https://outerbounds.com/docs/pass-xgboost-dmatrix-between-steps#loading-the-data,False,190.0,35.0
278,Run Flow,"This flow shows how to use `save_matrix` and `write_binary` so you can serialize an `xgboost.DMatrix`. 
 
 We can run the flow and see that the `DMatrix` contents are propagated as expected:
 
 
 
 
 
 
 ```py title=""pass_dmatrix_between_steps.py""
 from metaflow import FlowSpec, step, Parameter 
 import xgboost as xgb
 import numpy as np
 from tempfile import NamedTemporaryFile
 
 def save_matrix(dmatrix, file_name):
     dmatrix.save_binary(file_name)
     with open(file_name, 'rb') as f:
         xgb_data = f.read()
         return xgb_data
     
 def write_binary(xgb_data, file_name):
     with open(file_name, 'wb') as f:
         f.write(xgb_data)
 
 class SerializeXGBDataFlow(FlowSpec):
 
     file_name = Parameter('file_name',
                           default='xgb_data.xgb')
     
     @step
     def start(self):
         dmatrix = xgb.DMatrix(np.random.rand(5, 10))
         self.xgb_data = save_matrix(dmatrix, 
                                     self.file_name) 
         self.next(self.end)
     
     @step
     def end(self):
         write_binary(self.xgb_data, self.file_name)
         data = xgb.DMatrix(self.file_name)
         print(f'there are {data.num_row()} ' + \
                'rows in the data.')
     
 if __name__ == '__main__':
     SerializeXGBDataFlow()
 ```
 
 
 
 
 
 ```bash
 python pass_dmatrix_between_steps.py run
 ```
 
 <CodeOutputBlock lang=""bash"">
 
 ```
     ...
      [1654221299103134/end/2 (pid 71559)] Task is starting.
      [1654221299103134/end/2 (pid 71559)] there are 5 rows in the data.
      [1654221299103134/end/2 (pid 71559)] Task finished successfully.
     ...
 ```
 
 </CodeOutputBlock>",H3,https://outerbounds.com/docs/pass-xgboost-dmatrix-between-steps#run-flow,False,1660.0,399.0
279,Further Reading,"* [Working with big data in Metaflow](https://docs.metaflow.org/metaflow/data)
 * [XGBoost flow template](/docs/use-xgboost-with-metaflow/)
 * [Whether to use the self keyword](/docs/whether-to-self/)",H2,https://outerbounds.com/docs/pass-xgboost-dmatrix-between-steps#further-reading,False,200.0,18.0
280,Question,I have a [Parquet](https://parquet.apache.org) dataset stored in AWS S3 and want to access it in a Metaflow flow. How can I read one or several Parquet files at once from a flow and use them in an [Arrow](https://arrow.apache.org/docs/python/index.html) table?,H2,https://outerbounds.com/docs/load-parquet-data-to-arrow-table#question,True,260.0,39.0
281,Solution,You can access a Parquet dataset on S3 in a Metaflow flow using the `metaflow.S3` functionalities. This feature allows you to download and upload large datasets with high throughput from S3.,H2,https://outerbounds.com/docs/load-parquet-data-to-arrow-table#solution,False,190.0,31.0
282,Access Parquet Data in S3,"It is recommended to use `metaflow.S3` in a context manager. It is important to know that `metaflow.S3` saves temporary files for the duration of the context. This is why in the following example you will see file names rewritten for access after the scope closes. 
 
 To access one file you can use `metaflow.S3.get`. Often times Parquet datasets have many files which is a good use case for Metaflow's `s3.get_many` function. 
 
 This example uses Parquet data stored in S3 from [Ookla Global's AWS Open Data Submission](https://registry.opendata.aws/speedtest-global-performance).",H3,https://outerbounds.com/docs/load-parquet-data-to-arrow-table#access-parquet-data-in-s3,False,583.0,89.0
283,Run Flow,"This flow shows how to:
 * Download multiple Parquet files using Metaflow's `s3.get_many` function. 
 * Read the result of the first dataset chunk as a [PyArrow](https://arrow.apache.org/docs/python/index.html) table. 
 
 
 
 
 
 
 ```py title=""load_parquet_to_arrow.py""
 from metaflow import FlowSpec, step, S3
 
 BASE_URL = 's3://ookla-open-data/' + \
            'parquet/performance/type=fixed/'
 YEARS = ['2019', '2020', '2021', '2022']
 S3_PATHS = [
     f'year={y}/quarter=1/{y}-' + \
      '01-01_performance_fixed_tiles.parquet' 
     for y in YEARS
 ]
 
 class ParquetArrowFlow(FlowSpec):
     
     @step
     def start(self):
         self.next(self.load_parquet)
 
     @step
     def load_parquet(self):
         import pyarrow.parquet as pq
         #highlight-start
         with S3(s3root=BASE_URL) as s3:
             tmp_data_path = s3.get_many(S3_PATHS)
             first_path = tmp_data_path[0].path
             self.table = pq.read_table(first_path)
         #highlight-end
         self.next(self.end)
 
     @step
     def end(self):
         print('Table for first year' + \
               f'has shape {self.table.shape}.')
 
 if __name__ == '__main__':
     ParquetArrowFlow()
 ```
 
 
 
 
 
 ```bash
 python load_parquet_to_arrow.py run
 ```
 
 <CodeOutputBlock lang=""bash"">
 
 ```
     ...
      [637/end/3308 (pid 7081)] Task is starting.
      [637/end/3308 (pid 7081)] Table for first yearhas shape (4877036, 7).
      [637/end/3308 (pid 7081)] Task finished successfully.
     ...
 ```
 
 </CodeOutputBlock>",H3,https://outerbounds.com/docs/load-parquet-data-to-arrow-table#run-flow,False,1541.0,348.0
284,Access Artifacts Outside of Flow,"The following can be run in any script or notebook to access the contents of the table that was stored as a flow artifact with `self.table`. You can also run quick tests to assert the artifacts have expected properties.
 
 
 ```python
 from metaflow import Flow
 run = Flow('ParquetArrowFlow').latest_run
 table = run.data.table
 assert run.successful
 assert table.shape == (4877036, 7)
 table.select([1,2,3,4,5])
 ```
 
 <CodeOutputBlock lang=""python"">
 
 ```
     pyarrow.Table
     tile: string
     avg_d_kbps: int64
     avg_u_kbps: int64
     avg_lat_ms: int64
     tests: int64
 ```
 
 </CodeOutputBlock>",H3,https://outerbounds.com/docs/load-parquet-data-to-arrow-table#access-artifacts-outside-of-flow,False,612.0,104.0
285,Further Reading,"* Working with [cloud data in Metaflow](https://docs.metaflow.org/metaflow/data)
 * [Scaling Metaflow](https://docs.metaflow.org/metaflow/scaling) flows",H2,https://outerbounds.com/docs/load-parquet-data-to-arrow-table#further-reading,False,152.0,11.0
286,Question,How can I access data in S3 with a SQL query from my Metaflow flow?,H2,https://outerbounds.com/docs/run-sql-query-aws-athena#question,True,67.0,15.0
287,Solution,"You can run queries in any way you normally interact with AWS from your Python script in a Metaflow task. In addition to storing data in AWS S3, some helpful tools for doing this include [AWS Glue](https://aws.amazon.com/glue/?whats-new-cards.sort-by=item.additionalFields.postDateTime&whats-new-cards.sort-order=desc) and [AWS Athena](https://aws.amazon.com/athena/?whats-new-cards.sort-by=item.additionalFields.postDateTime&whats-new-cards.sort-order=desc). AWS Glue is a managed extract, transform, and load (ETL) service. AWS Athena is a serverless SQL service that allows to you to run queries against Glue databases.",H2,https://outerbounds.com/docs/run-sql-query-aws-athena#solution,False,622.0,68.0
288,Add Parquet files to AWS Glue DB,"<CustomCode height=""20vh"">
 
 
 
 ```py title=""create_glue_db.py""
 import pandas as pd
 import awswrangler as wr
 
 def create_db(database_name, bucket_uri, table_name):
     dataset = pd.DataFrame({
         ""id"": [1, 2],
         ""feature_1"": [""foo"", ""bar""],
         ""feature_2"": [""fizz"", ""buzz""]}
     )
 
     try: 
         # create AWS Glue database query S3 data
         wr.catalog.create_database(name=database_name)
     except wr.exceptions.AlreadyExists as error:
         # if database exists, ignore this step
         print(f""{database_name} exists!"")
 
     # store data in AWS Data Lake
     # here we use .parquet files
         # AWS Glue works with many other data formats
     _ = wr.s3.to_parquet(df=dataset, 
                          path=f""{bucket_uri}/dataset/"",
                          dataset=True, 
                          database=database_name,
                          table=table_name)
 ```
 
 </CustomCode>",H3,https://outerbounds.com/docs/run-sql-query-aws-athena#add-parquet-files-to-aws-glue-db,False,946.0,285.0
289,Run Flow,"This flow shows how to:
 * Access Parquet data with a SQL query using AWS Athena. 
 * Transform a data set. 
 * Write a pandas DataFrame to AWS S3 as `.parquet` files.
 
 
 
 
 
 
 ```py title=""sql_query_athena.py""
 from metaflow import FlowSpec, step, Parameter
 import awswrangler as wr
 from create_glue_db import create_db
 
 class AWSQueryFlow(FlowSpec):
     
     bucket_uri = Parameter(
                     ""bucket_uri"", 
                     default=""s3://outerbounds-how-tos""
                  )
     db_name = Parameter(""database_name"", 
                         default=""test_db"")
     table_name = Parameter(""table_name"", 
                            default=""test_table"")
 
     @step
     def start(self):
         create_db(self.db_name, self.bucket_uri, 
                   self.table_name)
         self.next(self.query)
 
     @step
     def query(self):
         QUERY = f""SELECT * FROM {self.table_name}""
         result = wr.athena.read_sql_query(
             QUERY, 
             database=self.db_name
         )
         self.dataset = result
         self.next(self.transform)
         
     @step
     def transform(self):
         concat = lambda x: x[""feat_1""] + x[""feat_2""]
         self.dataset[""feat_12""] = self.dataset.apply(
             concat, 
             axis=1
         )
         self.next(self.write)
         
     @step
     def write(self):
         path = f""{self.bucket_uri}/dataset/""
         _ = wr.s3.to_parquet(df=self.dataset, 
                              mode=""overwrite"",
                              path=path,
                              dataset=True, 
                              database=self.db_name,
                              table=self.table_name)
         self.next(self.end)
         
     @step
     def end(self):
         print(""Database is updated!"")
 
 if __name__ == '__main__':
     AWSQueryFlow()
 ```
 
 
 
 
 
 ```bash
 python sql_query_athena.py run
 ```
 
 <CodeOutputBlock lang=""bash"">
 
 ```
     ...
      [106/extract/481 (pid 8240)] Task is starting.
      [106/extract/481 (pid 8240)] Task finished successfully.
     ...
      [106/transform/482 (pid 8244)] Task is starting.
      [106/transform/482 (pid 8244)] Task finished successfully.
     ...
      [106/write/483 (pid 8249)] Task is starting.
      [106/write/483 (pid 8249)] Task finished successfully.
     ...
      [106/end/484 (pid 8253)] Task is starting.
      [106/end/484 (pid 8253)] Database is updated!
      [106/end/484 (pid 8253)] Task finished successfully.
     ...
 ```
 
 </CodeOutputBlock>",H3,https://outerbounds.com/docs/run-sql-query-aws-athena#run-flow,False,2560.0,823.0
290,Access Artifacts Outside of Flow,"The following can be run in any script or notebook to access the contents of the DataFrame that was stored as a flow artifact with `self.dataset`. 
 
 
 ```python
 from metaflow import Flow
 run_data = Flow('AWSQueryFlow').latest_run.data
 run_data.dataset
 ```
     
 <HTMLOutputBlock >
 
 
 
 
 ```html
 <div>
 <style scoped>
     .dataframe tbody tr th:only-of-type {
         vertical-align: middle;
     }
 
     .dataframe tbody tr th {
         vertical-align: top;
     }
 
     .dataframe thead th {
         text-align: right;
     }
 </style>
 <table border=""1"" class=""dataframe"">
   <thead>
     <tr style=""text-align: right;"">
       <th></th>
       <th>id</th>
       <th>feature_1</th>
       <th>feature_2</th>
       <th>feature_12</th>
     </tr>
   </thead>
   <tbody>
     <tr>
       <th>0</th>
       <td>1</td>
       <td>foo</td>
       <td>fizz</td>
       <td>foofizz</td>
     </tr>
     <tr>
       <th>1</th>
       <td>2</td>
       <td>bar</td>
       <td>buzz</td>
       <td>barbuzz</td>
     </tr>
   </tbody>
 </table>
 </div>
 ```
 
 
 
 </HTMLOutputBlock>",H3,https://outerbounds.com/docs/run-sql-query-aws-athena#access-artifacts-outside-of-flow,False,1093.0,287.0
291,Further Reading,"* [Loading and storing data](https://docs.metaflow.org/metaflow/data) with Metaflow
 * [AWS Data Wrangler quickstart](https://aws-data-wrangler.readthedocs.io/en/stable/)
 * [Connecting to AWS data sources](https://docs.aws.amazon.com/athena/latest/ug/work-with-data-stores.html)",H2,https://outerbounds.com/docs/run-sql-query-aws-athena#further-reading,False,279.0,18.0
292,Question,I have a CSV and want to access it in a Metaflow flow. How can I read this data into tasks and write it to disk?,H2,https://outerbounds.com/docs/load-csv-in-metaflow-steps#question,True,112.0,26.0
293,Solution,You can read a CSV into the flow using `IncludeFile`. This makes the data accessible for local and remote steps in the flow.,H2,https://outerbounds.com/docs/load-csv-in-metaflow-steps#solution,False,124.0,23.0
294,Acquire CSV,In this example a CSV from the [Metaflow tutorials](https://docs.metaflow.org/getting-started/tutorials/season-1-the-local-experience/episode00) is used. This is done in the `save_data_locally` function defined outside of the flow.,H3,https://outerbounds.com/docs/load-csv-in-metaflow-steps#acquire-csv,False,231.0,23.0
295,Run Flow,"This flow shows how to:
 * Include a CSV saved locally for all steps in the flow.
 * Add a feature to each data point. 
 * Save the new data as a flow artifact.
 
 
 
 
 
 
 ```py title=""load_csv_data.py""
 from metaflow import FlowSpec, step, IncludeFile
 import pandas as pd
 
 def save_data_locally():
     url = ""https://raw.githubusercontent.com/"" + \
           ""Netflix/metaflow/master/metaflow""
     data_path = ""/tutorials/02-statistics/movies.csv""
     local_path = ""./movies.csv""
     df = pd.read_csv(url+data_path)
     df.to_csv(local_path)
 
 class CSVFlow(FlowSpec):
     
     data = IncludeFile(""data"", default=""./movies.csv"")
     
     @step
     def start(self):
         self.next(self.use_csv)
         
     @step
     def use_csv(self):
         import pandas as pd 
         from io import StringIO
         df = pd.read_csv(StringIO(self.data),
                          index_col=0)
         f = lambda x: x < 2000
         df[""is_before_2000""] = df[""title_year""].apply(f)
         self.new_df = df
         self.next(self.end)
         
     @step
     def end(self):
         result = self.new_df.is_before_2000.sum() 
         print(f""Number of pre-2000 movies is {result}."")
         
 if __name__ == ""__main__"":
     save_data_locally()
     CSVFlow()
 ```
 
 
 
 
 
 ```bash
 python load_csv_data.py run
 ```
 
 <CodeOutputBlock lang=""bash"">
 
 ```
     ...
      [1654221300950244/end/3 (pid 71595)] Task is starting.
      [1654221300950244/end/3 (pid 71595)] Number of pre-2000 movies is 1023.
      [1654221300950244/end/3 (pid 71595)] Task finished successfully.
     ...
 ```
 
 </CodeOutputBlock>",H3,https://outerbounds.com/docs/load-csv-in-metaflow-steps#run-flow,False,1636.0,405.0
296,Access Artifacts Outside of Flow,"The following can be run in any script or notebook to access the contents of the DataFrame that was stored as a flow artifact with `self.new_df`. 
 
 
 ```python
 from metaflow import Flow 
 run = Flow(""CSVFlow"").latest_run
 assert run.successful
 run.data.new_df.is_before_2000.sum()
 ```
 
 <CodeOutputBlock lang=""python"">
 
 ```
     1023
 ```
 
 </CodeOutputBlock>",H3,https://outerbounds.com/docs/load-csv-in-metaflow-steps#access-artifacts-outside-of-flow,False,368.0,55.0
297,Further Reading,"* [Basics of Metaflow](https://docs.metaflow.org/metaflow/basics)
 * Working with [cloud data using Metaflow](https://docs.metaflow.org/metaflow/data)",H2,https://outerbounds.com/docs/load-csv-in-metaflow-steps#further-reading,False,150.0,11.0
298,Question,I have a [parquet](https://parquet.apache.org) dataset stored in AWS S3 and want to access it in a Metaflow flow. How can I read one or several Parquet files at once from a flow and use them in a [pandas](https://pandas.pydata.org) DataFrame?,H2,https://outerbounds.com/docs/load-parquet-data-to-pandas-df#question,True,242.0,39.0
299,Solution,,H2,https://outerbounds.com/docs/load-parquet-data-to-pandas-df#solution,False,0.0,1.0
300,Access Parquet Data in S3,"You can access a parquet dataset on S3 in a Metaflow flow using the `metaflow.S3` functionalities and load it into a pandas DataFrame for analysis.
 To access one file you can use `metaflow.S3.get`. Often times Parquet datasets have many files which is a good use case for Metaflow's `s3.get_many` function. 
 
 This example uses Parquet data stored in S3 from [Ookla Global's AWS Open Data Submission](https://registry.opendata.aws/speedtest-global-performance).",H3,https://outerbounds.com/docs/load-parquet-data-to-pandas-df#access-parquet-data-in-s3,False,463.0,67.0
301,Run Flow,"This flow shows how to:
 * Download multiple Parquet files using Metaflow's `s3.get_many` function. 
 * Read the result of one year of the dataset as a Pandas dataframe. 
 
 
 
 
 
 
 ```py title=""load_parquet_to_pandas.py""
 from metaflow import FlowSpec, step, S3
 
 BASE_URL = 's3://ookla-open-data/' + \
             'parquet/performance/type=fixed/'
 YEARS = ['2019', '2020', '2021', '2022']
 S3_PATHS = [
     f'year={y}/quarter=1/{y}-' + \
      '01-01_performance_fixed_tiles.parquet' 
     for y in YEARS
 ]
 
 class ParquetPandasFlow(FlowSpec):
 
     @step
     def start(self):
         self.next(self.load_parquet)
 
     @step
     def load_parquet(self):
         import pandas as pd
         #highlight-start
         with S3(s3root=BASE_URL) as s3:
             tmp_data_path = s3.get_many(S3_PATHS)
             first_path = tmp_data_path[0].path
             self.df = pd.read_parquet(first_path)
         #highlight-end
         self.next(self.end)
 
     @step
     def end(self):
         print('DataFrame for first year' + \
               f'has shape {self.df.shape}.')
 
 if __name__ == '__main__':
     ParquetPandasFlow()
 ```
 
 
 
 
 
 ```bash
 python load_parquet_to_pandas.py run
 ```
 
 <CodeOutputBlock lang=""bash"">
 
 ```
     ...
      [638/end/3312 (pid 7120)] Task is starting.
      [638/end/3312 (pid 7120)] DataFrame for first yearhas shape (4877036, 7).
      [638/end/3312 (pid 7120)] Task finished successfully.
     ...
 ```
 
 </CodeOutputBlock>",H3,https://outerbounds.com/docs/load-parquet-data-to-pandas-df#run-flow,False,1489.0,346.0
302,Access Artifacts Outside of Flow,"The following can be run in any script or notebook to access the contents of the DataFrame that was stored as a flow artifact with `self.df`.
 
 
 ```python
 from metaflow import Flow
 Flow('ParquetPandasFlow').latest_run.data.df.head()
 ```
     
 <HTMLOutputBlock >
 
 
 
 
 ```html
 <div>
 <style scoped>
     .dataframe tbody tr th:only-of-type {
         vertical-align: middle;
     }
 
     .dataframe tbody tr th {
         vertical-align: top;
     }
 
     .dataframe thead th {
         text-align: right;
     }
 </style>
 <table border=""1"" class=""dataframe"">
   <thead>
     <tr style=""text-align: right;"">
       <th></th>
       <th>quadkey</th>
       <th>tile</th>
       <th>avg_d_kbps</th>
       <th>avg_u_kbps</th>
       <th>avg_lat_ms</th>
       <th>tests</th>
       <th>devices</th>
     </tr>
   </thead>
   <tbody>
     <tr>
       <th>0</th>
       <td>0231113112003202</td>
       <td>POLYGON((-90.6591796875 38.4922941923613, -90....</td>
       <td>66216</td>
       <td>12490</td>
       <td>13</td>
       <td>28</td>
       <td>4</td>
     </tr>
     <tr>
       <th>1</th>
       <td>1322111021111001</td>
       <td>POLYGON((110.352172851562 21.2893743558604, 11...</td>
       <td>102598</td>
       <td>37356</td>
       <td>13</td>
       <td>15</td>
       <td>4</td>
     </tr>
     <tr>
       <th>2</th>
       <td>3112203030003110</td>
       <td>POLYGON((138.592529296875 -34.9219710361638, 1...</td>
       <td>24686</td>
       <td>18736</td>
       <td>18</td>
       <td>162</td>
       <td>106</td>
     </tr>
     <tr>
       <th>3</th>
       <td>0320000130321312</td>
       <td>POLYGON((-87.637939453125 40.225024210605, -87...</td>
       <td>17674</td>
       <td>13989</td>
       <td>78</td>
       <td>364</td>
       <td>4</td>
     </tr>
     <tr>
       <th>4</th>
       <td>0320001332313103</td>
       <td>POLYGON((-84.7430419921875 38.9209554204673, -...</td>
       <td>441192</td>
       <td>218955</td>
       <td>22</td>
       <td>14</td>
       <td>1</td>
     </tr>
   </tbody>
 </table>
 </div>
 ```
 
 
 
 </HTMLOutputBlock>",H3,https://outerbounds.com/docs/load-parquet-data-to-pandas-df#access-artifacts-outside-of-flow,False,2100.0,554.0
303,Further Reading,"* Working with [cloud data in Metaflow](https://docs.metaflow.org/metaflow/data)
 * [Scaling Metaflow](https://docs.metaflow.org/metaflow/scaling) flows",H2,https://outerbounds.com/docs/load-parquet-data-to-pandas-df#further-reading,False,152.0,11.0
304,Question,"I have a set of `.parquet` files in the cloud, and want to read them into memory on my remote workers quickly. How can I do this with Metaflow?",H2,https://outerbounds.com/docs/fast-cloud-to-table#question,True,143.0,29.0
305,Solution,"1. You can load data from S3 directly to memory very quickly, at tens of gigabits per second or more, using Metaflow’s optimized S3 client, metaflow.S3.
 2. Once in memory, Parquet data can be decoded efficiently using Apache Arrow.
 3. The in-memory tables produced by Arrow are interoperable with various modern data tools, so you can use the data in various ways without making additional copies, which speeds up processing and avoids unnecessary memory overhead.",H2,https://outerbounds.com/docs/fast-cloud-to-table#solution,False,466.0,75.0
306,Cloud to table,"Before writing a Metaflow flow, let's see how to use the [Metaflow S3 client](https://docs.metaflow.org/scaling/data) with [Apache Arrow](https://arrow.apache.org/).
 The main steps to pay attention to are that we use the [`metaflow.S3.get_many` function](https://docs.metaflow.org/api/S3#S3.get_many) to parallelize the retrieval of partitions of the `.parquet` file, loading the bytes into memory on the worker instance, and decoding the bytes so they are useful in a `pyarrow.Table` object.
 
 
 ```python
 from metaflow import S3
 import pyarrow.parquet as pq
 import pyarrow
 from concurrent.futures import ThreadPoolExecutor
 import multiprocessing
 ```
 
 
 ```python",H3,https://outerbounds.com/docs/fast-cloud-to-table#cloud-to-table,False,674.0,85.0
307,Performance benefits scale with instance size,"Using the basic pattern described above, you can now write Metaflow flows that scale this fast data speedup on cloud instances. 
 
 <Highlight>
 
 
 In this workflow, we organize the same operations presented in section 1 in a Metaflow flow. 
 Notice that the `data_processing` step is annotated with `@batch(..., use_tmpfs=True, ...)`. 
 The `tmpfs` feature extends the resources you request, 
 because it allows you to use memory on the Batch instance to instantiate a temporary file system;
 this makes the cloud-to-table workflow significantly faster and does not require using the local file system to temporarily store the `.parquet` bytes.
 
 To reiterate, the benefits of this workflow scale with the number of processors, available RAM, and I/O throughput of the machine you are loading a table on; so you will want to use an instance that can fit your entire Arrow table in memory to get maximal benefits. To get a sense of how fast this workflow can get, check out the [Fast Data: Loading Tables From S3 At Lightning Speed](/blog/metaflow-fast-data/) post.
 
 </Highlight>
 <RHS>
 
 
 
 ```py title=""fast_data_processing.py""
 from metaflow import Parameter, FlowSpec, step, S3, batch, conda
 from time import time
 
 class FastDataProcessing(FlowSpec):
 
     url = Parameter(
         ""data"", 
         default=""s3://outerbounds-datasets/ubiquant/investment_ids"", 
         help=""S3 prefix to Parquet files"")
 
     @step
     def start(self):
         self.next(self.data_processing)
 
     @conda(
         libraries={
             ""pandas"": ""2.0.1"", 
             ""pyarrow"": ""11.0.0""
         }, 
         python=""3.10.10""
     )
     @batch(memory=32000, cpu=8, use_tmpfs=True, tmpfs_size=16000)
     @step
     def data_processing(self):
         
         import pyarrow.parquet as pq
         import pyarrow
         from concurrent.futures import ThreadPoolExecutor
         import multiprocessing
         
         with S3() as s3:
             
             # Check metadata about is in the S3 url of interest.
             files = list(s3.list_recursive([self.url]))
             total_size = sum(f.size for f in files) / 1024**3
             msg = ""Loading%2.1dGB of data across %d files.""
             print(msg % (total_size, len(files)))
             
             # Download N parquet files in parallel.
             loaded = s3.get_many([f.url for f in files])
             local_tmp_file_paths = [f.path for f in loaded]
             
             # Read N PyArrow tables from bytes and concatenate.
             n_threads = multiprocessing.cpu_count()
             with ThreadPoolExecutor(max_workers = n_threads) as exe:
                 tables = exe.map(
                     lambda f: pq.read_table(f, use_threads=False), 
                     local_tmp_file_paths
                 )
                 table = pyarrow.concat_tables(tables)
                 
         msg = ""Table has %d rows and%2.1dGB bytes in memory.""
         print(msg % (table.shape[0], table.nbytes / 1024**3))
         
         self.next(self.end)
 
     @step
     def end(self):
         pass
 
 if __name__ == ""__main__"":
     FastDataProcessing()
 ```
 
 </RHS>
 
 
 
 ```bash
 python fast_data_processing.py --environment=conda run
 ```
 
 <CodeOutputBlock lang=""bash"">
 
 ```
      Workflow starting (run-id 199435):
      [199435/start/1097414 (pid 71466)] Task is starting.
      [199435/start/1097414 (pid 71466)] Task finished successfully.
      [199435/data_processing/1097415 (pid 71475)] Task is starting.
      [199435/data_processing/1097415 (pid 71475)] [b731b181-128d-4e9b-9ed7-dd88e7f6cf26] Task is starting (status SUBMITTED)...
      [199435/data_processing/1097415 (pid 71475)] [b731b181-128d-4e9b-9ed7-dd88e7f6cf26] Task is starting (status RUNNABLE)...
      [199435/data_processing/1097415 (pid 71475)] [b731b181-128d-4e9b-9ed7-dd88e7f6cf26] Task is starting (status RUNNABLE)...
      [199435/data_processing/1097415 (pid 71475)] [b731b181-128d-4e9b-9ed7-dd88e7f6cf26] Task is starting (status RUNNABLE)...
      [199435/data_processing/1097415 (pid 71475)] [b731b181-128d-4e9b-9ed7-dd88e7f6cf26] Task is starting (status RUNNABLE)...
      [199435/data_processing/1097415 (pid 71475)] [b731b181-128d-4e9b-9ed7-dd88e7f6cf26] Task is starting (status STARTING)...
      [199435/data_processing/1097415 (pid 71475)] [b731b181-128d-4e9b-9ed7-dd88e7f6cf26] Task is starting (status RUNNING)...
      [199435/data_processing/1097415 (pid 71475)] [b731b181-128d-4e9b-9ed7-dd88e7f6cf26] Setting up task environment.
      [199435/data_processing/1097415 (pid 71475)] [b731b181-128d-4e9b-9ed7-dd88e7f6cf26] Downloading code package...
      [199435/data_processing/1097415 (pid 71475)] [b731b181-128d-4e9b-9ed7-dd88e7f6cf26] Code package downloaded.
      [199435/data_processing/1097415 (pid 71475)] [b731b181-128d-4e9b-9ed7-dd88e7f6cf26] Task is starting.
      [199435/data_processing/1097415 (pid 71475)] [b731b181-128d-4e9b-9ed7-dd88e7f6cf26] Bootstrapping environment...
      [199435/data_processing/1097415 (pid 71475)] [b731b181-128d-4e9b-9ed7-dd88e7f6cf26] Environment bootstrapped.
      [199435/data_processing/1097415 (pid 71475)] [b731b181-128d-4e9b-9ed7-dd88e7f6cf26] Loading 7GB of data across 3579 files.
      [199435/data_processing/1097415 (pid 71475)] [b731b181-128d-4e9b-9ed7-dd88e7f6cf26] Table has 3141410 rows and 7GB bytes in memory.
      [199435/data_processing/1097415 (pid 71475)] [b731b181-128d-4e9b-9ed7-dd88e7f6cf26] Task finished with exit code 0.
      [199435/data_processing/1097415 (pid 71475)] Task finished successfully.
      [199435/end/1097416 (pid 71524)] Task is starting.
      [199435/end/1097416 (pid 71524)] Task finished successfully.
      Done!
 ```
 
 </CodeOutputBlock>
 
 </Wrapper>",H3,https://outerbounds.com/docs/fast-cloud-to-table#performance-benefits-scale-with-instance-size,False,5764.0,1188.0
308,Further Reading,,H2,https://outerbounds.com/docs/fast-cloud-to-table#further-reading,False,0.0,1.0
309,The Parquet Format,"The above data could be stored as a Parquet file that looks like this conceptually:
 
 | Name | Age | Title |
 | :---: | :---: | :---: |
 | String | Int | String |
 | Alice | 45 | Director | 
 | Bob | 22 | Engineer | 
 | Charlie | 34 | Designer |
 
 There are a few key differences to the CSV format:
 * Data is stored by columns, not by rows. It is particularly efficient to load and process data in a columnar fashion: column-by-column vs. row-by-row.
 * Each column has a name, type, and other optional metadata, which makes loading the data faster and more robust.
 * Data is compressed by default. An upside of this is reduced space requirements and faster processing, a downside is the loss of human-readability - the data is stored in a binary format.
 
 Today, Parquet is widely used by modern data warehouses and databases, like [Snowflake](https://docs.snowflake.com/en/user-guide/script-data-load-transform-parquet.html) or [AWS RDS](https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ExportSnapshot.html), [Athena](https://docs.aws.amazon.com/athena/latest/ug/parquet-serde.html), or [Google BigQuery](https://cloud.google.com/bigquery/docs/exporting-data#parquet_export_details), as an efficient format for importing, exporting, and processing data. You can load Parquet files either by using [read_parquet in pandas](https://pandas.pydata.org/docs/reference/api/pandas.read_parquet.html) or the [Arrow library](https://arrow.apache.org/docs/python/parquet.html). Note that you may be able to load much larger datasets in memory using Arrow instead of pandas - for more details read [Python data structures for tabular data](/docs/python-tabular-data-structures).
 Over the past years, the tooling for handling Parquet files across languages and libraries has advanced dramatically - thanks to the [Apache Arrow](https://arrow.apache.org/) project - so Parquet is now a solid choice for loading and storing tabular data in modern data applications.",H2,https://outerbounds.com/docs/tabular-file-formats#the-parquet-format,False,1970.0,266.0
310,Local Files vs. Files in the Cloud,"Besides the data format, where should you store the data? Business environments shun the idea of storing and sharing data as local files for good reasons: Local files can get lost, it is hard to make sure they follow the company’s data governance policies, and they are slow and inconvenient to move around.
 In the past, many data-hungry machine learning and data science applications preferred local files for performance reasons. For instance, it could have been 100x faster to train a model using local files versus loading the data from a database on the fly.
 Thanks to the advances in cloud computing, these days it can be faster to load files from a cloud storage like S3 than loading local files - e.g. Parquet files exported from a data warehouse - as long as the data doesn’t leave the cloud. Achieving the highest performance requires a relatively large cloud server and a sufficiently large dataset, which makes this a great pattern especially for applications handling hundreds of gigabytes of data or more. The situation can be summarized as follows:
 
 ![](/assets/tabular-file-formats-01.png)
 <Caption> Comparison of several ways to store and access data </Caption>
 
 Loading local files on a local workstation like a laptop is certainly fast, but a single workstation is not a particularly scalable solution and local files can be problematic, as discussed above. Loading data from the cloud to a local workstation can be slow, depending on the local network.
 You can achieve the highest data throughput by using [the @batch or @kubernetes decorators in Metaflow](https://docs.metaflow.org/metaflow/scaling-out-and-up) with [@resources](https://docs.metaflow.org/metaflow/scaling-out-and-up/effortless-scaling-with-kubernetes#requesting-resources-with-resources-decorator) requirements that exceed the size of the dataset or by using a large cloud-based workstation. Metaflow comes with a built-in S3 client, [metaflow.S3](https://docs.metaflow.org/metaflow/data), which is optimized for high-performance data loading. It shines at loading e.g. Parquet data from S3, as demonstrated in the article about Loading Parquet Data from S3.
 If your data resides in local files, you can still benefit from cloud computing either by first copying the data to S3 manually using the [AWS command line tools](https://aws.amazon.com/cli/) or by using the IncludeFile construct in Metaflow which snapshots the data as a Metaflow artifact automatically.",H2,https://outerbounds.com/docs/tabular-file-formats#local-files-vs-files-in-the-cloud,False,2460.0,356.0
311,How do I?,"[Load a Parquet File to pandas Dataframe](/docs/load-parquet-data-to-pandas-df)
 
 
 [Load CSV Data in Metaflow Steps](/docs/load-csv-in-metaflow-steps)
 
 
 [Include a File in a Metaflow Flow](https://docs.metaflow.org/metaflow/data#data-in-local-files)
 
 
 [Work with Tabular Data in Python](/docs/python-tabular-data-structures)",H2,https://outerbounds.com/docs/tabular-file-formats#how-do-i,False,332.0,32.0
312,Question,How do I query a database with SQL and load the results into Pandas?,H2,https://outerbounds.com/docs/query-local-db#question,True,68.0,14.0
313,Solution,"Pandas has utility functions that make it one line to create a table and store it in a database, and later run queries against the data. This page will show you how to run a SQL query against a self-hosted database.",H2,https://outerbounds.com/docs/query-local-db#solution,False,215.0,41.0
314,Add Table to MySQL DB,To run the full example locally you will need to [install MySQL](https://dev.mysql.com/doc/mysql-installation-excerpt/5.7/en/) and set up a database called `test`. This example uses a Python function defined in the script containing the flow to create the table but you could set up the table any way you prefer to interact with the database.,H3,https://outerbounds.com/docs/query-local-db#add-table-to-mysql-db,False,342.0,52.0
315,Run Flow,"The flow shows how to:
 * Access data in a [Pandas](https://pandas.pydata.org) dataframe by running a SQL query on a local database.
     * In this example a [MySQL](https://www.google.com/search?client=safari&rls=en&q=mysql&ie=UTF-8&oe=UTF-8) database is used, but you could also store data in [PostgreSQL](https://www.postgresql.org) for this example.
 * Make a transformation to the dataframe.
 * Save the result to a separate table in the database.
 
 
 
 
 
 
 ```py title=""sql_query_local.py""
 from metaflow import FlowSpec, step, Parameter
 from sqlalchemy import create_engine
 import pandas as pd
 
 class LocalQueryFlow(FlowSpec):
     
     @step
     def start(self):        
         self.next(self.extract)
         
     @step
     def extract(self):
         QUERY = f""SELECT * FROM {table_name}""
         self.result = pd.read_sql(QUERY, con=conn)
         self.next(self.transform)
         
     @step
     def transform(self):
         f = lambda x: x[""feat_1""] + x[""feat_2""]
         self.result[""feat_12""] = self.result.apply(f, 
                                                 axis=1)
         self.next(self.write)
         
     @step
     def write(self):
         self.result.to_sql(name=f""{table_name}_updated"", 
                            con=conn, 
                            if_exists=""replace"")
         self.next(self.end)
         
     @step
     def end(self):
         conn.close()",H3,https://outerbounds.com/docs/query-local-db#run-flow,False,1421.0,406.0
316,Access Artifacts Outside of Flow,"The following can be run in any script or notebook to access the contents of the DataFrame that was stored as a flow artifact with `self.result`. 
 
 
 ```python
 from metaflow import Flow
 run = Flow('LocalQueryFlow').latest_run
 run.data.result
 ```
     
 <HTMLOutputBlock >
 
 
 
 
 ```html
 <div>
 <style scoped>
     .dataframe tbody tr th:only-of-type {
         vertical-align: middle;
     }
 
     .dataframe tbody tr th {
         vertical-align: top;
     }
 
     .dataframe thead th {
         text-align: right;
     }
 </style>
 <table border=""1"" class=""dataframe"">
   <thead>
     <tr style=""text-align: right;"">
       <th></th>
       <th>index</th>
       <th>id</th>
       <th>feat_1</th>
       <th>feat_2</th>
       <th>feat_12</th>
     </tr>
   </thead>
   <tbody>
     <tr>
       <th>0</th>
       <td>0</td>
       <td>1</td>
       <td>foo</td>
       <td>fizz</td>
       <td>foofizz</td>
     </tr>
     <tr>
       <th>1</th>
       <td>1</td>
       <td>2</td>
       <td>bar</td>
       <td>buzz</td>
       <td>barbuzz</td>
     </tr>
   </tbody>
 </table>
 </div>
 ```
 
 
 
 </HTMLOutputBlock>",H3,https://outerbounds.com/docs/query-local-db#access-artifacts-outside-of-flow,False,1132.0,308.0
317,Further Reading,* [Loading and storing data](https://docs.metaflow.org/metaflow/data) with Metaflow,H2,https://outerbounds.com/docs/query-local-db#further-reading,False,83.0,7.0
318,Question,How do I load data from a local directory structure on AWS Batch using Metaflow's `IncludeFile`?,H2,https://outerbounds.com/docs/load-local-data-with-include#question,True,96.0,16.0
319,Solution,"When using Metaflow's `@batch` decorator as a compute environment for a step, there are several options for accessing data. This page will show how to use `metaflow.IncludeFile` to access a file on AWS Batch or Kubernetes.",H2,https://outerbounds.com/docs/load-local-data-with-include#solution,False,222.0,36.0
320,Acquire Data,"The example will access this CSV file from a step the runs on AWS Batch in several ways including:
 
 
 ```csv title=""local_data.csv""
 1, 2, 3
 4, 5, 6
 ```",H3,https://outerbounds.com/docs/load-local-data-with-include#acquire-data,False,156.0,30.0
321,Run Flow,"This flow shows how to:
 * Include flow artifacts with `self.little_data`.
 * Use artifacts to access the contents of a local file on AWS Batch.
 
 
 
 
 
 
 ```py title=""local_data_on_batch_include.py""
 from metaflow import FlowSpec, step, IncludeFile, batch
 
 class IncludeFileFlow(FlowSpec):
     data = IncludeFile('data', 
                        default='./local_data.csv')
 
     @batch(cpu=1)
     @step
     def start(self):
         print(self.data)
         self.next(self.end)
 
     @step
     def end(self):
         print('Finished reading the data!')
 
 if __name__ == '__main__':
     IncludeFileFlow()
 ```
 
 
 
 
 
 ```bash
 python local_data_on_batch_include.py run
 ```
 
 <CodeOutputBlock lang=""bash"">
 
 ```
     ...
      [468/end/2406 (pid 46569)] Task is starting.
      [468/end/2406 (pid 46569)] Finished reading the data!
      [468/end/2406 (pid 46569)] Task finished successfully.
     ...
 ```
 
 </CodeOutputBlock>",H3,https://outerbounds.com/docs/load-local-data-with-include#run-flow,False,949.0,207.0
322,Further Reading,"* [How to use Parameters Across Flows](/docs/reuse-parameters-across-flows/)
 * [Loading and storing data](https://docs.metaflow.org/metaflow/data) with Metaflow",H2,https://outerbounds.com/docs/load-local-data-with-include#further-reading,False,161.0,14.0
323,Question,How do I load data from a local directory structure on AWS Batch using Metaflow's S3 client?,H2,https://outerbounds.com/docs/sharing-local-data-with-s3#question,True,92.0,17.0
324,Solution,"When using Metaflow's `@batch` decorator as a compute environment for a step, there are several options for accessing data. This page will show how to:
 * Serialize data in a non-pickle format from a local step.
 * Upload it to S3 using Metaflow's client.
 * Read the data from a downstream step that runs on AWS Batch or Kubernetes.",H2,https://outerbounds.com/docs/sharing-local-data-with-s3#solution,False,333.0,59.0
325,Acquire Data,"The example will access this CSV file:
 
 
 ```csv title=""local_data.csv""
 1, 2, 3
 4, 5, 6
 ```",H3,https://outerbounds.com/docs/sharing-local-data-with-s3#acquire-data,False,96.0,18.0
326,Configure metaflow.S3,"When using Metaflow's `@batch` decorator you need to have an S3 bucket configured. When S3 is configured in `~/.metaflow_config/config.json` artifacts defined like `self.artifact_name` will be serialized and stored on S3. This means that for most cases you don't need to directly call Metaflow's S3 client. However, for a variety of reasons you may want to access arbitrary S3 bucket contents.",H3,https://outerbounds.com/docs/sharing-local-data-with-s3#configure-metaflows3,False,393.0,60.0
327,Run Flow,"This flow shows how to:
 * Read the contents of `local_data.csv` using `IncludeFile`.
 * Serialize the contents of the file using the `json` module.
 * Get the data on AWS S3.
 
 
 
 
 
 
 ```py title=""local_data_on_batch_s3.py""
 from metaflow import (FlowSpec, step, IncludeFile, 
                       batch, S3)
 import json
 
 class S3FileFlow(FlowSpec):
         
     data = IncludeFile('data', 
                        default='./local_data.csv')
 
     @step
     def start(self):
         with S3(run=self) as s3:
             res = json.dumps({'data': self.data})
             url = s3.put('data', res)
         self.next(self.read_from_batch)
         
     @batch(cpu=1)
     @step
     def read_from_batch(self):
         # change `run=self` to any run
         with S3(run=self) as s3: 
             data = s3.get('data').text
             print(f""File contents: {json.loads(data)}"")
         self.next(self.end)
 
     @step
     def end(self):
         print('Finished reading the data!')
 
 if __name__ == '__main__':
     S3FileFlow()
 ```
 
 
 
 
 
 ```bash
 python local_data_on_batch_s3.py run
 ```
 
 <CodeOutputBlock lang=""bash"">
 
 ```
     ...
      [467/end/2405 (pid 46565)] Task is starting.
      [467/end/2405 (pid 46565)] Finished reading the data!
      [467/end/2405 (pid 46565)] Task finished successfully.
     ...
 ```
 
 </CodeOutputBlock>",H3,https://outerbounds.com/docs/sharing-local-data-with-s3#run-flow,False,1377.0,369.0
328,Further Reading,* [Loading and storing data](https://docs.metaflow.org/metaflow/data) with Metaflow,H2,https://outerbounds.com/docs/sharing-local-data-with-s3#further-reading,False,83.0,7.0
329,Question,I have a large pandas dataframe in memory. How can I chunk it into Parquet files using Metaflow?,H2,https://outerbounds.com/docs/chunk-df#question,True,96.0,18.0
330,Solution,You can do this using Metaflow's foreach. A pattern you can use to do this is to use Apache Arrow's zero copy slice ability with Metaflow's foreach.,H2,https://outerbounds.com/docs/chunk-df#solution,False,148.0,27.0
331,Gather data,"Suppose you have curated a dataset: 
 
 
 
 
 
 
 ```python
 import numpy as np
 import pandas as pd
 import string 
 from datetime import datetime
 
 letters = list(string.ascii_lowercase)
 make_str = lambda n: ''.join(np.random.choice(letters, size=n))
 dates = pd.date_range(start=datetime(2010,1,1), 
                       end=datetime.today(),
                       freq=""min"")
 size = len(dates)
 df = pd.DataFrame({
     'date': dates,
     'num1': np.random.rand(size),
     'num2': np.random.rand(size),
     'str1': [make_str(20) for _ in range(size)],
     'str2': [make_str(20) for _ in range(size)]
 })
 
 df.to_csv(""./large_dataframe.csv"")
 ```
 
 
 
 
 
 ```python
 df.head(3)
 ```
     
 <HTMLOutputBlock >
 
 
 
 
 ```html
 <div>
 <style scoped>
     .dataframe tbody tr th:only-of-type {
         vertical-align: middle;
     }
 
     .dataframe tbody tr th {
         vertical-align: top;
     }
 
     .dataframe thead th {
         text-align: right;
     }
 </style>
 <table border=""1"" class=""dataframe"">
   <thead>
     <tr style=""text-align: right;"">
       <th></th>
       <th>date</th>
       <th>num1</th>
       <th>num2</th>
       <th>str1</th>
       <th>str2</th>
     </tr>
   </thead>
   <tbody>
     <tr>
       <th>0</th>
       <td>2010-01-01 00:00:00</td>
       <td>0.424410</td>
       <td>0.503014</td>
       <td>xyouzjaivrwtnqczcieb</td>
       <td>fonxhwjxdpdvnfvtvcar</td>
     </tr>
     <tr>
       <th>1</th>
       <td>2010-01-01 00:01:00</td>
       <td>0.650159</td>
       <td>0.184204</td>
       <td>dxrqtbmezgwobpqlpybt</td>
       <td>ihahasnbtgptjfwnvlic</td>
     </tr>
     <tr>
       <th>2</th>
       <td>2010-01-01 00:02:00</td>
       <td>0.602216</td>
       <td>0.647338</td>
       <td>kaatnygdfekoxmpnvbky</td>
       <td>wffzxlyzjnopahttvdxe</td>
     </tr>
   </tbody>
 </table>
 </div>
 ```
 
 
 
 </HTMLOutputBlock>
 
 and your goal is to store this data efficiently in Parquet files.",H3,https://outerbounds.com/docs/chunk-df#gather-data,False,1959.0,482.0
332,Determine How to Chunk the Data,"[Pyarrow](https://arrow.apache.org/docs/python/generated/pyarrow.Table.html) version `5.0.0` is used to split the dataframe into chunks. You can see how in this utility function that will be used in the following flow:
 
 
 ```py title=""dataframe_utils.py""
 import pyarrow as pa
 import pandas as pd
 from datetime import datetime
 from typing import List, Tuple
 
 def get_chunks(df:pd.DataFrame = None,
                num_chunks:int = 4) -> Tuple[pa.Table, List]:
     get_year = lambda x: datetime.strptime(
         x.split()[0], ""%Y-%m-%d"").year
     df['year'] = df.date.apply(get_year)
     num_records = df.shape[0] // num_chunks
     lengths = [num_records] * num_chunks
     lengths[-1] += df.shape[0] - num_chunks*num_records
     offsets = [sum(lengths[:i]) for i in range(num_chunks)]
     names = [""chunk_%s"" %i for i in range(num_chunks)]
     return (pa.Table.from_pandas(df), 
             list(zip(names, offsets, lengths)))
 ```",H3,https://outerbounds.com/docs/chunk-df#determine-how-to-chunk-the-data,False,948.0,173.0
333,Run Flow,"This flow shows how to load this into a pandas dataframe and apply the following steps:
 * Use `pyarrow.from_pandas` method to load the data to Arrow memory.
 * In parallel branches:
     * Use `pyarrow.Table.slice` to make zero-copy views of chunks of the table.
     * Apply a transformation to the table; appending a column in this case
     * Move the chunks to your S3 bucket using `pyarrow.parquet.write_table`. 
 * Pick a chunk and verify the existence of the new transformed column.
 
 If you have a dataframe in S3 that you want to read into memory, you can see an example specific to this topic [here](/docs/load-parquet-data-to-pandas-df). 
 
 
 
 
 
 
 
 ```py title=""chunk_dataframe.py""
 from metaflow import FlowSpec, step
 
 class ForEachChunkFlow(FlowSpec):
     
     bucket = ""s3://outerbounds-how-tos""
     s3_path = ""{}/dataframe-chunks/{}.parquet""
     df_path = ""./large_dataframe.csv""
     
     @step
     def start(self):
         import pandas as pd
         from dataframe_utils import get_chunks
         my_big_df = pd.read_csv(self.df_path)
         self.table, self.chunks = get_chunks(my_big_df)
         self.next(self.process_chunk, foreach='chunks')
     
     @step
     def process_chunk(self):
         import pyarrow as pa
         import pyarrow.parquet as pq
         
         # get view of this chunk only
         chunk_id, offset, length = self.input
         chunk = self.table.slice(offset=offset, length=length)
     
         # do transformation on table
         col1 = chunk['num1'].to_numpy()
         col2 = chunk['num2'].to_numpy()
         values = pa.array(col1 * col2)
         chunk = chunk.append_column('new col', values)
     
         # write chunk as parquet file in S3 bucket
         self.my_path = self.s3_path.format(self.bucket, chunk_id)
         pq.write_table(table=chunk, where=self.my_path)
         self.next(self.join)
         
     @step
     def join(self, inputs):
         self.next(self.end)
 
     @step
     def end(self):
         import pyarrow.parquet as pq
         test_id = 'chunk_1'
         path = self.s3_path.format(self.bucket, test_id)
         test_chunk = pq.read_table(source=path)
         assert 'new col' in test_chunk.column_names
     
 if __name__ == ""__main__"":
     ForEachChunkFlow()
 ```
 
 
 
 
 
 ```bash
 python chunk_dataframe.py run
 ```
 
 <CodeOutputBlock lang=""bash"">
 
 ```
      Workflow starting (run-id 1658839758360594):
      [1658839758360594/start/1 (pid 65431)] Task is starting.
      [1658839758360594/start/1 (pid 65431)] Foreach yields 4 child steps.
      [1658839758360594/start/1 (pid 65431)] Task finished successfully.
      [1658839758360594/process_chunk/2 (pid 65447)] Task is starting.
      [1658839758360594/process_chunk/3 (pid 65448)] Task is starting.
      [1658839758360594/process_chunk/4 (pid 65449)] Task is starting.
      [1658839758360594/process_chunk/5 (pid 65450)] Task is starting.
      [1658839758360594/process_chunk/5 (pid 65450)] Task finished successfully.
      [1658839758360594/process_chunk/4 (pid 65449)] Task finished successfully.
      [1658839758360594/process_chunk/2 (pid 65447)] Task finished successfully.
      [1658839758360594/process_chunk/3 (pid 65448)] Task finished successfully.
      [1658839758360594/join/6 (pid 65592)] Task is starting.
      [1658839758360594/join/6 (pid 65592)] Task finished successfully.
      [1658839758360594/end/7 (pid 65595)] Task is starting.
      [1658839758360594/end/7 (pid 65595)] Task finished successfully.
      Done!
 ```
 
 </CodeOutputBlock>",H3,https://outerbounds.com/docs/chunk-df#run-flow,False,3565.0,758.0
334,Further Reading,"* [Loading and Storing Data in Metaflow](https://docs.metaflow.org/metaflow/data#data-in-local-files)
 * [Load Parquet Data from S3 to Arrow Table](/docs/load-parquet-data-to-arrow-table)",H2,https://outerbounds.com/docs/chunk-df#further-reading,False,187.0,16.0
335,"NumPy - Efficient, Interoperable Arrays for Numeric Data","[NumPy](https://numpy.org/) is a performant array library for numeric data. It shines at handling arrays of data of uniform types - like individual columns of a table. In fact, pandas uses NumPy internally to store columns of a dataframe. NumPy can also represent higher-dimensional arrays which can come in handy as an input matrix, e.g. for model training or other mathematical operations.
 Under the hood, NumPy is implemented in the C programming language, making it very fast and memory-efficient. A downside is that it comes with a more limited set of data processing operations compared to a dataframe like pandas.
 A key upside of NumPy is that it can work as a conduit between various libraries. Most data science libraries in Python, such as [SciKit Learn](https://scikit-learn.org/stable/), can use, import, and export NumPy arrays natively. Many of them are smart enough to leverage NumPy in a manner that doesn’t require data to be copied explicitly, which makes it very fast to move even large amounts of data between libraries through NumPy.",H2,https://outerbounds.com/docs/python-tabular-data-structures#numpy-efficient-interoperable-arrays-for-numeric-data,False,1056.0,169.0
336,"Arrow - Efficient, Interoperable Tables","[Apache Arrow](https://arrow.apache.org/) is a newer, performance-oriented library for tabular data. In contrast to NumPy it can handle mixed columns like pandas, albeit as of today it doesn’t come with as many built-in operations for data processing. However, if you can express your operations [using the Arrow API](https://arrow.apache.org/docs/python/getstarted.html#performing-computations), the result can be much faster than with pandas. Also, thanks to Arrow’s efficient way of representing data, you can load much more data in memory than what would be possible using pandas.
 It is easy and efficient to move data between [pandas and Arrow](https://arrow.apache.org/docs/python/pandas.html) or [NumPy and Arrow](https://arrow.apache.org/docs/python/numpy.html), which can be performed in a zero-copy fashion.",H2,https://outerbounds.com/docs/python-tabular-data-structures#arrow-efficient-interoperable-tables,False,818.0,104.0
337,Scipy.Sparse - Efficient Sparse Arrays for Numeric Data,"The three libraries above are general-purpose in a sense that they come with a rich set of APIs and supporting modules that allow them to be used for a wide range of use cases. In contrast, [Scipy.Sparse](https://docs.scipy.org/doc/scipy/reference/sparse.html) is a more specialized library, targeted at handling sparse matrices i.e. numeric arrays where most values are empty or missing.
 For instance, a machine learning model may take an input matrix with tens of thousands of columns. In such cases, it is typical for most columns to be empty for any particular row. Processing such a dataset as a dense array e.g. using NumPy may be impractical due to a large amount of memory being consumed by empty values. If you use a library that is compatible with Scipy.Sparse matrices, such as [XGBoost](https://xgboost.readthedocs.io/en/stable/) or Scikit Learn, sparse matrices may allow you to handle much larger datasets than what would be feasible otherwise.",H2,https://outerbounds.com/docs/python-tabular-data-structures#scipysparse-efficient-sparse-arrays-for-numeric-data,False,959.0,149.0
338,Tensors and Other Library-specific Arrays,"Modern machine learning libraries like XGBoost, [TensorFlow](https://www.tensorflow.org/), and [PyTorch](https://pytorch.org/) are capable of crunching through a huge amount of data efficiently but configuring them for peak performance requires effort. You need suitable data loaders that load raw data into the model, as well as specialized to facilitate data movement within the model.
 For optimal performance, you are often required to use library-specific data structures, such as [DMatrix for XGBoost](https://xgboost.readthedocs.io/en/stable/python/python_api.html#xgboost.DMatrix), or various tensor objects in deep learning frameworks. These data structures are optimized for the needs of each particular library, which limits their usefulness as a generic way to store and process data. Fortunately, it is often possible to move data from tensors to NumPy arrays efficiently and vice versa.",H2,https://outerbounds.com/docs/python-tabular-data-structures#tensors-and-other-library-specific-arrays,False,900.0,118.0
339,Choosing the Library,"Here’s a simple rubric for choosing the right library for the job:
 Do you use a deep learning library? If yes, use library-specific objects and data loaders.
 Is your data small enough not to require special treatment (if you are unsure, assume yes)? Use pandas.
 Is your large data numerical and dense? Use NumPy.
 Is your large data numerical and sparse? Use Scipy.Sparse.
 Otherwise use Arrow.
 Note that in all these cases you can scale to larger datasets simply by requesting more resources from the cloud using Metaflow’s @resources decorator.",H2,https://outerbounds.com/docs/python-tabular-data-structures#choosing-the-library,False,550.0,90.0
340,How do I?,"[Pass XGBoost DMatrix between Metaflow steps](/docs/pass-xgboost-dmatrix-between-steps)
 
 
 [Understand common file formats for tabular data](/docs/tabular-file-formats)",H2,https://outerbounds.com/docs/python-tabular-data-structures#how-do-i,False,170.0,15.0
341,Run in your Browser,"Skip the setup:
 <OpenSandbox fileName=""Introduction to Metaflow"" link=""https://account.outerbounds.dev/account/?workspace=/home/workspace/workspaces/intro-to-mf/workspace.code-workspace"">
 </OpenSandbox>",H2,https://outerbounds.com/docs/intro-tutorial-setup#run-in-your-browser,False,204.0,9.0
342,Run on your own Infrastructure,"To run this tutorial locally or with your own Metaflow deployment, you will need to clone the repository, and then install dependencies using an environment manager of your choice. In the repository you will find the necessary dependencies in a `env.yml` file for conda. You can install them by following these instructions.",H2,https://outerbounds.com/docs/intro-tutorial-setup#run-on-your-own-infrastructure,False,324.0,52.0
343,Clone This Repository,"To get set up for this tutorial, clone this repository. You can do so by executing the following in your terminal:
 
 ```
 git clone https://github.com/outerbounds/tutorials.git
 ```",H3,https://outerbounds.com/docs/intro-tutorial-setup#clone-this-repository,False,182.0,27.0
344,Install Dependencies,To run these tutorials you need to install a few packages. The next two sections show two options for how you can do this using a) conda or b) pip.,H3,https://outerbounds.com/docs/intro-tutorial-setup#install-dependencies,False,147.0,30.0
345,Install env.yml with Conda,"[Conda](https://docs.conda.io/en/latest/) is an open source package management system and environment management system. 
 
 If you want to use conda do not already have it you should install the [Anaconda distribution](https://www.anaconda.com/download/) of Python 3 appropriate for your computer.
 
 We suggest you try [`mamba`](https://mamba.readthedocs.io/en/latest/) instead of `conda` because it can be significantly faster. You can install mamba [from mambaforge](https://github.com/conda-forge/miniforge#mambaforge) or [from an existing conda installation](https://mamba.readthedocs.io/en/latest/installation.html#existing-conda-install). The following shows the latter of these two paths in addition to how to create and activate your environment: 
 
 ```
 cd tutorials/intro-to-mf
 conda install mamba -n base -c conda-forge
 mamba env create -f env.yml
 conda activate intro-to-mf
 ```
 
 If you prefer not to use mamba, you can also use `conda`:
 ```
 cd tutorials/intro-to-mf
 conda env create -f env.yml
 conda activate intro-to-mf
 ```",H4,https://outerbounds.com/docs/intro-tutorial-setup#install-envyml-with-conda,False,1050.0,130.0
346,Install requirements.txt with venv,"If you prefer to use [venv](https://docs.python.org/3/library/venv.html) then you can create and activate a new environment:
 ```
 cd tutorials/intro-to-mf
 python -m venv ./intro-to-mf-env
 source ./intro-to-mf-env/bin/activate
 ```
 
 Then, if you have a Mac with M1 processor:
 
 ```
 pip install -r requirements-macM1.txt
 ```
 
 otherwise: 
 ```
 pip install -r requirements.txt
 ```
 
 If you were able to complete either `2a` or `2b` you are good to move on to the tutorials!",H4,https://outerbounds.com/docs/intro-tutorial-setup#install-requirementstxt-with-venv,False,482.0,71.0
347,Running the Tutorials,"There are many ways you can create and use Metaflow flows. In this first tutorial the primary actions to consider include: writing, editing, and running flows. 
 
 It is common to use your favorite text editor to write and edit Metaflow code and to run the flows from the command line. In the following videos, you can see this demonstrated in two ways. The first video shows how to use [VSCode](https://code.visualstudio.com/) as a place to write and edit code while using the terminal to run commands. The second video shows how to use [Jupyter Lab](https://jupyter.org/) to write, edit, and run Metaflow code.",H3,https://outerbounds.com/docs/intro-tutorial-setup#running-the-tutorials,False,612.0,103.0
348,Metaflow Tutorials with VSCode,"```mdx-code-block
 import LiteYouTubeEmbed from 'react-lite-youtube-embed';
 import 'react-lite-youtube-embed/dist/LiteYouTubeEmbed.css';
 
 <div className=""video-container"">
 <LiteYouTubeEmbed
         id=""S1FEdXUxXMg""
         title=""Metaflow Tutorials with VSCode""
 />
 </div>
 ````",H4,https://outerbounds.com/docs/intro-tutorial-setup#metaflow-tutorials-with-vscode,False,285.0,35.0
349,Metaflow Tutorials with Jupyter Lab,"```mdx-code-block
 <div className=""video-container"">
 <LiteYouTubeEmbed
         id=""BdyB2RZDFKc""
         title=""Metaflow Tutorials with Jupyter Lab""
 />
 </div>
 ````",H4,https://outerbounds.com/docs/intro-tutorial-setup#metaflow-tutorials-with-jupyter-lab,False,168.0,29.0
350,Write Your First Branching Flow,"The flow has the following structure:
 * Parameter values are defined at the beginning of the class.
 * The `start` step loads and splits a dataset to be used in downstream tasks.
     * Notice that this step calls two downstream steps in `self.next(self.train_rf, self.train_xgb)`. This is called branching. This means the `train_rf` and `train_xgb` steps will be run in parallel. 
 * The `train_rf` step fits a `sklearn.ensemble.RandomForestClassifier` for the classification task using cross-validation. 
 * The `train_xgb` step fits a `xgboost.XGBClassifier` for the classification task using cross-validation. 
 * The `score` step evaluates each classifier on a held-out dataset for testing.
     * This step is referred to as a join step. 
     * It takes in an extra argument that contains the results of the tasks that call `self.next(self.score)`. 
 * The `end` step prints the accuracy scores for each classifier.
 
 ![](/assets/intro-tutorial-dag-S2E3.png)
 
 
 ```py title=""branching_trees_flow.py""
 from metaflow import FlowSpec, step, Parameter
 
 class ParallelTreesFlow(FlowSpec):
 
     max_depth = Parameter(""max_depth"", default=None)
     random_state = Parameter(""seed"", default=21)
     n_estimators = Parameter(""n-est"", default=10)
     min_samples_split = Parameter(""min-samples"", default=2)
     eval_metric = Parameter(""eval-metric"", default='mlogloss')
     k_fold = Parameter(""k"", default=5)
 
     @step
     def start(self):
         from sklearn import datasets
         self.iris = datasets.load_iris()
         self.X = self.iris['data']
         self.y = self.iris['target']
         #highlight-next-line
         self.next(self.train_rf, self.train_xgb)
 
     @step
     def train_rf(self):
         from sklearn.ensemble import RandomForestClassifier
         from sklearn.model_selection import cross_val_score
         self.clf = RandomForestClassifier(
             n_estimators=self.n_estimators, 
             max_depth=self.max_depth,
             min_samples_split=self.min_samples_split, 
             random_state=self.random_state)
         self.model_name = ""Random Forest""
         self.scores = cross_val_score(
             self.clf, self.X, self.y, cv=self.k_fold)
         #highlight-next-line
         self.next(self.score)
 
     @step
     def train_xgb(self):
         from xgboost import XGBClassifier
         from sklearn.model_selection import cross_val_score
         self.clf = XGBClassifier(
             n_estimators=self.n_estimators,
             random_state=self.random_state,
             eval_metric=self.eval_metric,
             use_label_encoder=False
         )
         self.model_name = ""XGBoost""
         self.scores = cross_val_score(
             self.clf, self.X, self.y, cv=self.k_fold)
         #highlight-next-line
         self.next(self.score)
 
     @step
     def score(self, modeling_tasks):
         import numpy as np
         self.scores = [
             (model.model_name, 
              np.mean(model.scores), 
              np.std(model.scores))
             for model in modeling_tasks
         ]
         self.next(self.end)
 
     @step
     def end(self):
         self.experiment_results = []
         for name, mean, std in self.scores:
             self.experiment_results.append((name,mean,std))
             msg = ""{} Model Accuracy: {} \u00B1 {}%""
             print(msg.format(name, round(mean, 3), round(std, 3)))
         
 if __name__ == ""__main__"":
     ParallelTreesFlow()
 ```",H3,https://outerbounds.com/docs/intro-tutorial-S2E3#write-your-first-branching-flow,False,3487.0,835.0
351,Run the Flow,"```bash
 python branching_trees_flow.py run
 ```
 
 <CodeOutputBlock lang=""bash"">
 
 ```
      Workflow starting (run-id 1666720729727507):
      [1666720729727507/start/1 (pid 52718)] Task is starting.
      [1666720729727507/start/1 (pid 52718)] Task finished successfully.
      [1666720729727507/train_rf/2 (pid 52725)] Task is starting.
      [1666720729727507/train_xgb/3 (pid 52726)] Task is starting.
      [1666720729727507/train_rf/2 (pid 52725)] Task finished successfully.
      [1666720729727507/train_xgb/3 (pid 52726)] Task finished successfully.
      [1666720729727507/score/4 (pid 52731)] Task is starting.
      [1666720729727507/score/4 (pid 52731)] Task finished successfully.
      [1666720729727507/end/5 (pid 52734)] Task is starting.
      [1666720729727507/end/5 (pid 52734)] Random Forest Model Accuracy: 0.96 ± 0.025%
      [1666720729727507/end/5 (pid 52734)] XGBoost Model Accuracy: 0.967 ± 0.021%
      [1666720729727507/end/5 (pid 52734)] Task finished successfully.
      Done!
 ```
 
 </CodeOutputBlock>
 
 In this episode, you trained two models in parallel using multiple CPU cores. 
 In the next episode, you will transition from authoring and running flows to focusing on how to analyze the results produced by flows.
 See you there!",H3,https://outerbounds.com/docs/intro-tutorial-S2E3#run-the-flow,False,1271.0,208.0
352,Write a Gradient Boosted Trees Flow,"The flow has the following structure:
 * Parameter values are defined at the beginning of the class.
 * The `start` step loads and splits a dataset to be used in downstream tasks.
 * The `train_xgb` step fits an `xgboost.XGBClassifier` for the classification task using cross-validation. 
 * The `end` step prints the accuracy scores for the classifier.
 
 ![](/assets/intro-tutorial-dag-S2E2.png)
 
 
 ```py title=""gradient_boosted_trees_flow.py""
 from metaflow import FlowSpec, step, Parameter
 
 class GradientBoostedTreesFlow(FlowSpec):
 
     random_state = Parameter(""seed"", default=12)
     n_estimators = Parameter(""n-est"", default=10)
     eval_metric = Parameter(""eval-metric"", default='mlogloss')
     k_fold = Parameter(""k"", default=5)
     
     @step
     def start(self):
         from sklearn import datasets
         self.iris = datasets.load_iris()
         self.X = self.iris['data']
         self.y = self.iris['target']
         self.next(self.train_xgb)
 
     #highlight-start
     @step
     def train_xgb(self):
         from xgboost import XGBClassifier
         from sklearn.model_selection import cross_val_score
         self.clf = XGBClassifier(
             n_estimators=self.n_estimators,
             random_state=self.random_state,
             eval_metric=self.eval_metric,
             use_label_encoder=False)
         self.scores = cross_val_score(
             self.clf, self.X, self.y, cv=self.k_fold)
         self.next(self.end)
     #highlight-end
         
     @step
     def end(self):
         import numpy as np
         msg = ""Gradient Boosted Trees Model Accuracy: {} \u00B1 {}%""
         self.mean = round(100*np.mean(self.scores), 3)
         self.std = round(100*np.std(self.scores), 3)
         print(msg.format(self.mean, self.std))
 
 if __name__ == ""__main__"":
     GradientBoostedTreesFlow()
 ```",H3,https://outerbounds.com/docs/intro-tutorial-S2E2#write-a-gradient-boosted-trees-flow,False,1854.0,415.0
353,Run the Flow,"```bash
 python gradient_boosted_trees_flow.py run
 ```
 
 <CodeOutputBlock lang=""bash"">
 
 ```
      Workflow starting (run-id 1666720725993465):
      [1666720725993465/start/1 (pid 52705)] Task is starting.
      [1666720725993465/start/1 (pid 52705)] Task finished successfully.
      [1666720725993465/train_xgb/2 (pid 52708)] Task is starting.
      [1666720725993465/train_xgb/2 (pid 52708)] Task finished successfully.
      [1666720725993465/end/3 (pid 52714)] Task is starting.
      [1666720725993465/end/3 (pid 52714)] Gradient Boosted Trees Model Accuracy: 96.667 ± 2.108%
      [1666720725993465/end/3 (pid 52714)] Task finished successfully.
      Done!
 ```
 
 </CodeOutputBlock>
 
 Note that XGBoost has two ways to train a [booster](https://xgboost.readthedocs.io/en/stable/python/python_api.html#xgboost.Booster) model. This example uses [XGBoost's scikit-learn API](https://xgboost.readthedocs.io/en/stable/python/python_api.html#module-xgboost.sklearn). If you use the [XGBoost learning API](https://xgboost.readthedocs.io/en/stable/python/python_api.html#module-xgboost.training) you will have to use [xgboost.DMatrix](https://xgboost.readthedocs.io/en/stable/python/python_api.html?highlight=data#module-xgboost.core) objects for data. These objects can not be serialized by pickle so cannot be stored using `self` directly. See [this example](/docs/pass-xgboost-dmatrix-between-steps/) to learn how to deal with cases where objects you want to `self` cannot be pickled.
 
 In the last two episodes, you wrote flows to train random forest and XGBoost models. In the next episode, you will start to see the power of Metaflow as you merge these two flows and train the models in parallel. Metaflow allows you to run as many parallel tasks as you want, and the next lesson will provide a template for how to do this.",H3,https://outerbounds.com/docs/intro-tutorial-S2E2#run-the-flow,False,1836.0,244.0
354,What You Will Learn,"At the end of this season you will be able to:
 * Build models in a way you can operationalize efficiently.
 * Train models in parallel.
 * Store and analyze data produced by model training workflows.",H4,https://outerbounds.com/docs/intro-tutorial-season-2-overview#what-you-will-learn,False,200.0,36.0
355,To Run The Code,"Ensure you have followed the [setup steps](/docs/intro-tutorial-setup). Then,
 ```
 cd <YOUR PATH>/tutorials/intro-to-mf/season-2
 ```",H4,https://outerbounds.com/docs/intro-tutorial-season-2-overview#to-run-the-code,False,134.0,13.0
356,Write a Random Forest Flow,"The flow has the following structure:
 * Parameter values are defined at the beginning of the class.
     * Default values can be overridden using command line arguments as shown in episode 1.4.
 * The `start` step loads and splits a dataset to be used in downstream tasks.
     * The dataset for this task is small, so we can store it in `self` without introducing much copying and storage overhead. Remember that you can only use `self` for objects that can be [pickled](https://docs.python.org/3/library/pickle.html?highlight=pickle#module-pickle). To learn more about using `self`, [see episode 1.3](/docs/intro-tutorial-S1E3).
 * The `train_rf` step fits a `sklearn.ensemble.RandomForestClassifier` for the classification task using cross-validation. 
 * The `end` step prints the accuracy scores for the classifier.
 
 ![](/assets/intro-tutorial-dag-S2E1.png)
 
 
 ```py title=""random_forest_flow.py""
 from metaflow import FlowSpec, step, Parameter
 
 class RandomForestFlow(FlowSpec):
 
     max_depth = Parameter(""max_depth"", default=None)
     random_state = Parameter(""seed"", default=11)
     n_estimators = Parameter(""n-est"", default=10)
     min_samples_split = Parameter(""min-samples"", default=2)
     k_fold = Parameter(""k"", default=5)
 
     @step
     def start(self):
         from sklearn import datasets
         self.iris = datasets.load_iris()
         self.X = self.iris['data']
         self.y = self.iris['target']
         self.next(self.train_rf)
 
     @step
     def train_rf(self):
         from sklearn.ensemble import RandomForestClassifier
         from sklearn.model_selection import cross_val_score
         self.clf = RandomForestClassifier(
             n_estimators=self.n_estimators, 
             max_depth=self.max_depth,
             min_samples_split=self.min_samples_split, 
             random_state=self.random_state)
         self.scores = cross_val_score(
             self.clf, self.X, self.y, cv=self.k_fold)
         self.next(self.end)
 
     @step
     def end(self):
         import numpy as np
         msg = ""Random Forest Accuracy: {} \u00B1 {}%""
         self.mean = round(100*np.mean(self.scores), 3)
         self.std = round(100*np.std(self.scores), 3)
         print(msg.format(self.mean, self.std))
 
 if __name__ == ""__main__"":
     RandomForestFlow()
 ```",H3,https://outerbounds.com/docs/intro-tutorial-S2E1#write-a-random-forest-flow,False,2319.0,468.0
357,Run the Random Forest Flow,"```bash
 python random_forest_flow.py run
 ```
 
 <CodeOutputBlock lang=""bash"">
 
 ```
      Workflow starting (run-id 1666720721614183):
      [1666720721614183/start/1 (pid 52687)] Task is starting.
      [1666720721614183/start/1 (pid 52687)] Task finished successfully.
      [1666720721614183/train_rf/2 (pid 52691)] Task is starting.
      [1666720721614183/train_rf/2 (pid 52691)] Task finished successfully.
      [1666720721614183/end/3 (pid 52702)] Task is starting.
      [1666720721614183/end/3 (pid 52702)] Random Forest Accuracy: 96.0 ± 3.266%
      [1666720721614183/end/3 (pid 52702)] Task finished successfully.
      Done!
 ```
 
 </CodeOutputBlock>
 
 In this episode, you trained a random forest and evaluated its accuracy. Once you have a workflow setup, Metaflow will work with any model you can express in python code! Here are more examples of using scikit-learn and Metaflow together: 
 * [How to do cross-validation in parallel](/docs/cross-validation-in-parallel/)
 * [How to do a grid search in Metaflow](/docs/grid-search-with-metaflow/)
 
 In the next episode, you will see a similar workflow for an XGBoost model.",H3,https://outerbounds.com/docs/intro-tutorial-S2E1#run-the-random-forest-flow,False,1144.0,181.0
358,Check the Success of a Run,"You can use the client API to fetch the results of the latest run of each flow type and ensure they ran successfully. Results can be accessed from your favorite IDE or notebook environment.
 
 
 ```python
 from metaflow import Flow
 for name in ['RandomForestFlow', 'GradientBoostedTreesFlow', 'ParallelTreesFlow']:
     run = Flow(name).latest_run
     print(run.successful)
 ```
 
 <CodeOutputBlock lang=""python"">
 
 ```
     True
     True
     True
 ```
 
 </CodeOutputBlock>",H3,https://outerbounds.com/docs/intro-tutorial-S2E4#check-the-success-of-a-run,False,479.0,83.0
359,Ways to Fetch a Run,"In the previous example, you saw how to use the `metaflow.Flow` object to access the latest run of that flow like this:
 
 
 ```python
 flow_name = 'RandomForestFlow'
 flow = Flow(flow_name)
 latest_run = flow.latest_run
 ```
 
 If you know the `id` of a specific run, you can access it directly without using the flow. 
 This is helpful when flows have many runs and you want to access a specific run instead of filtering through many of them.
 
 For example, you can use the `id` property from the latest `RandomForestFlow` run. One way you can find a run's `id` is in the console output when you executed the command: `python <FLOW SCRIPT> run`. 
 
 
 ```python
 from metaflow import Run
 directly_accessed_run = Run('{}/{}'.format(
     flow_name, latest_run.id))
 ```
 
 For more examples of searching results with the client API see this guide on [How-to filter flows on a condition](/docs/filter-on-condition/).",H3,https://outerbounds.com/docs/intro-tutorial-S2E4#ways-to-fetch-a-run,False,918.0,153.0
360,What Does the Run Contain?,"Once you have fetched the run object, you can:
 * View the structure of your flow.
 * View metadata about the run such as the time it was run, and environment information like the Python and Metaflow versions.
 
 To access the steps of this run you can call `run.steps()`. This will return a [generator](https://docs.python.org/3/glossary.html#term-generator) that you can iterate over to inspect the steps in your flow. Once you have a step you can inspect the runtime tasks created from the step template like `step.tasks()`. This also returns a generator. Here is an example showing how to access steps from a run, and then access tasks from the last step:
 
 
 ```python
 run = Flow(name).latest_run
 steps = list(run.steps())[::-1]
 for task in steps[-1].tasks():
     print(task)
 ```
 
 <CodeOutputBlock lang=""python"">
 
 ```
     Task('ParallelTreesFlow/1666720729727507/end/5')
 ```
 
 </CodeOutputBlock>
 
 In addition to observing different versions of your flow and the runs it has produced, you can visualize the metadata of specific runs. You can use this for many things like tracking model hyperparameters, metric scores, and more.
 
 Concretely, you can access any artifact stored using `self` with `<RUN NAME>.data.<ARTIFACT NAME>`.
 
 Let's compare the accuracy of the models produced in the `RandomForestFlow` and the `GradientBoostedTreesFlow`. 
 
 
 ```python
 for name in ['RandomForestFlow', 'GradientBoostedTreesFlow']:
     run = Flow(name).latest_run
     artifacts = run.data
     print(""Latest {} had model accuracy = {}%"".format(name, artifacts.mean))
 ```
 
 <CodeOutputBlock lang=""python"">
 
 ```
     Latest RandomForestFlow had model accuracy = 96.0%
     Latest GradientBoostedTreesFlow had model accuracy = 96.667%
 ```
 
 </CodeOutputBlock>
 
 Bonus: If you are working on a team where multiple people run flows on the same Metaflow deployment you will most likely want to check out the related concept of [namespaces](https://docs.metaflow.org/metaflow/tagging#namespaces). 
 
 In this season you how to train models in a flow. 
 You saw how to create flows that can run the model training in parallel. 
 In the final episode saw how to analyze the results using the client API. 
 If you want to go further into the client API, you can find more details about the Metaflow object hierarchy [here](https://docs.metaflow.org/metaflow/client).
 
 In the next season, you will see how to quickly visualize the results of your flow in the browser and get an introduction to debugging flows when steps fail. See you there!",H3,https://outerbounds.com/docs/intro-tutorial-S2E4#what-does-the-run-contain,False,2554.0,406.0
361,Common Resume Scenario,"In this episode, we focus on using `resume` in the command line when debugging your flows. A common scenario of using `resume` might go something like this: 
 * You write `my_sweet_flow.py`
 * You run `python my_sweet_flow.py run`
     * Oh no, something broke! Analyzing stack trace...
     * Found the bug! 
     * Save `my_sweet_flow.py` with the fix. 
 * You resume the flow from the step that produced the bug: `python my_sweet_flow.py resume`
     * Pick up the state of the last flow execution *from the step that failed*.
     * Note: You can also specify a specific step to resume from like `python my_sweet_flow.py resume <DIFFERENT STEP NAME>`",H3,https://outerbounds.com/docs/intro-tutorial-S3E3#common-resume-scenario,False,654.0,127.0
362,Example,"Let's look at an example. In this flow:
 * The `time_consuming_step` mimics some process you'd rather not re-run because of a downstream error. Examples of such processes might be data transformations or model training.
 * The `error_prone_step` creates an `Exception` that halts your flow.
 
 
 ```py title=""debuggable_flow.py""
 from metaflow import FlowSpec, step
 
 class DebuggableFlow(FlowSpec):
     
     @step
     def start(self):
         self.next(self.time_consuming_step)
         
     @step
     def time_consuming_step(self):
         import time
         time.sleep(12)
         self.next(self.error_prone_step)
         
     @step
     def error_prone_step(self):
         #highlight-next-line
         raise Exception()
         self.next(self.end)
     
     @step
     def end(self):
         print(""Flow is done!"")
 
 if __name__ == ""__main__"":
     DebuggableFlow()
 ```",H3,https://outerbounds.com/docs/intro-tutorial-S3E3#example,False,894.0,215.0
363,Observe a Failed Task,"```bash
 python debuggable_flow.py run
 ```
 
 <CodeOutputBlock lang=""bash"">
 
 ```
     ...
      [1666720922151822/error_prone_step/3 (pid 52879)] Task is starting.
      [1666720922151822/error_prone_step/3 (pid 52879)] <flow DebuggableFlow step error_prone_step> failed:
      [1666720922151822/error_prone_step/3 (pid 52879)] Internal error
      [1666720922151822/error_prone_step/3 (pid 52879)] Traceback (most recent call last):
      [1666720922151822/error_prone_step/3 (pid 52879)] start(auto_envvar_prefix=""METAFLOW"", obj=state)
      [1666720922151822/error_prone_step/3 (pid 52879)] task.run_step(
      [1666720922151822/error_prone_step/3 (pid 52879)] self._exec_step_function(step_func)
      [1666720922151822/error_prone_step/3 (pid 52879)] step_function()
      [1666720922151822/error_prone_step/3 (pid 52879)] raise Exception()
      [1666720922151822/error_prone_step/3 (pid 52879)] Exception
      [1666720922151822/error_prone_step/3 (pid 52879)] 
      [1666720922151822/error_prone_step/3 (pid 52879)] Task failed.
     ...
 ```
 
 </CodeOutputBlock>",H4,https://outerbounds.com/docs/intro-tutorial-S3E3#observe-a-failed-task,False,1077.0,145.0
364,Fix the Issue,"You can resolve the issue by:
 1. Finding and fixing the bug
 
     In this case:",H4,https://outerbounds.com/docs/intro-tutorial-S3E3#fix-the-issue,False,81.0,20.0
365,Resume the Flow,"```bash
 python debuggable_flow.py resume
 ```
 
 <CodeOutputBlock lang=""bash"">
 
 ```
     Metaflow 2.7.12 executing DebuggableFlow for user:eddie
     Validating your flow...
         The graph looks good!
     Running pylint...
         Pylint is happy!
     2022-10-25 13:02:16.194 Gathering required information to resume run (this may take a bit of time)...
     2022-10-25 13:02:16.200 Workflow starting (run-id 1666720936193890):
     2022-10-25 13:02:16.202 [1666720936193890/start/1] Cloning results of a previously run task 1666720922151822/start/1
     2022-10-25 13:02:16.565 [1666720936193890/time_consuming_step/2] Cloning results of a previously run task 1666720922151822/time_consuming_step/2
     2022-10-25 13:02:16.925 [1666720936193890/error_prone_step/3 (pid 52891)] Task is starting.
     2022-10-25 13:02:17.220 [1666720936193890/error_prone_step/3 (pid 52891)] Squashed bug
     2022-10-25 13:02:17.266 [1666720936193890/error_prone_step/3 (pid 52891)] Task finished successfully.
     2022-10-25 13:02:17.273 [1666720936193890/end/4 (pid 52894)] Task is starting.
     2022-10-25 13:02:17.570 [1666720936193890/end/4 (pid 52894)] Flow is done!
     2022-10-25 13:02:17.615 [1666720936193890/end/4 (pid 52894)] Task finished successfully.
     2022-10-25 13:02:17.616 Done!
 ```
 
 </CodeOutputBlock>
 
 Congratulations, you have completed the Introduction to Metaflow tutorial! 
 Now you are ready to operationalize your machine learning workflows with Metaflow. 
 
 To keep progressing in your Metaflow journey you can:
 * Get to know Outerbounds' view on the [machine learning stack](/docs/infra-stack).
 * Check out the [open-source repository](https://github.com/Netflix/metaflow).
 * Join our slack community and engage in #ask-metaflow. There is a lot of machine learning wisdom to discover from the community!",H4,https://outerbounds.com/docs/intro-tutorial-S3E3#resume-the-flow,False,1842.0,269.0
366,Add a Card to Your Flow,"The flow is structured as follows:
 * The `start` step loads image data from Keras.
 * The `build_model` step builds and compiles a Keras model.
 * The `train` step fits the neural net.
     * This is the step that contains the Metaflow card. In this instance, we append `metaflow.cards.Image` objects to `current.card`. You can use [cards](https://docs.metaflow.org/metaflow/visualizing-results/easy-custom-reports-with-card-components) to create data visualizations containing plots, images, markdown, HTML, and more. 
 
 
 ```py title=""neural_net_card_flow.py""
 from metaflow import FlowSpec, step, card, current, Parameter
 from metaflow.cards import Image
 
 def plot_learning_curves(history):
     import matplotlib.pyplot as plt
     fig1, ax = plt.subplots(1,1)
     ax.plot(history.history['accuracy'])
     ax.plot(history.history['val_accuracy'])
     ax.set_title('model accuracy')
     ax.set_ylabel('accuracy')
     ax.set_xlabel('epoch')
     fig1.legend(['train', 'test'], loc='upper left')
     fig2, ax = plt.subplots(1,1)
     ax.plot(history.history['loss'])
     ax.plot(history.history['val_loss'])
     ax.set_title('model loss')
     ax.set_ylabel('loss')
     ax.set_xlabel('epoch')
     fig2.legend(['train', 'test'], loc='upper left')
     return fig1, fig2
 
 class NeuralNetCardFlow(FlowSpec):
     
     epochs = Parameter('e', default=10)
 
     @step
     def start(self):
         import numpy as np
         from tensorflow import keras
         self.num_classes = 10
         ((x_train, y_train), 
          (x_test, y_test)) = keras.datasets.mnist.load_data()
         x_train = x_train.astype(""float32"") / 255
         x_test = x_test.astype(""float32"") / 255
         self.x_train = np.expand_dims(x_train, -1)
         self.x_test = np.expand_dims(x_test, -1)
         self.y_train = keras.utils.to_categorical(
             y_train, self.num_classes)
         self.y_test = keras.utils.to_categorical(
             y_test, self.num_classes)
         self.next(self.build_model)
 
     @step
     def build_model(self):
         import tempfile
         import numpy as np
         import tensorflow as tf
         from tensorflow import keras
         from tensorflow.keras import layers # pylint: disable=import-error
         self.model = keras.Sequential([
             keras.Input(shape=(28,28,1)),
             layers.Conv2D(32, kernel_size=(3, 3), 
                           activation=""relu""),
             layers.MaxPooling2D(pool_size=(2, 2)),
             layers.Conv2D(64, kernel_size=(3, 3), 
                           activation=""relu""),
             layers.MaxPooling2D(pool_size=(2, 2)),
             layers.Flatten(),
             layers.Dropout(0.5),
             layers.Dense(self.num_classes, activation=""softmax""),
         ])
         self.model.compile(loss=""categorical_crossentropy"",
                            optimizer=""adam"", metrics=[""accuracy""])
         self.next(self.train)
 
     @card
     @step
     def train(self):
         import tempfile
         import tensorflow as tf
         self.batch_size = 128
         history = self.model.fit(
             self.x_train, self.y_train,
             batch_size=self.batch_size, 
             epochs=self.epochs, validation_split=0.1
         )
         fig_acc, fig_loss = plot_learning_curves(history)
         current.card.append(Image.from_matplotlib(fig_acc))
         current.card.append(Image.from_matplotlib(fig_loss))
         self.next(self.end)
 
     @step
     def end(self):
         print(""NeuralNetFlow is all done."")
 
 if __name__ == ""__main__"":
     NeuralNetCardFlow()
 ```",H3,https://outerbounds.com/docs/intro-tutorial-S3E2#add-a-card-to-your-flow,False,3614.0,879.0
367,Run the Flow,"```bash
 python neural_net_card_flow.py run
 ```
 
 <CodeOutputBlock lang=""bash"">
 
 ```
      Workflow starting (run-id 1666720921633922):
      [1666720921633922/start/1 (pid 52856)] Task is starting.
      [1666720921633922/start/1 (pid 52856)] Task finished successfully.
      [1666720921633922/build_model/2 (pid 52870)] Task is starting.
      [1666720921633922/build_model/2 (pid 52870)] WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
      [1666720921633922/build_model/2 (pid 52870)] Task finished successfully.
      [1666720921633922/train/3 (pid 52876)] Task is starting.
     422/422 [==============================] - 8s 19ms/step - loss: 0.3856 - accuracy: 0.8811 - val_loss: 0.0863 - val_accuracy: 0.97624 - loss: 2.3331 - accuracy: 0.09
     422/422 [==============================] - 7s 18ms/step - loss: 0.1164 - accuracy: 0.9650 - val_loss: 0.0579 - val_accuracy: 0.9847- loss: 0.2044 - accuracy: 0.96
     422/422 [==============================] - 7s 18ms/step - loss: 0.0874 - accuracy: 0.9732 - val_loss: 0.0475 - val_accuracy: 0.9862- loss: 0.2218 - accuracy: 0.95
     422/422 [==============================] - 7s 18ms/step - loss: 0.0709 - accuracy: 0.9778 - val_loss: 0.0418 - val_accuracy: 0.9883- loss: 0.0672 - accuracy: 0.97
     422/422 [==============================] - 8s 18ms/step - loss: 0.0658 - accuracy: 0.9794 - val_loss: 0.0398 - val_accuracy: 0.9887- loss: 0.0981 - accuracy: 0.96
     422/422 [==============================] - 8s 18ms/step - loss: 0.0595 - accuracy: 0.9816 - val_loss: 0.0366 - val_accuracy: 0.9895- loss: 0.0591 - accuracy: 0.9
     422/422 [==============================] - 8s 18ms/step - loss: 0.0542 - accuracy: 0.9829 - val_loss: 0.0346 - val_accuracy: 0.9897- loss: 0.0287 - accuracy: 0.98
     422/422 [==============================] - 8s 19ms/step - loss: 0.0503 - accuracy: 0.9836 - val_loss: 0.0333 - val_accuracy: 0.9912- loss: 0.0148 - accuracy: 1.00
     422/422 [==============================] - 9s 21ms/step - loss: 0.0480 - accuracy: 0.9848 - val_loss: 0.0325 - val_accuracy: 0.9913- loss: 0.0515 - accuracy: 0.97
     422/422 [==============================] - 8s 19ms/step - loss: 0.0436 - accuracy: 0.9866 - val_loss: 0.0323 - val_accuracy: 0.9915- loss: 0.0092 - accuracy: 1.00
      [1666720921633922/train/3 (pid 52876)] WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
      [1666720921633922/train/3 (pid 52876)] Task finished successfully.
      [1666720921633922/end/4 (pid 52939)] Task is starting.
      [1666720921633922/end/4 (pid 52939)] NeuralNetFlow is all done.
      [1666720921633922/end/4 (pid 52939)] Task finished successfully.
      Done!
 ```
 
 </CodeOutputBlock>",H3,https://outerbounds.com/docs/intro-tutorial-S3E2#run-the-flow,False,2971.0,449.0
368,Visualize the Card,"Now you can view your cards with one command. Moreover, they are [versioned](/docs/versioning-code-and-models/) like all Metaflow data. Take a look at the card containing the matplotlib figures produced in the `train` step with this command:
 
 
 ```bash
 python neural_net_card_flow.py card view train
 ```
 
 <CodeOutputBlock lang=""bash"">
 
 ```
     Metaflow 2.7.12 executing NeuralNetCardFlow for user:eddie
     Resolving card: NeuralNetCardFlow/1666720921633922/train/3
 ```
 
 </CodeOutputBlock>
 
 You can also render cards like the one produced in this example in a [Jupyter Notebook](https://github.com/outerbounds/tutorials/tree/main/intro-to-mf/season-2/S2E4-analysis.ipynb). 
 
 You can open the notebook in the command line from the directory where you ran the flow like: 
 ```
 jupyter lab S3E2-analysis.ipynb
 ```
 
 In this episode you saw how using `@card` can help you quickly visualize images that are produced in Metaflow flows. This can help you log results and iterate quickly. In the next episode, you will see another Metaflow pattern designed to help you iterate faster. Specifically, what do you do when a task in your flow fails? See you in the next episode to find out!",H3,https://outerbounds.com/docs/intro-tutorial-S3E2#visualize-the-card,False,1198.0,179.0
369,hide_table_of_contents: false,"This season introduces some of the many interactive aspects of machine learning workflows. You will see how to train a neural network, generate data visualizations, and debug your flows when steps fail. This season is about highlighting how Metaflow can help improve your experience as a developer. When working with heavy resource-consuming models like neural networks, the importance of quick visualizations and the ability to checkpoint and resume your workflows becomes increasingly important.",H2,https://outerbounds.com/docs/intro-tutorial-season-3-overview#hide-table-of-contents-false,False,497.0,73.0
370,What You Will Learn,"At the end of this season you will be able to:
 * Build neural networks that you can operationalize.
 * Visualize the results of workflows.
 * Checkpoint and resume your machine learning code from a specific step.",H4,https://outerbounds.com/docs/intro-tutorial-season-3-overview#what-you-will-learn,False,213.0,37.0
371,To Run The Code,"Ensure you have followed the [setup steps](/docs/intro-tutorial-setup). Then,
 ```
 cd <YOUR PATH>/tutorials/intro-to-mf/season-3
 ```",H4,https://outerbounds.com/docs/intro-tutorial-season-3-overview#to-run-the-code,False,134.0,13.0
372,Write a Neural Network Flow,"The flow shows:
 * The `start` step loads image data from Keras.
 * The `build_model` step builds and compiles a Keras model.
 * The `train` step fits the neural net.
 
 ![](/assets/intro-tutorial-dag-S3E1.png)
 
 
 ```py title=""neural_net_flow.py""
 from metaflow import FlowSpec, step, Parameter
 
 class NeuralNetFlow(FlowSpec):
     
     epochs = Parameter('e', default=10)
 
     @step
     def start(self):
         import numpy as np
         from tensorflow import keras
         self.num_classes = 10
         ((x_train, y_train), 
          (x_test, y_test)) = keras.datasets.mnist.load_data()
         x_train = x_train.astype(""float32"") / 255
         x_test = x_test.astype(""float32"") / 255
         self.x_train = np.expand_dims(x_train, -1)
         self.x_test = np.expand_dims(x_test, -1)
         self.y_train = keras.utils.to_categorical(
             y_train, self.num_classes)
         self.y_test = keras.utils.to_categorical(
             y_test, self.num_classes)
         self.next(self.build_model)
 
     @step
     def build_model(self):
         import tempfile
         import numpy as np
         import tensorflow as tf
         from tensorflow import keras
         from tensorflow.keras import layers # pylint: disable=import-error
         self.model = keras.Sequential([
             keras.Input(shape=(28,28,1)),
             layers.Conv2D(32, kernel_size=(3, 3), 
                           activation=""relu""),
             layers.MaxPooling2D(pool_size=(2, 2)),
             layers.Conv2D(64, kernel_size=(3, 3), 
                           activation=""relu""),
             layers.MaxPooling2D(pool_size=(2, 2)),
             layers.Flatten(),
             layers.Dropout(0.5),
             layers.Dense(self.num_classes, activation=""softmax"")
         ])
         self.model.compile(loss=""categorical_crossentropy"",
                            optimizer=""adam"", metrics=[""accuracy""])
         self.next(self.train)
 
     @step
     def train(self):
         import tempfile
         import tensorflow as tf
         self.batch_size = 128
         self.model.fit(
             self.x_train, self.y_train,
             batch_size=self.batch_size,
             epochs=self.epochs, validation_split=0.1
         )
         self.next(self.end)
 
     @step
     def end(self):
         print(""NeuralNetFlow is all done."")
 
 if __name__ == ""__main__"":
     NeuralNetFlow()
 ```",H3,https://outerbounds.com/docs/intro-tutorial-S3E1#write-a-neural-network-flow,False,2413.0,693.0
373,Run the Flow,"```bash
 python neural_net_flow.py run
 ```
 
 <CodeOutputBlock lang=""bash"">
 
 ```
      Workflow starting (run-id 1666720917686061):
      [1666720917686061/start/1 (pid 52844)] Task is starting.
      [1666720917686061/start/1 (pid 52844)] Task finished successfully.
      [1666720917686061/build_model/2 (pid 52862)] Task is starting.
      [1666720917686061/build_model/2 (pid 52862)] Task finished successfully.
      [1666720917686061/train/3 (pid 52873)] Task is starting.
     422/422 [==============================] - 8s 18ms/step - loss: 0.3670 - accuracy: 0.8887 - val_loss: 0.0776 - val_accuracy: 0.98004 - loss: 2.2965 - accuracy: 0.13
     422/422 [==============================] - 8s 19ms/step - loss: 0.1067 - accuracy: 0.9674 - val_loss: 0.0540 - val_accuracy: 0.9855- loss: 0.1289 - accuracy: 0.95
     422/422 [==============================] - 8s 18ms/step - loss: 0.0811 - accuracy: 0.9757 - val_loss: 0.0464 - val_accuracy: 0.9867- loss: 0.1024 - accuracy: 0.9
     422/422 [==============================] - 8s 18ms/step - loss: 0.0660 - accuracy: 0.9789 - val_loss: 0.0450 - val_accuracy: 0.9872- loss: 0.1125 - accuracy: 0.96
     422/422 [==============================] - 8s 18ms/step - loss: 0.0585 - accuracy: 0.9820 - val_loss: 0.0363 - val_accuracy: 0.9895- loss: 0.0410 - accuracy: 0.97
     422/422 [==============================] - 7s 18ms/step - loss: 0.0530 - accuracy: 0.9832 - val_loss: 0.0360 - val_accuracy: 0.9910- loss: 0.0488 - accuracy: 0.98
     422/422 [==============================] - 8s 18ms/step - loss: 0.0468 - accuracy: 0.9853 - val_loss: 0.0337 - val_accuracy: 0.9918- loss: 0.0540 - accuracy: 0.96
     422/422 [==============================] - 8s 19ms/step - loss: 0.0433 - accuracy: 0.9861 - val_loss: 0.0324 - val_accuracy: 0.9910- loss: 0.0201 - accuracy: 1.00
     422/422 [==============================] - 9s 21ms/step - loss: 0.0417 - accuracy: 0.9868 - val_loss: 0.0353 - val_accuracy: 0.9907- loss: 0.0363 - accuracy: 0.98
     422/422 [==============================] - 8s 20ms/step - loss: 0.0396 - accuracy: 0.9877 - val_loss: 0.0335 - val_accuracy: 0.9910- loss: 0.0416 - accuracy: 0.99
      [1666720917686061/train/3 (pid 52873)] Task finished successfully.
      [1666720917686061/end/4 (pid 52933)] Task is starting.
      [1666720917686061/end/4 (pid 52933)] NeuralNetFlow is all done.
      [1666720917686061/end/4 (pid 52933)] Task finished successfully.
      Done!
 ```
 
 </CodeOutputBlock>
 
 In this episode, you saw how to train and evaluate a neural network. 
 Here are more examples of using Metaflow with neural networks: 
 * [How to use Keras with Metaflow](/docs/use-keras-with-metaflow/)
 * [How to use PyTorch with Metaflow](/docs/use-pytorch-with-metaflow/)
 * [How to use GPUs with Metaflow](/docs/scale-model-training-and-tuning/)
 
 In the next episode, you will see how to use Metaflow's cards feature to add data visualization to this flow. See you there!",H3,https://outerbounds.com/docs/intro-tutorial-S3E1#run-the-flow,False,2957.0,460.0
374,Why Artifacts?,"Machine learning centers around moving data. Machine learning workflows involve many forms of data including:
 * Raw data and feature data.
 * Training and testing data.
 * Model state and hyperparameters.
 * Metadata and metrics.
 
 In this episode, you will see a way to track the state of data types like this with Metaflow. 
 
 In Metaflow you can store data in one step and access it in any later step of the flow or after the run is complete. To store data with your flow run you use the `self` keyword. When you use the `self` keyword to store data, Metaflow automatically makes the data accessible in downstream tasks of your flow no matter where the tasks run. 
 
 When using Metaflow we refer to data stored using `self` as an artifact. Storing data as an artifact of a flow run is especially useful when you run different steps of the flow on different computers. In this case, Metaflow handles moving the data to where you need it for you!",H3,https://outerbounds.com/docs/intro-tutorial-S1E3#why-artifacts,False,951.0,174.0
375,Write a Flow,"This flow shows using `self` to track the state of artifacts named `dataset` and `metadata_description`. 
 
 ![](/assets/intro-tutorial-dag-S1E3.png)
 
 The objects in this example are a list and a string. You can store any Python object this way, so long as it can be serialized with [pickle](https://docs.python.org/3/library/pickle.html). 
 
 
 ```py title=""artifact_flow.py""
 from metaflow import FlowSpec, step
 
 class ArtifactFlow(FlowSpec):
 
     @step
     def start(self):
         self.next(self.create_artifact)
 
     @step
     def create_artifact(self):
         self.dataset = [[1,2,3], [4,5,6], [7,8,9]]
         self.metadata_description = ""created""
         self.next(self.transform_artifact)
 
     @step
     def transform_artifact(self):
         self.dataset = [
             [value * 10 for value in row] 
             for row in self.dataset
         ]
         self.metadata_description = ""transformed""
         self.next(self.end)
 
     @step
     def end(self):
         print(""Artifact is in state `{}` with values {}"".format(
             self.metadata_description, self.dataset))
 
 if __name__ == ""__main__"":
     ArtifactFlow()
 ```",H3,https://outerbounds.com/docs/intro-tutorial-S1E3#write-a-flow,False,1167.0,267.0
376,Run the Flow,"```bash
 python artifact_flow.py run
 ```
 
 <CodeOutputBlock lang=""bash"">
 
 ```
      Workflow starting (run-id 1666720673449590):
      [1666720673449590/start/1 (pid 52589)] Task is starting.
      [1666720673449590/start/1 (pid 52589)] Task finished successfully.
      [1666720673449590/create_artifact/2 (pid 52592)] Task is starting.
      [1666720673449590/create_artifact/2 (pid 52592)] Task finished successfully.
      [1666720673449590/transform_artifact/3 (pid 52595)] Task is starting.
      [1666720673449590/transform_artifact/3 (pid 52595)] Task finished successfully.
      [1666720673449590/end/4 (pid 52598)] Task is starting.
      [1666720673449590/end/4 (pid 52598)] Artifact is in state `transformed` with values [[10, 20, 30], [40, 50, 60], [70, 80, 90]]
      [1666720673449590/end/4 (pid 52598)] Task finished successfully.
      Done!
 ```
 
 </CodeOutputBlock>
 
 In machine learning workflows, artifacts are useful as a way to track experiment pipeline properties like metrics, model hyper-parameters, and other metadata. This pattern can deal with many cases but is not ideal for storing big data. In the case where you have a large dataset (e.g., a training dataset of many images) you may want to consider using [Metaflow's S3 utilities](https://docs.metaflow.org/api/S3).",H3,https://outerbounds.com/docs/intro-tutorial-S1E3#run-the-flow,False,1306.0,203.0
377,Access the Flow Artifacts,"In addition to observing and updating artifact state during the flow, you can access artifacts after flow runs are complete from any Python environment. For example, you can open this notebook from the directory where you ran this flow in a [Jupyter Notebook](https://github.com/outerbounds/tutorials/tree/main/intro-to-mf/season-1/S1E3-analysis.ipynb) like:
 ```
 jupyter lab S1E3-analysis.ipynb
 ```
 
 
 ```python
 from metaflow import Flow
 run_artifacts = Flow(""ArtifactFlow"").latest_run.data
 run_artifacts.dataset
 ```
 
 <CodeOutputBlock lang=""python"">
 
 ```
     [[10, 20, 30], [40, 50, 60], [70, 80, 90]]
 ```
 
 </CodeOutputBlock>
 
 In this episode, you have seen how to store artifacts of your flow runs, and access the data later. In the next lesson, you will see how to add parameters to your flow so you can pass values to your flow from the command line.",H3,https://outerbounds.com/docs/intro-tutorial-S1E3#access-the-flow-artifacts,False,872.0,128.0
378,To Run The Code,"Ensure you have followed the [setup steps](/docs/intro-tutorial-setup). Then,
 ```
 cd <YOUR PATH>/tutorials/intro-to-mf/season-1
 ```",H4,https://outerbounds.com/docs/intro-tutorial-season-1-overview#to-run-the-code,False,134.0,13.0
379,What You Will Learn,"At the end of this season you will be able to:
 * Express ML applications as workflows.
 * Read, transform, and write data in the computational steps of your machine learning workflows.
 * Analyze and visualize the results of machine learning workflows.",H4,https://outerbounds.com/docs/intro-tutorial-season-1-overview#what-you-will-learn,False,253.0,42.0
380,Why Decorators?,"Using Metaflow requires the use of decorators. In Python code, a decorator is a function that takes another function and extends its behavior without the need to modify it directly. You can find an in-depth introduction to decorators [here](https://realpython.com/primer-on-python-decorators/) and the documentation [here](https://docs.python.org/3/glossary.html#term-decorator). For now, you just need to know that they make it easier to work with your steps.
 
 In [season 1 episode 1](/docs/intro-tutorial-S1E1) you saw Metaflow's `@step` decorators in action. This is just the beginning. There are many decorators built into Metaflow and plugins built by community members.
 
 For example, there are function- or step-level decorators including:
 * You can use `@conda` to handle dependency management for a step. This decorator helps you organize dependencies across environments in your machine learning system. 
 * You can use `@batch` or `@kubernetes` to run a step remotely on AWS Batch or a Kubernetes cluster, respectively. A powerful feature of Metaflow is that you can also run many replications of this step in parallel with [minimal code changes](https://docs.metaflow.org/metaflow/basics#foreach).
 
 There are also class- or flow-level decorators including:
 * You can use `@conda_base` to handle dependency management for each step in a flow. This is similar to `@conda` but instead of creating different conda environments for each task, `@conda_base` will create one environment, package it, and reuse it across each task in the run. 
 * You can use `@schedule` to trigger flows automatically when they are deployed on a production orchestrator such as [Argo](https://argoproj.github.io/argo-workflows/) or [AWS Step Functions](https://aws.amazon.com/step-functions/). This decorator helps you run flows at a specific time without needing to intervene. You can also trigger flows at any time using the command line functionality that comes with Metaflow.",H3,https://outerbounds.com/docs/intro-tutorial-S1E2#why-decorators,False,1974.0,280.0
381,Decorate Your Flow,"In this flow, we have a step `my_decorated_func` to which the `@card` decorator is applied. 
 
 ![](/assets/intro-tutorial-dag-S1E2.png)
 
 Using `@card` creates a canvas where you can compose data visualizations using any data produced in that step or prior to it. After running the flow, you will see how to access the contents of the card from the command line. 
 
 
 ```py title=""decorator_flow.py""
 from metaflow import FlowSpec, step, card
 
 
 class DecoratorFlow(FlowSpec):
     
     @step
     def start(self):
         self.next(self.my_decorated_func)
         
     @card
     @step
     def my_decorated_func(self):
         self.data = [1, 2, 3]
         self.next(self.end)
     
     @step
     def end(self):
         print(""Flow is done!"")
 
 if __name__ == ""__main__"":
     DecoratorFlow()
 ```",H3,https://outerbounds.com/docs/intro-tutorial-S1E2#decorate-your-flow,False,814.0,185.0
382,Run the Flow,"```bash
 python decorator_flow.py run
 ```
 
 <CodeOutputBlock lang=""bash"">
 
 ```
      Workflow starting (run-id 1666720670441830):
      [1666720670441830/start/1 (pid 52568)] Task is starting.
      [1666720670441830/start/1 (pid 52568)] Task finished successfully.
      [1666720670441830/my_decorated_func/2 (pid 52571)] Task is starting.
      [1666720670441830/my_decorated_func/2 (pid 52571)] Task finished successfully.
      [1666720670441830/end/3 (pid 52578)] Task is starting.
      [1666720670441830/end/3 (pid 52578)] Flow is done!
      [1666720670441830/end/3 (pid 52578)] Task finished successfully.
      Done!
 ```
 
 </CodeOutputBlock>",H3,https://outerbounds.com/docs/intro-tutorial-S1E2#run-the-flow,False,657.0,105.0
383,View the Card,"Now that we have run `@card` for the `my_decorated_func` step, we can use the following command to visualize our flow:
 
 
 ```bash
 python decorator_flow.py card view my_decorated_func
 ```
 
 <CodeOutputBlock lang=""bash"">
 
 ```
     Resolving card: DecoratorFlow/1666720670441830/my_decorated_func/2
 ```
 
 </CodeOutputBlock>
 
 You should now see a browser tab open where you can inspect flow results.
 
 ![](/assets/decorator-tutorial-card.gif)
 
 Using `@card` in this way is one example of a decorator extending the functionality of a step. There are many other decorators you can use to extend your steps in Metaflow. You can view a list of all step decorators [here](https://docs.metaflow.org/api/step-decorators) and all flow decorators [here](https://docs.metaflow.org/api/flow-decorators).
 
 In the next episode, you will learn how to store and analyze the results of your flows.",H3,https://outerbounds.com/docs/intro-tutorial-S1E2#view-the-card,False,893.0,126.0
384,Why Flows?,"You may be familiar with authoring data science projects in a notebook. A notebook is a list of cells that contain Python code that is executed sequentially, one after another. Metaflow extends this concept by allowing you to define a graph of cells that Metaflow calls steps.
 
 A big benefit of this graph approach is that some steps may be executed concurrently, which makes the code run much faster! As you will learn soon, structuring your projects as Metaflow flows brings many other benefits too. Luckily, if you know how to author a notebook, there isn't much new to learn when it comes to authoring Metaflow flows.",H3,https://outerbounds.com/docs/intro-tutorial-S1E1#why-flows,False,623.0,108.0
385,Write Your First Flow,"Let's start with the simplest possible flow. 
 
 ![](/assets/intro-tutorial-dag-S1E1.png)
 
 Every flow you create must contain a `start` and `end` function. Above these functions, you will see the `@step` decorator. You will learn all about this and more decorators in the next episode. You tell Metaflow the order to execute steps by using `self.next`. Here you can see an example of a flow that contains only `start` and `end` steps. Note that you can write Python scripts containing flows in any text editor or notebook environment.
 
 
 ```py title=""minimum_flow.py""
 from metaflow import FlowSpec, step
 
 class MinimumFlow(FlowSpec):
     
     @step
     def start(self):
         self.next(self.end)
     
     @step
     def end(self):
         print(""Flow is done!"")
 
 if __name__ == ""__main__"":
     MinimumFlow()
 ```
 
 All of your flows [inherit](https://en.wikipedia.org/wiki/Inheritance_(object-oriented_programming)) from `FlowSpec`. In this example, you can see the `MinimumFlow` object doing so. That is the only thing you need to know about object-oriented programming to use Metaflow. You only need to write an object that uses a `FlowSpec` in a Python script.",H3,https://outerbounds.com/docs/intro-tutorial-S1E1#write-your-first-flow,False,1183.0,211.0
386,Run Your First Flow,"Once the Python script containing your flow is defined, the flow can be run from the command line using the `run` command: 
 
 
 ```bash
 python minimum_flow.py run
 ```
 
 <CodeOutputBlock lang=""bash"">
 
 ```
      Workflow starting (run-id 1666720668130276):
      [1666720668130276/start/1 (pid 52554)] Task is starting.
      [1666720668130276/start/1 (pid 52554)] Task finished successfully.
      [1666720668130276/end/2 (pid 52557)] Task is starting.
      [1666720668130276/end/2 (pid 52557)] Flow is done!
      [1666720668130276/end/2 (pid 52557)] Task finished successfully.
      Done!
 ```
 
 </CodeOutputBlock>
 
 There is a lot of information you can view in the console including:
 * Every Metaflow run gets a unique ID so that you can keep track of your experiments and have an unambiguous way to refer to the results of any particular run.
 * A run executes the steps in order. The step that is currently being executed is denoted by the step name. 
 * When runtime processes are created for steps, they are called tasks. Each task is executed by a separate process (potentially in parallel) in your operating system, identified by a process ID aka `pid`. You can use any operating system-level monitoring tool such as `top` to monitor the resource consumption of a task based on its process ID.
 * The combination of a flow name, run ID, step name, and task ID, uniquely identifies a task in your Metaflow environment, amongst all runs of any flows. Here, the flow name is omitted since it is the same for all lines. We call this globally unique identifier a `pathspec`.
 
 Congratulations on running your first flow!
 
 In the next episode, you will see how you can expand flows with decorators. Metaflow decorators can be used to send steps to the cloud, build experiment trackers, data visualizations, and more.",H3,https://outerbounds.com/docs/intro-tutorial-S1E1#run-your-first-flow,False,1833.0,323.0
387,Why Parameters?,"Sometimes you need to pass a value to a flow at runtime. For example, in a machine learning system with automated aspects, you may not know how to select an input to the model training flow, such as a hyperparameter search space, until it is time to run the flow. 
 
 To address these cases you can pass values to a `metaflow.Parameter` in your flow. When you write a flow you can define which parameters the flow will take. Then you can pass corresponding values to the command that runs your flow:
 ```
 python <FLOW SCRIPT> run --<PARAM NAME> <PARAM VALUE>
 ```",H3,https://outerbounds.com/docs/intro-tutorial-S1E4#why-parameters,False,564.0,103.0
388,Write a Flow with Parameters,"Using parameters is a convenient way to quickly iterate in prototyping. For example, you might want to change a hyperparameter like a model's learning rate. This flow adds a parameter called `learning_rate` to the `MinimumFlow` example from episode 1. 
 
 ![](/assets/intro-tutorial-dag-S1E4.png)
 
 <CustomCode height=""50vh"">
 
 
 
 ```py title=""parameter_flow.py""
 from metaflow import FlowSpec, step, Parameter
 
 class ParameterizedFlow(FlowSpec):
     
     #highlight-next-line
     learning_rate = Parameter('lr', default=.01)
     
     @step
     def start(self):
         self.next(self.end)
     
     @step
     def end(self):
         print(""Learning rate value is {}"".format(self.learning_rate))
 
 if __name__ == ""__main__"":
     ParameterizedFlow()
 ```
 
 </CustomCode>",H3,https://outerbounds.com/docs/intro-tutorial-S1E4#write-a-flow-with-parameters,False,786.0,144.0
389,Run the Flow,"You can run the flow using the generic command and the default value will be used:
 
 
 ```bash
 python parameter_flow.py run
 ```
 
 <CodeOutputBlock lang=""bash"">
 
 ```
      Workflow starting (run-id 1666720680013279):
      [1666720680013279/start/1 (pid 52610)] Task is starting.
      [1666720680013279/start/1 (pid 52610)] Task finished successfully.
      [1666720680013279/end/2 (pid 52613)] Task is starting.
      [1666720680013279/end/2 (pid 52613)] Learning rate value is 0.01
      [1666720680013279/end/2 (pid 52613)] Task finished successfully.
      Done!
 ```
 
 </CodeOutputBlock>
 
 Or you can pass in the parameter's value at run time:
 
 
 ```bash
 python parameter_flow.py run --lr .001
 ```
 
 <CodeOutputBlock lang=""bash"">
 
 ```
      Workflow starting (run-id 1666720681347916):
      [1666720681347916/start/1 (pid 52631)] Task is starting.
      [1666720681347916/start/1 (pid 52631)] Task finished successfully.
      [1666720681347916/end/2 (pid 52634)] Task is starting.
      [1666720681347916/end/2 (pid 52634)] Learning rate value is 0.001
      [1666720681347916/end/2 (pid 52634)] Task finished successfully.
      Done!
 ```
 
 </CodeOutputBlock>
 
 Congratulations on finishing the first season of the tutorial! 
 
 In this season you have seen:
 * In Metaflow, a flow defines the structure of your code.
 * You can create flows by wrapping a `FlowSpec` object in a python script.  
 * Flows can store data using the `self` keyword. These objects are referred to as artifacts.
 * Flow data can be accessed from previous steps, or after the flow using the client API. 
 * You can pass values to your flows at run time using parameters.
 
 In the next season, you will see how to apply many of these concepts in machine learning workflows.",H3,https://outerbounds.com/docs/intro-tutorial-S1E4#run-the-flow,False,1776.0,315.0
390,Recommender Systems with Metaflow: Beginner Tutorial,"This tutorial on RecSys is the first adventure in training recommender systems using Metaflow to provide a scalable workflow you can use for experimentation and production. The goal is to develop a relatively simple, effective, and general pipeline for sequential recommendations, and show how you can use popular open-source libraries and tools including DuckDB, Gensim, Metaflow, and Keras to build a fully working cloud endpoint that serves predictions in real-time, starting from raw data.
 
 <CustomCode height=""60vh"" sandboxLink=""https://account.outerbounds.dev/account/?workspace=/home/workspace/workspaces/recsys/workspace.code-workspace"">
 
 ```python title=""Recommender System with Metaflow""",H1,https://outerbounds.com/docs/recsys-tutorial-overview#recommender-systems-with-metaflow-beginner-tutorial,False,701.0,84.0
391,Objectives,"Our use case is: given a training set made by music playlists (list of songs hand-curated by users), can we suggest what to listen to next when presented with a new song?
 
 By following along you will learn how to:
 * take a recommender system idea from prototype to real-time production;
 * leverage Metaflow to train different versions of the same model and pick the best one;
 * use Metaflow cards to save important details about model performance;
 * package a representation of your data in a Keras object that you can deploy directly from the flow to a cloud endpoint with AWS Sagemaker.
 
 ![](../../../../static/assets/recsys-tutorial-RecSysSagemakerDeployment.png)",H2,https://outerbounds.com/docs/recsys-tutorial-overview#objectives,False,674.0,107.0
392,Prerequisites,"This tutorial does not assume knowledge about recommender systems, but does assume the following:
 * you are familiar with the basic concepts of Metaflow (flow, steps, tasks, client API, etc.), know how to run a flow;
 * you are familiar with the basic concepts of machine learning such as training, validation, and test split for model evaluation.
 
 If you are new to Metaflow we recommend starting with the [introductory tutorials](/docs/intro-tutorial-overview) to get up to speed on the basics of Metaflow before returning to this tutorial.
 
 ![](/assets/intro-tutorial-dag-S1E1.png)
 
 Bonus points (no worries, we will explain all of this) if you:
 * have experience with model serving;
 * know a little bit about what embeddings or neural networks are;
 * are comfortable with AWS concepts such as storing data in S3 and using SageMaker.",H2,https://outerbounds.com/docs/recsys-tutorial-overview#prerequisites,False,846.0,134.0
393,Tutorial Structure,"The content includes the following:
 * [Episode 1](/docs/recsys-tutorial-L1): Understand the Data
 * [Episode 2](/docs/recsys-tutorial-L2): Query Faster with DuckDB
 * [Episode 3](/docs/recsys-tutorial-L3): Build a Recommender Model
 * [Episode 4](/docs/recsys-tutorial-L4): Tune Hyperparameters
 * [Episode 5](/docs/recsys-tutorial-L5): Analyze Results and Iterate
 * [Episode 6](/docs/recsys-tutorial-L6): Make Real-time Predictions
 
 Each episode contains either a Metaflow script to run or a Jupyter notebook. 
 You do not need access to cloud computing or a [Metaflow deployment](/engineering/welcome/) to complete the first five episodes.
 If you want to run the final episode that deploys your model to an endpoint, you will need access to an AWS account that has an IAM role that can execute operations on Sagemaker. We will explain these details in that episode. As always, feel free to reach us in the #ask-metaflow channel on [Slack](http://slack.outerbounds.co/) if you need help deploying Metaflow on your infrastructure! 
 The estimated time to complete all episodes is 1-2 hours.",H2,https://outerbounds.com/docs/recsys-tutorial-overview#tutorial-structure,False,1095.0,151.0
394,Why Metaflow?,"The main benefit of using a data science workflow solution like Metaflow when prototyping is that your code will be built on a strong foundation for deploying to a production environment. Metaflow is most useful when projects have scaling requirements, are mission-critical, and/or have many interacting parts. You can read more at these links:
 * [Should I Use Metaflow?](https://docs.metaflow.org/introduction/what-is-metaflow#should-i-use-metaflow)
 * [Why Metaflow?](https://docs.metaflow.org/introduction/why-metaflow)
 
 After completing the lessons, you can transfer insights and code from the tutorial to your real-world data science projects. This is a beginner tutorial so it will not reflect many important challenges to consider in production ML environments. For example, in production, you may consider using Metaflow features such as the `@conda` decorator for dependency management, `@batch` or `@kubernetes` for remote execution, and `@schedule` to automatically trigger jobs.",H2,https://outerbounds.com/docs/recsys-tutorial-overview#why-metaflow,False,993.0,131.0
395,Recommender System Resources,"* A gentle introduction to [RecSys concepts](https://github.com/jacopotagliabue/jacopotagliabue.github.io/blob/gh-pages/public/Wild_Wild_Tests_MLOPS_World_2022.pdf), with a focus on metrics and testing.
 * A research paper on the methodology ([prod2vec](https://arxiv.org/abs/2007.14906)) we adopted to solve the use case demonstrated in this tutorial.",H2,https://outerbounds.com/docs/recsys-tutorial-overview#recommender-system-resources,False,352.0,33.0
396,Run in your Browser,"<OpenSandbox fileName=""Recommender System with Metaflow"" link=""https://account.outerbounds.dev/account/?workspace=/home/workspace/workspaces/recsys/workspace.code-workspace"">
 </OpenSandbox>",H2,https://outerbounds.com/docs/recsys-tutorial-setup#run-in-your-browser,False,190.0,7.0
397,Run on your own Infrastructure,"To run this tutorial locally or with your own Metaflow deployment, you will need to clone the repository, and then install dependencies using an environment manager of your choice. In the repository you will find the necessary dependencies in a `env.yml` file for conda. You can install them by following these instructions.",H2,https://outerbounds.com/docs/recsys-tutorial-setup#run-on-your-own-infrastructure,False,324.0,52.0
398,Clone the Repository,"```
 git clone https://github.com/outerbounds/tutorials.git
 ```",H3,https://outerbounds.com/docs/recsys-tutorial-setup#clone-the-repository,False,64.0,5.0
399,Install Dependencies with Conda,"To run the tutorial, you need to install the required dependencies via conda. We have included a conda environment in the [env.yml](https://github.com/outerbounds/tutorials/recsys/env.yml) file.  You can install and activate the environment with this command:
 
 > We are using [`mamba`](https://mamba.readthedocs.io/en/latest/) instead of `conda` because it is significantly faster. However, you can use `conda` if you want to.
 
 ```
 cd tutorials/recsys
 conda install mamba -n base -c conda-forge
 mamba env create -f env.yml
 conda activate mf-tutorial-recsys
 ```",H3,https://outerbounds.com/docs/recsys-tutorial-setup#install-dependencies-with-conda,False,569.0,77.0
400,Running the Tutorials,Please follow the instructions in each lesson for running either the associated Python script or Jupyter Notebook.,H3,https://outerbounds.com/docs/recsys-tutorial-setup#running-the-tutorials,False,114.0,17.0
401,Embeddings and modeling,"Now we are ready to add model training to our flow. Let's leverage the datasets we prepared with duckDB to train an embedding model for the song data. If you want to follow along, you can find the code in this [flow](https://github.com/outerbounds/tutorials/blob/main/recsys/embed_and_model.py). In particular, `self.generate_embeddings` trains embeddings for the songs in our dataset. 
 
 The intuition for the modeling approach comes from word2vec, an NLP technique that produces a embedding space for [words based on their distribution]( https://www.coveo.com/blog/clothes-in-space-real-time-personalization-in-less-than-100-lines-of-code/ ): words that are similar tend to occur in similar contexts. Word2Vec is an algorithm that uses a neural network model to learn word associations by mapping each word to a vector. The mathematical space these vectors live in is the embedding space. At prediction time, we use this space as a way encode input data in a space where we can run the [k-nearest neighbors (KNN)](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm) algorithm to classify and predict the next song. In this example we compute a standard information retrieval metric - hit rate - to get a sense of how well the model is performing. Again, this whole process is to produce a model that, given a list of songs a user has listened to, learns to suggest a next song the user.  
 
 When the latent space has been trained, our candidate model is tested once again, but this time on the unseen test set. The performance on the test set is our estimate of the how the performance of the model will generalize. While summarizing a model with one quantitative test is convenient when getting started with recommender systems, in practice it is recommended to supplement these tests with more robust assessments (https://reclist.io/).",H3,https://outerbounds.com/docs/recsys-tutorial-L3/#embeddings-and-modeling,False,1850.0,281.0
402,Extend the flow,"In `RecModelTrainingFlow`, you will see a few new steps after the `prepare_dataset` step you saw in the [last episode](/docs/recsys-tutorial-L2/). 
 * The `generate_embeddings` step uses Word2Vec to process the output of `prepare_dataset`.
 * The `predict_next_track` function uses a k-nearest neighbors model to predict the next best song given the embeddings of songs we know the listener enjoys.
 
 ![](/assets/recsys-tutorial-RecModelTrainingFlow.png)
 
 <CustomCode height=""60vh"">
 
 
 
 ```py title=""embed_and_model.py""
 from metaflow import FlowSpec, step, S3, Parameter, current
 import os
 import json
 import time
 from random import choice
 
 
 class RecModelTrainingFlow(FlowSpec):
 
     IS_DEV = Parameter(
         name='is_dev',
         help='Flag for dev development, with a smaller dataset',
         default='1'
     )
 
     #highlight-start
     KNN_K = Parameter(
         name='knn_k',
         help='Number of neighbors we retrieve from the vector space',
         default='100'
     ) 
     #highlight-end
 
     @step
     def start(self):
         print(""flow name: %s"" % current.flow_name)
         print(""run id: %s"" % current.run_id)
         print(""username: %s"" % current.username)
         if self.IS_DEV == '1':
             print(""ATTENTION: RUNNING AS DEV VERSION - DATA WILL BE SUB-SAMPLED!!!"")
         self.next(self.prepare_dataset)
 
     @step
     def prepare_dataset(self):
         """"""
         Get the data in the right shape by reading the parquet dataset
         and using duckdb SQL-based wrangling to quickly prepare the datasets for
         training our Recommender System.
         """"""
         import duckdb
         import numpy as np
         con = duckdb.connect(database=':memory:')
         con.execute(""""""
             CREATE TABLE playlists AS 
             SELECT *, 
             CONCAT (user_id, '-', playlist) as playlist_id,
             CONCAT (artist, '|||', track) as track_id,
             FROM 'cleaned_spotify_dataset.parquet'
             ;
         """""")
         con.execute(""SELECT * FROM playlists LIMIT 1;"")
         print(con.fetchone())
         
         tables = ['row_id', 'user_id', 'track_id', 'playlist_id', 'artist']
         for t in tables:
             con.execute(""SELECT COUNT(DISTINCT({})) FROM playlists;"".format(t))
             print(""# of {}"".format(t), con.fetchone()[0])
             
         sampling_cmd = ''
         if self.IS_DEV == '1':
             print(""Subsampling data, since this is DEV"")
             sampling_cmd = ' USING SAMPLE 10 PERCENT (bernoulli)'
             
         dataset_query = """"""
             SELECT * FROM
             (   
                 SELECT 
                     playlist_id,
                     LIST(artist ORDER BY row_id ASC) as artist_sequence,
                     LIST(track_id ORDER BY row_id ASC) as track_sequence,
                     array_pop_back(LIST(track_id ORDER BY row_id ASC)) as track_test_x,
                     LIST(track_id ORDER BY row_id ASC)[-1] as track_test_y
                 FROM 
                     playlists
                 GROUP BY playlist_id 
                 HAVING len(track_sequence) > 2
             ) 
             {}
             ;
             """""".format(sampling_cmd)
         
         con.execute(dataset_query)
         df = con.fetch_df()
         print(""# rows: {}"".format(len(df)))
         print(df.iloc[0].tolist())
         con.close()
 
         train, validate, test = np.split(
             df.sample(frac=1, random_state=42), 
             [int(.7 * len(df)), int(.9 * len(df))])
         
         self.df_dataset = df
         self.df_train = train
         self.df_validate = validate
         self.df_test = test
         print(""# testing rows: {}"".format(len(self.df_test)))
         
         self.hyper_string = json.dumps({ 
             'min_count': 3, 
             'epochs': 30, 
             'vector_size': 48, 
             'window': 10,
             'ns_exponent': 0.75 })
         
         self.next(self.generate_embeddings)
 
     #highlight-next-line
     def predict_next_track(self, vector_space, input_sequence, k):
         """"""        
         Given an embedding space, predict best next song with KNN.
         Initially, we just take the LAST item in the input playlist as the query item for KNN
         and retrieve the top K nearest vectors (you could think of taking the smoothed average embedding
         of the input list, for example, as a refinement).
 
         If the query item is not in the vector space, we make a random bet. We could refine this by taking
         for example the vector of the artist (average of all songs), or with some other strategy (sampling
         by popularity). 
 
         For more options on how to generate vectors for ""cold items"" see for example the paper:
         https://dl.acm.org/doi/10.1145/3383313.3411477
         """"""
         query_item = input_sequence[-1]
         if query_item not in vector_space:
             query_item = choice(list(vector_space.index_to_key))
         
         return [_[0] for _ in vector_space.most_similar(query_item, topn=k)]
 
     #highlight-next-line
     def evaluate_model(self, _df, vector_space, k):
         lambda_predict = lambda row: self.predict_next_track(vector_space, row['track_test_x'], k)
         _df['predictions'] = _df.apply(lambda_predict, axis=1)
         lambda_hit = lambda row: 1 if row['track_test_y'] in row['predictions'] else 0
         _df['hit'] = _df.apply(lambda_hit, axis=1)
         hit_rate = _df['hit'].sum() / len(_df)
         return hit_rate
 
     #highlight-start
     @step
     def generate_embeddings(self):
     #highlight-end
         """"""
         Generate vector representations for songs, based on the Prod2Vec idea.
 
         For an overview of the algorithm and the evaluation, see for example:
         https://arxiv.org/abs/2007.14906
         """"""
         from gensim.models.word2vec import Word2Vec
         self.hypers = json.loads(self.hyper_string)
         track2vec_model = Word2Vec(self.df_train['track_sequence'], **self.hypers)
         print(""Training with hypers {} is completed!"".format(self.hyper_string))
         print(""Vector space size: {}"".format(len(track2vec_model.wv.index_to_key)))
         test_track = choice(list(track2vec_model.wv.index_to_key))
         print(""Example track: '{}'"".format(test_track))
         test_vector = track2vec_model.wv[test_track]
         print(""Test vector for '{}': {}"".format(test_track, test_vector[:5]))
         test_sims = track2vec_model.wv.most_similar(test_track, topn=3)
         print(""Similar songs to '{}': {}"".format(test_track, test_sims))
         self.validation_metric = self.evaluate_model(
             self.df_validate,
             track2vec_model.wv,
             k=int(self.KNN_K))
         print(""Hit Rate@{} is: {}"".format(self.KNN_K, self.validation_metric))
         self.track_vectors = track2vec_model.wv
         self.next(self.model_testing)
 
     #highlight-start
     @step
     def model_testing(self):
     #highlight-end
         """"""
         Test the generalization abilities of the best model by running predictions
         on the unseen test data.
 
         We report a quantitative point-wise metric, hit rate @ K, as an initial implementation. However,
         evaluating recommender systems is a very complex task, and better metrics, through good abstractions, 
         are available, i.e. https://reclist.io/.
         """"""
         self.test_metric = self.evaluate_model(
             self.df_test,
             self.track_vectors,
             k=int(self.KNN_K))
         print(""Hit Rate@{} on the test set is: {}"".format(self.KNN_K, self.test_metric))
         self.next(self.end)
 
     @step
     def end(self):
         pass
 
 if __name__ == '__main__':
     RecModelTrainingFlow()
 ```
 
 </CustomCode>",H3,https://outerbounds.com/docs/recsys-tutorial-L3/#extend-the-flow,False,7883.0,2227.0
403,Run your flow,"```bash
 python embed_and_model.py run
 ```
 
 <CodeOutputBlock lang=""bash"">
 
 ```
      Workflow starting (run-id 188128):
      [188128/start/1014265 (pid 80106)] Task is starting.
      [188128/start/1014265 (pid 80106)] flow name: RecModelTrainingFlow
      [188128/start/1014265 (pid 80106)] run id: 188128
      [188128/start/1014265 (pid 80106)] username: eddie
      [188128/start/1014265 (pid 80106)] ATTENTION: RUNNING AS DEV VERSION - DATA WILL BE SUB-SAMPLED!!!
      [188128/start/1014265 (pid 80106)] Task finished successfully.
      [188128/prepare_dataset/1014266 (pid 80110)] Task is starting.
      [188128/prepare_dataset/1014266 (pid 80110)] (0, '9cc0cfd4d7d7885102480dd99e7a90d6', 'Elvis Costello', '(The Angels Wanna Wear My) Red Shoes', 'HARD ROCK 2010', '9cc0cfd4d7d7885102480dd99e7a90d6-HARD ROCK 2010', 'Elvis Costello|||(The Angels Wanna Wear My) Red Shoes')
      [188128/prepare_dataset/1014266 (pid 80110)] # of row_id 12891680
      [188128/prepare_dataset/1014266 (pid 80110)] # of user_id 15918
      [188128/prepare_dataset/1014266 (pid 80110)] # of track_id 2819059
      [188128/prepare_dataset/1014266 (pid 80110)] # of playlist_id 232369
      [188128/prepare_dataset/1014266 (pid 80110)] # of artist 289821
      [188128/prepare_dataset/1014266 (pid 80110)] Subsampling data, since this is DEV
      [188128/prepare_dataset/1014266 (pid 80110)] # rows: 21913
      [188128/prepare_dataset/1014266 (pid 80110)] ['f502838601e234f5a7299588546c0070-Babooshka', ['Kate Bush', 'Joni Mitchell', 'Kate Bush', 'Kate Bush', 'Kate Bush', 'Kate Bush'], ['Kate Bush|||Babooshka', 'Joni Mitchell|||Both Sides Now', 'Kate Bush|||Hounds Of Love', 'Kate Bush|||The Man With The Child In His Eyes', ""Kate Bush|||This Woman's Work"", 'Kate Bush|||Wuthering Heights'], ['Kate Bush|||Babooshka', 'Joni Mitchell|||Both Sides Now', 'Kate Bush|||Hounds Of Love', 'Kate Bush|||The Man With The Child In His Eyes', ""Kate Bush|||This Woman's Work""], 'Kate Bush|||Wuthering Heights']
      [188128/prepare_dataset/1014266 (pid 80110)] # testing rows: 2192
      [188128/prepare_dataset/1014266 (pid 80110)] Task finished successfully.
      [188128/generate_embeddings/1014267 (pid 80150)] Task is starting.
      [188128/generate_embeddings/1014267 (pid 80150)] Training with hypers {""min_count"": 3, ""epochs"": 30, ""vector_size"": 48, ""window"": 10, ""ns_exponent"": 0.75} is completed!
      [188128/generate_embeddings/1014267 (pid 80150)] Vector space size: 63350
      [188128/generate_embeddings/1014267 (pid 80150)] Example track: 'Faith No More|||Malpractice'
      [188128/generate_embeddings/1014267 (pid 80150)] Test vector for 'Faith No More|||Malpractice': [ 0.14538704 -0.37973246  0.0037712  -0.01834163  0.6416138 ]
      [188128/generate_embeddings/1014267 (pid 80150)] Similar songs to 'Faith No More|||Malpractice': [('Audioslave|||Man Or Animal', 0.9970235228538513), ('Living Colour|||Love Rears Its Ugly Head', 0.9931139349937439), ('A Perfect Circle|||Magdalena', 0.9929249286651611)]
      [188128/generate_embeddings/1014267 (pid 80150)] Hit Rate@100 is: 0.0807850296668188
      [188128/generate_embeddings/1014267 (pid 80150)] Task finished successfully.
      [188128/model_testing/1014268 (pid 80194)] Task is starting.
      [188128/model_testing/1014268 (pid 80194)] Hit Rate@100 on the test set is: 0.08074817518248176
      [188128/model_testing/1014268 (pid 80194)] Task finished successfully.
      [188128/end/1014269 (pid 80239)] Task is starting.
      [188128/end/1014269 (pid 80239)] Task finished successfully.
      Done!
 ```
 
 </CodeOutputBlock>
 
 In this lesson, you extended the `DataFlow` to incorporate feature embeddings and model training. 
 Next, you will see how to improve our embedding by tuning associated hyperparameters in parallel, leveraging Metaflow's ability to parallelize your code locally or on remote machines with minimal changes to the code.",H3,https://outerbounds.com/docs/recsys-tutorial-L3/#run-your-flow,False,3917.0,569.0
404,How to deploy our model behind an endpoint,"The final episode of this tutorial extends our flow once more into a full-fledged, end-to-end workflow. We introduce a few new parameters to govern SageMaker, the AWS service for hosted ML inference. You can follow along with in this [flow](https://github.com/outerbounds/tutorials/blob/main/recsys/recsys_deploy.py). 
 
 The idea is pretty simple: since now we have a (versioned) artifact that is our tested model, how do we deploy it in real life so that users can get recommendations on what songs should be next in their digital radio?",H3,https://outerbounds.com/docs/recsys-tutorial-L6/#how-to-deploy-our-model-behind-an-endpoint,False,539.0,83.0
405,Using SageMaker and Metaflow,"There are many possible solutions for deploying a KNN model. We pick Sagemaker endpoints here for a few reasons:
 * As an AWS resource, you can spin up and delete the endpoint from Python, directly in your flow - no other configuration is needed!
 * If you're using Metaflow with AWS as a data store (recommended in general, and required by the code below), SageMaker deployments are an elegant way to leverage Metaflow artifact storage: in fact, just point SageMaker to the model on s3!
 * SageMaker is easier to use with one of the pre-defined model types - in this case Tensorflow. In fact, our deployment strategy for the KNN-based model we trained is to first ""export"" it to a [TF-Recs](https://www.tensorflow.org/recommenders) model with keras (the function `keras_model`), and then deploy it to SageMaker with their TensorFlowModel abstraction. If you wish, you can build your own container and serve predictions directly from the gensim model that we trained - for simplicity, and to showcase another open-source library, we opted here to convert the space to a TF model.
 
 Note that after deployment, getting actual, live predictions in code is as easy as calling `result = predictor.predict(input)`.",H3,https://outerbounds.com/docs/recsys-tutorial-L6/#using-sagemaker-and-metaflow,False,1210.0,197.0
406,Setup,"You will need access to the appropriate SageMaker execution role to run this code. You can read more about the `AmazonSageMakerFullAccess` IAM role, as well as more granular permissions in this [guide](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html). Once this role is in your IAM policy, we can use it to create SageMaker resources for tasks like training jobs and deploying models to endpoints. 
 
 :::caution
 Please consider that:
 * SageMaker allows you to pick a Docker image and hardware. The image we picked is compatible with TensorFlow models. The hardware can be changed but please do so with caution as SageMaker can be very expensive;
 * the code in the following flow automatically deletes the endpoint after making one test prediction - this is to save money. If you wish to test the endpoint for longer, comment out the delete endpoint line and use the same Python code to get predictions from a notebook, for example.
 :::",H4,https://outerbounds.com/docs/recsys-tutorial-L6/#setup,False,961.0,153.0
407,Deploy your model from a flow!,"This flow extends [episode 4](/docs/recsys-tutorial-L4/) where we trained several models in parallel. The new steps include
 * The `keras_model` function helps us package our KNN model using generic TensorFlow abstractions so we can leverage the TensorFlow and Sagemaker integration.
 * The `build_retrieval_model` calls the `keras_model` function, zips up the model, and sends it to S3 using Metaflow's built-in S3 client. 
 * The `deploy` step calls the `build_retrieval_model` function and deploys the resulting model as a Sagemaker endpoint with our choice of the underlying Docker image and hardware capabilities. 
 
 ![](../../../../static/assets/recsys-tutorial-RecSysSagemakerDeployment.png)
 
 <CustomCode height=""60vh"">
 
 
 
 ```python",H3,https://outerbounds.com/docs/recsys-tutorial-L6/#deploy-your-model-from-a-flow,False,746.0,99.0
408,Run your flow,"To run this flow and deploy to Sagemaker, you will need:
 * Access to a Metaflow deployment on AWS. Reach us on [Slack](http://slack.outerbounds.co/) if you need help getting set up!
 * Your active Metaflow profile to be configured with an S3 as the `DATASTORE_SYSROOT_S3` variable. You can find the default config at `$HOME/.metaflowconfig/config.json`. Read more [here](/docs/use-multiple-metaflow-configs/).
 * To set the argument `--sagemaker_deploy 1`. 
 * To set the argument `--sagemaker_role <YOUR SAGEMAKER EXECUTION ROLE>`.
 
 ```bash
 python recsys_deploy.py run --sagemaker_deploy 1 --sagemaker_role <SAGEMAKER_ROLE>
 ```
 
 <CodeOutputBlock lang=""bash"">
 
 ```
      Workflow starting (run-id 187483):
      [187483/start/1012249 (pid 74483)] Task is starting.
      [187483/start/1012249 (pid 74483)] flow name: RecSysSagemakerDeployment
      [187483/start/1012249 (pid 74483)] run id: 187483
      [187483/start/1012249 (pid 74483)] username: eddie
      [187483/start/1012249 (pid 74483)] ATTENTION: RUNNING AS DEV VERSION - DATA WILL BE SUB-SAMPLED!!!
      [187483/start/1012249 (pid 74483)] ATTENTION: DEPLOYMENT TO SAGEMAKER IS ENABLED!
      [187483/start/1012249 (pid 74483)] Task finished successfully.
      [187483/prepare_dataset/1012250 (pid 74487)] Task is starting.
      [187483/prepare_dataset/1012250 (pid 74487)] (0, '9cc0cfd4d7d7885102480dd99e7a90d6', 'Elvis Costello', '(The Angels Wanna Wear My) Red Shoes', 'HARD ROCK 2010', '9cc0cfd4d7d7885102480dd99e7a90d6-HARD ROCK 2010', 'Elvis Costello|||(The Angels Wanna Wear My) Red Shoes')
      [187483/prepare_dataset/1012250 (pid 74487)] # of row_id 12891680
      [187483/prepare_dataset/1012250 (pid 74487)] # of user_id 15918
      [187483/prepare_dataset/1012250 (pid 74487)] # of track_id 2819059
      [187483/prepare_dataset/1012250 (pid 74487)] # of playlist_id 232369
      [187483/prepare_dataset/1012250 (pid 74487)] # of artist 289821
      [187483/prepare_dataset/1012250 (pid 74487)] Subsampling data, since this is DEV
      [187483/prepare_dataset/1012250 (pid 74487)] # rows: 21521
      [187483/prepare_dataset/1012250 (pid 74487)] ['59f776b2f9de1de9969bc43bcd3fca59-Rush', ['Iron Maiden', 'Rush', 'Rush', 'Judas Priest'], ['Iron Maiden|||2 Minutes To Midnight', 'Rush|||Limelight', 'Rush|||Tom Sawyer', ""Judas Priest|||You've Got Another Thing Comin'""], ['Iron Maiden|||2 Minutes To Midnight', 'Rush|||Limelight', 'Rush|||Tom Sawyer'], ""Judas Priest|||You've Got Another Thing Comin'""]
      [187483/prepare_dataset/1012250 (pid 74487)] # testing rows: 2153
      [187483/prepare_dataset/1012250 (pid 74487)] Foreach yields 3 child steps.
      [187483/prepare_dataset/1012250 (pid 74487)] Task finished successfully.
      [187483/generate_embeddings/1012251 (pid 74518)] Task is starting.
      [187483/generate_embeddings/1012252 (pid 74521)] Task is starting.
      [187483/generate_embeddings/1012253 (pid 74524)] Task is starting.
      [187483/generate_embeddings/1012253 (pid 74524)] Training with hypers {""min_count"": 10, ""epochs"": 30, ""vector_size"": 48, ""window"": 10, ""ns_exponent"": 0.75} is completed!
      [187483/generate_embeddings/1012253 (pid 74524)] Vector space size: 9379
      [187483/generate_embeddings/1012253 (pid 74524)] Example track: 'Allah-Las|||Catamaran'
      [187483/generate_embeddings/1012253 (pid 74524)] Test vector for 'Allah-Las|||Catamaran': [-0.5164605  -0.39848462  0.48366475  0.06304073  0.16176549]
      [187483/generate_embeddings/1012253 (pid 74524)] Similar songs to 'Allah-Las|||Catamaran': [('Kimbra|||Cameo Lover', 0.965700626373291), ('Augustines|||Chapel Song', 0.9608513712882996), ('My Morning Jacket|||Circuital', 0.960833728313446)]
      [187483/generate_embeddings/1012253 (pid 74524)] Hit Rate@100 is: 0.06342936802973978
      [187483/generate_embeddings/1012251 (pid 74518)] Training with hypers {""min_count"": 3, ""epochs"": 30, ""vector_size"": 48, ""window"": 10, ""ns_exponent"": 0.75} is completed!
      [187483/generate_embeddings/1012251 (pid 74518)] Vector space size: 62838
      [187483/generate_embeddings/1012251 (pid 74518)] Example track: 'Queen|||Crazy Little Thing Called Love - 2011 Remaster'
      [187483/generate_embeddings/1012251 (pid 74518)] Test vector for 'Queen|||Crazy Little Thing Called Love - 2011 Remaster': [-0.40840358 -0.28218955 -0.67459786  0.5966493   0.71324164]
      [187483/generate_embeddings/1012251 (pid 74518)] Similar songs to 'Queen|||Crazy Little Thing Called Love - 2011 Remaster': [('Brandon Flowers|||Crossfire', 0.9929628372192383), ('Styx|||Crystal Ball', 0.9924074411392212), ('Madonna|||Crazy for You', 0.9904692769050598)]
      [187483/generate_embeddings/1012252 (pid 74521)] Training with hypers {""min_count"": 5, ""epochs"": 30, ""vector_size"": 48, ""window"": 10, ""ns_exponent"": 0.75} is completed!
      [187483/generate_embeddings/1012252 (pid 74521)] Vector space size: 28303
      [187483/generate_embeddings/1012252 (pid 74521)] Example track: 'Bone Thugs-N-Harmony|||1st Of Tha Month'
      [187483/generate_embeddings/1012252 (pid 74521)] Test vector for 'Bone Thugs-N-Harmony|||1st Of Tha Month': [-0.03574348 -0.05404772  0.24944994 -0.16587584  0.324895  ]
      [187483/generate_embeddings/1012252 (pid 74521)] Similar songs to 'Bone Thugs-N-Harmony|||1st Of Tha Month': [('Ana Tijoux|||1977', 0.885529637336731), ('Hamilton Leithauser|||Alexandra', 0.8741297125816345), ('Ja Rule|||Always On Time', 0.8725541234016418)]
      [187483/generate_embeddings/1012251 (pid 74518)] Hit Rate@100 is: 0.07969330855018587
      [187483/generate_embeddings/1012252 (pid 74521)] Hit Rate@100 is: 0.08921933085501858
      [187483/generate_embeddings/1012253 (pid 74524)] Task finished successfully.
      [187483/generate_embeddings/1012251 (pid 74518)] Task finished successfully.
      [187483/generate_embeddings/1012252 (pid 74521)] Task finished successfully.
      [187483/join_runs/1012254 (pid 74569)] Task is starting.
      [187483/join_runs/1012254 (pid 74569)] Current result map: {'{""min_count"": 3, ""epochs"": 30, ""vector_size"": 48, ""window"": 10, ""ns_exponent"": 0.75}': 0.07969330855018587, '{""min_count"": 5, ""epochs"": 30, ""vector_size"": 48, ""window"": 10, ""ns_exponent"": 0.75}': 0.08921933085501858, '{""min_count"": 10, ""epochs"": 30, ""vector_size"": 48, ""window"": 10, ""ns_exponent"": 0.75}': 0.06342936802973978}
      [187483/join_runs/1012254 (pid 74569)] The best validation score is for model: {""min_count"": 5, ""epochs"": 30, ""vector_size"": 48, ""window"": 10, ""ns_exponent"": 0.75}, 0.08921933085501858
      [187483/join_runs/1012254 (pid 74569)] Task finished successfully.
      [187483/model_testing/1012255 (pid 74580)] Task is starting.
      [187483/model_testing/1012255 (pid 74580)] Hit Rate@100 on the test set is: 0.08360427310729215
      [187483/model_testing/1012255 (pid 74580)] Task finished successfully.
      [187483/deploy/1012256 (pid 74588)] Task is starting.
      [187483/deploy/1012256 (pid 74588)] Vector space dims: 48
      [187483/deploy/1012256 (pid 74588)] (28303, 48) (1, 48)
      [187483/deploy/1012256 (pid 74588)] (28304, 48)
      [187483/deploy/1012256 (pid 74588)] [-0.44227722  0.62302357  2.6069138 ]
      [187483/deploy/1012256 (pid 74588)] tf.Tensor([-0.44227722  0.62302357  2.6069138 ], shape=(3,), dtype=float32)
      [187483/deploy/1012256 (pid 74588)] Test unknown id:
      [187483/deploy/1012256 (pid 74588)] tf.Tensor([0. 0. 0.], shape=(3,), dtype=float32)
      [187483/deploy/1012256 (pid 74588)] Recommendations after track 'Daft Punk|||Get Lucky - Radio Edit': [b'Daft Punk|||Get Lucky - Radio Edit' b'Daft Punk|||Get Lucky'
      [187483/deploy/1012256 (pid 74588)] WARNING:absl:Found untraced functions such as query_with_exclusions while saving (showing 1 of 1). These functions will not be directly callable after loading.
      [187483/deploy/1012256 (pid 74588)] b'PSY|||Gangnam Style (\xea\xb0\x95\xeb\x82\xa8\xec\x8a\xa4\xed\x83\x80\xec\x9d\xbc)']
      [187483/deploy/1012256 (pid 74588)] 
      [187483/deploy/1012256 (pid 74588)] WARNING:sagemaker.deprecations:update_endpoint is a no-op in sagemaker>=2.
      [187483/deploy/1012256 (pid 74588)] 
      [187483/deploy/1012256 (pid 74588)] ================
      [187483/deploy/1012256 (pid 74588)] Endpoint name is: playlist-recs-1669682006179-endpoint
      [187483/deploy/1012256 (pid 74588)] 
      [187483/deploy/1012256 (pid 74588)] 
      [187483/deploy/1012256 (pid 74588)] ------------!{'instances': array(['Daft Punk|||Get Lucky - Radio Edit'], dtype='<U34')} {'predictions': [{'output_1': [327.358551, 309.735748, 239.649109, 236.526245, 230.304749, 227.55574, 226.75943, 226.298782, 225.014877, 221.921616], 'output_2': ['Daft Punk|||Get Lucky - Radio Edit', 'Daft Punk|||Get Lucky', 'PSY|||Gangnam Style (강남스타일)', 'Avicii|||Hey Brother', 'Pharrell Williams|||Happy - From Despicable Me 2""""', 'Pharrell Williams|||Happy', 'Katy Perry|||Firework', 'Daft Punk|||Giorgio by Moroder', ""Swedish House Mafia|||Don't You Worry Child (Radio Edit) [feat. John Martin]"", 'Swedish House Mafia|||Greyhound']}]}
      1 task is running: deploy (1 running; 0 done).
      No tasks are waiting in the queue.
      end step has not started
      [187483/deploy/1012256 (pid 74588)] Deleting endpoint now...
      [187483/deploy/1012256 (pid 74588)] Endpoint deleted!
      [187483/deploy/1012256 (pid 74588)] See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.
      [187483/deploy/1012256 (pid 74588)] Task finished successfully.
      [187483/end/1012257 (pid 74676)] Task is starting.
      [187483/end/1012257 (pid 74676)] All done
      [187483/end/1012257 (pid 74676)] 
      [187483/end/1012257 (pid 74676)] See you, space cowboy
      [187483/end/1012257 (pid 74676)] 
      [187483/end/1012257 (pid 74676)] Task finished successfully.
      Done!
 ```
 
 </CodeOutputBlock>",H3,https://outerbounds.com/docs/recsys-tutorial-L6/#run-your-flow,False,9836.0,1383.0
409,Conclusion,"Congratulations, you have completed Metaflow's introductory tutorial on recommender system workflows! You have learned how to:
 * take a recommender system idea from prototype to real-time production;
 * leverage Metaflow to train different versions of the same model and pick the best one;
 * use Metaflow cards to save important details about model performance;
 * package a representation of your data in a keras object that you can deploy directly from the flow to a cloud endpoint with AWS Sagemaker.
 
 To keep progressing in your Metaflow journey you can:
 * Check out the open-source [repository](https://github.com/Netflix/metaflow).
 * Join our [Slack community](http://slack.outerbounds.co/) and learn with us in #ask-metaflow.
     * We are actively working on more advanced recommender system tutorials. Please send us your suggestions and questions!",H3,https://outerbounds.com/docs/recsys-tutorial-L6/#conclusion,False,863.0,129.0
410,How to use DuckDB,"Given our dataset is in a parquet file, in this lesson you will learn how to leverage an open-source, a hyper-performant database for analytics workloads called [DuckDB](https://duckdb.org/docs/api/python/overview). You can follow along with the code in this [flow](https://github.com/outerbounds/tutorials/blob/main/recsys/data_flow.py). DuckDB has become popular as a fast way to keep Pandas DataFrame interfaces while processing data faster and consuming less memory, as demonstrated in these [public benchmarks](https://colab.research.google.com/drive/1eg_TJpPQr2tyYKWjISJlX8IEAi8Qln3U?usp=sharing) and described in this [post](https://duckdb.org/2021/05/14/sql-on-pandas.html#:~:text=For%20many%20queries%2C%20you%20can,%2C%20Pandas%2Dout%E2%80%9D). 
 
 If you're familiar with basic SQL commands, all data preparation should be immediately understandable: DuckDB reads the parquet file (`cleaned_spotify_dataset.parquet`) in memory and produces the dataset we need. Remember, a playlist with:
 
 * song_525, song_22, song_814, song_4255
 
 needs to become:
 
 * query: song_525, song_22, song_814
 * label: song_4255
 
 for our model. The label is the event we want our model to predict, so we can suggest songs the listener likes. Following standard best practices, we divide our sequences of songs into a train, validation and test dataset.",H3,https://outerbounds.com/docs/recsys-tutorial-L2/#how-to-use-duckdb,False,1348.0,162.0
411,Ingest and split data in a flow,"In this flow, you will see how to prepare the dataset using DuckDB queries. The data is then split into train, validation, and test splits. In general, it is good practice to have a validation set for choosing the best hyperparameters and a held out test set to give an estimate of performance on unseen data. Later, you will extend this flow to evaluate, tune, and deploy a model to make real-time predictions.
 
 You can also observe two more tricks in the `PlaylistRecsFlow`:
 * we declare and use an `IS_DEV` parameter to sample down the dataset in case we are iterating quickly in ""Developer Mode"". You can imagine running the flow on a few thousand lines initially just to get the logic correct, and then running it in ""Evaluation Mode"" without any sampling on the full dataset;
 * we make use of Metaflow's built-in versioning capability to version precisely the datasets we just created - this will be very important later on for debugging and inspection.
 
 ![](/assets/recsys-tutorial-DataFlow.png)
 
 <CustomCode height=""60vh"">
 
 
 
 ```py title=""data_flow.py""
 from metaflow import FlowSpec, step, S3, Parameter, current
 
 class DataFlow(FlowSpec):
 
     IS_DEV = Parameter(
         name='is_dev',
         help='Flag for dev development, with a smaller dataset',
         default='1'
     )
     
     @step
     def start(self):
         self.next(self.prepare_dataset)
 
     @step
     def prepare_dataset(self):
         """"""
         Get the data in the right shape by reading the parquet dataset
         and using DuckDB SQL-based wrangling to quickly prepare the datasets for
         training our Recommender System.
         """"""
         import duckdb
         import numpy as np
         # highlight-next-line
         con = duckdb.connect(database=':memory:')
         # highlight-start
         con.execute(""""""
             CREATE TABLE playlists AS 
             SELECT *, 
             CONCAT (user_id, '-', playlist) as playlist_id,
             CONCAT (artist, '|||', track) as track_id,
             FROM 'cleaned_spotify_dataset.parquet'
             ;
         """""")
         # highlight-end
         # highlight-next-line
         con.execute(""SELECT * FROM playlists LIMIT 1;"")
         print(con.fetchone())
         tables = ['row_id', 'user_id', 'track_id', 'playlist_id', 'artist']
         for t in tables:
             # highlight-next-line
             con.execute(""SELECT COUNT(DISTINCT({})) FROM playlists;"".format(t))
             print(""# of {}"".format(t), con.fetchone()[0])
         sampling_cmd = ''
         if self.IS_DEV == '1':
             print(""Subsampling data, since this is DEV"")
             # highlight-next-line
             sampling_cmd = ' USING SAMPLE 10 PERCENT (bernoulli)'
         # highlight-start
         dataset_query = """"""
             SELECT * FROM
             (   
                 SELECT 
                     playlist_id,
                     LIST(artist ORDER BY row_id ASC) as artist_sequence,
                     LIST(track_id ORDER BY row_id ASC) as track_sequence,
                     array_pop_back(LIST(track_id ORDER BY row_id ASC)) as track_test_x,
                     LIST(track_id ORDER BY row_id ASC)[-1] as track_test_y
                 FROM 
                     playlists
                 GROUP BY playlist_id 
                 HAVING len(track_sequence) > 2
             ) 
             {}
             ;
             """""".format(sampling_cmd)
         con.execute(dataset_query)
         df = con.fetch_df()
         # highlight-end
         print(""# rows: {}"".format(len(df)))
         print(df.iloc[0].tolist())
         con.close()
         train, validate, test = np.split(
             df.sample(frac=1, random_state=42), 
             [int(.7 * len(df)), int(.9 * len(df))])
         self.df_dataset = df
         self.df_train = train
         self.df_validate = validate
         self.df_test = test
         print(""# testing rows: {}"".format(len(self.df_test)))
         self.next(self.end)
 
     @step
     def end(self):
         pass
 
 if __name__ == '__main__':
     DataFlow()
 ```
 
 </CustomCode>",H3,https://outerbounds.com/docs/recsys-tutorial-L2/#ingest-and-split-data-in-a-flow,False,4114.0,1233.0
412,Run your flow,"```bash
 python data_flow.py run
 ```
 
 <CodeOutputBlock lang=""bash"">
 
 ```
      Workflow starting (run-id 188127):
      [188127/start/1014261 (pid 80025)] Task is starting.
      [188127/start/1014261 (pid 80025)] Task finished successfully.
      [188127/prepare_dataset/1014262 (pid 80029)] Task is starting.
      [188127/prepare_dataset/1014262 (pid 80029)] (0, '9cc0cfd4d7d7885102480dd99e7a90d6', 'Elvis Costello', '(The Angels Wanna Wear My) Red Shoes', 'HARD ROCK 2010', '9cc0cfd4d7d7885102480dd99e7a90d6-HARD ROCK 2010', 'Elvis Costello|||(The Angels Wanna Wear My) Red Shoes')
      [188127/prepare_dataset/1014262 (pid 80029)] # of row_id 12891680
      [188127/prepare_dataset/1014262 (pid 80029)] # of user_id 15918
      [188127/prepare_dataset/1014262 (pid 80029)] # of track_id 2819059
      [188127/prepare_dataset/1014262 (pid 80029)] # of playlist_id 232369
      [188127/prepare_dataset/1014262 (pid 80029)] # of artist 289821
      [188127/prepare_dataset/1014262 (pid 80029)] Subsampling data, since this is DEV
      [188127/prepare_dataset/1014262 (pid 80029)] # rows: 21716
      [188127/prepare_dataset/1014262 (pid 80029)] ['fdf079dbafcf8405f82917e3474c6ca4-New playlist...Gary Moore', ['Gary Moore', 'The Jezabels', 'Gary Moore', 'Gary Moore', 'Gary Moore', 'Gary Moore', 'Gary Moore', 'Gary Moore', 'Gary Moore', 'Gary Moore', 'Gary Moore', 'Gary Moore', 'Gary Moore', 'Gary Moore'], ['Gary Moore|||All Your Love [Live 1995]', 'The Jezabels|||Austerlitz', ""Gary Moore|||Can't Find My Baby"", 'Gary Moore|||Cold Day In Hell - Live', ""Gary Moore|||Jumpin' At Shadows"", 'Gary Moore|||King Of The Blues [Live 1990]', 'Gary Moore|||Midnight Blues', 'Gary Moore|||Moving On [Live 1990]', 'Gary Moore|||Need Your Love So Bad [Live 1995]', 'Gary Moore|||Oh Pretty Woman - Live', 'Gary Moore|||Parisienne Walkways - Live', 'Gary Moore|||Since I Met You Baby - Live', 'Gary Moore|||Still Got The Blues - Single Version', 'Gary Moore|||Walking By Myself - Live'], ['Gary Moore|||All Your Love [Live 1995]', 'The Jezabels|||Austerlitz', ""Gary Moore|||Can't Find My Baby"", 'Gary Moore|||Cold Day In Hell - Live', ""Gary Moore|||Jumpin' At Shadows"", 'Gary Moore|||King Of The Blues [Live 1990]', 'Gary Moore|||Midnight Blues', 'Gary Moore|||Moving On [Live 1990]', 'Gary Moore|||Need Your Love So Bad [Live 1995]', 'Gary Moore|||Oh Pretty Woman - Live', 'Gary Moore|||Parisienne Walkways - Live', 'Gary Moore|||Since I Met You Baby - Live', 'Gary Moore|||Still Got The Blues - Single Version'], 'Gary Moore|||Walking By Myself - Live']
      [188127/prepare_dataset/1014262 (pid 80029)] # testing rows: 2172
      [188127/prepare_dataset/1014262 (pid 80029)] Task finished successfully.
      [188127/end/1014263 (pid 80064)] Task is starting.
      [188127/end/1014263 (pid 80064)] Task finished successfully.
      Done!
 ```
 
 </CodeOutputBlock>
 
 In this lesson, you structured a data ingestion workflow using DuckDB and Metaflow. This pattern can be used on a wide variety of ML tasks to help you efficiently move between fast, local data storage and cloud resources. In the next lesson, you will build on this flow by building a predictive model to predict the next track to suggest.",H3,https://outerbounds.com/docs/recsys-tutorial-L2/#run-your-flow,False,3208.0,484.0
413,Access results with Metaflow's client API,"First we import the packages we need and define some config variables:
 
 
 ```python
 from metaflow import Flow
 import numpy as np
 from random import choice
 import matplotlib.pyplot as plt
 from collections import Counter
 from sklearn.manifold import TSNE
 ```
 
 
 ```python
 FLOW_NAME = 'RecSysTuningFlow'
 ```
 
 Let's retrieved the artifacts from the latest successful run. 
 The `get_latest_successful_run` uses the `metaflow.Flow` object to get results of runs using the (class) name of your flows. 
 
 
 ```python
 def get_latest_successful_run(flow_name: str):
     ""Gets the latest successful run.""
     for r in Flow(flow_name).runs():
         if r.successful: 
             return r
 ```
 
 
 ```python
 latest_run = get_latest_successful_run(FLOW_NAME)
 latest_model = latest_run.data.final_vectors
 latest_dataset = latest_run.data.final_dataset
 ```
 
 First, check all is in order by printing out datasets and rows and stats:
 
 
 ```python
 latest_dataset.head(3)
 ```
     
 <HTMLOutputBlock >
 
 
 
 
 ```html
 <div>
 <style scoped>
     .dataframe tbody tr th:only-of-type {
         vertical-align: middle;
     }
 
     .dataframe tbody tr th {
         vertical-align: top;
     }
 
     .dataframe thead th {
         text-align: right;
     }
 </style>
 <table border=""1"" class=""dataframe"">
   <thead>
     <tr style=""text-align: right;"">
       <th></th>
       <th>playlist_id</th>
       <th>artist_sequence</th>
       <th>track_sequence</th>
       <th>track_test_x</th>
       <th>track_test_y</th>
       <th>predictions</th>
       <th>hit</th>
     </tr>
   </thead>
   <tbody>
     <tr>
       <th>56437</th>
       <td>69080ca9b4d90cc7c6425ccc32626df7-Arcade Fire -...</td>
       <td>[Arcade Fire, Arcade Fire, Arcade Fire, Arcade...</td>
       <td>[Arcade Fire|||Afterlife, Arcade Fire|||Awful ...</td>
       <td>[Arcade Fire|||Afterlife, Arcade Fire|||Awful ...</td>
       <td>Arcade Fire|||You Already Know</td>
       <td>[Nick Cave &amp; The Bad Seeds|||We No Who U R, Pa...</td>
       <td>0</td>
     </tr>
     <tr>
       <th>9442</th>
       <td>0a94b98aa949dbb6c9acfd78a79671e2-Double Jointed</td>
       <td>[Mark Kozelek, Sun Kil Moon, The Flaming Lips,...</td>
       <td>[Mark Kozelek|||Around and Around, Sun Kil Moo...</td>
       <td>[Mark Kozelek|||Around and Around, Sun Kil Moo...</td>
       <td>Mojave 3|||Writing to St. Peter</td>
       <td>[Alexandre Desplat|||Mr. Fox In The Fields Med...</td>
       <td>0</td>
     </tr>
     <tr>
       <th>96621</th>
       <td>1954a8f3f1a377582fd9b21db7301d32-Joel</td>
       <td>[Death Cab for Cutie, Ben Folds Five, Real Est...</td>
       <td>[Death Cab for Cutie|||A Lack Of Color, Ben Fo...</td>
       <td>[Death Cab for Cutie|||A Lack Of Color, Ben Fo...</td>
       <td>Ben Folds|||Zak and Sara</td>
       <td>[tUnE-yArDs|||You Yes You, Perfume Genius|||Yo...</td>
       <td>0</td>
     </tr>
   </tbody>
 </table>
 </div>
 ```
 
 
 
 </HTMLOutputBlock>
 
 
 ```python
 len(latest_dataset)
 ```
 
 <CodeOutputBlock lang=""python"">
 
 ```
     21785
 ```
 
 </CodeOutputBlock>",H3,https://outerbounds.com/docs/recsys-tutorial-L5/#access-results-with-metaflow-s-client-api,False,3096.0,644.0
414,Iterate and improve models in a notebook,"Now, let's turn our attention to the model - the embedding space we trained: let's check how big it is and use it to make a test prediction.
 
 
 ```python
 print(""# track vectors in the space: {}"".format(len(latest_model)))
 test_track = choice(list(latest_model.index_to_key))
 print(""Example track: '{}'"".format(test_track))
 test_vector = latest_model[test_track]
 print(""Test vector for '{}': {}"".format(test_track, test_vector[:5]))
 test_sims = latest_model.most_similar(test_track, topn=3)
 print(""Similar songs to '{}': {}"".format(test_track, test_sims))
 ```
 
 <CodeOutputBlock lang=""python"">
 
 ```
     # track vectors in the space: 316725
     Example track: 'D'Angelo|||Brown Sugar'
     Test vector for 'D'Angelo|||Brown Sugar': [ 1.8937136  -2.6552134  -0.8126778   0.40475285  5.2605653 ]
     Similar songs to 'D'Angelo|||Brown Sugar': [(""Jill Scott|||Can't Explain (42nd Street Happenstance)"", 0.9238367080688477), ('Erykah Badu|||Bump It (Part 1 & 2)', 0.9221848845481873), ('Angie Stone|||Brotha', 0.9144230484962463)]
 ```
 
 </CodeOutputBlock>
 
 The skip-gram model we trained is an embedding space: if we did our job correctly, the space is such that tracks closer in the space are actually similar, and tracks that are far apart are pretty unrelated.
 
 [Judging the quality of ""fantastic embeddings"" is hard](https://arxiv.org/abs/2007.14906), but we point here to some common qualitative checks you can run.
 
 
 ```python",H3,https://outerbounds.com/docs/recsys-tutorial-L5/#iterate-and-improve-models-in-a-notebook,False,1451.0,201.0
415,RecSys 101,"Recommender systems (RSs) are some of the most ubiquitous ML systems in production: whether Netflix suggesting you what movie to watch, Amazon what books to buy, or Linkedin which data influencer to follow, RSs play a pivotal role in our digital life (it is estimated the RSs market will be around 15BN in 2026!).
 
 The model architecture, and therefore many MLOps choices, of a given RS, depends heavily on the use case. While a full taxonomy is beyond the scope of this tutorial, we can provide a simple taxonomy of RSs based on the type of input and output they process.
 
 ![](/assets/recsys-io-taxonomy.png)
 
 1. input user, output item - example: Netflix recommends you a movie that they think you would enjoy;
 2. input item, output item - example: while browsing a book page, Amazon recommends you another book because ""people often look at X as well"";
 3. input a list of items, output the next items - Spotify is picking songs to suggest in your discover weekly playlist based on what songs you have listened to lately.
 
 Finally, as far as input data goes, there is important distinction practitioners make between content and behavioral data.
 
 ![](/assets/content-and-behavioral-data.png)
 
 Content data is data that does not depend on any interaction: think for example of the author of a book on Amazon, or a movie poster on Netflix - even if nobody will ever watch that movie, we could still use some basic metadata to decide how likely we are to like it. Behavioral data is the result of user interactions with a system: it may be add-to-cart events for e-commerce or previous people you added on Facebook - generally speaking, behavioral data needs systems in place to capture and store these signals, often under time constraints.
 
 While the general rule of ML applies and more data is better, in practice the use case and modeling technique(s) will constrain what is feasible: for example, if you are building a RS for a completely new product, with 0 or few active users, content-based data is your only option! The trick to building a recommender system for a product is to be able to ship something that is good enough to generate interest in the product, so over time you can build an increasingly useful behavioral dataset as the product improves.",H3,https://outerbounds.com/docs/recsys-tutorial-L1/#recsys-101,False,2279.0,391.0
416,Next event prediction for songs,"Armed with our taxonomy, we can explore what is the use case we are trying to solve today:
 
 _Can we suggest what to listen to next when presented with a song?_
 
 You will build a sequential recommender system that matches case 3 above. The model will learn from existing sequences (playlists by real users) how to continue extending an arbitrary new list. More generally, this task is also known as next event prediction (NEP). The modeling technique we picked will only leverage behavioral data in the form of interactions created by users when composing their playlists.
 
 The training set is a list of playlists, e.g.:
 * song_1, song_414, song_42425
 * song_412, song_2214, song_525, song_11, song_414, song_42425
 * song_12, song_416
 * ...
 
 The key intuition about our modeling is that ""songs that often appear in similar contexts"" are similar. If we observe that ""Imagine"" and ""Hey Jude"" tend to appear in similar playlists, they must have something in common!
 
 At prediction time, our input will be an unseen playlist with N songs: we will take the first N - 1 songs as the input (or query) for our model, and ask it to predict the last, missing item, that is:
 
 * song_525, song_22, song_814, song_4255
 
 will become:
 
 * query: song_525, song_22, song_814
 * label: song_4255
 
 If our model is able to guess ""song_4255"", we will count it as a successful prediction. Of course, we have left all the juicy details out - so no worries if things feel a bit vague: for now, we just want to be very clear about what problem we are solving, and which type of input/output data our model should deal with.
 
 In the rest of the notebook, we will read our dataset and start getting familiar with the main entities of characters of our story, tracks, and playlists.",H3,https://outerbounds.com/docs/recsys-tutorial-L1/#next-event-prediction-for-songs,False,1777.0,313.0
417,Download the dataset,"You can download the dataset from Kaggle [here](https://www.kaggle.com/datasets/andrewmvd/spotify-playlists?resource=download).
 Place the downloaded file in the `recsys` directory and unzip it. 
 
 ```
 unzip ./archive.zip
 rm ./archive.zip
 ```
 
 We need to do so minor data cleaning, which can be handled by running the following script.
 
 
 ```bash
 python clean_dataset.py
 ```
 
 <CodeOutputBlock lang=""bash"">
 
 ```
        row_id  ...        playlist
     0       0  ...  HARD ROCK 2010
     1       1  ...  HARD ROCK 2010
     2       2  ...  HARD ROCK 2010
     3       3  ...  HARD ROCK 2010
     4       4  ...  HARD ROCK 2010
     
     [5 rows x 5 columns]
     Total rows: 12891680
     All done
     
     See you, space cowboy
     
 ```
 
 </CodeOutputBlock>",H3,https://outerbounds.com/docs/recsys-tutorial-L1/#download-the-dataset,False,778.0,212.0
418,What does the data look like?,"Before loading the data, there are a few packages to import: 
 
 
 ```python
 import pandas as pd
 import matplotlib.pyplot as plt
 from collections import Counter
 import powerlaw
 ```
 
 Now we can load the dataset and explore its structure. The dataset is stored in a `.parquet` file. [Loading parquet files into dataframes](/docs/load-parquet-data-to-pandas-df/) is a common pattern when working with large tabular datasets like the kind often found in RSs. If you are curious, we have a post all about common [file formats for tabular datasets](/docs/tabular-file-formats/).
 
 
 ```python
 df = pd.read_parquet('cleaned_spotify_dataset.parquet')
 df.head(3)
 ```
     
 <HTMLOutputBlock >
 
 
 
 
 ```html
 <div>
 <style scoped>
     .dataframe tbody tr th:only-of-type {
         vertical-align: middle;
     }
 
     .dataframe tbody tr th {
         vertical-align: top;
     }
 
     .dataframe thead th {
         text-align: right;
     }
 </style>
 <table border=""1"" class=""dataframe"">
   <thead>
     <tr style=""text-align: right;"">
       <th></th>
       <th>row_id</th>
       <th>user_id</th>
       <th>artist</th>
       <th>track</th>
       <th>playlist</th>
     </tr>
   </thead>
   <tbody>
     <tr>
       <th>0</th>
       <td>0</td>
       <td>9cc0cfd4d7d7885102480dd99e7a90d6</td>
       <td>Elvis Costello</td>
       <td>(The Angels Wanna Wear My) Red Shoes</td>
       <td>HARD ROCK 2010</td>
     </tr>
     <tr>
       <th>1</th>
       <td>1</td>
       <td>9cc0cfd4d7d7885102480dd99e7a90d6</td>
       <td>Elvis Costello &amp; The Attractions</td>
       <td>(What's So Funny 'Bout) Peace, Love And Unders...</td>
       <td>HARD ROCK 2010</td>
     </tr>
     <tr>
       <th>2</th>
       <td>2</td>
       <td>9cc0cfd4d7d7885102480dd99e7a90d6</td>
       <td>Tiffany Page</td>
       <td>7 Years Too Late</td>
       <td>HARD ROCK 2010</td>
     </tr>
   </tbody>
 </table>
 </div>
 ```
 
 
 
 </HTMLOutputBlock>
 
 How many data samples are there?
 
 
 ```python
 len(df)
 ```
 
 <CodeOutputBlock lang=""python"">
 
 ```
     12891680
 ```
 
 </CodeOutputBlock>
 
 What artists and songs are most popular?
 
 
 ```python
 artist_counter = Counter(list(df['artist']))
 song_counter = Counter(list(df['track']))
 print(""\nTop artists: {}\n"".format(artist_counter.most_common(20)))
 print(""\nTop songs: {}\n"".format(song_counter.most_common(20)))
 ```
 
 <CodeOutputBlock lang=""python"">
 
 ```
     
     Top artists: [('Daft Punk', 36086), ('Coldplay', 35485), (None, 33568), ('Radiohead', 31429), ('The Rolling Stones', 30832), ('Kanye West', 29111), ('JAY Z', 28928), ('Eminem', 28894), ('Queen', 28079), ('David Bowie', 27802), ('Michael Jackson', 26336), ('Muse', 24159), ('U2', 23455), ('Rihanna', 23315), ('Arctic Monkeys', 23288), ('Pearl Jam', 23085), ('Foo Fighters', 21999), ('David Guetta', 21798), ('Bruce Springsteen', 21764), ('Nirvana', 21184)]
     
     
     Top songs: [('Intro', 6676), ('Home', 5600), ('Closer', 3549), ('Runaway', 3350), ('Hold On', 3224), ('Radioactive', 3189), ('Forever', 3055), ('Stay', 2993), ('Alive', 2936), ('Wake Me Up', 2794), ('Heaven', 2793), ('Trouble', 2789), ('Kids', 2714), ('Breathe', 2696), ('Crazy', 2692), ('Dreams', 2691), ('Angel', 2683), ('Happy', 2660), ('You', 2645), ('One', 2622)]
     
 ```
 
 </CodeOutputBlock>
 
 Let's visualize the distribution of tracks and artist in our dataset. 
 
 
 ```python
 def plot_distribution(artists, tracks, n_bins: int=50, outpath = './artist-track-dist.png'):
     """"""
     Plot distributions of tracks and artists in the final dataset.
     """"""
     import numpy as np
     from itertools import product
     import seaborn as sns
     sns.set_style()
     import matplotlib.pyplot as plt
     plt.ioff()
     fig, axs = plt.subplots(1, 2, tight_layout=True, figsize=(8,4))
     axs[0].hist(artist_counter.values(), bins=n_bins, color='#2E3454')
     axs[0].set_title('Artists', fontsize=16)
     axs[0].set_yscale('log')
     axs[0].ticklabel_format(axis=""x"", style=""sci"", scilimits=(0,0))
     axs[0].set_xlabel('# of artists')
     axs[0].set_ylabel('# of times artist is in a playlist')
     axs[1].hist(song_counter.values(), bins=n_bins, color='#2E3454')
     axs[1].set_title('Songs', fontsize=16)
     axs[1].set_yscale('log')
     axs[1].ticklabel_format(axis=""x"", style=""sci"", scilimits=(0,0))
     axs[1].set_xlabel('# of songs')
     axs[1].set_ylabel('# of times song is in a playlist')
     for (i,side) in list(product([0,1], ['top', 'right'])):
         axs[i].spines[side].set_visible(False)
     fig.savefig(outpath)
     plt.close()
     return
 ```
 
 
 ```python
 plot_distribution(artist_counter, song_counter);
 ```
 
 ![](artist-track-dist.png)
 
 Since it looks like our data is very skewed, we can use the `powerlaw` library and formally compare the distribution of how artists are represented in playlists to a powerlaw. Specifically, we use the package to visualize the [probability density function](https://pythonhosted.org/powerlaw/#powerlaw.Fit.plot_pdf) for the theoretical distribution estimated using the number of times artists are represented in playlists.
 
 
 ```python
 data = list(artist_counter.values())
 fit = powerlaw.Fit(data, discrete=True)
 ```
 
 <CodeOutputBlock lang=""python"">
 
 ```
     Calculating best minimal value for power law fit
     xmin progress: 99%
 ```
 
 </CodeOutputBlock>
 
 
 ```python
 fig, ax = plt.subplots(1,1,figsize=(4,4))
 ax.spines['top'].set_visible(False)
 ax.spines['right'].set_visible(False)
 data = list(artist_counter.values())
 fit = powerlaw.Fit(data, discrete=True)
 figCCDF = fit.plot_pdf(color='#2E3454', linewidth=2, ax=ax)
 fit.power_law.plot_pdf(color='#2E3454', linestyle='--', ax=figCCDF)
 fig.savefig('./powerlaw.png');
 ```
 
 <CodeOutputBlock lang=""python"">
 
 ```
     Calculating best minimal value for power law fit
     xmin progress: 99%
 ```
 
 </CodeOutputBlock>
 
 ![](./powerlaw.png)
 
 Nice work! In this lesson, you explored a dataset with millions of Spotify songs and their playlist groupings. 
 You saw which artists and songs are most popular and observed how the distribution of how artists are represented in playlists follows a power law. 
 In the next episode, we will see how to leverage DuckDB to query the dataset efficiently. See you there!",H3,https://outerbounds.com/docs/recsys-tutorial-L1/#what-does-the-data-look-like,False,6299.0,1075.0
419,Seamless parallelism with Metaflow,"Once you have a functioning process for embedding features and a model that uses them for predictions, we can leverage Metaflow's built-in capabilities for parallelization to take this workflow to the next level. With a slight modification of our flow (note the  `foreach='hypers_sets'` parameter), we can scale tasks that can be parallelized on as many machines as your cloud budget allows. 
 
 You can follow along with these modifications in the [flow code for this episode](https://github.com/outerbounds/tutorials/blob/main/recsys/recsys_tuning_flow.py). The flow includes tasks like processing data, or in this case hyperparameter tuning. The same logic applies as the flow in the [previous episode](/docs/recsys-tutorial-L3/), but now we tune over many embedding spaces in parallel and pick the best one (on the validation set) to use as our candidate model. As before, we then test our candidate model once again on the held-out set, to give us a sense of its generalization abilities.",H3,https://outerbounds.com/docs/recsys-tutorial-L4/#seamless-parallelism-with-metaflow,False,993.0,150.0
420,Organize Metaflow run results with cards,"We can use Metaflow card abstractions to create cards to document specific components of the pipeline in a versioned, shareable format. For example, `@card(type='blank', id='hyperCard')` records the performance for all the models we trained. If you wish to use a separate tool for tracking experiments, you can leverage Metaflow integrations with tools like [Comet ML](/docs/track-comet/) and [Weights and Biases](/docs/track-wandb/).",H3,https://outerbounds.com/docs/recsys-tutorial-L4/#organize-metaflow-run-results-with-cards,False,434.0,59.0
421,Find better models with a hyperparemeter tuning flow,"In the following code you will see the `RecSysTuningFlow`. This flow is nearly identical to the previous one. 
 The changes include:
 * Metaflow's `foreach` pattern at the end of the `prepare_dataset` step. The line `self.next(self.generate_embeddings, foreach='hypers_sets')` indicates that the `generate_embeddings` step will be created for all the combinations of parameters defined in the `self.hypers_sets` variable.
 * The `join_runs` step is required (it doesn't have to be any particular name, but the ""join"" step concept needs to exist) to merge all of the artifacts produced in the parallel `generate_embeddings` steps. Notice this function receives the `inputs` argument, containing results for each of the embeddings evaluated. This step includes the `card` that organizes the results of the parameter combinations and the validation metric scores they produced.  
 
 ![](../../../../static/assets/recsys-tutorial-RecSysTuningFlow.png)
 
 <CustomCode height=""60vh"">
 
 
 
 ```py title=""recsys_tuning_flow.py""
 from metaflow import FlowSpec, step, S3, Parameter, current, card
 from metaflow.cards import Markdown, Table
 import os
 import json
 import time
 from random import choice
 
 
 class RecSysTuningFlow(FlowSpec):
 
     IS_DEV = Parameter(
         name='is_dev',
         help='Flag for dev development, with a smaller dataset',
         default='1'
     )
 
     KNN_K = Parameter(
         name='knn_k',
         help='Number of neighbors we retrieve from the vector space',
         default='100'
     ) 
 
     @step
     def start(self):
         print(""flow name: %s"" % current.flow_name)
         print(""run id: %s"" % current.run_id)
         print(""username: %s"" % current.username)
         if self.IS_DEV == '1':
             print(""ATTENTION: RUNNING AS DEV VERSION - DATA WILL BE SUB-SAMPLED!!!"") 
         self.next(self.prepare_dataset)
 
     @step
     def prepare_dataset(self):
         """"""
         Get the data in the right shape by reading the parquet dataset
         and using duckdb SQL-based wrangling to quickly prepare the datasets for
         training our Recommender System.
         """"""
         import duckdb
         import numpy as np
         con = duckdb.connect(database=':memory:')
         
         con.execute(""""""
             CREATE TABLE playlists AS 
             SELECT *, 
             CONCAT (user_id, '-', playlist) as playlist_id,
             CONCAT (artist, '|||', track) as track_id,
             FROM 'cleaned_spotify_dataset.parquet'
             ;
         """""")
         con.execute(""SELECT * FROM playlists LIMIT 1;"")
         print(con.fetchone())
         
         tables = ['row_id', 'user_id', 'track_id', 'playlist_id', 'artist']
         for t in tables:
             con.execute(""SELECT COUNT(DISTINCT({})) FROM playlists;"".format(t))
             print(""# of {}"".format(t), con.fetchone()[0])
             
         sampling_cmd = ''
         if self.IS_DEV == '1':
             print(""Subsampling data, since this is DEV"")
             sampling_cmd = ' USING SAMPLE 1 PERCENT (bernoulli)'
             
         dataset_query = """"""
             SELECT * FROM
             (   
                 SELECT 
                     playlist_id,
                     LIST(artist ORDER BY row_id ASC) as artist_sequence,
                     LIST(track_id ORDER BY row_id ASC) as track_sequence,
                     array_pop_back(LIST(track_id ORDER BY row_id ASC)) as track_test_x,
                     LIST(track_id ORDER BY row_id ASC)[-1] as track_test_y
                 FROM 
                     playlists
                 GROUP BY playlist_id 
                 HAVING len(track_sequence) > 2
             ) 
             {}
             ;
             """""".format(sampling_cmd)
         
         con.execute(dataset_query)
         df = con.fetch_df()
         print(""# rows: {}"".format(len(df)))
         print(df.iloc[0].tolist())
         con.close()
         
         train, validate, test = np.split(
             df.sample(frac=1, random_state=42), 
             [int(.7 * len(df)), int(.9 * len(df))])
         
         self.df_dataset = df
         self.df_train = train
         self.df_validate = validate
         self.df_test = test
         print(""# testing rows: {}"".format(len(self.df_test)))
         
         #highlight-start
         self.hypers_sets = [json.dumps(_) for _ in [
             { 'min_count': 5, 'epochs': 30, 'vector_size': 48, 'window': 10, 'ns_exponent': 0.75 },
             { 'min_count': 10, 'epochs': 30, 'vector_size': 48, 'window': 10, 'ns_exponent': 0.75 }
         ]]
         #highlight-start
         # we train K models in parallel, depending how many configurations of hypers 
         # we set - we generate K set of vectors, and evaluate them on the validation
         # set to pick the best combination of parameters!
         # highlight-next-line
         self.next(self.generate_embeddings, foreach='hypers_sets')
 
     def predict_next_track(self, vector_space, input_sequence, k):
         """"""        
         Given an embedding space, predict best next song with KNN.
         Initially, we just take the LAST item in the input playlist as the query item for KNN
         and retrieve the top K nearest vectors (you could think of taking the smoothed average embedding
         of the input list, for example, as a refinement).
 
         If the query item is not in the vector space, we make a random bet. We could refine this by taking
         for example the vector of the artist (average of all songs), or with some other strategy (sampling
         by popularity). 
 
         For more options on how to generate vectors for ""cold items"" see for example the paper:
         https://dl.acm.org/doi/10.1145/3383313.3411477
         """"""
         query_item = input_sequence[-1]
         if query_item not in vector_space:
             query_item = choice(list(vector_space.index_to_key))
         
         return [_[0] for _ in vector_space.most_similar(query_item, topn=k)]
 
     def evaluate_model(self, _df, vector_space, k):
         lambda_predict = lambda row: self.predict_next_track(vector_space, row['track_test_x'], k)
         _df['predictions'] = _df.apply(lambda_predict, axis=1)
         lambda_hit = lambda row: 1 if row['track_test_y'] in row['predictions'] else 0
         _df['hit'] = _df.apply(lambda_hit, axis=1)
         hit_rate = _df['hit'].sum() / len(_df)
         return hit_rate
 
     @step
     def generate_embeddings(self):
         """"""
         Generate vector representations for songs, based on the Prod2Vec idea.
 
         For an overview of the algorithm and the evaluation, see for example:
         https://arxiv.org/abs/2007.14906
         """"""
         from gensim.models.word2vec import Word2Vec
         self.hyper_string = self.input
         self.hypers = json.loads(self.hyper_string)
         track2vec_model = Word2Vec(self.df_train['track_sequence'], **self.hypers)
         print(""Training with hypers {} is completed!"".format(self.hyper_string))
         print(""Vector space size: {}"".format(len(track2vec_model.wv.index_to_key)))
         test_track = choice(list(track2vec_model.wv.index_to_key))
         print(""Example track: '{}'"".format(test_track))
         test_vector = track2vec_model.wv[test_track]
         print(""Test vector for '{}': {}"".format(test_track, test_vector[:5]))
         test_sims = track2vec_model.wv.most_similar(test_track, topn=3)
         print(""Similar songs to '{}': {}"".format(test_track, test_sims))
         self.validation_metric = self.evaluate_model(
             self.df_validate,
             track2vec_model.wv,
             k=int(self.KNN_K))
         print(""Hit Rate@{} is: {}"".format(self.KNN_K, self.validation_metric))
         self.track_vectors = track2vec_model.wv
         self.next(self.join_runs)
 
     # highlight-next-line
     @card(type='blank', id='hyperCard')
     @step
     def join_runs(self, inputs):
         """"""
         Join the parallel runs and merge results into a dictionary.
         """"""
         self.all_vectors = { inp.hyper_string: inp.track_vectors for inp in inputs}
         self.all_results = { inp.hyper_string: inp.validation_metric for inp in inputs}
         print(""Current result map: {}"".format(self.all_results))
         self.best_model, self_best_result = sorted(self.all_results.items(), key=lambda x: x[1], reverse=True)[0]
         print(""The best validation score is for model: {}, {}"".format(self.best_model, self_best_result))
         self.final_vectors = self.all_vectors[self.best_model]
         self.final_dataset = inputs[0].df_test
         # highlight-start
         current.card.append(Markdown(""## Results from parallel training""))
         current.card.append(
             Table([
                 [inp.hyper_string, inp.validation_metric] for inp in inputs
             ])
         )
         # highlight-end
         # next, test the best model on unseen data, and report the final Hit Rate as 
         # our best point-wise estimate of ""in the wild"" performance
         self.next(self.model_testing)
 
     @step
     def model_testing(self):
         """"""
         Test the generalization abilities of the best model by running predictions
         on the unseen test data.
 
         We report a quantitative point-wise metric, hit rate @ K, as an initial implementation. However,
         evaluating recommender systems is a very complex task, and better metrics, through good abstractions, 
         are available, i.e. https://reclist.io/.
         """"""
         self.test_metric = self.evaluate_model(
             self.final_dataset,
             self.final_vectors,
             k=int(self.KNN_K))
         print(""Hit Rate@{} on the test set is: {}"".format(self.KNN_K, self.test_metric))
         self.next(self.end)
 
     @step
     def end(self):
         """"""
         Just say bye!
         """"""
         print(""All done\n\nSee you, space cowboy\n"")
         return
 
 
 if __name__ == '__main__':
     RecSysTuningFlow()
 ```
 
 </CustomCode>",H3,https://outerbounds.com/docs/recsys-tutorial-L4/#find-better-models-with-a-hyperparemeter-tuning-flow,False,10070.0,2718.0
422,Run your flow,"```bash
 python recsys_tuning_flow.py run --is_dev 0
 ```
 
 <CodeOutputBlock lang=""bash"">
 
 ```
      Workflow starting (run-id 188134):
      [188134/start/1014307 (pid 87579)] Task is starting.
      [188134/start/1014307 (pid 87579)] flow name: RecSysTuningFlow
      [188134/start/1014307 (pid 87579)] run id: 188134
      [188134/start/1014307 (pid 87579)] username: eddie
      [188134/start/1014307 (pid 87579)] Task finished successfully.
      [188134/prepare_dataset/1014308 (pid 87583)] Task is starting.
      [188134/prepare_dataset/1014308 (pid 87583)] (0, '9cc0cfd4d7d7885102480dd99e7a90d6', 'Elvis Costello', '(The Angels Wanna Wear My) Red Shoes', 'HARD ROCK 2010', '9cc0cfd4d7d7885102480dd99e7a90d6-HARD ROCK 2010', 'Elvis Costello|||(The Angels Wanna Wear My) Red Shoes')
      [188134/prepare_dataset/1014308 (pid 87583)] # of row_id 12891680
      [188134/prepare_dataset/1014308 (pid 87583)] # of user_id 15918
      [188134/prepare_dataset/1014308 (pid 87583)] # of track_id 2819059
      [188134/prepare_dataset/1014308 (pid 87583)] # of playlist_id 232369
      [188134/prepare_dataset/1014308 (pid 87583)] # of artist 289821
      [188134/prepare_dataset/1014308 (pid 87583)] # rows: 217843
      [188134/prepare_dataset/1014308 (pid 87583)] ['b741ad9438bcf049085e58aa184a4be1-EA Sports - MVP Baseball 2005 Soundtrack!', ['The Bravery', 'Louis XIV', 'Rock n Roll Soldiers', '...And You Will Know Us By The Trail Of Dead', 'Dropkick Murphys', 'The High Speed Scene', 'Steriogram', 'Hot Hot Heat'], ['The Bravery|||An Honest Mistake', 'Louis XIV|||Finding Out True Love Is Blind - Album/EP Version', 'Rock n Roll Soldiers|||Funny Little Feeling', '...And You Will Know Us By The Trail Of Dead|||Let It Dive', 'Dropkick Murphys|||Tessie (Radio Version)', 'The High Speed Scene|||The I Roc Z Song', 'Steriogram|||Walkie Talkie Man', 'Hot Hot Heat|||You Owe Me An IOU'], ['The Bravery|||An Honest Mistake', 'Louis XIV|||Finding Out True Love Is Blind - Album/EP Version', 'Rock n Roll Soldiers|||Funny Little Feeling', '...And You Will Know Us By The Trail Of Dead|||Let It Dive', 'Dropkick Murphys|||Tessie (Radio Version)', 'The High Speed Scene|||The I Roc Z Song', 'Steriogram|||Walkie Talkie Man'], 'Hot Hot Heat|||You Owe Me An IOU']
      [188134/prepare_dataset/1014308 (pid 87583)] # testing rows: 21785
      [188134/prepare_dataset/1014308 (pid 87583)] Foreach yields 2 child steps.
      [188134/prepare_dataset/1014308 (pid 87583)] Task finished successfully.
      [188134/generate_embeddings/1014309 (pid 87633)] Task is starting.
      [188134/generate_embeddings/1014310 (pid 87636)] Task is starting.
      [188134/generate_embeddings/1014310 (pid 87636)] Training with hypers {""min_count"": 10, ""epochs"": 30, ""vector_size"": 48, ""window"": 10, ""ns_exponent"": 0.75} is completed!
      [188134/generate_embeddings/1014310 (pid 87636)] Vector space size: 148016
      [188134/generate_embeddings/1014310 (pid 87636)] Example track: 'Buddy Holly|||Rock Around With Ollie Vee'
      [188134/generate_embeddings/1014310 (pid 87636)] Test vector for 'Buddy Holly|||Rock Around With Ollie Vee': [-0.57586724  0.25871253  1.0829923  -0.40932345  1.0014604 ]
      [188134/generate_embeddings/1014310 (pid 87636)] Similar songs to 'Buddy Holly|||Rock Around With Ollie Vee': [('Duane Eddy|||Rebel Rouser', 0.9402039051055908), ('Carl Perkins|||Put Your Cat Clothes On', 0.936795175075531), ('Buddy Holly|||Ready Teddy', 0.9180619716644287)]
      [188134/generate_embeddings/1014309 (pid 87633)] Training with hypers {""min_count"": 5, ""epochs"": 30, ""vector_size"": 48, ""window"": 10, ""ns_exponent"": 0.75} is completed!
      2 tasks are running: generate_embeddings (2 running; 0 done).
      No tasks are waiting in the queue.
      3 steps have not started: join_runs, end, model_testing.
      [188134/generate_embeddings/1014309 (pid 87633)] Vector space size: 316725
      [188134/generate_embeddings/1014309 (pid 87633)] Example track: 'Bananarama|||Long Train Running [7-inch version]'
      [188134/generate_embeddings/1014309 (pid 87633)] Test vector for 'Bananarama|||Long Train Running [7-inch version]': [-0.11822549 -0.00500495  0.5621464   0.2686343   0.64628214]
      [188134/generate_embeddings/1014309 (pid 87633)] Similar songs to 'Bananarama|||Long Train Running [7-inch version]': [('Bananarama|||Long Train Running', 0.9727321863174438), ('Belinda Carlisle|||Love In The Key Of C', 0.9349488019943237), ('Tröckener Kecks|||Met Hart En Ziel', 0.930341899394989)]
      2 tasks are running: generate_embeddings (2 running; 0 done).
      No tasks are waiting in the queue.
      3 steps have not started: join_runs, end, model_testing.
      [188134/generate_embeddings/1014310 (pid 87636)] Hit Rate@100 is: 0.17409566654425265
      [188134/generate_embeddings/1014310 (pid 87636)] Task finished successfully.
      [188134/generate_embeddings/1014309 (pid 87633)] Hit Rate@100 is: 0.1834373852368711
      1 task is running: generate_embeddings (1 running; 1 done).
      No tasks are waiting in the queue.
      3 steps have not started: join_runs, end, model_testing.
      [188134/generate_embeddings/1014309 (pid 87633)] Task finished successfully.
      [188134/join_runs/1014311 (pid 89559)] Task is starting.
      [188134/join_runs/1014311 (pid 89559)] Current result map: {'{""min_count"": 5, ""epochs"": 30, ""vector_size"": 48, ""window"": 10, ""ns_exponent"": 0.75}': 0.1834373852368711, '{""min_count"": 10, ""epochs"": 30, ""vector_size"": 48, ""window"": 10, ""ns_exponent"": 0.75}': 0.17409566654425265}
      [188134/join_runs/1014311 (pid 89559)] The best validation score is for model: {""min_count"": 5, ""epochs"": 30, ""vector_size"": 48, ""window"": 10, ""ns_exponent"": 0.75}, 0.1834373852368711
      [188134/join_runs/1014311 (pid 89559)] Task finished successfully.
      [188134/model_testing/1014312 (pid 89566)] Task is starting.
      [188134/model_testing/1014312 (pid 89566)] Hit Rate@100 on the test set is: 0.18613725040165252
      [188134/model_testing/1014312 (pid 89566)] Task finished successfully.
      [188134/end/1014313 (pid 89584)] Task is starting.
      [188134/end/1014313 (pid 89584)] All done
      [188134/end/1014313 (pid 89584)] 
      [188134/end/1014313 (pid 89584)] See you, space cowboy
      [188134/end/1014313 (pid 89584)] 
      [188134/end/1014313 (pid 89584)] Task finished successfully.
      Done!
 ```
 
 </CodeOutputBlock>
 
 Now you have a flow that will not only help you operationalize the training of your model but can help you train many variations in parallel while seamlessly tracking the results in a variety of modes. Stay tuned for the next episode where you will learn to use Metaflow's Client API to access the models you trained in Python code.",H3,https://outerbounds.com/docs/recsys-tutorial-L4/#run-your-flow,False,6745.0,1033.0
423,Recommender Systems with Metaflow: Beginner Tutorial,"This tutorial on RecSys is the first adventure in training recommender systems using Metaflow to provide a scalable workflow you can use for experimentation and production. The goal is to develop a relatively simple, effective, and general pipeline for sequential recommendations, and show how you can use popular open-source libraries and tools including DuckDB, Gensim, Metaflow, and Keras to build a fully working cloud endpoint that serves predictions in real-time, starting from raw data.
 
 <CustomCode height=""60vh"" sandboxLink=""https://account.outerbounds.dev/account/?workspace=/home/workspace/workspaces/recsys/workspace.code-workspace"">
 
 ```python title=""Recommender System with Metaflow""",H1,https://outerbounds.com/docs/recsys-tutorial-overview#recommender-systems-with-metaflow-beginner-tutorial,False,701.0,84.0
424,Objectives,"Our use case is: given a training set made by music playlists (list of songs hand-curated by users), can we suggest what to listen to next when presented with a new song?
 
 By following along you will learn how to:
 * take a recommender system idea from prototype to real-time production;
 * leverage Metaflow to train different versions of the same model and pick the best one;
 * use Metaflow cards to save important details about model performance;
 * package a representation of your data in a Keras object that you can deploy directly from the flow to a cloud endpoint with AWS Sagemaker.
 
 ![](../../../../static/assets/recsys-tutorial-RecSysSagemakerDeployment.png)",H2,https://outerbounds.com/docs/recsys-tutorial-overview#objectives,False,674.0,107.0
425,Prerequisites,"This tutorial does not assume knowledge about recommender systems, but does assume the following:
 * you are familiar with the basic concepts of Metaflow (flow, steps, tasks, client API, etc.), know how to run a flow;
 * you are familiar with the basic concepts of machine learning such as training, validation, and test split for model evaluation.
 
 If you are new to Metaflow we recommend starting with the [introductory tutorials](/docs/intro-tutorial-overview) to get up to speed on the basics of Metaflow before returning to this tutorial.
 
 ![](/assets/intro-tutorial-dag-S1E1.png)
 
 Bonus points (no worries, we will explain all of this) if you:
 * have experience with model serving;
 * know a little bit about what embeddings or neural networks are;
 * are comfortable with AWS concepts such as storing data in S3 and using SageMaker.",H2,https://outerbounds.com/docs/recsys-tutorial-overview#prerequisites,False,846.0,134.0
426,Tutorial Structure,"The content includes the following:
 * [Episode 1](/docs/recsys-tutorial-L1): Understand the Data
 * [Episode 2](/docs/recsys-tutorial-L2): Query Faster with DuckDB
 * [Episode 3](/docs/recsys-tutorial-L3): Build a Recommender Model
 * [Episode 4](/docs/recsys-tutorial-L4): Tune Hyperparameters
 * [Episode 5](/docs/recsys-tutorial-L5): Analyze Results and Iterate
 * [Episode 6](/docs/recsys-tutorial-L6): Make Real-time Predictions
 
 Each episode contains either a Metaflow script to run or a Jupyter notebook. 
 You do not need access to cloud computing or a [Metaflow deployment](/engineering/welcome/) to complete the first five episodes.
 If you want to run the final episode that deploys your model to an endpoint, you will need access to an AWS account that has an IAM role that can execute operations on Sagemaker. We will explain these details in that episode. As always, feel free to reach us in the #ask-metaflow channel on [Slack](http://slack.outerbounds.co/) if you need help deploying Metaflow on your infrastructure! 
 The estimated time to complete all episodes is 1-2 hours.",H2,https://outerbounds.com/docs/recsys-tutorial-overview#tutorial-structure,False,1095.0,151.0
427,Why Metaflow?,"The main benefit of using a data science workflow solution like Metaflow when prototyping is that your code will be built on a strong foundation for deploying to a production environment. Metaflow is most useful when projects have scaling requirements, are mission-critical, and/or have many interacting parts. You can read more at these links:
 * [Should I Use Metaflow?](https://docs.metaflow.org/introduction/what-is-metaflow#should-i-use-metaflow)
 * [Why Metaflow?](https://docs.metaflow.org/introduction/why-metaflow)
 
 After completing the lessons, you can transfer insights and code from the tutorial to your real-world data science projects. This is a beginner tutorial so it will not reflect many important challenges to consider in production ML environments. For example, in production, you may consider using Metaflow features such as the `@conda` decorator for dependency management, `@batch` or `@kubernetes` for remote execution, and `@schedule` to automatically trigger jobs.",H2,https://outerbounds.com/docs/recsys-tutorial-overview#why-metaflow,False,993.0,131.0
428,Recommender System Resources,"* A gentle introduction to [RecSys concepts](https://github.com/jacopotagliabue/jacopotagliabue.github.io/blob/gh-pages/public/Wild_Wild_Tests_MLOPS_World_2022.pdf), with a focus on metrics and testing.
 * A research paper on the methodology ([prod2vec](https://arxiv.org/abs/2007.14906)) we adopted to solve the use case demonstrated in this tutorial.",H2,https://outerbounds.com/docs/recsys-tutorial-overview#recommender-system-resources,False,352.0,33.0
429,Run in your Browser,"<OpenSandbox fileName=""Recommender System with Metaflow"" link=""https://account.outerbounds.dev/account/?workspace=/home/workspace/workspaces/recsys/workspace.code-workspace"">
 </OpenSandbox>",H2,https://outerbounds.com/docs/recsys-tutorial-setup#run-in-your-browser,False,190.0,7.0
430,Run on your own Infrastructure,"To run this tutorial locally or with your own Metaflow deployment, you will need to clone the repository, and then install dependencies using an environment manager of your choice. In the repository you will find the necessary dependencies in a `env.yml` file for conda. You can install them by following these instructions.",H2,https://outerbounds.com/docs/recsys-tutorial-setup#run-on-your-own-infrastructure,False,324.0,52.0
431,Clone the Repository,"```
 git clone https://github.com/outerbounds/tutorials.git
 ```",H3,https://outerbounds.com/docs/recsys-tutorial-setup#clone-the-repository,False,64.0,5.0
432,Install Dependencies with Conda,"To run the tutorial, you need to install the required dependencies via conda. We have included a conda environment in the [env.yml](https://github.com/outerbounds/tutorials/recsys/env.yml) file.  You can install and activate the environment with this command:
 
 > We are using [`mamba`](https://mamba.readthedocs.io/en/latest/) instead of `conda` because it is significantly faster. However, you can use `conda` if you want to.
 
 ```
 cd tutorials/recsys
 conda install mamba -n base -c conda-forge
 mamba env create -f env.yml
 conda activate mf-tutorial-recsys
 ```",H3,https://outerbounds.com/docs/recsys-tutorial-setup#install-dependencies-with-conda,False,569.0,77.0
433,Running the Tutorials,Please follow the instructions in each lesson for running either the associated Python script or Jupyter Notebook.,H3,https://outerbounds.com/docs/recsys-tutorial-setup#running-the-tutorials,False,114.0,17.0
434,What about feature engineering?,"Once the data is available through SQL, we make use of Python for the final feature engineering. As is common in deep learning workflows, feature engineering is minimal, and here it is mostly bookkeeping for categorical variables. This bookkeeping helps us make sure the same exact transformations happen uniformly at training time and test time. 
 
 The `build_workflow` step uses NVTabular, an Nvidia library for tabular processing, to go from the ""human-readable"" datasets we had to the machine-optimized datasets we need for the deep learning model. Once the datasets are ready, we take notes of all the available IDs for both shoppers and items, as Merlin will use its own ids for training and predictions: the two mappings (`self.id_2_user_id` and `self.id_2_item_id`) will be crucial at the end to translate the model IDs into human-readable entities.",H3,https://outerbounds.com/docs/recsys-tutorial-S2E2#what-about-feature-engineering,False,858.0,136.0
435,What are we doing again? :shrug:,"Before going too deep into the model, let's recall the learning task. At test time, our model will receive a user identifier as the query and will be tasked with suggesting k (fashion) items that she is likely to buy. How are we going to accomplish this? The easiest way to build our intuition for this user-item model is to start from our previous recommender system tutorial, where we build a model to recommend new songs to continue a playlist. [In that case](https://outerbounds.com/docs/recsys-tutorial-L3/), we built a latent space for the songs and ""placed"" the user based on what she was listening to - the goal is to represent user interests _through the latest song in a list_: recommendations then boiled down to a KNN search in that space - what is the most similar song to the one I'm listening now?
 
 Our new model goes one step further, and builds an _explicit representation for users_, through their own embeddings: we call this architecture a two-tower model because it builds a latent space containing both users and items in parallel. 
 
 ![](../../../../static/assets/two-tower-recsys.png)
 <Caption> Source: https://nvidia-merlin.github.io/models/v0.5.0/_images/TwoTower.png </Caption>
 
 If the previous model could be trained ""implicitly"" by relying on the sequential nature of playlists, this model requires ""explicit"" feedback in the form of past transactions: when we train our network, user and item embeddings will adjust depends on the historical transactions that actually happened. Why go through the additional trouble? In a nutshell, two-tower models are very flexible, and allow us to incorporate user and item meta-data into our algorithm: thanks to deep learning's inherent flexibility, the model learns not only from purchases but also from the features associated with each transaction - how old was the shopper? How expensive was the item?
 
 Implementing such a model seems complex - and it is! Thankfully, our friends at Nvidia Merlin are developing an open-source library for deep learning recommendations to allow us to implement this  [state-of-the-art architecture in few lines of code](https://nvidia-merlin.github.io/models/v0.5.0/examples/05-Retrieval-Model.html). The training code is remarkably straightforward for such a complex system, resembling the simplicity of Keras-like models. We define user and item schemas in parallel, including their embedding dimensions, and then fit the model on our training data set with standard information retrieval metrics: the model is evaluated on the validation set and then serialized and versioned.
 
 :::note
 The usage of sampling and the default number of epochs will allow the flow to run reasonably fast in a local or hybrid environment with no GPU. However, one of the most useful features of Metaflow when scaling large models is the ease of adding the `gpu` parameter to the `@batch` decorator, and benefit from the hardware speed-up. Like many data science scenarios, the same code will work on CPU or GPU setup in this tutorial, but to train the model to good performance levels a GPU is needed.
 :::
 
 Finally, we introduce a Metaflow customization `@magicdir`. Since Merlin flows are parquet-based, intermediate computations are often stored as a series of files. The `@magicdir` decorator comes in handy dealing with this situation, allowing us to serialize and deserialize the merlin folder and seamlessly make sure these intermediate files are available to any step, local or remote!",H3,https://outerbounds.com/docs/recsys-tutorial-S2E2#what-are-we-doing-again-shrug,False,3494.0,539.0
436,What does the model consider good? :dart:,"There are two metrics that guide how `model.compile` is instantiated in the `train_model` step. 
 
 ```python
 model.compile(
     optimizer=opt,  
     metrics=[mm.RecallAt(int(self.TOP_K)), mm.NDCGAt(int(self.TOP_K))],)
 ```
 
 **Recall@K**: Also known as `HitRate@K`. This measures whether a relevant/correct item is among the recommended items list.
 
 **Normalized Discounted Cumulative Gain (NDCG@K)**: NDCG accounts for rank, and is a more nuanced measure than hit rate. It is a normalization of a measure called Discounted Cumulative Gain (DCG). The G, gain, measures the relevance of an item our model can recommend to someone. Relevance labels can be whether a user actually interacted with an item or where an item placed in search rankings. DCG penalizes highly relevant items being placed at the bottom of the recommendation list.",H3,https://outerbounds.com/docs/recsys-tutorial-S2E2#what-does-the-model-consider-good-dart,False,843.0,130.0
437,Train your model with Nvidia Merlin,"<CustomCode height=""60vh"">
 
 
 ![](../../../../static/assets/train-merlin.png)
 
 
 ```py title=""train_merlin.py""
 from metaflow import FlowSpec, step, batch, Parameter, current
 from custom_decorators import magicdir
 import json
 from datetime import datetime
 
 
 class TrainMerlin(FlowSpec):
 
     ### MERLIN PARAMETERS ###
 
     MODEL_FOLDER = Parameter(
         name='model_folder',
         help='Folder to store the model from Merlin, between steps',
         default='merlin_model'
     )
 
     ### DATA PARAMETERS ###
 
     ROW_SAMPLING = Parameter(
         name='row_sampling',
         help='Row sampling: if 0, NO sampling is applied. Needs to be an int between 1 and 100',
         default='1'
     )
 
     #NOTE: data parameters - we split by time, leaving the last two weeks for validation and tests
     # The first date in the table is 2018-09-20
     # The last date in the table is 2020-09-22
     TRAINING_END_DATE = Parameter(
         name='training_end_date',
         help='Data up until this date is used for training, format yyyy-mm-dd',
         default='2020-09-08'
     )
 
     VALIDATION_END_DATE = Parameter(
         name='validation_end_date',
         help='Data up after training end and until this date is used for validation, format yyyy-mm-dd',
         default='2020-09-15'
     )
 
     ### TRAINING PARAMETERS ###
     VALIDATION_METRIC = Parameter(
         name='validation_metric',
         help='Merlin metric to use for picking the best set of hyperparameter',
         default='recall_at_10'
     )
 
     N_EPOCHS = Parameter(
         name='n_epoch',
         help='Number of epochs to train the Merlin model',
         default='1' # default to 1 for quick testing
     )
 
     TOP_K = Parameter(
         name='top_k',
         help='Number of products to recommend for a giver shopper',
         default='10'
     )
     
     @step
     def start(self):
         """"""
         Start-up: check everything works or fail fast!
         """"""
         # print out some debug info
         print(""flow name: %s"" % current.flow_name)
         print(""run id: %s"" % current.run_id)
         print(""username: %s"" % current.username)
         # we need to check if Metaflow is running with remote (s3) data store or not
         from metaflow.metaflow_config import DATASTORE_SYSROOT_S3 
         print(""DATASTORE_SYSROOT_S3: %s"" % DATASTORE_SYSROOT_S3)
         if DATASTORE_SYSROOT_S3 is None:
             print(""ATTENTION: LOCAL DATASTORE ENABLED"")
         # check variables and connections are working fine
         assert int(self.ROW_SAMPLING)
         # check the data range makes sense
         self.training_end_date = datetime.strptime(self.TRAINING_END_DATE, '%Y-%m-%d')
         self.validation_end_date = datetime.strptime(self.VALIDATION_END_DATE, '%Y-%m-%d')
         assert self.validation_end_date > self.training_end_date
 
         self.next(self.get_dataset)
 
     @step
     def get_dataset(self):
         """"""
         Get the data in the right shape using duckDb, after the dbt transformation
         """"""
         from pyarrow import Table as pt
         import duckdb
         # check if we need to sample - this is useful to iterate on the code with a real setup
         # without reading in too much data...
         _sampling = int(self.ROW_SAMPLING)
         sampling_expression = '' if _sampling == 0 else 'USING SAMPLE {} PERCENT (bernoulli)'.format(_sampling)
         # thanks to our dbt preparation, the ML models can read in directly the data without additional logic
         query = """"""
             SELECT 
                 ARTICLE_ID,
                 PRODUCT_CODE, 
                 PRODUCT_TYPE_NO,
                 PRODUCT_GROUP_NAME,
                 GRAPHICAL_APPEARANCE_NO,
                 COLOUR_GROUP_CODE,
                 PERCEIVED_COLOUR_VALUE_ID,
                 PERCEIVED_COLOUR_MASTER_ID,
                 DEPARTMENT_NO,
                 INDEX_CODE,
                 INDEX_GROUP_NO,
                 SECTION_NO,
                 GARMENT_GROUP_NO,
                 ACTIVE,
                 FN,
                 AGE,
                 CLUB_MEMBER_STATUS,
                 CUSTOMER_ID,
                 FASHION_NEWS_FREQUENCY,
                 POSTAL_CODE,
                 PRICE,
                 SALES_CHANNEL_ID,
                 T_DAT
             FROM
                 read_parquet('filtered_dataframe.parquet')
                 {}
             ORDER BY
                 T_DAT ASC
         """""".format(sampling_expression)
         print(""Fetching rows with query: \n {} \n\nIt may take a while...\n"".format(query))
         # fetch raw dataset
         con = duckdb.connect(database=':memory:')
         con.execute(query)
         dataset = con.fetchall()
         # convert the COLS to lower case (Keras does complain downstream otherwise)
         cols = [c[0].lower() for c in con.description]
         dataset = [{ k: v for k, v in zip(cols, row) } for row in dataset]
         # debug
         print(""Example row"", dataset[0])
         self.item_id_2_meta = { str(r['article_id']): r for r in dataset }
         # we split by time window, using the dates specified as parameters
         # NOTE: we could actually return Arrow table directly, by then running three queries over
         # a different date range (e.g. https://duckdb.org/2021/12/03/duck-arrow.html)
         # For simplicity, we kept here the original flow compatible with warehouse processing
         train_dataset = pt.from_pylist([row for row in dataset if row['t_dat'] < self.training_end_date])
         validation_dataset = pt.from_pylist([row for row in dataset 
             if row['t_dat'] >= self.training_end_date and row['t_dat'] < self.validation_end_date])
         test_dataset = pt.from_pylist([row for row in dataset if row['t_dat'] >= self.validation_end_date])
         print(""# {:,} events in the training set, {:,} for validation, {:,} for test"".format(
             len(train_dataset),
             len(validation_dataset),
             len(test_dataset)
         ))
         # store and version datasets as a map label -> datasets, for consist processing later on
         self.label_to_dataset = {
             'train': train_dataset,
             'valid': validation_dataset,
             'test': test_dataset
         }
         # go to the next step for NV tabular data
         self.next(self.build_workflow)
     
     # NOTE: we use the magicdir package (https://github.com/outerbounds/metaflow_magicdir)
     # to simplify moving the parquet files that Merlin needs / consumes across steps
     @magicdir
     @step
     def build_workflow(self):
         """"""
         Use NVTabular to transform the original data into the final dataframes for training,
         validation, testing.
         """"""
         from workflow_builder import get_nvt_workflow, read_to_dataframe
         import pandas as pd
         import nvtabular as nvt # pylint: disable=import-error
         import itertools
         # read dataset into frames
         label_to_df = {}
         for label, dataset in self.label_to_dataset.items():
             label_to_df[label] = read_to_dataframe(dataset, label)
         full_dataset = nvt.Dataset(pd.concat(list(label_to_df.values())))
         # get the workflow and fit the dataset
         workflow = get_nvt_workflow()
         workflow.fit(full_dataset)
         self.label_to_melin_dataset = {}
         for label, _df in label_to_df.items():
             cnt_dataset = nvt.Dataset(_df)
             self.label_to_melin_dataset[label] = cnt_dataset
             workflow.transform(cnt_dataset).to_parquet(output_path=""merlin/{}/"".format(label))
         # store the mapping Merlin ID -> article_id and Merlin ID -> customer_id
         user_unique_ids = list(pd.read_parquet('categories/unique.customer_id.parquet')['customer_id'])
         items_unique_ids = list(pd.read_parquet('categories/unique.article_id.parquet')['article_id'])
         self.id_2_user_id = { idx:_ for idx, _ in enumerate(user_unique_ids) }
         self.id_2_item_id = { idx:_ for idx, _ in enumerate(items_unique_ids) }
         # sets of hypers 
         self.LEARNING_RATE = 0.05
         self.BATCH_SIZE = 4096
         self.next(self.train_model)
 
     @batch(
         gpu=1,
         memory=24000,
         image='public.ecr.aws/outerbounds/merlin-reasonable-scale:22.11-latest'
     )
     @magicdir
     @step
     def train_model(self):
         """"""
         Train models in parallel and store artifacts and validation KPIs for downstream consumption.
         """"""
         import hashlib
         import merlin.models.tf as mm # pylint: disable=import-error
         from merlin.io.dataset import Dataset # pylint: disable=import-error
         from merlin.schema.tags import Tags # pylint: disable=import-error
         import tensorflow as tf # pylint: disable=import-error
         train = Dataset('merlin/train/*.parquet')
         valid = Dataset('merlin/valid/*.parquet')
         print(""Train dataset shape: {}, Validation: {}"".format(
             train.to_ddf().compute().shape,
             valid.to_ddf().compute().shape
             ))
         # train the model and evaluate it on validation set
         user_schema = train.schema.select_by_tag(Tags.USER) # MERLIN WARNING
         user_inputs = mm.InputBlockV2(user_schema)
         query = mm.Encoder(user_inputs, mm.MLPBlock([128, 64]))
         item_schema = train.schema.select_by_tag(Tags.ITEM)
         item_inputs = mm.InputBlockV2(item_schema,)
         candidate = mm.Encoder(item_inputs, mm.MLPBlock([128, 64]))
         model = mm.TwoTowerModelV2(query, candidate)
         opt = tf.keras.optimizers.Adagrad(learning_rate=self.LEARNING_RATE)
         model.compile(
             optimizer=opt, 
             run_eagerly=False, 
             metrics=[mm.RecallAt(int(self.TOP_K)), mm.NDCGAt(int(self.TOP_K))],)
         model.fit(
             train, 
             validation_data=valid, 
             batch_size=self.BATCH_SIZE, 
             epochs=int(self.N_EPOCHS))
         self.metrics = model.evaluate(valid, batch_size=1024, return_dict=True)
         print(""\n\n====> Eval results: {}\n\n"".format(self.metrics))
         self.model_path = 'merlin/model/'
         model.save(self.model_path)
         print(f""Model saved to {self.model_path}!"")
         self.next(self.end)
 
     def get_items_topk_recommender_model(
         self,
         train_dataset, 
         model, 
         k: int
     ):
         from merlin.models.utils.dataset import unique_rows_by_features # pylint: disable=import-error
         from merlin.schema.tags import Tags # pylint: disable=import-error
         candidate_features = unique_rows_by_features(train_dataset, Tags.ITEM, Tags.ITEM_ID)
         topk_model = model.to_top_k_encoder(candidate_features, k=k, batch_size=128)
         topk_model.compile(run_eagerly=False)
 
         return topk_model
 
     @step
     def end(self):
         """"""
         Just say bye!
         """"""
         print(""All done\n\nSee you, recSys cowboy\n"")
         return
 
 
 if __name__ == '__main__':
     TrainMerlin()
 ```
 
 </CustomCode>",H3,https://outerbounds.com/docs/recsys-tutorial-S2E2#train-your-model-with-nvidia-merlin,False,11175.0,3082.0
438,Run your flow :arrow_forward:,"```bash
 python train_merlin.py run
 ```
 
 <CodeOutputBlock lang=""bash"">
 
 ```
     ...
      [196110/start/1073741 (pid 58764)] Task is starting.
      [196110/start/1073741 (pid 58764)] flow name: IntermediateRecSysFlow
      [196110/start/1073741 (pid 58764)] run id: 196110
      [196110/start/1073741 (pid 58764)] username: eddie
      [196110/start/1073741 (pid 58764)] DATASTORE_SYSROOT_S3: s3://oleg2-s3-mztdpcvj/metaflow
      [196110/start/1073741 (pid 58764)] Task finished successfully.
     ...
      [196110/train_model/1073750 (pid 58845)] Task is starting.
      [196110/train_model/1073750 (pid 58845)] WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.data_structures has been moved to tensorflow.python.trackable.data_structures. The old module will be deleted in version 2.11.
      [196110/train_model/1073750 (pid 58845)] Train dataset shape: (107368, 23), Validation: (814, 23)
      [196110/train_model/1073750 (pid 58845)] 199: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz
      [196110/train_model/1073750 (pid 58845)] 
      [196110/train_model/1073750 (pid 58845)] WARNING:absl:Found untraced functions such as train_compute_metrics, model_context_layer_call_fn, model_context_layer_call_and_return_conditional_losses, _update_step_xla, dot_product_layer_call_fn while saving (showing 5 of 122). These functions will not be directly callable after loading.
      [196110/train_model/1073750 (pid 58845)] 
      [196110/train_model/1073750 (pid 58845)] ====> Eval results: {'loss': 6.701679229736328, 'recall_at_10': 0.013513513840734959, 'ndcg_at_10': 0.006188767030835152, 'regularization_loss': 0.0, 'loss_batch': 6.701679229736328}
      [196110/train_model/1073750 (pid 58845)] 
      [196110/train_model/1073750 (pid 58845)] 
      [196110/train_model/1073750 (pid 58845)] Model saved to merlin/model/!
      [196110/train_model/1073750 (pid 58845)] Task finished successfully.
     ...
      [196110/end/1073761 (pid 58957)] Task is starting.
      [196110/end/1073761 (pid 58957)] All done
      [196110/end/1073761 (pid 58957)] 
      [196110/end/1073761 (pid 58957)] See you, recSys cowboy
      [196110/end/1073761 (pid 58957)] 
      [196110/end/1073761 (pid 58957)] Task finished successfully.
     ...
 ```
 
 </CodeOutputBlock>
 
 In this tutorial, you built on the data workflow from the previous lesson and extended your flow by adding a modeling step with Nvidia Merlin. You also learned about the two-tower model and a little bit about how it relates to our user-item recommendation task. In the next lesson, we will keep moving forward, improving model performance by adding hyperparameter tuning and model selection steps. See you there!",H3,https://outerbounds.com/docs/recsys-tutorial-S2E2#run-your-flow-arrow-forward,False,2771.0,414.0
439,Find your best Merlin,"In the last lesson, we trained one model. In this lesson we will extend that workflow in two key ways:
 1. We add a new flow variable `self.hypers_sets` at the end of `build_worflow`. The training will now happen in parallel, with different models training `foreach='hypers_sets'` in the `train_model` step, and results being aggregated in the `join_runs` step. Importantly we pick the best model and save it, so that we can use `magicdir` to access the best model in all downstream steps. Remember that the best is determined by our metrics 
 2. We now have a `model_testing` step. 
 
 If you have followed previous tutorials, this generalization has the same pattern as a [typical Flow](https://outerbounds.com/docs/recsys-tutorial-L4/). Once multiple models have been trained in parallel, `join_runs` will collect the metrics for each model on the validation set and pick the best one as the ""model we promote to production"". Using Metaflow, we store and version both the best combination of parameters and the location of the model in the data store through `self.final_model_path`.
 
 In the final step, `model_testing`, we load back in memory the best model (according to the validation above), and we do two important things:
 
 * we run the best model on the unseen, held-out test set, and report back the relevant metrics: this is important because the test set is our quantitative proxy for generalization (of course, other evaluation protocols are just as important: if you want to extend the code, you should look into [RecList](https://github.com/jacopotagliabue/reclist)!);
 * we run the best model on the user in the test set, simulating in this way our ""target shoppers"": if this pipeline runs every night for example, you may think of them as the shoppers we expect to come to our website tomorrow. When they do, we will serve them the recommendations that we are computing, and saving, now!",H3,https://outerbounds.com/docs/recsys-tutorial-S2E3#find-your-best-merlin,False,1908.0,310.0
440,Tune Merlin for production :magic_wand::sparkles:,"![](../../../../static/assets/tune-merlin.png)
 
 <CustomCode height=""60vh"">
 
 
 
 ```py title=""tune_merlin.py""
 from metaflow import FlowSpec, step, batch, Parameter, current
 from custom_decorators import magicdir
 import os
 import json
 from datetime import datetime
 
 
 class MerlinTuningFlow(FlowSpec):
 
     ### MERLIN PARAMETERS ###
 
     MODEL_FOLDER = Parameter(
         name='model_folder',
         help='Folder to store the model from Merlin, between steps',
         default='merlin_model'
     )
 
     ### DATA PARAMETERS ###
 
     ROW_SAMPLING = Parameter(
         name='row_sampling',
         help='Row sampling: if 0, NO sampling is applied. Needs to be an int between 1 and 100',
         default='1'
     )
 
     #NOTE: data parameters - we split by time, leaving the last two weeks for validation and tests
     # The first date in the table is 2018-09-20
     # The last date in the table is 2020-09-22
     TRAINING_END_DATE = Parameter(
         name='training_end_date',
         help='Data up until this date is used for training, format yyyy-mm-dd',
         default='2020-09-08'
     )
 
     VALIDATION_END_DATE = Parameter(
         name='validation_end_date',
         help='Data up after training end and until this date is used for validation, format yyyy-mm-dd',
         default='2020-09-15'
     )
 
     ### TRAINING PARAMETERS ###
 
     VALIDATION_METRIC = Parameter(
         name='validation_metric',
         help='Merlin metric to use for picking the best set of hyperparameter',
         default='recall_at_10'
     )
 
     N_EPOCHS = Parameter(
         name='n_epoch',
         help='Number of epochs to train the Merlin model',
         default='1' # default to 1 for quick testing
     )
     
     TOP_K = Parameter(
         name='top_k',
         help='Number of products to recommend for a giver shopper',
         default='10'
     )
 
     @step
     def start(self):
         """"""
         Start-up: check everything works or fail fast!
         """"""
         # print out some debug info
         print(""flow name: %s"" % current.flow_name)
         print(""run id: %s"" % current.run_id)
         print(""username: %s"" % current.username)
         # we need to check if Metaflow is running with remote (s3) data store or not
         from metaflow.metaflow_config import DATASTORE_SYSROOT_S3 
         print(""DATASTORE_SYSROOT_S3: %s"" % DATASTORE_SYSROOT_S3)
         if DATASTORE_SYSROOT_S3 is None:
             print(""ATTENTION: LOCAL DATASTORE ENABLED"")
         # check variables and connections are working fine
         assert int(self.ROW_SAMPLING)
         # check the data range makes sense
         self.training_end_date = datetime.strptime(self.TRAINING_END_DATE, '%Y-%m-%d')
         self.validation_end_date = datetime.strptime(self.VALIDATION_END_DATE, '%Y-%m-%d')
         assert self.validation_end_date > self.training_end_date
 
         self.next(self.get_dataset)
 
     @step
     def get_dataset(self):
         """"""
         Get the data in the right shape using duckDb, after the dbt transformation
         """"""
         from pyarrow import Table as pt
         import duckdb
         # check if we need to sample - this is useful to iterate on the code with a real setup
         # without reading in too much data...
         _sampling = int(self.ROW_SAMPLING)
         sampling_expression = '' if _sampling == 0 else 'USING SAMPLE {} PERCENT (bernoulli)'.format(_sampling)
         # thanks to our dbt preparation, the ML models can read in directly the data without additional logic
         query = """"""
             SELECT 
                 ARTICLE_ID,
                 PRODUCT_CODE, 
                 PRODUCT_TYPE_NO,
                 PRODUCT_GROUP_NAME,
                 GRAPHICAL_APPEARANCE_NO,
                 COLOUR_GROUP_CODE,
                 PERCEIVED_COLOUR_VALUE_ID,
                 PERCEIVED_COLOUR_MASTER_ID,
                 DEPARTMENT_NO,
                 INDEX_CODE,
                 INDEX_GROUP_NO,
                 SECTION_NO,
                 GARMENT_GROUP_NO,
                 ACTIVE,
                 FN,
                 AGE,
                 CLUB_MEMBER_STATUS,
                 CUSTOMER_ID,
                 FASHION_NEWS_FREQUENCY,
                 POSTAL_CODE,
                 PRICE,
                 SALES_CHANNEL_ID,
                 T_DAT
             FROM
                 read_parquet('filtered_dataframe.parquet')
                 {}
             ORDER BY
                 T_DAT ASC
         """""".format(sampling_expression)
         print(""Fetching rows with query: \n {} \n\nIt may take a while...\n"".format(query))
         # fetch raw dataset
         con = duckdb.connect(database=':memory:')
         con.execute(query)
         dataset = con.fetchall()
         # convert the COLS to lower case (Keras does complain downstream otherwise)
         cols = [c[0].lower() for c in con.description]
         dataset = [{ k: v for k, v in zip(cols, row) } for row in dataset]
         # debug
         print(""Example row"", dataset[0])
         self.item_id_2_meta = { str(r['article_id']): r for r in dataset }
         # we split by time window, using the dates specified as parameters
         # NOTE: we could actually return Arrow table directly, by then running three queries over
         # a different date range (e.g. https://duckdb.org/2021/12/03/duck-arrow.html)
         # For simplicity, we kept here the original flow compatible with warehouse processing
         train_dataset = pt.from_pylist([row for row in dataset if row['t_dat'] < self.training_end_date])
         validation_dataset = pt.from_pylist([row for row in dataset 
             if row['t_dat'] >= self.training_end_date and row['t_dat'] < self.validation_end_date])
         test_dataset = pt.from_pylist([row for row in dataset if row['t_dat'] >= self.validation_end_date])
         print(""# {:,} events in the training set, {:,} for validation, {:,} for test"".format(
             len(train_dataset),
             len(validation_dataset),
             len(test_dataset)
         ))
         # store and version datasets as a map label -> datasets, for consist processing later on
         self.label_to_dataset = {
             'train': train_dataset,
             'valid': validation_dataset,
             'test': test_dataset
         }
         # go to the next step for NV tabular data
         self.next(self.build_workflow)
     
     # NOTE: we use the magicdir package (https://github.com/outerbounds/metaflow_magicdir)
     # to simplify moving the parquet files that Merlin needs / consumes across steps
     @magicdir
     @step
     def build_workflow(self):
         """"""
         Use NVTabular to transform the original data into the final dataframes for training,
         validation, testing.
         """"""
         from workflow_builder import get_nvt_workflow, read_to_dataframe
         import pandas as pd
         import nvtabular as nvt # pylint: disable=import-error
         import itertools
         # read dataset into frames
         label_to_df = {}
         for label, dataset in self.label_to_dataset.items():
             label_to_df[label] = read_to_dataframe(dataset, label)
         full_dataset = nvt.Dataset(pd.concat(list(label_to_df.values())))
         # get the workflow and fit the dataset
         workflow = get_nvt_workflow()
         workflow.fit(full_dataset)
         self.label_to_melin_dataset = {}
         for label, _df in label_to_df.items():
             cnt_dataset = nvt.Dataset(_df)
             self.label_to_melin_dataset[label] = cnt_dataset
             workflow.transform(cnt_dataset).to_parquet(output_path=""merlin/{}/"".format(label))
         # store the mapping Merlin ID -> article_id and Merlin ID -> customer_id
         user_unique_ids = list(pd.read_parquet('categories/unique.customer_id.parquet')['customer_id'])
         items_unique_ids = list(pd.read_parquet('categories/unique.article_id.parquet')['article_id'])
         self.id_2_user_id = { idx:_ for idx, _ in enumerate(user_unique_ids) }
         self.id_2_item_id = { idx:_ for idx, _ in enumerate(items_unique_ids) }
         # sets of hypers
         # batch size
         batch_sizes = [ 16384, 4096]
         # learning rate
         learning_rates = [0.04, 0.02]
         grid_search = []
         for params in itertools.product(batch_sizes, learning_rates):
             grid_search.append({
                 'BATCH_SIZE': params[0],
                 'LEARNING_RATE': params[1]
             })
         # we serialize hypers to a string and pass them to the foreach below
         # NOTE: to save time and compute, we limit the foreach to two hypers
         self.hypers_sets = [json.dumps(_) for _ in grid_search[:3]]
         # debug
         print(self.hypers_sets)
         self.next(self.train_model, foreach='hypers_sets')
 
     @batch(
         gpu=1,
         memory=24000,
         image='public.ecr.aws/outerbounds/merlin-reasonable-scale:22.11-latest'
     )
     @magicdir
     @step
     def train_model(self):
         """"""
         Train models in parallel and store artifacts and validation KPIs for downstream consumption.
         """"""
         import hashlib
         import merlin.models.tf as mm # pylint: disable=import-error
         from merlin.io.dataset import Dataset # pylint: disable=import-error
         from merlin.schema.tags import Tags # pylint: disable=import-error
         import tensorflow as tf # pylint: disable=import-error
         # this is the CURRENT hyper param JSON in the fan-out
         # each copy of this step in the parallelization will have its own value
         self.hyper_string = self.input
         self.hypers = json.loads(self.hyper_string)
         train = Dataset('merlin/train/*.parquet')
         valid = Dataset('merlin/valid/*.parquet')
         print(""Train dataset shape: {}, Validation: {}"".format(
             train.to_ddf().compute().shape,
             valid.to_ddf().compute().shape
             ))
         # train the model and evaluate it on validation set
         user_schema = train.schema.select_by_tag(Tags.USER) # MERLIN WARNING
         user_inputs = mm.InputBlockV2(user_schema)
         query = mm.Encoder(user_inputs, mm.MLPBlock([128, 64]))
         item_schema = train.schema.select_by_tag(Tags.ITEM)
         item_inputs = mm.InputBlockV2(item_schema,)
         candidate = mm.Encoder(item_inputs, mm.MLPBlock([128, 64]))
         model = mm.TwoTowerModelV2(query, candidate)
         opt = tf.keras.optimizers.Adagrad(learning_rate=self.hypers['LEARNING_RATE'])
         model.compile(
             optimizer=opt, 
             run_eagerly=False, 
             metrics=[mm.RecallAt(int(self.TOP_K)), mm.NDCGAt(int(self.TOP_K))],)
         model.fit(
             train, 
             validation_data=valid, 
             batch_size=self.hypers['BATCH_SIZE'], 
             epochs=int(self.N_EPOCHS))
         self.metrics = model.evaluate(valid, batch_size=1024, return_dict=True)
         print(""\n\n====> Eval results: {}\n\n"".format(self.metrics))
         # save the model
         model_hash = str(hashlib.md5(self.hyper_string.encode('utf-8')).hexdigest())
         self.model_path = 'merlin/model{}/'.format(model_hash)
         model.save(self.model_path)
         print(f""Model saved to {self.model_path}!"")
         self.next(self.join_runs)
 
     def get_items_topk_recommender_model(
         self,
         train_dataset, 
         model, 
         k: int
     ):
         from merlin.models.utils.dataset import unique_rows_by_features # pylint: disable=import-error
         from merlin.schema.tags import Tags # pylint: disable=import-error
         candidate_features = unique_rows_by_features(train_dataset, Tags.ITEM, Tags.ITEM_ID)
         topk_model = model.to_top_k_encoder(candidate_features, k=k, batch_size=128)
         topk_model.compile(run_eagerly=False)
 
         return topk_model
 
     @step
     def join_runs(self, inputs):
         """"""
         Join the parallel runs and merge results into a dictionary.
         """"""
         # merge results from runs with different parameters (key is hyper settings as a string)
         self.model_paths = { inp.hyper_string: inp.model_path for inp in inputs}
         self.results_from_runs = { inp.hyper_string: inp.metrics[self.VALIDATION_METRIC] for inp in inputs}
         print(""Current results: {}"".format(self.results_from_runs))
          # pick one according to some logic, e.g. higher VALIDATION_METRIC
         self.best_model, self_best_result = sorted(self.results_from_runs.items(), key=lambda x: x[1], reverse=True)[0]
         print(""Best model is: {}, best path is {}"".format(
             self.best_model,
             self.model_paths[self.best_model]
             ))
         # assign the variable for the ""final"" (the best) model path in S3 and its corresponding name
         self.final_model_path = self.model_paths[self.best_model]
         # pick a final mapping for metadata and other service variables
         best_model_idx = list(self.model_paths.keys()).index(self.best_model)
         self.item_id_2_meta = inputs[best_model_idx].item_id_2_meta
         self.id_2_item_id = inputs[best_model_idx].id_2_item_id
         self.id_2_user_id = inputs[best_model_idx].id_2_user_id
         self.magicdir = inputs[best_model_idx].magicdir
         # next, for the best model do more testing  
         self.next(self.model_testing)
 
     def load_merlin_model(
         self,
         dataset,
         path
     ):
         import tensorflow as tf # pylint: disable=import-error
         import merlin.models.tf as mm # pylint: disable=import-error
         loaded_model = tf.keras.models.load_model(path, compile=False)
         # this is necessary when re-loading the model, before building the top K
         _ = loaded_model(mm.sample_batch(dataset, batch_size=128, include_targets=False))
         # debug
         print(""Model re-loaded!"")
 
         return loaded_model
 
     @batch(
         gpu=1,
         memory=24000,
         image='public.ecr.aws/outerbounds/merlin-reasonable-scale:22.11-latest'
     )
     @magicdir
     @step
     def model_testing(self):
         """"""
         Test the generalization abilities of the best model through the held-out set...
         and RecList Beta (Forthcoming!)
         """"""
         from merlin.io.dataset import Dataset # pylint: disable=import-error
         import merlin.models.tf as mm # pylint: disable=import-error
         from merlin.schema import Tags # pylint: disable=import-error
         # loading back datasets and the model for final testing
         test = Dataset('merlin/test/*.parquet')
         train = Dataset('merlin/train/*.parquet')
         loaded_model = self.load_merlin_model(train, self.final_model_path)
         topk_rec_model = self.get_items_topk_recommender_model(test, loaded_model, k=int(self.TOP_K))
         # extract the target item id from the inputs
         test_loader = mm.Loader(test, batch_size=1024, transform=mm.ToTarget(test.schema, Tags.ITEM_ID))
         self.test_metrics = topk_rec_model.evaluate(test_loader, batch_size=1024, return_dict=True)
         print(""\n\n====> Test results: {}\n\n"".format(self.test_metrics))        
         # calculate recommendations 
         topk_rec_model = self.get_items_topk_recommender_model(train, loaded_model, k=int(self.TOP_K))
         self.best_predictions = self.get_recommendations(test, topk_rec_model)
         self.next(self.end)
             
     def get_recommendations(
         self,
         test,
         topk_rec_model 
     ):
         """"""
         Run predictions on a target dataset of shoppers (in this case, the testing dataset) 
         and store the predictions for the cache downstream.
         """"""
         import merlin.models.tf as mm # pylint: disable=import-error
         # export ONLY the users in the test set to simulate the set of shoppers we need to recommend items to
         test_dataset = mm.Loader(test, batch_size=1024, shuffle=False)
         # predict returns a tuple with two elements, scores and product IDs: we get the IDs only
         self.raw_predictions = topk_rec_model.predict(test_dataset)[1]
         n_rows = self.raw_predictions.shape[0]
         self.target_shoppers = test_dataset.data.to_ddf().compute()['customer_id']
         print(""Inspect the shopper object for debugging...{}"".format(type(self.target_shoppers)))
         # check we have as many predictions as we have shoppers in the test set
         assert n_rows == len(self.target_shoppers)
         # map predictions to a final dictionary, with the actual H and M IDs for users and products
         self.h_m_shoppers = [str(self.id_2_user_id[_]) for _ in self.target_shoppers.to_numpy().tolist()]
         print(""Example target shoppers: "", self.h_m_shoppers[:3])
         self.target_items = test_dataset.data.to_ddf().compute()['article_id']
         print(""Example target items: "", self.target_items[:3])
         predictions = self.serialize_predictions(
             self.h_m_shoppers,
             self.id_2_item_id,
             self.raw_predictions,
             self.target_items,
             n_rows
         )
         print(""Example target predictions"", predictions[self.h_m_shoppers[0]])
         # debug, if rows > len(predictions), same user appears at least twice in test set
         print(n_rows, len(predictions))
         
         return predictions
 
     def serialize_predictions(
         self,
         h_m_shoppers,
         id_2_item_id,
         raw_predictions,
         target_items,
         n_rows
     ):
         """"""
         Convert raw predictions to a dictionary user -> items for easy re-use 
         later in the pipeline (e.g. dump the predicted items to a cache!)
         """"""
         sku_convert = lambda x: [str(id_2_item_id[_]) for _ in x]
         predictions = {}
         for _ in range(n_rows):
             cnt_user = h_m_shoppers[_]
             cnt_raw_preds = raw_predictions[_].tolist()
             cnt_target = target_items[_]
             # don't overwite if we already have a prediction for this user
             if cnt_user not in predictions:
                 predictions[cnt_user] = {
                     'items': sku_convert(cnt_raw_preds),
                     'target': sku_convert([cnt_target])[0]
                 }
 
         return predictions
 
     @step
     def end(self):
         """"""
         Just say bye!
         """"""
         print(""All done\n\nSee you, recSys cowboy\n"")
         return
 
 
 if __name__ == '__main__':
     MerlinTuningFlow()
 ```
 
 </CustomCode>",H3,https://outerbounds.com/docs/recsys-tutorial-S2E3#tune-merlin-for-production-magic-wand-sparkles,False,18660.0,5017.0
441,Run your flow :arrow_forward:,"```bash
 python tune_merlin.py run
 ```
 
 <CodeOutputBlock lang=""bash"">
 
 ```
     ...
      [196123/start/1073837 (pid 61431)] Task is starting.
      [196123/start/1073837 (pid 61431)] flow name: MerlinTuningFlow
      [196123/start/1073837 (pid 61431)] run id: 196123
      [196123/start/1073837 (pid 61431)] username: eddie
      [196123/start/1073837 (pid 61431)] DATASTORE_SYSROOT_S3: s3://oleg2-s3-mztdpcvj/metaflow
      [196123/start/1073837 (pid 61431)] Task finished successfully.
     ...
      [196123/join_runs/1073868 (pid 61725)] Task is starting.
      [196123/join_runs/1073868 (pid 61725)] Current results: {'{""BATCH_SIZE"": 16384, ""LEARNING_RATE"": 0.04}': 0.012106537818908691, '{""BATCH_SIZE"": 16384, ""LEARNING_RATE"": 0.02}': 0.012106537818908691, '{""BATCH_SIZE"": 4096, ""LEARNING_RATE"": 0.04}': 0.012106537818908691}
      [196123/join_runs/1073868 (pid 61725)] Best model is: {""BATCH_SIZE"": 16384, ""LEARNING_RATE"": 0.04}, best path is merlin/model2548b9008d0990d99af495627cdf35ab/
      [196123/join_runs/1073868 (pid 61725)] Task finished successfully.
     ...
      [196123/model_testing/1073869 (pid 61749)] Task is starting.
      [196123/model_testing/1073869 (pid 61749)] [c2989bbe-6522-4200-9b05-1967971844d5] Task is starting (status SUBMITTED)...
      [196123/model_testing/1073869 (pid 61749)] [c2989bbe-6522-4200-9b05-1967971844d5] Task is starting (status RUNNABLE)...
      [196123/model_testing/1073869 (pid 61749)] [c2989bbe-6522-4200-9b05-1967971844d5] Task is starting (status STARTING)...
      [196123/model_testing/1073869 (pid 61749)] [c2989bbe-6522-4200-9b05-1967971844d5] Task is starting (status STARTING)...
      [196123/model_testing/1073869 (pid 61749)] [c2989bbe-6522-4200-9b05-1967971844d5] Task is starting (status STARTING)...
      [196123/model_testing/1073869 (pid 61749)] [c2989bbe-6522-4200-9b05-1967971844d5] Task is starting (status STARTING)...
      [196123/model_testing/1073869 (pid 61749)] [c2989bbe-6522-4200-9b05-1967971844d5] Task is starting (status STARTING)...
      [196123/model_testing/1073869 (pid 61749)] [c2989bbe-6522-4200-9b05-1967971844d5] Task is starting (status STARTING)...
      [196123/model_testing/1073869 (pid 61749)] [c2989bbe-6522-4200-9b05-1967971844d5] Task is starting (status STARTING)...
      [196123/model_testing/1073869 (pid 61749)] [c2989bbe-6522-4200-9b05-1967971844d5] Task is starting (status STARTING)...
      [196123/model_testing/1073869 (pid 61749)] [c2989bbe-6522-4200-9b05-1967971844d5] Task is starting (status STARTING)...
      [196123/model_testing/1073869 (pid 61749)] [c2989bbe-6522-4200-9b05-1967971844d5] Task is starting (status STARTING)...
      [196123/model_testing/1073869 (pid 61749)] [c2989bbe-6522-4200-9b05-1967971844d5] Task is starting (status STARTING)...
      [196123/model_testing/1073869 (pid 61749)] [c2989bbe-6522-4200-9b05-1967971844d5] Task is starting (status STARTING)...
      [196123/model_testing/1073869 (pid 61749)] [c2989bbe-6522-4200-9b05-1967971844d5] Task is starting (status STARTING)...
      [196123/model_testing/1073869 (pid 61749)] [c2989bbe-6522-4200-9b05-1967971844d5] Task is starting (status RUNNING)...
      [196123/model_testing/1073869 (pid 61749)] [c2989bbe-6522-4200-9b05-1967971844d5] Setting up task environment.
      [196123/model_testing/1073869 (pid 61749)] [c2989bbe-6522-4200-9b05-1967971844d5] Downloading code package...
      [196123/model_testing/1073869 (pid 61749)] [c2989bbe-6522-4200-9b05-1967971844d5] Code package downloaded.
      [196123/model_testing/1073869 (pid 61749)] [c2989bbe-6522-4200-9b05-1967971844d5] Task is starting.
      [196123/model_testing/1073869 (pid 61749)] [c2989bbe-6522-4200-9b05-1967971844d5] /usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.12) or chardet (3.0.4) doesn't match a supported version!
      [196123/model_testing/1073869 (pid 61749)] [c2989bbe-6522-4200-9b05-1967971844d5]   warnings.warn(""urllib3 ({}) or chardet ({}) doesn't match a supported ""
      [196123/model_testing/1073869 (pid 61749)] [c2989bbe-6522-4200-9b05-1967971844d5] 699: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
      [196123/model_testing/1073869 (pid 61749)] [c2989bbe-6522-4200-9b05-1967971844d5] 316: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
      [196123/model_testing/1073869 (pid 61749)] [c2989bbe-6522-4200-9b05-1967971844d5] 572: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
      [196123/model_testing/1073869 (pid 61749)] [c2989bbe-6522-4200-9b05-1967971844d5] 445: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
      [196123/model_testing/1073869 (pid 61749)] [c2989bbe-6522-4200-9b05-1967971844d5] To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
      [196123/model_testing/1073869 (pid 61749)] [c2989bbe-6522-4200-9b05-1967971844d5] 627: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
      [196123/model_testing/1073869 (pid 61749)] [c2989bbe-6522-4200-9b05-1967971844d5] 991: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
      [196123/model_testing/1073869 (pid 61749)] [c2989bbe-6522-4200-9b05-1967971844d5] 294: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
      [196123/model_testing/1073869 (pid 61749)] [c2989bbe-6522-4200-9b05-1967971844d5] 690: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
      [196123/model_testing/1073869 (pid 61749)] [c2989bbe-6522-4200-9b05-1967971844d5] 072: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
      [196123/model_testing/1073869 (pid 61749)] [c2989bbe-6522-4200-9b05-1967971844d5] 435: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:991] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
      [196123/model_testing/1073869 (pid 61749)] [c2989bbe-6522-4200-9b05-1967971844d5] 706: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 8080 MB memory:  -> device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:1e.0, compute capability: 7.0
      [196123/model_testing/1073869 (pid 61749)] [c2989bbe-6522-4200-9b05-1967971844d5] Model re-loaded!
      [196123/model_testing/1073869 (pid 61749)] [c2989bbe-6522-4200-9b05-1967971844d5] WARNING:absl:Found untraced functions such as concat_features_1_layer_call_fn, concat_features_1_layer_call_and_return_conditional_losses, model_context_layer_call_fn, model_context_layer_call_and_return_conditional_losses, embeddings_layer_call_fn while saving (showing 5 of 68). These functions will not be directly callable after loading.
      [196123/model_testing/1073869 (pid 61749)] [c2989bbe-6522-4200-9b05-1967971844d5] WARNING:tensorflow:No training configuration found in save file, so the model was not compiled. Compile it manually.
      [196123/model_testing/1073869 (pid 61749)] [c2989bbe-6522-4200-9b05-1967971844d5] WARNING:tensorflow:No training configuration found in save file, so the model was not compiled. Compile it manually.
      [196123/model_testing/1073869 (pid 61749)] [c2989bbe-6522-4200-9b05-1967971844d5]
      [196123/model_testing/1073869 (pid 61749)] [c2989bbe-6522-4200-9b05-1967971844d5]
      [196123/model_testing/1073869 (pid 61749)] [c2989bbe-6522-4200-9b05-1967971844d5] ====> Test results: {'loss': 0.03424941003322601, 'recall_at_10': 0.014864864759147167, 'mrr_at_10': 0.004014371428638697, 'ndcg_at_10': 0.006472568027675152, 'map_at_10': 0.004014371428638697, 'precision_at_10': 0.0014864865224808455, 'regularization_loss': 0.0, 'loss_batch': 0.03424941003322601}
      [196123/model_testing/1073869 (pid 61749)] [c2989bbe-6522-4200-9b05-1967971844d5]
      [196123/model_testing/1073869 (pid 61749)] [c2989bbe-6522-4200-9b05-1967971844d5]
      [196123/model_testing/1073869 (pid 61749)] [c2989bbe-6522-4200-9b05-1967971844d5] WARNING:absl:Found untraced functions such as concat_features_1_layer_call_fn, concat_features_1_layer_call_and_return_conditional_losses, model_context_layer_call_fn, model_context_layer_call_and_return_conditional_losses, embeddings_layer_call_fn while saving (showing 5 of 68). These functions will not be directly callable after loading.
      [196123/model_testing/1073869 (pid 61749)] [c2989bbe-6522-4200-9b05-1967971844d5] WARNING:tensorflow:No training configuration found in save file, so the model was not compiled. Compile it manually.
      [196123/model_testing/1073869 (pid 61749)] [c2989bbe-6522-4200-9b05-1967971844d5] WARNING:tensorflow:No training configuration found in save file, so the model was not compiled. Compile it manually.
      [196123/model_testing/1073869 (pid 61749)] [c2989bbe-6522-4200-9b05-1967971844d5] Inspect the shopper object for debugging...<class 'cudf.core.series.Series'>
      [196123/model_testing/1073869 (pid 61749)] [c2989bbe-6522-4200-9b05-1967971844d5] Example target shoppers:  ['b241b978da584a3f7e6f269ab2b988d7bad1a224730645591c3ed62e893b9e57', 'f476cc29c16c4e3bb499a73b8a300c174e69a8ee9009ed47984be0e3485074ae', 'b209da2fb96abac946ed71289340d9285dc4d99ed87ca8aee221237fc6dfe046']
      [196123/model_testing/1073869 (pid 61749)] [c2989bbe-6522-4200-9b05-1967971844d5] Example target items:  0    5333
      [196123/model_testing/1073869 (pid 61749)] [c2989bbe-6522-4200-9b05-1967971844d5] 1      41
      [196123/model_testing/1073869 (pid 61749)] [c2989bbe-6522-4200-9b05-1967971844d5] 2    6883
      [196123/model_testing/1073869 (pid 61749)] [c2989bbe-6522-4200-9b05-1967971844d5] Name: article_id, dtype: int64
      [196123/model_testing/1073869 (pid 61749)] [c2989bbe-6522-4200-9b05-1967971844d5] Example target predictions {'items': ['735572001', '639338001', '780031004', '738399001', '844112002', '737636001', '782129001', '682576001', '670485001', '876342001'], 'target': '200182002'}
      [196123/model_testing/1073869 (pid 61749)] [c2989bbe-6522-4200-9b05-1967971844d5] 740 736
      [196123/model_testing/1073869 (pid 61749)] [c2989bbe-6522-4200-9b05-1967971844d5] Task finished with exit code 0.
      [196123/model_testing/1073869 (pid 61749)] Task finished successfully.
     ...
      [196123/end/1073872 (pid 62062)] Task is starting.
      [196123/end/1073872 (pid 62062)] All done
      [196123/end/1073872 (pid 62062)] 
      [196123/end/1073872 (pid 62062)] See you, recSys cowboy
      [196123/end/1073872 (pid 62062)] 
      [196123/end/1073872 (pid 62062)] Task finished successfully.
     ...
 ```
 
 </CodeOutputBlock>
 
 In this lesson, you trained several models in parallel, in the cloud, on separate GPU nodes (if you uncommented the `@batch` decorator). 
 Metaflow can scale this pattern up and out to work with any state-of-the-art models you want.
 In the final lesson of this tutorial, we will shift focus to production, by pushing our predictions to a DynamoDB table.",H3,https://outerbounds.com/docs/recsys-tutorial-S2E3#run-your-flow-arrow-forward,False,12156.0,1432.0
442,Run in your Browser,"<OpenSandbox fileName=""Recommender System with Metaflow"" link=""https://account.outerbounds.dev/account/?workspace=/home/workspace/workspaces/recsys-2/workspace.code-workspace"">
 </OpenSandbox>",H2,https://outerbounds.com/docs/recsys-tutorial-S2-setup#run-in-your-browser,False,192.0,7.0
443,Run on your own Infrastructure,"To run this tutorial locally or with your own Metaflow deployment, you will need to clone the repository, and then install dependencies using an environment manager of your choice. In the repository you will find the necessary dependencies in a `env.yml` file for conda. You can install them by following these instructions.",H2,https://outerbounds.com/docs/recsys-tutorial-S2-setup#run-on-your-own-infrastructure,False,324.0,52.0
444,Clone the Repository,"```
 git clone https://github.com/outerbounds/tutorials.git
 ```",H3,https://outerbounds.com/docs/recsys-tutorial-S2-setup#clone-the-repository,False,64.0,5.0
445,Install Dependencies with Conda,"To run the tutorial, you need to install the required dependencies via conda. We have included a conda environment in the [env.yml](https://github.com/outerbounds/tutorials/recsys/env.yml) file.  You can install and activate the environment with this command:
 
 > We are using [`mamba`](https://mamba.readthedocs.io/en/latest/) instead of `conda` because it is significantly faster. However, you can use `conda` if you want to.
 
 ```
 cd tutorials/recsys-2
 conda install mamba -n base -c conda-forge
 mamba env create -f env.yml
 conda activate mf-tutorial-recsys-2
 ```",H3,https://outerbounds.com/docs/recsys-tutorial-S2-setup#install-dependencies-with-conda,False,573.0,77.0
446,Running the Tutorials,Please follow the instructions in each lesson for running either the associated Python script or Jupyter Notebook.,H3,https://outerbounds.com/docs/recsys-tutorial-S2-setup#running-the-tutorials,False,114.0,17.0
447,Recommender Systems with Metaflow: Intermediate Tutorial,"This tutorial is an adventure in training recommender systems using Metaflow. In the end, you will have a scalable workflow to use for experimentation and in production systems. After dipping our toes into recommender systems in our [first tutorial](/docs/recsys-tutorial-overview), we are ready for more complex data flows and architecture. 
 
 In particular, in this tutorial we use the H&M dataset to reproduce a classic user-item, offline training-offline serving recommendation use case. 
 
 ![](../../../../static/assets/recsys-2-architecture.png)
 
 You will implement your own [reasonable scale recommender system](https://github.com/jacopotagliabue/recs-at-resonable-scale). To read more about the original work from [Ronay Ak](https://www.linkedin.com/in/ronay-ak/) and [Jacopo Tagliabue](https://www.linkedin.com/in/jacopotagliabue/) that this tutorial is based on, check out their [blog post](https://medium.com/nvidia-merlin/nvidia-merlin-meets-the-mlops-ecosystem-building-a-production-ready-recsys-pipeline-on-cloud-1a16c156166b) and this [webinar](https://www.youtube.com/live/ojsi3P-4zoI?feature=share).
 
 Stay tuned to get hands on and learn much more!
 
 <CustomCode height=""60vh"" sandboxLink=""https://account.outerbounds.dev/account/?workspace=/home/workspace/workspaces/recsys-2/workspace.code-workspace"">
 
 ```python title=""Tuning Merlin with Metaflow""
 from metaflow import FlowSpec, step, kubernetes, Parameter, current
 from custom_decorators import magicdir
 import os
 import json
 from datetime import datetime
 
 
 class MerlinTuningFlow(FlowSpec):
 
     ### MERLIN PARAMETERS ###
 
     MODEL_FOLDER = Parameter(
         name=""model_folder"",
         help=""Folder to store the model from Merlin, between steps"",
         default=""merlin_model"",
     )
 
     ### DATA PARAMETERS ###
 
     ROW_SAMPLING = Parameter(
         name=""row_sampling"",
         help=""Row sampling: if 0, NO sampling is applied. Needs to be an int between 1 and 100"",
         default=""1"",
     )
 
     # NOTE: data parameters - we split by time, leaving the last two weeks for validation and tests
     # The first date in the table is 2018-09-20
     # The last date in the table is 2020-09-22
     TRAINING_END_DATE = Parameter(
         name=""training_end_date"",
         help=""Data up until this date is used for training, format yyyy-mm-dd"",
         default=""2018-10-20"",
     )
 
     VALIDATION_END_DATE = Parameter(
         name=""validation_end_date"",
         help=""Data up after training end and until this date is used for validation, format yyyy-mm-dd"",
         default=""2018-11-20"",
     )
 
     ### TRAINING PARAMETERS ###
 
     VALIDATION_METRIC = Parameter(
         name=""validation_metric"",
         help=""Merlin metric to use for picking the best set of hyperparameter"",
         default=""recall_at_10"",
     )
 
     N_EPOCHS = Parameter(
         name=""n_epoch"",
         help=""Number of epochs to train the Merlin model"",
         default=""1"",  # default to 1 for quick testing
     )
 
     TOP_K = Parameter(
         name=""top_k"",
         help=""Number of products to recommend for a giver shopper"",
         default=""10"",
     )
 
     @step
     def start(self):
         """"""
         Start-up: check everything works or fail fast!
         """"""
         # print out some debug info
         print(""flow name: %s"" % current.flow_name)
         print(""run id: %s"" % current.run_id)
         print(""username: %s"" % current.username)
         # we need to check if Metaflow is running with remote (s3) data store or not
         from metaflow.metaflow_config import DATASTORE_SYSROOT_S3
 
         print(""DATASTORE_SYSROOT_S3: %s"" % DATASTORE_SYSROOT_S3)
         if DATASTORE_SYSROOT_S3 is None:
             print(""ATTENTION: LOCAL DATASTORE ENABLED"")
         # check variables and connections are working fine
         assert int(self.ROW_SAMPLING)
         # check the data range makes sense
         self.training_end_date = datetime.strptime(self.TRAINING_END_DATE, ""%Y-%m-%d"")
         self.validation_end_date = datetime.strptime(
             self.VALIDATION_END_DATE, ""%Y-%m-%d""
         )
         assert self.validation_end_date > self.training_end_date
 
         self.next(self.get_dataset)
 
     @step
     def get_dataset(self):
         """"""
         Get the data in the right shape using duckDb, after the dbt transformation
         """"""
         from pyarrow import Table as pt
         import duckdb
 
         # check if we need to sample - this is useful to iterate on the code with a real setup
         # without reading in too much data...
         _sampling = int(self.ROW_SAMPLING)
         sampling_expression = (
             """"
             if _sampling == 0
             else ""USING SAMPLE {} PERCENT (bernoulli)"".format(_sampling)
         )
         # thanks to our dbt preparation, the ML models can read in directly the data without additional logic
         query = """"""
             SELECT
                 ARTICLE_ID,
                 PRODUCT_CODE,
                 PRODUCT_TYPE_NO,
                 PRODUCT_GROUP_NAME,
                 GRAPHICAL_APPEARANCE_NO,
                 COLOUR_GROUP_CODE,
                 PERCEIVED_COLOUR_VALUE_ID,
                 PERCEIVED_COLOUR_MASTER_ID,
                 DEPARTMENT_NO,
                 INDEX_CODE,
                 INDEX_GROUP_NO,
                 SECTION_NO,
                 GARMENT_GROUP_NO,
                 ACTIVE,
                 FN,
                 AGE,
                 CLUB_MEMBER_STATUS,
                 CUSTOMER_ID,
                 FASHION_NEWS_FREQUENCY,
                 POSTAL_CODE,
                 PRICE,
                 SALES_CHANNEL_ID,
                 T_DAT
             FROM
                 read_parquet('filtered_dataframe.parquet')
                 {}
             ORDER BY
                 T_DAT ASC
         """""".format(
             sampling_expression
         )
         print(
             ""Fetching rows with query: \n {} \n\nIt may take a while...\n"".format(query)
         )
         # fetch raw dataset
         con = duckdb.connect(database="":memory:"")
         con.execute(query)
         dataset = con.fetchall()
         # convert the COLS to lower case (Keras does complain downstream otherwise)
         cols = [c[0].lower() for c in con.description]
         dataset = [{k: v for k, v in zip(cols, row)} for row in dataset]
         # debug
         print(""Example row"", dataset[0])
         self.item_id_2_meta = {str(r[""article_id""]): r for r in dataset}
         # we split by time window, using the dates specified as parameters
         # NOTE: we could actually return Arrow table directly, by then running three queries over
         # a different date range (e.g. https://duckdb.org/2021/12/03/duck-arrow.html)
         # For simplicity, we kept here the original flow compatible with warehouse processing
         train_dataset = pt.from_pylist(
             [row for row in dataset if row[""t_dat""] < self.training_end_date]
         )
         validation_dataset = pt.from_pylist(
             [
                 row
                 for row in dataset
                 if row[""t_dat""] >= self.training_end_date
                 and row[""t_dat""] < self.validation_end_date
             ]
         )
         test_dataset = pt.from_pylist(
             [row for row in dataset if row[""t_dat""] >= self.validation_end_date]
         )
         print(
             ""# {:,} events in the training set, {:,} for validation, {:,} for test"".format(
                 len(train_dataset), len(validation_dataset), len(test_dataset)
             )
         )
         # store and version datasets as a map label -> datasets, for consist processing later on
         self.label_to_dataset = {
             ""train"": train_dataset,
             ""valid"": validation_dataset,
             ""test"": test_dataset,
         }
         # go to the next step for NV tabular data
         self.next(self.build_workflow)
 
     # NOTE: we use the magicdir package (https://github.com/outerbounds/metaflow_magicdir)
     # to simplify moving the parquet files that Merlin needs / consumes across steps
     @magicdir
     @step
     def build_workflow(self):
         """"""
         Use NVTabular to transform the original data into the final dataframes for training,
         validation, testing.
         """"""
         from workflow_builder import get_nvt_workflow, read_to_dataframe
         import pandas as pd
         import nvtabular as nvt  # pylint: disable=import-error
         import itertools
 
         # read dataset into frames
         label_to_df = {}
         for label, dataset in self.label_to_dataset.items():
             label_to_df[label] = read_to_dataframe(dataset, label)
         full_dataset = nvt.Dataset(pd.concat(list(label_to_df.values())))
         # get the workflow and fit the dataset
         workflow = get_nvt_workflow()
         workflow.fit(full_dataset)
         self.label_to_melin_dataset = {}
         for label, _df in label_to_df.items():
             cnt_dataset = nvt.Dataset(_df)
             self.label_to_melin_dataset[label] = cnt_dataset
             workflow.transform(cnt_dataset).to_parquet(
                 output_path=""merlin/{}/"".format(label)
             )
         # store the mapping Merlin ID -> article_id and Merlin ID -> customer_id
         user_unique_ids = list(
             pd.read_parquet(""categories/unique.customer_id.parquet"")[""customer_id""]
         )
         items_unique_ids = list(
             pd.read_parquet(""categories/unique.article_id.parquet"")[""article_id""]
         )
         self.id_2_user_id = {idx: _ for idx, _ in enumerate(user_unique_ids)}
         self.id_2_item_id = {idx: _ for idx, _ in enumerate(items_unique_ids)}
         # sets of hypers
         # batch size
         batch_sizes = [16384, 4096]
         # learning rate
         learning_rates = [0.04, 0.02]
         grid_search = []
         for params in itertools.product(batch_sizes, learning_rates):
             grid_search.append({""BATCH_SIZE"": params[0], ""LEARNING_RATE"": params[1]})
         # we serialize hypers to a string and pass them to the foreach below
         # NOTE: to save time and compute, we limit the foreach to two hypers
         self.hypers_sets = [json.dumps(_) for _ in grid_search[:3]]
         # debug
         print(self.hypers_sets)
         self.next(self.train_model, foreach=""hypers_sets"")
 
     # @kubernetes(
     #     cpu=1,
     #     memory=12288,
     #     image='public.ecr.aws/outerbounds/merlin-reasonable-scale:22.11-latest'
     # )
     @magicdir
     @step
     def train_model(self):
         """"""
         Train models in parallel and store artifacts and validation KPIs for downstream consumption.
         """"""
         import hashlib
         import merlin.models.tf as mm  # pylint: disable=import-error
         from merlin.io.dataset import Dataset  # pylint: disable=import-error
         from merlin.schema.tags import Tags  # pylint: disable=import-error
         import tensorflow as tf  # pylint: disable=import-error
 
         # this is the CURRENT hyper param JSON in the fan-out
         # each copy of this step in the parallelization will have its own value
         self.hyper_string = self.input
         self.hypers = json.loads(self.hyper_string)
         train = Dataset(""merlin/train/*.parquet"")
         valid = Dataset(""merlin/valid/*.parquet"")
         print(
             ""Train dataset shape: {}, Validation: {}"".format(
                 train.to_ddf().compute().shape, valid.to_ddf().compute().shape
             )
         )
         # train the model and evaluate it on validation set
         user_schema = train.schema.select_by_tag(Tags.USER)  # MERLIN WARNING
         user_inputs = mm.InputBlockV2(user_schema)
         query = mm.Encoder(user_inputs, mm.MLPBlock([128, 64]))
         item_schema = train.schema.select_by_tag(Tags.ITEM)
         item_inputs = mm.InputBlockV2(
             item_schema,
         )
         candidate = mm.Encoder(item_inputs, mm.MLPBlock([128, 64]))
         model = mm.TwoTowerModelV2(query, candidate)
         opt = tf.keras.optimizers.Adagrad(learning_rate=self.hypers[""LEARNING_RATE""])
         model.compile(
             optimizer=opt,
             run_eagerly=False,
             metrics=[mm.RecallAt(int(self.TOP_K)), mm.NDCGAt(int(self.TOP_K))],
         )
         model.fit(
             train,
             validation_data=valid,
             batch_size=self.hypers[""BATCH_SIZE""],
             epochs=int(self.N_EPOCHS),
         )
         self.metrics = model.evaluate(valid, batch_size=1024, return_dict=True)
         print(""\n\n====> Eval results: {}\n\n"".format(self.metrics))
         # save the model
         model_hash = str(hashlib.md5(self.hyper_string.encode(""utf-8"")).hexdigest())
         self.model_path = ""merlin/model{}/"".format(model_hash)
         model.save(self.model_path)
         print(f""Model saved to {self.model_path}!"")
         self.next(self.join_runs)
 
     def get_items_topk_recommender_model(self, train_dataset, model, k: int):
         from merlin.models.utils.dataset import (
             unique_rows_by_features,
         )  # pylint: disable=import-error
         from merlin.schema.tags import Tags  # pylint: disable=import-error
 
         candidate_features = unique_rows_by_features(
             train_dataset, Tags.ITEM, Tags.ITEM_ID
         )
         topk_model = model.to_top_k_encoder(candidate_features, k=k, batch_size=128)
         topk_model.compile(run_eagerly=False)
 
         return topk_model
 
     @step
     def join_runs(self, inputs):
         """"""
         Join the parallel runs and merge results into a dictionary.
         """"""
         # merge results from runs with different parameters (key is hyper settings as a string)
         self.model_paths = {inp.hyper_string: inp.model_path for inp in inputs}
         self.results_from_runs = {
             inp.hyper_string: inp.metrics[self.VALIDATION_METRIC] for inp in inputs
         }
         print(""Current results: {}"".format(self.results_from_runs))
         # pick one according to some logic, e.g. higher VALIDATION_METRIC
         self.best_model, self_best_result = sorted(
             self.results_from_runs.items(), key=lambda x: x[1], reverse=True
         )[0]
         print(
             ""Best model is: {}, best path is {}"".format(
                 self.best_model, self.model_paths[self.best_model]
             )
         )
         # assign the variable for the ""final"" (the best) model path in S3 and its corresponding name
         self.final_model_path = self.model_paths[self.best_model]
         # pick a final mapping for metadata and other service variables
         best_model_idx = list(self.model_paths.keys()).index(self.best_model)
         self.item_id_2_meta = inputs[best_model_idx].item_id_2_meta
         self.id_2_item_id = inputs[best_model_idx].id_2_item_id
         self.id_2_user_id = inputs[best_model_idx].id_2_user_id
         self.magicdir = inputs[best_model_idx].magicdir
         # next, for the best model do more testing
         self.next(self.model_testing)
 
     def load_merlin_model(self, dataset, path):
         import tensorflow as tf  # pylint: disable=import-error
         import merlin.models.tf as mm  # pylint: disable=import-error
 
         loaded_model = tf.keras.models.load_model(path, compile=False)
         # this is necessary when re-loading the model, before building the top K
         _ = loaded_model(
             mm.sample_batch(dataset, batch_size=128, include_targets=False)
         )
         # debug
         print(""Model re-loaded!"")
 
         return loaded_model
 
     # @kubernetes(
     #     cpu=1,
     #     memory=12288,
     #     image='public.ecr.aws/outerbounds/merlin-reasonable-scale:22.11-latest'
     # )
     @magicdir
     @step
     def model_testing(self):
         """"""
         Test the generalization abilities of the best model through the held-out set...
         and RecList Beta (Forthcoming!)
         """"""
         from merlin.io.dataset import Dataset  # pylint: disable=import-error
         import merlin.models.tf as mm  # pylint: disable=import-error
         from merlin.schema import Tags  # pylint: disable=import-error
 
         # loading back datasets and the model for final testing
         test = Dataset(""merlin/test/*.parquet"")
         train = Dataset(""merlin/train/*.parquet"")
         loaded_model = self.load_merlin_model(train, self.final_model_path)
         topk_rec_model = self.get_items_topk_recommender_model(
             test, loaded_model, k=int(self.TOP_K)
         )
         # extract the target item id from the inputs
         test_loader = mm.Loader(
             test, batch_size=1024, transform=mm.ToTarget(test.schema, Tags.ITEM_ID)
         )
         self.test_metrics = topk_rec_model.evaluate(
             test_loader, batch_size=1024, return_dict=True
         )
         print(""\n\n====> Test results: {}\n\n"".format(self.test_metrics))
         # calculate recommendations
         topk_rec_model = self.get_items_topk_recommender_model(
             train, loaded_model, k=int(self.TOP_K)
         )
         self.best_predictions = self.get_recommendations(test, topk_rec_model)
         self.next(self.end)
 
     def get_recommendations(self, test, topk_rec_model):
         """"""
         Run predictions on a target dataset of shoppers (in this case, the testing dataset)
         and store the predictions for the cache downstream.
         """"""
         import merlin.models.tf as mm  # pylint: disable=import-error
 
         # export ONLY the users in the test set to simulate the set of shoppers we need to recommend items to
         test_dataset = mm.Loader(test, batch_size=1024, shuffle=False)
         # predict returns a tuple with two elements, scores and product IDs: we get the IDs only
         self.raw_predictions = topk_rec_model.predict(test_dataset)[1]
         n_rows = self.raw_predictions.shape[0]
         self.target_shoppers = test_dataset.data.to_ddf().compute()[""customer_id""]
         print(
             ""Inspect the shopper object for debugging...{}"".format(
                 type(self.target_shoppers)
             )
         )
         # check we have as many predictions as we have shoppers in the test set
         assert n_rows == len(self.target_shoppers)
         # map predictions to a final dictionary, with the actual H and M IDs for users and products
         self.h_m_shoppers = [
             str(self.id_2_user_id[_]) for _ in self.target_shoppers.to_numpy().tolist()
         ]
         print(""Example target shoppers: "", self.h_m_shoppers[:3])
         self.target_items = test_dataset.data.to_ddf().compute()[""article_id""]
         print(""Example target items: "", self.target_items[:3])
         predictions = self.serialize_predictions(
             self.h_m_shoppers,
             self.id_2_item_id,
             self.raw_predictions,
             self.target_items,
             n_rows,
         )
         print(""Example target predictions"", predictions[self.h_m_shoppers[0]])
         # debug, if rows > len(predictions), same user appears at least twice in test set
         print(n_rows, len(predictions))
 
         return predictions
 
     def serialize_predictions(
         self, h_m_shoppers, id_2_item_id, raw_predictions, target_items, n_rows
     ):
         """"""
         Convert raw predictions to a dictionary user -> items for easy re-use
         later in the pipeline (e.g. dump the predicted items to a cache!)
         """"""
         sku_convert = lambda x: [str(id_2_item_id[_]) for _ in x]
         predictions = {}
         for _ in range(n_rows):
             cnt_user = h_m_shoppers[_]
             cnt_raw_preds = raw_predictions[_].tolist()
             cnt_target = target_items[_]
             # don't overwite if we already have a prediction for this user
             if cnt_user not in predictions:
                 predictions[cnt_user] = {
                     ""items"": sku_convert(cnt_raw_preds),
                     ""target"": sku_convert([cnt_target])[0],
                 }
 
         return predictions
 
     @step
     def end(self):
         """"""
         Just say bye!
         """"""
         print(""All done\n\nSee you, recSys cowboy\n"")
         return
 
 
 if __name__ == ""__main__"":
     MerlinTuningFlow()
 ```
 
 </CustomCode>",H1,https://outerbounds.com/docs/recsys-tutorial-S2-overview#recommender-systems-with-metaflow-intermediate-tutorial,False,20459.0,5592.0
448,Objectives,,H2,https://outerbounds.com/docs/recsys-tutorial-S2-overview#objectives,False,0.0,1.0
449,Prerequisites,"Before starting this tutorial, we suggest you feel confident with the following topics
 * a good understanding of Metaflow and basic decorators including: `@step`, `@batch`, etc.; and familiarity with the basic concepts of machine learning such as training, validation, and test split for model evaluation;
 * a basic understanding of RecSys use cases: Jacopo's [intro using embeddings](https://github.com/jacopotagliabue/MLSys-NYU-2022/tree/main/weeks/12) is a great place to start;
 * a good understanding of RecSys pipelines: complete our [Beginner Recommender Systems Tutorial](https://outerbounds.com/docs/recsys-tutorial-overview/) if you have not done so already;
 * nice to have: familiarity with deep learning for the modeling part, and working knowledge of AWS Lambda and DynamoDB for the deployment part.",H2,https://outerbounds.com/docs/recsys-tutorial-S2-overview#prerequisites,False,815.0,107.0
450,Tutorial Structure,"The content includes the following:
 * [Episode 1: Process E-commerce Data](/docs/recsys-tutorial-S2E1)
 ![](../../../../static/assets/data-prep.png)
 * [Episode 2: Train Merlin](/docs/recsys-tutorial-S2E2)
 ![](../../../../static/assets/train-merlin.png)
 * [Episode 3: Tune Hyperparameters](/docs/recsys-tutorial-S2E3)
 ![](../../../../static/assets/tune-merlin.png)
 * [Episode 4: Deploy Serverless Predictions](/docs/recsys-tutorial-S2E4)
 ![](../../../../static/assets/e2e-recommender.png)",H2,https://outerbounds.com/docs/recsys-tutorial-S2-overview#tutorial-structure,False,494.0,31.0
451,Serverless deployment,"Once recommendations are computed and stored, the last thing we need to do is make sure they are available with low latency in a highly scalable fashion. Thanks to [PaaS](https://en.wikipedia.org/wiki/Platform_as_a_service) and [FaaS](https://en.wikipedia.org/wiki/Function_as_a_service) services such as AWS Lambda and DynamoDB, we can achieve this primarily from a workflow contained in a single Python file.
 
 If you wish to deploy your predictions and make them available through a cloud endpoint, you need to run the serverless project in the `serverless` folder _before_ running the flow: the project will create _both_ a DynamoDB table and a working GET endpoint. 
 
 
 First, install [Serverless Framework](https://www.serverless.com/framework/): 
 ```
 npm install -g serverless
 ```
 
 Then, [connect serverless with your AWS account credentials](https://www.serverless.com/framework/docs/providers/aws/guide/credentials/).  
 
 Second, `cd` into the `serverless` folder, and run `serverless deploy`. 
 
 ```bash
 cd serverless
 serverless deploy
 ```",H3,https://outerbounds.com/docs/recsys-tutorial-S2E4#serverless-deployment,False,1062.0,141.0
452,Test Your Endpoint,"If all goes well, the `serverless deploy` command will create the relevant resources and print out the URL for your public recommendation API, e.g. `endpoint: GET - https://xafacoa313.execute-api.us-west-2.amazonaws.com/dev/itemRecs`.
 
 :::tip
 Save the URL in the output somewhere. You may want to store it to an environment variable:
 ```bash
 export RECS_ENDPOINT=<YOUR_URL>
 ```
 :::
 
 You can verify the endpoint is working by pasting the URL in the browser (response will be empty as you need to run the flow to populate the cache). Make sure the region of deployment in the `serverless/serverless.yml` file is the same as the one in the Metaflow pipeline. Note that while we use the _serverless_ framework for convenience, the same setup can be done manually, if preferred.",H4,https://outerbounds.com/docs/recsys-tutorial-S2E4#test-your-endpoint,False,782.0,121.0
453,The end-to-end flow :ocean:,"Now that the endpoint is up and running, we need to populate our cache, DynamoDB, with the recommendations computed by our model. If the relevant Metaflow variable is set, the final step - `cache_predictions` - will loop over the stored predictions for our users and write documents to a key-value store, where the key represents the shopper id, and the value is a list of items the model thinks she would like.
 
 At prediction time, the endpoint will receive from our website a shopper ID, and it will retrieve the available list of items from the store!
 
 ![](../../../../static/assets/e2e-recommender.png)
 
 <CustomCode height=""60vh"">
 
 
 
 ```py title=""e2e_recommender_flow.py""
 from metaflow import FlowSpec, step, batch, Parameter, current, retry
 from custom_decorators import magicdir
 import os
 import json
 from datetime import datetime
 
 
 class End2EndRecommenderFlow(FlowSpec):
 
     ### MERLIN PARAMETERS ###
 
     MODEL_FOLDER = Parameter(
         name='model_folder',
         help='Folder to store the model from Merlin, between steps',
         default='merlin_model'
     )
 
     ### DATA PARAMETERS ###
 
     ROW_SAMPLING = Parameter(
         name='row_sampling',
         help='Row sampling: if 0, NO sampling is applied. Needs to be an int between 1 and 100',
         default='1'
     )
 
     #NOTE: data parameters - we split by time, leaving the last two weeks for validation and tests
     # The first date in the table is 2018-09-20
     # The last date in the table is 2020-09-22
     TRAINING_END_DATE = Parameter(
         name='training_end_date',
         help='Data up until this date is used for training, format yyyy-mm-dd',
         default='2020-09-08'
     )
 
     VALIDATION_END_DATE = Parameter(
         name='validation_end_date',
         help='Data up after training end and until this date is used for validation, format yyyy-mm-dd',
         default='2020-09-15'
     )
 
     ### TRAINING PARAMETERS ###
 
     VALIDATION_METRIC = Parameter(
         name='validation_metric',
         help='Merlin metric to use for picking the best set of hyperparameter',
         default='recall_at_10'
     )
 
     N_EPOCHS = Parameter(
         name='n_epoch',
         help='Number of epochs to train the Merlin model',
         default='1' # default to 1 for quick testing
     )
 
     ### SERVING PARAMETERS ###
 
     SAVE_TO_CACHE = Parameter(
         name='save_to_cache',
         help='Enable / disable (1/0) saving the best predictions to a key value store',
         default='0' # default to 0 - NO CACHING
     )
 
     DYNAMO_TABLE = Parameter(
         name='dynamo_table',
         help='Name of dynamo db table to store the pre-computed recs. Default is same as in the serverless application',
         default='userItemTable'
     )
 
     TOP_K = Parameter(
         name='top_k',
         help='Number of products to recommend for a giver shopper',
         default='10'
     )
 
     @step
     def start(self):
         """"""
         Start-up: check everything works or fail fast!
         """"""
         # print out some debug info
         print(""flow name: %s"" % current.flow_name)
         print(""run id: %s"" % current.run_id)
         print(""username: %s"" % current.username)
         # we need to check if Metaflow is running with remote (s3) data store or not
         from metaflow.metaflow_config import DATASTORE_SYSROOT_S3 
         print(""DATASTORE_SYSROOT_S3: %s"" % DATASTORE_SYSROOT_S3)
         if DATASTORE_SYSROOT_S3 is None:
             print(""ATTENTION: LOCAL DATASTORE ENABLED"")
         # check variables and connections are working fine
         assert int(self.ROW_SAMPLING)
         # check the data range makes sense
         self.training_end_date = datetime.strptime(self.TRAINING_END_DATE, '%Y-%m-%d')
         self.validation_end_date = datetime.strptime(self.VALIDATION_END_DATE, '%Y-%m-%d')
         assert self.validation_end_date > self.training_end_date
 
         self.next(self.get_dataset)
 
     @step
     def get_dataset(self):
         """"""
         Get the data in the right shape using duckDb, after the dbt transformation
         """"""
         from pyarrow import Table as pt
         import duckdb
         # check if we need to sample - this is useful to iterate on the code with a real setup
         # without reading in too much data...
         _sampling = int(self.ROW_SAMPLING)
         sampling_expression = '' if _sampling == 0 else 'USING SAMPLE {} PERCENT (bernoulli)'.format(_sampling)
         # thanks to our dbt preparation, the ML models can read in directly the data without additional logic
         query = """"""
             SELECT 
                 ARTICLE_ID,
                 PRODUCT_CODE, 
                 PRODUCT_TYPE_NO,
                 PRODUCT_GROUP_NAME,
                 GRAPHICAL_APPEARANCE_NO,
                 COLOUR_GROUP_CODE,
                 PERCEIVED_COLOUR_VALUE_ID,
                 PERCEIVED_COLOUR_MASTER_ID,
                 DEPARTMENT_NO,
                 INDEX_CODE,
                 INDEX_GROUP_NO,
                 SECTION_NO,
                 GARMENT_GROUP_NO,
                 ACTIVE,
                 FN,
                 AGE,
                 CLUB_MEMBER_STATUS,
                 CUSTOMER_ID,
                 FASHION_NEWS_FREQUENCY,
                 POSTAL_CODE,
                 PRICE,
                 SALES_CHANNEL_ID,
                 T_DAT
             FROM
                 read_parquet('filtered_dataframe.parquet')
                 {}
             ORDER BY
                 T_DAT ASC
         """""".format(sampling_expression)
         print(""Fetching rows with query: \n {} \n\nIt may take a while...\n"".format(query))
         # fetch raw dataset
         con = duckdb.connect(database=':memory:')
         con.execute(query)
         dataset = con.fetchall()
         # convert the COLS to lower case (Keras does complain downstream otherwise)
         cols = [c[0].lower() for c in con.description]
         dataset = [{ k: v for k, v in zip(cols, row) } for row in dataset]
         # debug
         print(""Example row"", dataset[0])
         self.item_id_2_meta = { str(r['article_id']): r for r in dataset }
         # we split by time window, using the dates specified as parameters
         # NOTE: we could actually return Arrow table directly, by then running three queries over
         # a different date range (e.g. https://duckdb.org/2021/12/03/duck-arrow.html)
         # For simplicity, we kept here the original flow compatible with warehouse processing
         train_dataset = pt.from_pylist([row for row in dataset if row['t_dat'] < self.training_end_date])
         validation_dataset = pt.from_pylist([row for row in dataset 
             if row['t_dat'] >= self.training_end_date and row['t_dat'] < self.validation_end_date])
         test_dataset = pt.from_pylist([row for row in dataset if row['t_dat'] >= self.validation_end_date])
         print(""# {:,} events in the training set, {:,} for validation, {:,} for test"".format(
             len(train_dataset),
             len(validation_dataset),
             len(test_dataset)
         ))
         # store and version datasets as a map label -> datasets, for consist processing later on
         self.label_to_dataset = {
             'train': train_dataset,
             'valid': validation_dataset,
             'test': test_dataset
         }
         # go to the next step for NV tabular data
         self.next(self.build_workflow)
     
     # NOTE: we use the magicdir package (https://github.com/outerbounds/metaflow_magicdir)
     # to simplify moving the parquet files that Merlin needs / consumes across steps
     @magicdir
     @step
     def build_workflow(self):
         """"""
         Use NVTabular to transform the original data into the final dataframes for training,
         validation, testing.
         """"""
         from workflow_builder import get_nvt_workflow, read_to_dataframe
         import pandas as pd
         import nvtabular as nvt # pylint: disable=import-error
         import itertools
         # read dataset into frames
         label_to_df = {}
         for label, dataset in self.label_to_dataset.items():
             label_to_df[label] = read_to_dataframe(dataset, label)
         full_dataset = nvt.Dataset(pd.concat(list(label_to_df.values())))
         # get the workflow and fit the dataset
         workflow = get_nvt_workflow()
         workflow.fit(full_dataset)
         self.label_to_melin_dataset = {}
         for label, _df in label_to_df.items():
             cnt_dataset = nvt.Dataset(_df)
             self.label_to_melin_dataset[label] = cnt_dataset
             workflow.transform(cnt_dataset).to_parquet(output_path=""merlin/{}/"".format(label))
         # store the mapping Merlin ID -> article_id and Merlin ID -> customer_id
         user_unique_ids = list(pd.read_parquet('categories/unique.customer_id.parquet')['customer_id'])
         items_unique_ids = list(pd.read_parquet('categories/unique.article_id.parquet')['article_id'])
         self.id_2_user_id = { idx:_ for idx, _ in enumerate(user_unique_ids) }
         self.id_2_item_id = { idx:_ for idx, _ in enumerate(items_unique_ids) }
         # sets of hypers
         # batch size
         batch_sizes = [ 16384, 4096]
         # learning rate
         learning_rates = [0.04, 0.02]
         grid_search = []
         for params in itertools.product(batch_sizes, learning_rates):
             grid_search.append({
                 'BATCH_SIZE': params[0],
                 'LEARNING_RATE': params[1]
             })
         # we serialize hypers to a string and pass them to the foreach below
         # NOTE: to save time and compute, we limit the foreach to two hypers
         self.hypers_sets = [json.dumps(_) for _ in grid_search[:3]]
         # debug
         print(self.hypers_sets)
         self.next(self.train_model, foreach='hypers_sets')
 
     @retry
     @batch(
         gpu=1,
         memory=24000,
         image='public.ecr.aws/outerbounds/merlin-reasonable-scale:22.11-latest'
     )
     @magicdir
     @step
     def train_model(self):
         """"""
         Train models in parallel and store artifacts and validation KPIs for downstream consumption.
         """"""
         import hashlib
         import merlin.models.tf as mm # pylint: disable=import-error
         from merlin.io.dataset import Dataset # pylint: disable=import-error
         from merlin.schema.tags import Tags # pylint: disable=import-error
         import tensorflow as tf # pylint: disable=import-error
         # this is the CURRENT hyper param JSON in the fan-out
         # each copy of this step in the parallelization will have its own value
         self.hyper_string = self.input
         self.hypers = json.loads(self.hyper_string)
         train = Dataset('merlin/train/*.parquet')
         valid = Dataset('merlin/valid/*.parquet')
         print(""Train dataset shape: {}, Validation: {}"".format(
             train.to_ddf().compute().shape,
             valid.to_ddf().compute().shape
             ))
         # train the model and evaluate it on validation set
         user_schema = train.schema.select_by_tag(Tags.USER) # MERLIN WARNING
         user_inputs = mm.InputBlockV2(user_schema)
         query = mm.Encoder(user_inputs, mm.MLPBlock([128, 64]))
         item_schema = train.schema.select_by_tag(Tags.ITEM)
         item_inputs = mm.InputBlockV2(item_schema,)
         candidate = mm.Encoder(item_inputs, mm.MLPBlock([128, 64]))
         model = mm.TwoTowerModelV2(query, candidate)
         opt = tf.keras.optimizers.Adagrad(learning_rate=self.hypers['LEARNING_RATE'])
         model.compile(
             optimizer=opt, 
             run_eagerly=False, 
             metrics=[mm.RecallAt(int(self.TOP_K)), mm.NDCGAt(int(self.TOP_K))],)
         model.fit(
             train, 
             validation_data=valid, 
             batch_size=self.hypers['BATCH_SIZE'], 
             epochs=int(self.N_EPOCHS))
         self.metrics = model.evaluate(valid, batch_size=1024, return_dict=True)
         print(""\n\n====> Eval results: {}\n\n"".format(self.metrics))
         # save the model
         model_hash = str(hashlib.md5(self.hyper_string.encode('utf-8')).hexdigest())
         self.model_path = 'merlin/model{}/'.format(model_hash)
         model.save(self.model_path)
         print(f""Model saved to {self.model_path}!"")
         self.next(self.join_runs)
 
     def get_items_topk_recommender_model(
         self,
         train_dataset, 
         model, 
         k: int
     ):
         from merlin.models.utils.dataset import unique_rows_by_features # pylint: disable=import-error
         from merlin.schema.tags import Tags # pylint: disable=import-error
         candidate_features = unique_rows_by_features(train_dataset, Tags.ITEM, Tags.ITEM_ID)
         topk_model = model.to_top_k_encoder(candidate_features, k=k, batch_size=128)
         topk_model.compile(run_eagerly=False)
 
         return topk_model
 
     @step
     def join_runs(self, inputs):
         """"""
         Join the parallel runs and merge results into a dictionary.
         """"""
         # merge results from runs with different parameters (key is hyper settings as a string)
         self.model_paths = { inp.hyper_string: inp.model_path for inp in inputs}
         self.results_from_runs = { inp.hyper_string: inp.metrics[self.VALIDATION_METRIC] for inp in inputs}
         print(""Current results: {}"".format(self.results_from_runs))
          # pick one according to some logic, e.g. higher VALIDATION_METRIC
         self.best_model, self_best_result = sorted(self.results_from_runs.items(), key=lambda x: x[1], reverse=True)[0]
         print(""Best model is: {}, best path is {}"".format(
             self.best_model,
             self.model_paths[self.best_model]
             ))
         # assign the variable for the ""final"" (the best) model path in S3 and its corresponding name
         self.final_model_path = self.model_paths[self.best_model]
         # pick a final mapping for metadata and other service variables
         best_model_idx = list(self.model_paths.keys()).index(self.best_model)
         self.item_id_2_meta = inputs[best_model_idx].item_id_2_meta
         self.id_2_item_id = inputs[best_model_idx].id_2_item_id
         self.id_2_user_id = inputs[best_model_idx].id_2_user_id
         self.magicdir = inputs[best_model_idx].magicdir
         # next, for the best model do more testing  
         self.next(self.model_testing)
 
     def load_merlin_model(
         self,
         dataset,
         path
     ):
         import tensorflow as tf # pylint: disable=import-error
         import merlin.models.tf as mm # pylint: disable=import-error
         loaded_model = tf.keras.models.load_model(path, compile=False)
         # this is necessary when re-loading the model, before building the top K
         _ = loaded_model(mm.sample_batch(dataset, batch_size=128, include_targets=False))
         # debug
         print(""Model re-loaded!"")
 
         return loaded_model
 
     @retry
     @batch(
         gpu=1,
         memory=24000,
         image='public.ecr.aws/outerbounds/merlin-reasonable-scale:22.11-latest'
     )
     @magicdir
     @step
     def model_testing(self):
         """"""
         Test the generalization abilities of the best model through the held-out set...
         and RecList Beta (Forthcoming!)
         """"""
         from merlin.io.dataset import Dataset # pylint: disable=import-error
         import merlin.models.tf as mm # pylint: disable=import-error
         from merlin.schema import Tags # pylint: disable=import-error
         # loading back datasets and the model for final testing
         test = Dataset('merlin/test/*.parquet')
         train = Dataset('merlin/train/*.parquet')
         loaded_model = self.load_merlin_model(train, self.final_model_path)
         topk_rec_model = self.get_items_topk_recommender_model(test, loaded_model, k=int(self.TOP_K))
         # extract the target item id from the inputs
         test_loader = mm.Loader(test, batch_size=1024, transform=mm.ToTarget(test.schema, Tags.ITEM_ID))
         self.test_metrics = topk_rec_model.evaluate(test_loader, batch_size=1024, return_dict=True)
         print(""\n\n====> Test results: {}\n\n"".format(self.test_metrics))        
         # calculate recommendations 
         topk_rec_model = self.get_items_topk_recommender_model(train, loaded_model, k=int(self.TOP_K))
         self.best_predictions = self.get_recommendations(test, topk_rec_model)
         # cache predictions
         self.next(self.cache_predictions)
             
     def get_recommendations(
         self,
         test,
         topk_rec_model 
     ):
         """"""
         Run predictions on a target dataset of shoppers (in this case, the testing dataset) 
         and store the predictions for the cache downstream.
         """"""
         import merlin.models.tf as mm # pylint: disable=import-error
         # export ONLY the users in the test set to simulate the set of shoppers we need to recommend items to
         test_dataset = mm.Loader(test, batch_size=1024, shuffle=False)
         # predict returns a tuple with two elements, scores and product IDs: we get the IDs only
         self.raw_predictions = topk_rec_model.predict(test_dataset)[1]
         n_rows = self.raw_predictions.shape[0]
         self.target_shoppers = test_dataset.data.to_ddf().compute()['customer_id']
         print(""Inspect the shopper object for debugging...{}"".format(type(self.target_shoppers)))
         # check we have as many predictions as we have shoppers in the test set
         assert n_rows == len(self.target_shoppers)
         # map predictions to a final dictionary, with the actual H and M IDs for users and products
         self.h_m_shoppers = [str(self.id_2_user_id[_]) for _ in self.target_shoppers.to_numpy().tolist()]
         print(""Example target shoppers: "", self.h_m_shoppers[:3])
         self.target_items = test_dataset.data.to_ddf().compute()['article_id']
         print(""Example target items: "", self.target_items[:3])
         predictions = self.serialize_predictions(
             self.h_m_shoppers,
             self.id_2_item_id,
             self.raw_predictions,
             self.target_items,
             n_rows
         )
         print(""Example target predictions"", predictions[self.h_m_shoppers[0]])
         # debug, if rows > len(predictions), same user appears at least twice in test set
         print(n_rows, len(predictions))
         
         return predictions
 
     def serialize_predictions(
         self,
         h_m_shoppers,
         id_2_item_id,
         raw_predictions,
         target_items,
         n_rows
     ):
         """"""
         Convert raw predictions to a dictionary user -> items for easy re-use 
         later in the pipeline (e.g. dump the predicted items to a cache!)
         """"""
         sku_convert = lambda x: [str(id_2_item_id[_]) for _ in x]
         predictions = {}
         for _ in range(n_rows):
             cnt_user = h_m_shoppers[_]
             cnt_raw_preds = raw_predictions[_].tolist()
             cnt_target = target_items[_]
             # don't overwite if we already have a prediction for this user
             if cnt_user not in predictions:
                 predictions[cnt_user] = {
                     'items': sku_convert(cnt_raw_preds),
                     'target': sku_convert([cnt_target])[0]
                 }
 
         return predictions
 
     @step
     def cache_predictions(self):
         """"""
         Use DynamoDb as a cache and a Lambda (in the serverless folder, check the README)
         to serve pre-computed predictions in a PaaS/FaaS manner.
 
         Note (see train_model above): we are just storing the predictions for the winning model, as 
         computed in the training step.
 
         """"""
 
         # skip the deployment if not needed
         if not bool(int(self.SAVE_TO_CACHE)):
             print(""Skipping deployment"")
             print(self.SAVE_TO_CACHE)
         else:
             print(""Caching predictions in DynamoDB"")
             import boto3
             dynamodb = boto3.resource('dynamodb')
             table = dynamodb.Table(self.DYNAMO_TABLE)
             # upload some static items as a test
             data = [{'userId': user, 'recs': json.dumps(recs) } for user, recs in self.best_predictions.items()] 
             # finally add test user
             data.append({'userId': 'no_user', 'recs': json.dumps(['test_rec_{}'.format(_) for _ in range(int(self.TOP_K))])})
             # loop over predictions and store them in the table
             with table.batch_writer() as writer:
                 for item in data:
                     writer.put_item(Item=item)
             print(""Predictions are all cached in DynamoDB"")
 
         self.next(self.end)
 
     @step
     def end(self):
         """"""
         Just say bye!
         """"""
         print(""All done\n\nSee you, recSys cowboy\n"")
         return
 
 
 if __name__ == '__main__':
     End2EndRecommenderFlow()
 ```
 
 </CustomCode>",H3,https://outerbounds.com/docs/recsys-tutorial-S2E4#the-end-to-end-flow-ocean,False,21155.0,5679.0
454,Run your flow :arrow_forward:,"```bash
 python e2e_recommender_flow.py run --save_to_cache=1
 ```
 
 <CodeOutputBlock lang=""bash"">
 
 ```
     ...
      [196136/start/1073932 (pid 63596)] Task is starting.
      [196136/start/1073932 (pid 63596)] flow name: End2EndRecommenderFlow
      [196136/start/1073932 (pid 63596)] run id: 196136
      [196136/start/1073932 (pid 63596)] username: eddie
      [196136/start/1073932 (pid 63596)] DATASTORE_SYSROOT_S3: s3://oleg2-s3-mztdpcvj/metaflow
      [196136/start/1073932 (pid 63596)] Task finished successfully.
     ...
      [196136/cache_predictions/1073940 (pid 64025)] Task is starting.
      [196136/cache_predictions/1073940 (pid 64025)] Skipping deployment
      [196136/cache_predictions/1073940 (pid 64025)] 0
      [196136/cache_predictions/1073940 (pid 64025)] Task finished successfully.
     ...
      [196136/end/1073941 (pid 64038)] Task is starting.
      [196136/end/1073941 (pid 64038)] All done
      [196136/end/1073941 (pid 64038)] 
      [196136/end/1073941 (pid 64038)] See you, recSys cowboy
      [196136/end/1073941 (pid 64038)] 
      [196136/end/1073941 (pid 64038)] Task finished successfully.
     ...
 ```
 
 </CodeOutputBlock>",H3,https://outerbounds.com/docs/recsys-tutorial-S2E4#run-your-flow-arrow-forward,False,1174.0,201.0
455,Request predictions from your Serverless endpoint :tada: :sparkles:,"Now you can easily request predictions from the endpoint using the command line, a notebook, and in your applications.
 Copy the URL that the `serverless deploy` command printed out for you at the beginning of this lesson and set it as an environment variable:
 ```
 export RECS_ENDPOINT=<YOUR URL>
 ```
 
 Open up a Python script or notebook and lets hit the endpoint!
 
 
 ```python
 import boto3
 import random
 import requests
 import os
 ```
 
 
 ```python",H3,https://outerbounds.com/docs/recsys-tutorial-S2E4#request-predictions-from-your-serverless-endpoint-tada-sparkles,False,461.0,77.0
456,Conclusion,,H3,https://outerbounds.com/docs/recsys-tutorial-S2E4#conclusion,False,0.0,1.0
457,What is the machine learning? :robot_face::brain:,"In particular, in this tutorial we use the [H&M dataset](https://www.kaggle.com/competitions/h-and-m-personalized-fashion-recommendations/data) to reproduce a classic user-item, offline training-offline serving recommendation use case, that is:
 
 * **user-item**: at test time, our model will receive a user identifier as the query, and will be tasked with suggesting k  (fashion) items that she is likely to buy. Since we have ground truth for past behavior, we can use previous purchases to test our model generalization abilities as we train it; 
 * **offline training-offline serving**: we assume our shoppers are, for the vast part, constants - that will allow us to build a pipeline that runs Monday night, makes k predictions for each user based on the data we have so far and stores those predictions in a cache. While not all use cases can be solved in this way, offline serving is a powerful pattern: a major benefit of this approach is that recommendations can be served with very low latency at a massive scale while maintaining high availability. This pattern is used by some of the most popular recommenders in the industry, such as [movie recommendations at Netflix.com](https://towardsdatascience.com/deep-dive-into-netflixs-recommender-system-341806ae3b48). For an example of offline training and online serving, you could check the end of our [previous tutorial](https://outerbounds.com/docs/recsys-tutorial-L6/).",H3,https://outerbounds.com/docs/recsys-tutorial-S2E1#what-is-the-machine-learning-robot-face-brain,False,1432.0,206.0
458,Download the data :arrow_down:,"The original dataset is [here](https://www.kaggle.com/competitions/h-and-m-personalized-fashion-recommendations/data). Download and unzip these three `.csv` files:
 * `articles.csv`
 * `customers.csv`
 * `transactions_train.csv`
 
 Then run `create_parquet_dataset.py` to obtain the basic parquet files for the flow. If you have concerns about your laptop capabilities, or you later run in to an `Out of Memory Error`, you can change the single argument of this script, which is a number between 0 and 1 that specifies the fraction of the original data you want to use in the downstream workflow.
 
 ```bash
 python create_parquet_dataset.py 0.25
 ```
 
 The result will of running this script will produce three `parquet` files, corresponding to each of the `.csv` files you downloaded.
 * `articles`
 * `customers`
 * `transactions_train`",H3,https://outerbounds.com/docs/recsys-tutorial-S2E1#download-the-data-arrow-down,False,840.0,117.0
459,Setup dbt profile and transform raw data,"Now we want to run transformations on the parquet files. 
 
 To simplify the downstream machine learning flow, we run data transformation *before* starting the flow in this example. If you want, you can also incorporate the transformation code in a flow, as demonstrated in this [project](https://github.com/jacopotagliabue/post-modern-stack), and explained in previous [Outerbounds posts](https://outerbounds.com/blog/dataops-mlops-reasonable-organizations/).",H3,https://outerbounds.com/docs/recsys-tutorial-S2E1#setup-dbt-profile-and-transform-raw-data,False,460.0,54.0
460,Motivation: Why use DuckDB in the transformation layer?,"In this tutorial, will use the [dbt DuckDB](https://github.com/jwills/dbt-duckdb) extension to run our [directed acyclic graph (DAG)](/docs/dags-in-data-science/) of transformations using DuckDB as a fast local analytic engine. This runs our transformations locally, in lieu of running transformations in a data warehouse. Using DuckDB in this way affords extremely powerful workflows for enterprise data scientists, where changing from a flexible local setup to a robust production environment becomes a simple config change.
 
 ![](../../../../static/assets/dbt-duckDB-snowflake.png)
 
 Running dbt transformations locally with DuckDB means we only need to change dbt profiles to run the transformations on a full-sized production dataset in a data warehouse like Snowflake or BigQuery. Later, you will see how Metaflow extends this parity between training and production environments to a [full machine learning stack](/docs/infra-stack/), making the dbt, DuckDB, and Metaflow combination a lean and portable toolchain for developing data-intensive applications.",H4,https://outerbounds.com/docs/recsys-tutorial-S2E1#motivation-why-use-duckdb-in-the-transformation-layer,False,1065.0,140.0
461,Setup,"To setup your `duckdb-metaflow` [profile for dbt](https://docs.getdbt.com/docs/get-started/connection-profiles) (you can pick another profile name if you want, the key here is the `type` and `path` sections), add the following in `~/.dbt/profiles.yml`:
 
 ```
 duckdb-metaflow:
   outputs:
    dev:
      type: duckdb
      path: ':memory:'
   target: dev
 ```
 
 In our example, the `dbt_project.yml` file in your working directory is *already configured* to use the profile when running the project. 
 
 To use it:
 * cd into the `dbt` folder;
 * `dbt run` will run the SQL transformations and produce the final output we need for our Python flow!
 
 ```bash
 cd dbt
 dbt run
 ```
 
 
 You can check that dbt ran the DAG successfully by seeing if a `filtered_dataframe.parquet` gets created in the project folder at the end - this contains the table we need, with all the data pre-joined, to run the flow.
 
 :::tip
 If you see an `Out of Memory Error`, return to section 1 of this page and select a lower number as the argument to the `create_parquet_dataset.py` script. This number in between 0 to 1 is proportional to the size of the `filtered_dataframe.parquet` output.
 :::",H4,https://outerbounds.com/docs/recsys-tutorial-S2E1#setup,False,1180.0,205.0
462,Query the data with DuckDB,"In the `DataPrepFlow`, you will see how to query the `filtered_dataframe.parquet` file using DuckDB, so it is usable in downstream steps.
 The objective is to run a SQL query that produces three [PyArrow tables](https://arrow.apache.org/docs/python/generated/pyarrow.Table.html) for training, validation, and test dataset. 
 
 Our final table is a wide, flat dataset containing a transaction for each row. A transaction is minimally defined by a timestamp, the ID of the customer, and the ID of the product. However, to leverage the power of the two-tower model you will meet in the next lesson of this tutorial we also report meta-data for the customer and the product, hoping this will facilitate the learning of better embeddings.
 
 ![](../../../../static/assets/data-prep.png)
 
 <CustomCode height=""60vh"">
 
 
 
 ```py title=""data_prep.py""
 from metaflow import FlowSpec, step, batch, Parameter, current
 from datetime import datetime
 
 class DataPrepFlow(FlowSpec):
     
     ### DATA PARAMETERS ###
     ROW_SAMPLING = Parameter(
         name='row_sampling',
         help='Row sampling: if 0, NO sampling is applied. Needs to be an int between 1 and 100',
         default='1'
     )
 
     # NOTE: data parameters - we split by time, leaving the last two weeks for validation and tests
     # The first date in the table is 2018-09-20
     # The last date in the table is 2020-09-22
     TRAINING_END_DATE = Parameter(
         name='training_end_date',
         help='Data up until this date is used for training, format yyyy-mm-dd',
         default='2020-09-08'
     )
 
     VALIDATION_END_DATE = Parameter(
         name='validation_end_date',
         help='Data up after training end and until this date is used for validation, format yyyy-mm-dd',
         default='2020-09-15'
     )
 
     @step
     def start(self):
         """"""
         Start-up: check everything works or fail fast!
         """"""
         # print out some debug info
         print(""flow name: %s"" % current.flow_name)
         print(""run id: %s"" % current.run_id)
         print(""username: %s"" % current.username)
         # we need to check if Metaflow is running with remote (s3) data store or not
         from metaflow.metaflow_config import DATASTORE_SYSROOT_S3 
         print(""DATASTORE_SYSROOT_S3: %s"" % DATASTORE_SYSROOT_S3)
         if DATASTORE_SYSROOT_S3 is None:
             print(""ATTENTION: LOCAL DATASTORE ENABLED"")
         # check variables and connections are working fine
         assert int(self.ROW_SAMPLING)
         # check the data range makes sense
         self.training_end_date = datetime.strptime(self.TRAINING_END_DATE, '%Y-%m-%d')
         self.validation_end_date = datetime.strptime(self.VALIDATION_END_DATE, '%Y-%m-%d')
         assert self.validation_end_date > self.training_end_date
 
         self.next(self.get_dataset)
 
     @step
     def get_dataset(self):
         """"""
         Get the data in the right shape using duckDb, after the dbt transformation
         """"""
         from pyarrow import Table as pt
         import duckdb
         # check if we need to sample - this is useful to iterate on the code with a real setup
         # without reading in too much data...
         _sampling = int(self.ROW_SAMPLING)
         sampling_expression = '' if _sampling == 0 else 'USING SAMPLE {} PERCENT (bernoulli)'.format(_sampling)
         # thanks to our dbt preparation, the ML models can read in directly the data without additional logic
         query = """"""
             SELECT 
                 ARTICLE_ID,
                 PRODUCT_CODE, 
                 PRODUCT_TYPE_NO,
                 PRODUCT_GROUP_NAME,
                 GRAPHICAL_APPEARANCE_NO,
                 COLOUR_GROUP_CODE,
                 PERCEIVED_COLOUR_VALUE_ID,
                 PERCEIVED_COLOUR_MASTER_ID,
                 DEPARTMENT_NO,
                 INDEX_CODE,
                 INDEX_GROUP_NO,
                 SECTION_NO,
                 GARMENT_GROUP_NO,
                 ACTIVE,
                 FN,
                 AGE,
                 CLUB_MEMBER_STATUS,
                 CUSTOMER_ID,
                 FASHION_NEWS_FREQUENCY,
                 POSTAL_CODE,
                 PRICE,
                 SALES_CHANNEL_ID,
                 T_DAT
             FROM
                 read_parquet('filtered_dataframe.parquet')
                 {}
             ORDER BY
                 T_DAT ASC
         """""".format(sampling_expression)
         print(""Fetching rows with query: \n {} \n\nIt may take a while...\n"".format(query))
         # fetch raw dataset
         con = duckdb.connect(database=':memory:')
         con.execute(query)
         dataset = con.fetchall()
         # convert the COLS to lower case (Keras does complain downstream otherwise)
         cols = [c[0].lower() for c in con.description]
         dataset = [{ k: v for k, v in zip(cols, row) } for row in dataset]
         # debug
         print(""Example row"", dataset[0])
         self.item_id_2_meta = { str(r['article_id']): r for r in dataset }
         # we split by time window, using the dates specified as parameters
         # NOTE: we could actually return Arrow table directly, by then running three queries over
         # a different date range (e.g. https://duckdb.org/2021/12/03/duck-arrow.html)
         # For simplicity, we kept here the original flow compatible with warehouse processing
         train_dataset = pt.from_pylist([row for row in dataset if row['t_dat'] < self.training_end_date])
         validation_dataset = pt.from_pylist([row for row in dataset 
             if row['t_dat'] >= self.training_end_date and row['t_dat'] < self.validation_end_date])
         test_dataset = pt.from_pylist([row for row in dataset if row['t_dat'] >= self.validation_end_date])
         print(""# {:,} events in the training set, {:,} for validation, {:,} for test"".format(
             len(train_dataset),
             len(validation_dataset),
             len(test_dataset)
         ))
         # store and version datasets as a map label -> datasets, for consist processing later on
         self.label_to_dataset = {
             'train': train_dataset,
             'valid': validation_dataset,
             'test': test_dataset
         }
         # go to the next step for NV tabular data
         self.next(self.end)
     
     @step
     def end(self):
         """"""
         Just say bye!
         """"""
         print(""All done\n\nSee you, recSys cowboy\n"")
         return
 
 if __name__ == '__main__':
     DataPrepFlow()
 ```
 
 </CustomCode>",H3,https://outerbounds.com/docs/recsys-tutorial-S2E1#query-the-data-with-duckdb,False,6560.0,1875.0
463,Run your flow :arrow_forward:,"```bash
 python data_prep.py run
 ```
 
 <CodeOutputBlock lang=""bash"">
 
 ```
      Workflow starting (run-id 1680413352353001):
      [1680413352353001/start/1 (pid 63488)] Task is starting.
      [1680413352353001/start/1 (pid 63488)] flow name: DataPrepFlow
      [1680413352353001/start/1 (pid 63488)] run id: 1680413352353001
      [1680413352353001/start/1 (pid 63488)] username: eddie
      [1680413352353001/start/1 (pid 63488)] DATASTORE_SYSROOT_S3: None
      [1680413352353001/start/1 (pid 63488)] ATTENTION: LOCAL DATASTORE ENABLED
      [1680413352353001/start/1 (pid 63488)] Task finished successfully.
      [1680413352353001/get_dataset/2 (pid 63491)] Task is starting.
      [1680413352353001/get_dataset/2 (pid 63491)] Fetching rows with query:
      [1680413352353001/get_dataset/2 (pid 63491)] 
      [1680413352353001/get_dataset/2 (pid 63491)] SELECT
      [1680413352353001/get_dataset/2 (pid 63491)] ARTICLE_ID,
      [1680413352353001/get_dataset/2 (pid 63491)] PRODUCT_CODE,
      [1680413352353001/get_dataset/2 (pid 63491)] PRODUCT_TYPE_NO,
      [1680413352353001/get_dataset/2 (pid 63491)] PRODUCT_GROUP_NAME,
      [1680413352353001/get_dataset/2 (pid 63491)] GRAPHICAL_APPEARANCE_NO,
      [1680413352353001/get_dataset/2 (pid 63491)] COLOUR_GROUP_CODE,
      [1680413352353001/get_dataset/2 (pid 63491)] PERCEIVED_COLOUR_VALUE_ID,
      [1680413352353001/get_dataset/2 (pid 63491)] PERCEIVED_COLOUR_MASTER_ID,
      [1680413352353001/get_dataset/2 (pid 63491)] DEPARTMENT_NO,
      [1680413352353001/get_dataset/2 (pid 63491)] INDEX_CODE,
      [1680413352353001/get_dataset/2 (pid 63491)] INDEX_GROUP_NO,
      [1680413352353001/get_dataset/2 (pid 63491)] SECTION_NO,
      [1680413352353001/get_dataset/2 (pid 63491)] GARMENT_GROUP_NO,
      [1680413352353001/get_dataset/2 (pid 63491)] ACTIVE,
      [1680413352353001/get_dataset/2 (pid 63491)] FN,
      [1680413352353001/get_dataset/2 (pid 63491)] AGE,
      [1680413352353001/get_dataset/2 (pid 63491)] CLUB_MEMBER_STATUS,
      [1680413352353001/get_dataset/2 (pid 63491)] CUSTOMER_ID,
      [1680413352353001/get_dataset/2 (pid 63491)] FASHION_NEWS_FREQUENCY,
      [1680413352353001/get_dataset/2 (pid 63491)] POSTAL_CODE,
      [1680413352353001/get_dataset/2 (pid 63491)] PRICE,
      [1680413352353001/get_dataset/2 (pid 63491)] SALES_CHANNEL_ID,
      [1680413352353001/get_dataset/2 (pid 63491)] T_DAT
      [1680413352353001/get_dataset/2 (pid 63491)] FROM
      [1680413352353001/get_dataset/2 (pid 63491)] read_parquet('filtered_dataframe.parquet')
      [1680413352353001/get_dataset/2 (pid 63491)] USING SAMPLE 1 PERCENT (bernoulli)
      [1680413352353001/get_dataset/2 (pid 63491)] ORDER BY
      [1680413352353001/get_dataset/2 (pid 63491)] T_DAT ASC
      [1680413352353001/get_dataset/2 (pid 63491)] 
      [1680413352353001/get_dataset/2 (pid 63491)] 
      [1680413352353001/get_dataset/2 (pid 63491)] It may take a while...
      [1680413352353001/get_dataset/2 (pid 63491)] 
      [1680413352353001/get_dataset/2 (pid 63491)] Example row {'article_id': 577513001, 'product_code': 577513, 'product_type_no': 265, 'product_group_name': 'Garment Full body', 'graphical_appearance_no': 1010016, 'colour_group_code': 73, 'perceived_colour_value_id': 4, 'perceived_colour_master_id': 2, 'department_no': 1313, 'index_code': 'A', 'index_group_no': 1, 'section_no': 11, 'garment_group_no': 1013, 'active': 1.0, 'fn': 1.0, 'age': 51.0, 'club_member_status': 'ACTIVE', 'customer_id': '1ae3948335af96c11695b7d88dd01edf086a3d0fc5fc1f41a3405789ef044454', 'fashion_news_frequency': 'Regularly', 'postal_code': 'd434fc1b4bd8a0d14111bfdc90ea72b8a9ccfe40adedd561de3c19a793b4ccde', 'price': 0.05083050951361656, 'sales_channel_id': 2, 't_dat': datetime.datetime(2018, 9, 20, 0, 0)}
      [1680413352353001/get_dataset/2 (pid 63491)] # 63,802 events in the training set, 468 for validation, 449 for test
      [1680413352353001/get_dataset/2 (pid 63491)] Task finished successfully.
      [1680413352353001/end/3 (pid 63494)] Task is starting.
      [1680413352353001/end/3 (pid 63494)] All done
      [1680413352353001/end/3 (pid 63494)] 
      [1680413352353001/end/3 (pid 63494)] See you, recSys cowboy
      [1680413352353001/end/3 (pid 63494)] 
      [1680413352353001/end/3 (pid 63494)] Task finished successfully.
      Done!
 ```
 
 </CodeOutputBlock>
 
 If this flow ran successfully, you have already done a lot! You now have a workflow that you can use to keep data transformation workflows in perfect parity locally and in your production warehouse(s). Moreover, you have built a solid foundation to start leveraging the power of Metaflow to extend this workflow by building and deploying complex models. Click through to the next lesson to start building up your recommender system training loop.",H3,https://outerbounds.com/docs/recsys-tutorial-S2E1#run-your-flow-arrow-forward,False,4801.0,673.0
464,How Metaflow Helps You Read and Write Data in the Cloud,"You can use the [AWS CLI](https://aws.amazon.com/cli/) or [boto3](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html) to communicate with AWS resources from Metaflow code, but using the Metaflow tools has a few advantages.",H3,https://outerbounds.com/docs/cv-tutorial-S2E3/#how-metaflow-helps-you-read-and-write-data-in-the-cloud,False,238.0,25.0
465,Move a Local Image Dataset to S3,"When working with valuable data in an organization, such as a large image dataset you have curated, you will eventually want to store it in the cloud.
 In [Episode 1](/docs/cv-tutorial-S2E1), you saw how to download the dataset. 
 Now, you will see how to push the data to an S3 bucket of your choosing.
 If you wish to run the code yourself you will need to choose an S3 bucket that you can write to. 
 You can read more about S3 policies and see examples [here](https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_examples_s3_rw-bucket.html).
 
 The following code snippet shows how you can upload the dataset, the two zip files downloaded in [Episode 1](/docs/cv-tutorial-S2E1/), to S3. 
 We use the `put_files` functionality from Metaflow's [S3 client](https://docs.metaflow.org/scaling/data#data-in-s3-metaflows3) for this.
 
 
 ```python
 from metaflow import S3",H3,https://outerbounds.com/docs/cv-tutorial-S2E3/#move-a-local-image-dataset-to-s3,False,884.0,128.0
466,Download an Image Dataset from S3,"We can also use Metaflow's S3 client to download the data.
 *The following code isn't necessary to run if you have been running the code this far, since you have already downloaded the data locally in the first episode of the tutorial.*
 
 The `_download_from_s3` function is used in flows to move the data from S3 and then unzip it on the instance where model training is done.
 In the next episode, you will see how this function is used in context in the `TrainHandGestureClassifier` flow.
 
 
 ```python
 import zipfile
 import os
 from metaflow import S3
 
 def _download_from_s3(file):
     with S3(s3root = self.S3_URI) as s3:
         result = s3.get(file)
         with zipfile.ZipFile(result.path, 'r') as zip_ref:
             zip_ref.extractall(
                 os.path.join(DATA_ROOT, file.split('.zip')[0])
             )",H3,https://outerbounds.com/docs/cv-tutorial-S2E3/#download-an-image-dataset-from-s3,False,836.0,178.0
467,Why use a Torch DataLoader?,"PyTorch's built-in `Dataset` and `Dataloader` objects simplify the processes between ingesting data and feeding it to a model.
 The objects provide abstractions that address requirements common to most, if not all, deep learning scenarios. 
 * The `Dataset` defines the structure and how to fetch data instances. 
 * The `Dataloader` leverages the `Dataset` to load batches of data that can easily be shuffled, sampled, transformed, etc.
 
 Importantly for many computer vision cases, this PyTorch functionality is built to scale to training large networks on large datasets and there are many optimization avenues to explore for advanced users.",H3,https://outerbounds.com/docs/cv-tutorial-S2E2/#why-use-a-torch-dataloader,False,645.0,99.0
468,What is a Torch DataLoader?,"The [torch.utils.data.Dataloader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) class helps you efficiently access batches from a dataset so you can feed them into your model.
 The `DataLoader` constructor has this signature:
 
 ```
 DataLoader(dataset, batch_size=1, shuffle=False, sampler=None,
            batch_sampler=None, num_workers=0, collate_fn=None,
            pin_memory=False, drop_last=False, timeout=0,
            worker_init_fn=None, *, prefetch_factor=2,
            persistent_workers=False)
 ```
 You can read more detail [here](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader). 
 The most important argument is the `dataset`, which should be an instance of a `torch.utils.data.DataLoader` object. 
 This object is what we will customize next.
 Then we can use it to instantiate `Dataloader` objects that follow the standard pattern for feeding data into a PyTorch model.",H3,https://outerbounds.com/docs/cv-tutorial-S2E2/#what-is-a-torch-dataloader,False,939.0,139.0
469,Build a Torch Dataset,"To create a `Dataloader`, we need to pass it a `Dataset`.  
 There are two ways to define a Torch `Dataset` object, the map and the iterable style. 
 The difference is whether the `torch.utils.data.Dataset` class defines the `__len__` and `__getitem__` functions (map type) or the `__iter__` function (iterable type). 
 You can read more about this distinction [here](https://pytorch.org/docs/stable/data.html#dataset-types).
 For now, all you need to know in the rest of this episode you will build a custom dataset with the HaGRID data `GestureDataset`.",H3,https://outerbounds.com/docs/cv-tutorial-S2E2/#build-a-torch-dataset,False,555.0,84.0
470,Example: Components of the GestureDataset,"In all remaining notebook examples and flows in this tutorial, we will use the `GestureDataset`. 
 Much of the code is reused from the original source, which you can view [here](https://github.com/hukenovs/hagrid/blob/master/classifier/dataset.py).
 The end goal is to create a `GestureDataset` object that we can easily use in model training code like the following snippet:
 
 ```python
 model = _initialize_model(model_name, checkpoint_path, device)
 train_dataset = GestureDataset(is_train=True, transform=get_transform())
 test_dataset = GestureDataset(is_train=False, transform=get_transform())
 TrainClassifier.train(model, train_dataset, test_dataset, device)
 ```
 
 This section shows how to implement the methods needed to use `GestureDataset`, or any custom dataset, as depicted in the above code. 
 More than the details of this specific example code, the main takeaway of this section is that when working with a custom `Dataset` class you need to:
 1. Your class should be a subclass of `torch.utils.data.Dataset`.
 2. You need to define the constructor.
 3. You either need to define the `__getitem__` and `__len__` methods, or define the `__iter__` method. You can put whatever you want in the different methods of your `Dataset` classes so long as the function signatures follow the [PyTorch protocol](https://pytorch.org/docs/stable/data.html#dataset-types).",H3,https://outerbounds.com/docs/cv-tutorial-S2E2/#example-components-of-the-gesturedataset,False,1377.0,180.0
471,The Dataset Constructor,"The `Dataset` constructor is called upon to create the dataset. 
 For `GestureDataset`, the constructor does the following:
 * Assign class variables for a configuration file, transformations, and dataset labels. 
 * Split the images and their annotations into training and validation sets.
 
 ```python
 class GestureDataset(torch.utils.data.Dataset):
 
     def __init__(self, is_train, conf, transform = None, is_test = False):
         self.conf = conf
         self.transform = transform
         self.is_train = is_train
         self.labels = {
             label: num for (label, num) in zip(self.conf.dataset.targets, range(len(self.conf.dataset.targets)))
         }
         self.leading_hand = {""right"": 0, ""left"": 1}
         subset = self.conf.dataset.get(""subset"", None)
         self.annotations = self.__read_annotations(subset)
         users = self.annotations[""user_id""].unique()
         users = sorted(users)
         random.Random(self.conf.random_state).shuffle(users)
         train_users = users[: int(len(users) * 0.8)]
         val_users = users[int(len(users) * 0.8) :]
         self.annotations = self.annotations.copy()
         if not is_test:
             if is_train:
                 self.annotations = self.annotations[self.annotations[""user_id""].isin(train_users)]
             else:
                 self.annotations = self.annotations[self.annotations[""user_id""].isin(val_users)]
                 
     ...
 ```",H4,https://outerbounds.com/docs/cv-tutorial-S2E2/#the-dataset-constructor,False,1450.0,341.0
472,Getting a Data Instance,"The `__getitem__` is a class method that allows instances of the `Dataset` class to be indexed like a list using `[]`. 
 In our case, we want this function to take an integer `index` and return an appropriately sized image and its label.
 
 ```python
 
 class GestureDataset(torch.utils.data.Dataset):
     
     ...
 
     def __getitem__(self, index: int):
         row = self.annotations.iloc[[index]].to_dict(""records"")[0]
         image_resized, gesture, leading_hand = self.__prepare_image_target(
             row[""target""], row[""name""], row[""bboxes""], row[""labels""], row[""leading_hand""]
         )
         label = {""gesture"": self.labels[gesture], ""leading_hand"": self.leading_hand[leading_hand]}
         if self.transform is not None:
             image_resized, label = self.transform(image_resized, label)
         return image_resized, label
     
     ...
 ```",H4,https://outerbounds.com/docs/cv-tutorial-S2E2/#getting-a-data-instance,False,875.0,183.0
473,Example: Using the GestureDataset,"In this section, you will use the `GestureDataset` to instantiate a `Dataloader` and visualize one batch of images with their labels.
 
 First, we will import dependencies. 
 
 
 ```python
 import torch
 from hagrid.classifier.dataset import GestureDataset
 from hagrid.classifier.preprocess import get_transform
 from hagrid.classifier.utils import collate_fn
 from omegaconf import OmegaConf
 from math import sqrt
 import matplotlib.pyplot as plt
 
 path_to_config = './hagrid/classifier/config/default.yaml'
 conf = OmegaConf.load(path_to_config)
 ```
 
 Then we instantiate the `GestureDataset` implemented [here](https://github.com/outerbounds/tutorials/cv-2/hagrid/classifier/dataset.py).
 
 
 ```python
 train_dataset = GestureDataset(is_train=True, conf=conf, transform=get_transform())
 ```
 
 Now, you can use the `train_dataset` to create a data loader to request batches from.
 
 
 ```python
 BATCH_SIZE = 16
 train_dataloader = torch.utils.data.DataLoader(
     train_dataset,
     batch_size=BATCH_SIZE,
     num_workers=1, # change this to load data faster. feasible values depend on your machine specs. 
     collate_fn=collate_fn, 
     shuffle=True 
     # What happens 
     # to the image grid displayed by the view_batch function
     # when you set shuffle=False in this constructor?
 )
 ```
 
 Here is a helper function to show the contents of a batch:
 
 
 ```python
 def view_batch(images, labels, batch_size):
     import matplotlib.pyplot as plt
     plt.ioff()
     grid_dim = (
         int(sqrt(batch_size)), 
         int(sqrt(batch_size)) + (1 if sqrt(batch_size) % 1 > 0 else 0)
     )
     fig, axes = plt.subplots(*grid_dim)
     for i, (image, label) in enumerate(zip(images, labels)):
         x, y = i//grid_dim[1], i%grid_dim[1]
         image = image.permute(1,2,0)
         axes[x, y].imshow(image) 
         axes[x, y].set_title(conf.dataset.targets[label['gesture']], fontsize=10)
         [axes[x, y].spines[_dir].set_visible(False) for _dir in ['right', 'left', 'top', 'bottom']]
         axes[x, y].set_xticks([])
         axes[x, y].set_yticks([])
     fig.tight_layout()
     fig.savefig(fname='./dataloader-sample.png')
 ```
 
 Now we can take the next batch from the `train_dataloader` and view a grid of each image and its corresponding label. 
 
 
 ```python
 images, labels = next(iter(train_dataloader))
 view_batch(images, labels, BATCH_SIZE)
 ```
 
 ![](dataloader-sample.png)
 
 Nice! Getting a reliable data flow is a big step in any machine learning project.
 In this lesson, you have just scratched the surface of the tools PyTorch offers to help you do this. 
 You learned about PyTorch datasets and data loaders in this episode.
 You saw to use them to efficiently and reliably load HaGRID dataset samples for training PyTorch models.
 Looking forward you will pair PyTorch data loaders with Metaflow features to extend the concepts when working with datasets in models in the cloud. 
 See you there!",H3,https://outerbounds.com/docs/cv-tutorial-S2E2/#example-using-the-gesturedataset,False,2963.0,500.0
474,Controlling TensorBoard Logs,"With TensorBoard, you can control where to log results using the `log_dir` parameter.
 You may want to do this in cases like the `train` step of the `TrainHandGestureClassifier`, 
 where we are writing TensorBoard logs from an ephemeral compute instance.
 The goal is to write these logs to a persistent location that we can read from any computer with access to the S3 object. 
 The approach taken here is to use the existing Metaflow datastore, and its built-in versioning capabilities, to organize TensorBoard logs produced in Metaflow runs.",H3,https://outerbounds.com/docs/cv-tutorial-S2E6/#controlling-tensorboard-logs,False,544.0,89.0
475,Store TensorBoard Results in S3,"In our case, we are using the [TensorBoard and PyTorch integration](https://pytorch.org/docs/stable/tensorboard.html). We can set the `log_dir` location like:
 
 ```python
 log_dir = os.path.join(tensorboard_s3_prefix, experiment_path, ""logs"")
 writer = torch.utils.tensorboard.SummaryWriter(log_dir=log_dir)
 ...
 writer.add_scalar(f""loss/train"", loss_value, step)
 ```
 
 If an s3 prefix is used for the `log_dir` argument of `SummaryWriter`, then TensorBoard will log results. 
 We can use the Metaflow config to determine where we want to write the results. 
 For example, you will see the following logic to set the TensorBoard log storage location in the `TrainHandGestureClassifier` code:
 ```python
 datastore = metaflow_config.METAFLOW_CONFIG['METAFLOW_DATASTORE_SYSROOT_S3']
 self.experiment_storage_prefix = os.path.join(datastore, current.flow_name, current.run_id)
 ```
 
 The `train` step will then write TensorBoard logs to `<experiment_storage_prefix>/experiments/logs`.",H3,https://outerbounds.com/docs/cv-tutorial-S2E6/#store-tensorboard-results-in-s3,False,986.0,108.0
476,View TensorBoard Results in S3,"```mdx-code-block
 import LiteYouTubeEmbed from 'react-lite-youtube-embed';
 import 'react-lite-youtube-embed/dist/LiteYouTubeEmbed.css';
 
 <div className=""video-container"">
 <LiteYouTubeEmbed
         id=""ZERRZjfS8Do""
         title=""Computer Vision Tutorial: Accessing TensorBoard""
 />
 </div>
 ```
 
 <Caption> A demonstration of accessing TensorBoard results using what the TrainHandGestureClassifier flow writes to stdout</Caption>
 
 
 After running the `TrainHandGestureClassifier` flow you will see a URI printed with the location where TensorBoard logs are stored. 
 You can run the following with your path:
 
 ```bash
 tensorboard --logdir=<tensorboard_s3_prefix>/experiments
 ```
 
 This can be run from the command line on your computer, assuming you have access to the S3 bucket which will be in the AWS account where your Metaflow deployment is.",H3,https://outerbounds.com/docs/cv-tutorial-S2E6/#view-tensorboard-results-in-s3,False,861.0,119.0
477,Summary,Congratulations! You have completed all of the episodes in our Computer Vision Training in the Cloud tutorial.,H3,https://outerbounds.com/docs/cv-tutorial-S2E6/#summary,False,110.0,17.0
478,Run in your Browser,"<OpenSandbox fileName=""Computer Vision with Metaflow - Intermediate"" link=""https://account.outerbounds.dev/account/?workspace=/home/workspace/workspaces/cv-2/workspace.code-workspace"">
 </OpenSandbox>",H2,https://outerbounds.com/docs/cv-tutorial-S2-setup#run-in-your-browser,False,200.0,9.0
479,Run on your own Infrastructure,"To run this tutorial locally or with your own Metaflow deployment, you will need to clone the repository, and then install dependencies using an environment manager of your choice. In the repository you will find the necessary dependencies in a `env.yml` file for conda. You can install them by following these instructions.",H2,https://outerbounds.com/docs/cv-tutorial-S2-setup#run-on-your-own-infrastructure,False,324.0,52.0
480,Clone the Repository,"```
 git clone https://github.com/outerbounds/tutorials.git
 ```",H3,https://outerbounds.com/docs/cv-tutorial-S2-setup#clone-the-repository,False,64.0,5.0
481,Install Dependencies with Conda,"To run the tutorial, you need to install the required dependencies via conda. We have included a conda environment in the [env.yml](https://github.com/outerbounds/tutorials/cv-2/env.yml) file.  You can install and activate the environment with this command:
 
 > We are using [`mamba`](https://mamba.readthedocs.io/en/latest/) instead of `conda` because it is significantly faster. However, you can use `conda` if you want to.
 
 ```
 cd tutorials/cv-2
 conda install mamba -n base -c conda-forge
 mamba env create -f env.yml
 conda activate mf-tutorial-cv-2
 ```",H3,https://outerbounds.com/docs/cv-tutorial-S2-setup#install-dependencies-with-conda,False,563.0,77.0
482,Running the Tutorials,Please follow the instructions in each lesson for running either the associated Python script or Jupyter Notebook.,H3,https://outerbounds.com/docs/cv-tutorial-S2-setup#running-the-tutorials,False,114.0,17.0
483,Introduction,"In this tutorial, you will build an image classifier on a large image dataset. You will learn how to move large amounts of data between your local environment, S3 storage, and remote compute instances where models are trained. You will fine-tune state-of-the-art model architectures on cloud GPUs and track results with Tensorboard. Before diving into these details, let's meet the dataset we will use to guide the tutorial.
 
 This tutorial has six episodes. If you want to run the code, you can follow along with this first page in this [Jupyter notebook](https://github.com/outerbounds/tutorials/tree/main/cv-2/cv-S2E1.ipynb).",H3,https://outerbounds.com/docs/cv-tutorial-S2E1/#introduction,False,629.0,93.0
484,What is the HaGRID Dataset?,"![](../../../../static/assets/hagrid.jpg)
 
 HaGRID is a large image dataset with labels and annotations for classification or detection tasks.
 The full HaGRID dataset is 716GB with 552,992 images divided into [18 classes of hand gestures](https://github.com/hukenovs/hagrid#tranval). Conveniently, the authors provide an evenly split (by class) 2GB sample that leads to cloud runs you can complete in one sitting.
 You can find more details in the [GitHub repository](https://github.com/hukenovs/hagrid) and corresponding paper, [HaGRID - HAnd Gesture Recognition Image Dataset](https://arxiv.org/abs/2206.08219).",H3,https://outerbounds.com/docs/cv-tutorial-S2E1/#what-is-the-hagrid-dataset,False,615.0,74.0
485,Download the Data,"You can use [wget](https://www.gnu.org/software/wget/) to download the subsample data from the URLs provided by the authors. The subsample will download 100 images from each class. Run the following from the command line to fetch the zipped data and place the zip file in the `data` directory.
 
 
 ```bash
 mkdir data && wget 'https://sc.link/AO5l' -O 'data/subsample.zip'
 ```
 
 Then you can unzip the resulting `subsample.zip` file. 
 
 
 ```bash
 unzip -qq 'data/subsample.zip' -d 'data/subsample'
 ```",H3,https://outerbounds.com/docs/cv-tutorial-S2E1/#download-the-data,False,507.0,76.0
486,View Sample Images,"Let's look at one class of images.
 You can see the available gesture labels by looking at the directories created when you unzipped the subsample. 
 
 
 ```bash
 ls 'data/subsample'
 ```
 
 <CodeOutputBlock lang=""bash"">
 
 ```
     call            like            palm            stop            two_up
     dislike         mute            peace           stop_inverted   two_up_inverted
     fist            ok              peace_inverted  three
     four            one             rock            three2
 ```
 
 </CodeOutputBlock>
 
 In the next cell, pick a `gesture` variable from one of the [18 dataset labels](https://github.com/hukenovs/hagrid#tranval).
 
 
 ```python
 relative_data_path = 'data/subsample'
 gesture = 'peace'
 ```
 
 Then we can grab sample images from the corresponding folder and visualize the result:
 
 <CustomCode height=""30vh"">
 
 
 
 ```python
 import os
 import glob
 import random
 import matplotlib.pyplot as plt
 from PIL import Image
 
 
 N_IMAGES = 3
 AX_DIM = 3
 path = (os.getcwd(), relative_data_path, gesture, '*.jpg')
 sample_images = random.sample(glob.glob(os.path.join(*path)), N_IMAGES)
 plt.ioff()
 fig, axes = plt.subplots(
     1, len(sample_images), 
     figsize = (AX_DIM * len(sample_images), AX_DIM)
 )
 fig.tight_layout()
 
 for img, ax in zip(sample_images, axes):
 
     # configure axis
     ax.spines['right'].set_visible(False)
     ax.spines['left'].set_visible(False)
     ax.spines['top'].set_visible(False)
     ax.spines['bottom'].set_visible(False)
     ax.set_xticks([])
     ax.set_yticks([])
 
     # display image
     ax.imshow(Image.open(img))
 
 fig.savefig(fname='{}-sample.png'.format(gesture));
 ```
 
 </CustomCode>
 
 
 ![](peace-sample.png)
 
 Similar to the command to download the images, you can download annotations using `wget`:
 
 
 ```bash
 wget 'https://sc.link/EQ5g' -O 'data/subsample-annotations.zip'
  unzip -qq 'data/subsample-annotations.zip'  -d 'data/subsample-annotations'
 ```
 
 <CodeOutputBlock lang=""bash"">
 
 ```
     --2022-11-14 14:00:23--  https://sc.link/EQ5g
     Resolving sc.link (sc.link)... 37.230.233.245
     Connecting to sc.link (sc.link)|37.230.233.245|:443... connected.
     HTTP request sent, awaiting response... 302 Found
     Location: https://n-usr-2uzac.s3pd02.sbercloud.ru/b-usr-2uzac-mv4/hagrid/ann_subsample.zip [following]
     --2022-11-14 14:00:24--  https://n-usr-2uzac.s3pd02.sbercloud.ru/b-usr-2uzac-mv4/hagrid/ann_subsample.zip
     Resolving n-usr-2uzac.s3pd02.sbercloud.ru (n-usr-2uzac.s3pd02.sbercloud.ru)... 37.18.122.129
     Connecting to n-usr-2uzac.s3pd02.sbercloud.ru (n-usr-2uzac.s3pd02.sbercloud.ru)|37.18.122.129|:443... connected.
     HTTP request sent, awaiting response... 200 OK
     Length: 1297620 (1.2M) [application/zip]
     Saving to: ‘./data/subsample-annotations.zip’
     
     ./data/subsample-an 100%[===================>]   1.24M  1.28MB/s    in 1.0s    
     
     2022-11-14 14:00:26 (1.28 MB/s) - ‘./data/subsample-annotations.zip’ saved [1297620/1297620]
     
 ```
 
 </CodeOutputBlock>
 
 Let's inspect the annotations. 
 The following code will draw a green box around the gesture of interest and a red box around other hands labeled in the image that are not making a gesture.
 These boxes correspond to the `bboxes` property that comes with each image annotation.
 
 <CustomCode height=""30vh"">
 
 
 
 ```python
 import json
 import cv2
 
 relative_annotation_path = 'data/subsample-annotations/ann_subsample/{}.json'.format(gesture)
 result = json.load(open(relative_annotation_path))
 
 color = None
 AX_DIM = 3
 plt.ioff()
 fig, axes = plt.subplots(1, len(sample_images), figsize = (AX_DIM * len(sample_images), AX_DIM))
 
 for im_file, ax in zip(sample_images, axes):
 
     # get image
     img_key = im_file.split('/')[-1].split('.')[0]
     image = cv2.imread(im_file)
     
     # openCV dims are BGR 
     b,g,r = cv2.split(image)  
     image = cv2.merge([r,g,b]) 
 
     # fetch bounding box for gesture
     for i, label in enumerate(result[img_key]['labels']):
 
         # determine annotation type
         if label == gesture:
             color = (0, 255, 0)
         elif label == 'no_gesture':
             color = (255, 0, 0)
 
         # unpack annotation format
         bbox = result[img_key]['bboxes'][i]
         top_left_x, top_left_y, w, h = bbox
         scale_x = image.shape[1]
         scale_y = image.shape[0]
 
         # draw bounding box to image scale
         x1 = int(top_left_x * scale_x)
         y1 = int(top_left_y * scale_y)
         x2 = int(x1 + scale_x * w)
         y2 = int(y1 + scale_y * h)
         cv2.rectangle(image, (x1, y1), (x2, y2), color, thickness=3)
         
         # display image
         ax.imshow(image)
 
     # configure axis
     ax.spines['right'].set_visible(False)
     ax.spines['left'].set_visible(False)
     ax.spines['top'].set_visible(False)
     ax.spines['bottom'].set_visible(False)
     ax.set_xticks([])
     ax.set_yticks([])
     ax.set_title(img_key, fontsize=8)
     
 fig.savefig('{}-sample-bbox.png'.format(gesture))
 ```
 
 </CustomCode>
 
 
 ![](peace-sample-bbox.png)",H3,https://outerbounds.com/docs/cv-tutorial-S2E1/#view-sample-images,False,5133.0,1027.0
487,A Baseline Gesture Classification Model,"The learning task of interest in this tutorial is to classify images by gesture. 
 In the previous section, you saw that each image comes with a `gesture` label and a bounding box in the corresponding annotation.
 Let's build a baseline model to predict the gesture for each image. 
 We use the majority-class classifier, which measures what happens when we predict all of examples in the test set with the majority class. 
 
 First, lets load the dataset using PyTorch objects you will learn about in the next episode.
 
 
 ```python
 import torch
 from hagrid.classifier.dataset import GestureDataset
 from hagrid.classifier.preprocess import get_transform
 from hagrid.classifier.utils import collate_fn
 from omegaconf import OmegaConf
 from torch import nn, Tensor
 
 path_to_config = './hagrid/classifier/config/default.yaml'
 conf = OmegaConf.load(path_to_config)
 N_CLASSES = 19 
 
 test_dataset = GestureDataset(is_train=False, conf=conf, transform=get_transform())
 test_dataloader = torch.utils.data.DataLoader(
     test_dataset,
     batch_size=conf.train_params.test_batch_size,
     num_workers=conf.train_params.num_workers,
     shuffle='random',
     collate_fn=collate_fn,
     persistent_workers = True,
     prefetch_factor=conf.train_params.prefetch_factor,
 )
 criterion = nn.CrossEntropyLoss()
 ```
 
 Then let's check the performance of the baseline model (always predict class 0) on one pass through the test set.
 
 Next, we collect the true targets next to compare to our benchmark approach.
 
 
 ```python
 from collections import defaultdict
 
 targets = defaultdict(list)
 n_targets_seen = defaultdict(int)
 for i, (images, labels) in enumerate(test_dataloader):
     accuracies = {target:[] for target in list(labels)[0].keys()}
     for target in list(labels)[0].keys():
         target_labels = [label[target] for label in labels]
         targets[target] += target_labels
         n_targets_seen[target] += len(target_labels)
         
 target = 'gesture'
 targets = torch.tensor(targets[target], dtype=torch.int32)
 predicts_labels = torch.zeros(n_targets_seen[target], dtype=torch.int32)
 ```
 
 Finally, we compute metric scores that we will be tracking on data subsets that we evaluate at the end of each epoch.
 
 
 ```python
 from torchmetrics.functional import accuracy, f1_score, precision, recall, auroc, confusion_matrix
 num_classes = 19
 average = conf.metric_params[""average""]
 metrics = conf.metric_params[""metrics""]
 scores = {
     ""accuracy"": accuracy(predicts_labels, targets, average=average, num_classes=num_classes).item(),
     ""f1_score"": f1_score(predicts_labels, targets, average=average, num_classes=num_classes).item(),
     ""precision"": precision(predicts_labels, targets, average=average, num_classes=num_classes).item(),
     ""recall"": recall(predicts_labels, targets, average=average, num_classes=num_classes).item()
 }
 scores
 ```
 
 <CodeOutputBlock lang=""python"">
 
 ```
     {'accuracy': 0.05177111551165581,
      'f1_score': 0.0050966376438736916,
      'precision': 0.002680248348042369,
      'recall': 0.05177111551165581}
 ```
 
 </CodeOutputBlock>
 
 In our baseline model, we see accuracy somewhere around 5% which makes sense given we have 18 evenly distributed classes.
 
 In this episode, you were introduced to the HaGRID dataset. Each data point is labeled with a class from 18 different hand gesture labels. In the rest of this tutorial, you will learn how to build a computer vision model training workflow to predict hand gesture classes using this data. The next episode starts this journey by introducing the fundamentals of PyTorch data loaders.",H3,https://outerbounds.com/docs/cv-tutorial-S2E1/#a-baseline-gesture-classification-model,False,3635.0,518.0
488,Checkpoint Models,"Checkpointing in model development essentially means that you save the state of a model, so you can resume it at a later time. This way you can make sure you do not lose results, such as your trained model. It also ensures you have a process to load an already trained model in future training and production scenarios while avoiding duplication of costly computation.
 
 In the PyTorch example used in the `TrainHandGestureClassifier` flow, a ""checkpoint"" refers to this code.
 
 ```python
 checkpoint_dict = {
     'state_dict': model.cpu().state_dict(),
     'optimizer_state_dict': optimizer.state_dict(),
     'epoch': epoch,
     'config': config_dict
 }
 torch.save(checkpoint_dict, checkpoint_path)
 ```
 
 You can save any artifact of the training work you have done in a `checkpoint_dict` like this.
 Then, you can resume the model state from the checkpoint.
 
 ```python
 from models.mobilenetv3 import MobileNetV3
 model = MobileNetV3(num_classes=num_classes, size='small', pretrained=pretrained, freezed=freezed)
 ...
 checkpoint = torch.load(checkpoint, map_location=torch.device(device))[""state_dict""]
 model.load_state_dict(checkpoint, strict=False)
 ...
 model.to(device)
 return model
 ```
 
 [Here](https://pytorch.org/tutorials/beginner/saving_loading_models.html#saving-loading-a-general-checkpoint-for-inference-and-or-resuming-training) are more general resources from the PyTorch documentation on checkpointing.",H3,https://outerbounds.com/docs/cv-tutorial-S2E5/#checkpoint-models,False,1435.0,177.0
489,Uploading the Best Model to the Cloud,"Model checkpoints in this example are written to the `best_model.pth` location. 
 But if we are running on a remote compute instance, how do we move this checkpoint to a cloud resource that will persist beyond the lifecycle of the compute task? Again, Metaflow's S3 client makes this easy!
 
 :::note
 There are many ways to structure model development workflows. You may not have to store model checkpoints in the cloud, for example. You also might prefer to use Metaflow's [IncludeFile pattern](/docs/load-local-data-with-include/) to move data of this type onto remote compute instances. 
 :::
 
 After each time model performs better than the previous best one, it is checkpointed and the result is uploaded to the cloud using this snippet:
 
 ```python
 path_to_best_model = os.path.join(experiment_path, 'best_model.pth')
 with S3(s3root = experiment_cloud_storage_path) as s3:
     s3.put_files([(path_to_best_model, path_to_best_model)])
 ```",H3,https://outerbounds.com/docs/cv-tutorial-S2E5/#uploading-the-best-model-to-the-cloud,False,950.0,139.0
490,Resuming the Best Model State,"The payoff of checkpointing in this way is that now you can easily resume the model from this state.
 
 In a notebook or Python script you can now evaluate the model, train it further, or iterate on the model architecture (PyTorch allows you to build [dynamic graphs](https://cs230.stanford.edu/section/5/)). 
 
 
 ```python
 from hagrid.classifier.run import _initialize_model
 from omegaconf import OmegaConf
 
 model_path = 'best_model.pth'
 
 try:
     model = _initialize_model(
         conf = OmegaConf.load('hagrid/classifier/config/default.yaml'),
         model_name = 'ResNet18', 
         checkpoint_path = model_path, # can be local or S3 URI. 
         device = 'cpu'
     )
 except FileNotFoundError:
     print(""Are you sure you trained a model and saved the file to {}"".format(model_path))
 ```
 
 <CodeOutputBlock lang=""python"">
 
 ```
     Building ResNet18
     Building model from local checkpoint at: best_model.pth
 ```
 
 </CodeOutputBlock>
 
 Because of how the `TrainHandGestureClassifier` uses Metaflow's built-in versioning capabilities, we can also resume model training with the `--checkpoint` parameter when you run the `TrainHandGestureClassifier` defined in `classifier_flow.py`. 
 
 ![](../../../../static/assets/cv-tutorial-2-IteratingTrainHandGestureClassifier.png)
 
 <Caption> Using model state at the end of run N of the TrainHandGestureClassifier flow as the initial state of the model in run N+1</Caption>
 
 
 This checkpoint parameter can either be a `.pth` file in an S3 bucket or a path to a local `.pth` file. In general, you can add arbitrary parameter types like this to your Metaflow flows in one line of code. This helps you write flexible code that can read in data from external systems in a variety of ways.
 
 ```mdx-code-block
 import LiteYouTubeEmbed from 'react-lite-youtube-embed';
 import 'react-lite-youtube-embed/dist/LiteYouTubeEmbed.css';
 
 <div className=""video-container"">
 <LiteYouTubeEmbed
         id=""-eyCldzlYgg""
         title=""Computer Vision Tutorial: Model Checkpointing""
 />
 </div>
 ```
 
 
 <Caption> A demonstration of using a model checkpoint from a past run in the command to run the TrainHandGestureClassifier flow</Caption>
 
 
 
 ```bash
 python classifier_flow.py --package-suffixes '.yaml' run --epochs 1 --model 'ResNet18' --checkpoint 'best_model.pth'
 ```
 
 <CodeOutputBlock lang=""bash"">
 
 ```
      Workflow starting (run-id 187938):
      [187938/start/1013688 (pid 28793)] Task is starting.
      [187938/start/1013688 (pid 28793)] Training ResNet18 in flow TrainHandGestureClassifier
      [187938/start/1013688 (pid 28793)] Task finished successfully.
      [187938/train/1013689 (pid 28800)] Task is starting.
      [187938/train/1013689 (pid 28800)] Downloading images...
      [187938/train/1013689 (pid 28800)] Done!
      [187938/train/1013689 (pid 28800)] Downloading annotations...
      [187938/train/1013689 (pid 28800)] Done!
      [187938/train/1013689 (pid 28800)] Building ResNet18
      [187938/train/1013689 (pid 28800)] Building model from local checkpoint at: best_model.pth
      [187938/train/1013689 (pid 28800)] Best model checkpoint saved at s3://oleg2-s3-mztdpcvj/metaflow/TrainHandGestureClassifier/187938/experiments/ResNet18/best_model.pth
      [187938/train/1013689 (pid 28800)] 
      [187938/train/1013689 (pid 28800)] View Tensorboard results in your browser with this command:
      [187938/train/1013689 (pid 28800)] tensorboard --logdir=s3://oleg2-s3-mztdpcvj/metaflow/TrainHandGestureClassifier/187938/experiments/ResNet18/logs
      [187938/train/1013689 (pid 28800)] 
      [187938/train/1013689 (pid 28800)] Best model checkpoint saved at s3://oleg2-s3-mztdpcvj/metaflow/TrainHandGestureClassifier/187938/experiments/ResNet18/best_model.pth
      [187938/train/1013689 (pid 28800)] Task finished successfully.
      [187938/end/1013690 (pid 28927)] Task is starting.
      [187938/end/1013690 (pid 28927)] Task finished successfully.
      Done!
 ```
 
 </CodeOutputBlock>
 
 In this lesson, you saw how to ensure you don't lose progress as you iterate on your model using checkpoints. 
 You learned how to store model checkpoints and resume that state from a notebook or as the starting point in a subsequent flow. 
 In the next lesson, we will complete the tutorial by demonstrating the use of [TensorBoard](https://www.tensorflow.org/tensorboard)'s experiment tracking solution with Metaflow.",H3,https://outerbounds.com/docs/cv-tutorial-S2E5/#resuming-the-best-model-state,False,4429.0,673.0
491,Transfer Learning 101,"Transfer learning helps you approach state-of-the-art results without spending too much on resources. It is a common pattern where a machine learning developer resumes the expensive work done to train models (trained by Google, Meta, Microsoft, etc.) instead of discarding it. In this example, we use a general image processing model and fine-tune it to our hand gesture classification use case. To do this, we leverage the `build_model` function defined in the `hagrid` package.
 
 
 ```python
 from hagrid.classifier.utils import build_model, get_device
 model = build_model(model_name = 'MobileNetV3_small', num_classes = 19, device = get_device())
 ```
 
 <CodeOutputBlock lang=""python"">
 
 ```
     Building MobileNetV3_small
 ```
 
 </CodeOutputBlock>
 
 Under the hood, this package calls one of the classes defined in the `models` directory, which you can inspect [here](https://github.com/hukenovs/hagrid/tree/master/classifier/models). The key lines that load the model leverage a custom class like `MobileNetV3` that subclasses [`torch.nn.module`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html).
 
 ```python
 from models.mobilenetv3 import MobileNetV3
 model = MobileNetV3(num_classes=num_classes, size='small', pretrained=pretrained, freezed=freezed)
 ```
 
 The custom classes, such as `MobileNetV3` in this case, each instantiate a torchvision model.
 
 ```python
 weights = torchvision.models.MobileNet_V3_Small_Weights if pretrained else None
 torchvision_model = torchvision.models.mobilenet_v3_small(weights=weights)
 ```
 
 Each custom class has its own constructor and `forward` step, like all neural networks in PyTorch. 
 An important part of working with transfer learning models is knowing how to stitch together a new output on the end of an existing classifier.
 For example, in the next code snippet, you can see the contents of the torch constructor for the `MobileNetV3` class, and how it uses torchvision to load the models above. Moreover, you can see how the loaded `torchvision_model` is then loaded with three parts including the ""transferred"" `backbone`, the `gesture_classifier`, and the `leading_hand_classifier`. 
 
 ```python
 if size == ""small"":
     weights = torchvision.models.MobileNet_V3_Small_Weights if pretrained else None
     torchvision_model = torchvision.models.mobilenet_v3_small(weights=weights)
     in_features = 576
     out_features = 1024
 else:
     weights = torchvision.models.MobileNet_V3_Large_Weights if pretrained else None
     torchvision_model = torchvision.models.mobilenet_v3_large(weights=weights)
     in_features = 960
     out_features = 1280
 
 if freezed:
     for param in torchvision_model.parameters():
         param.requires_grad = False
 
 self.backbone = nn.Sequential(
     torchvision_model.features,
     torchvision_model.avgpool
 )
 
 self.gesture_classifier = nn.Sequential(
     nn.Linear(in_features=in_features, out_features=out_features),
     nn.Hardswish(),
     nn.Dropout(p=0.2, inplace=True),
     nn.Linear(in_features=out_features, out_features=num_classes),
 )
 self.leading_hand_classifier = nn.Sequential(
     nn.Linear(in_features=in_features, out_features=out_features),
     nn.Hardswish(),
     nn.Dropout(p=0.2, inplace=True),
     nn.Linear(in_features=out_features, out_features=2),
 )
 
 ```
 
 Then, the `forward` function that is called when the neural net produces a prediction can then feed the output of the `backbone` part of the network into both classifiers. 
 
 ```python
 def forward(self, x: Tensor) -> Dict:
     x = self.backbone(x)
     x = x.view(x.size(0), -1)
     gesture = self.gesture_classifier(x)
     leading_hand = self.leading_hand_classifier(x)
     return {""gesture"": gesture, ""leading_hand"": leading_hand}
 ```
 
 This is an example of multi-task learning, a pattern that larger, more general models often use. As the model trains, its weights will be updated to improve both the `gesture_classifier` and `leading_hand_classifier`. You can view the model architecture with the `torchsummary` package:
 
 
 ```python
 from torchsummary import summary
 summary(model)
 ```
 
 <CodeOutputBlock lang=""python"">
 
 ```
     ======================================================================
     Layer (type:depth-idx)                        Param #
     ======================================================================
     ├─Sequential: 1-1                             --
     |    └─Sequential: 2-1                        --
     |    |    └─Conv2dNormActivation: 3-1         464
     |    |    └─InvertedResidual: 3-2             744
     |    |    └─InvertedResidual: 3-3             3,864
     |    |    └─InvertedResidual: 3-4             5,416
     |    |    └─InvertedResidual: 3-5             13,736
     |    |    └─InvertedResidual: 3-6             57,264
     |    |    └─InvertedResidual: 3-7             57,264
     |    |    └─InvertedResidual: 3-8             21,968
     |    |    └─InvertedResidual: 3-9             29,800
     |    |    └─InvertedResidual: 3-10            91,848
     |    |    └─InvertedResidual: 3-11            294,096
     |    |    └─InvertedResidual: 3-12            294,096
     |    |    └─Conv2dNormActivation: 3-13        56,448
     |    └─AdaptiveAvgPool2d: 2-2                 --
     ├─Sequential: 1-2                             --
     |    └─Linear: 2-3                            590,848
     |    └─Hardswish: 2-4                         --
     |    └─Dropout: 2-5                           --
     |    └─Linear: 2-6                            19,475
     ├─Sequential: 1-3                             --
     |    └─Linear: 2-7                            590,848
     |    └─Hardswish: 2-8                         --
     |    └─Dropout: 2-9                           --
     |    └─Linear: 2-10                           2,050
     ======================================================================
     Total params: 2,130,229
     Trainable params: 2,130,229
     Non-trainable params: 0
     ======================================================================
 
 
 
 
 
     ======================================================================
     Layer (type:depth-idx)                        Param #
     ======================================================================
     ├─Sequential: 1-1                             --
     |    └─Sequential: 2-1                        --
     |    |    └─Conv2dNormActivation: 3-1         464
     |    |    └─InvertedResidual: 3-2             744
     |    |    └─InvertedResidual: 3-3             3,864
     |    |    └─InvertedResidual: 3-4             5,416
     |    |    └─InvertedResidual: 3-5             13,736
     |    |    └─InvertedResidual: 3-6             57,264
     |    |    └─InvertedResidual: 3-7             57,264
     |    |    └─InvertedResidual: 3-8             21,968
     |    |    └─InvertedResidual: 3-9             29,800
     |    |    └─InvertedResidual: 3-10            91,848
     |    |    └─InvertedResidual: 3-11            294,096
     |    |    └─InvertedResidual: 3-12            294,096
     |    |    └─Conv2dNormActivation: 3-13        56,448
     |    └─AdaptiveAvgPool2d: 2-2                 --
     ├─Sequential: 1-2                             --
     |    └─Linear: 2-3                            590,848
     |    └─Hardswish: 2-4                         --
     |    └─Dropout: 2-5                           --
     |    └─Linear: 2-6                            19,475
     ├─Sequential: 1-3                             --
     |    └─Linear: 2-7                            590,848
     |    └─Hardswish: 2-8                         --
     |    └─Dropout: 2-9                           --
     |    └─Linear: 2-10                           2,050
     ======================================================================
     Total params: 2,130,229
     Trainable params: 2,130,229
     Non-trainable params: 0
     ======================================================================
 ```
 
 </CodeOutputBlock>
 
 We looked at a single example in this section.
 You can load one of the following models with the same process: [ResNet18](https://pytorch.org/vision/stable/models/generated/torchvision.models.resnet18.html#torchvision.models.resnet18), [ResNext50](https://pytorch.org/vision/stable/models/generated/torchvision.models.resnext50_32x4d.html#torchvision.models.resnext50_32x4d), [ResNet152](https://pytorch.org/vision/stable/models/generated/torchvision.models.resnet152.html#torchvision.models.resnet152), [MobileNetV3_small](https://pytorch.org/vision/stable/models/generated/torchvision.models.mobilenet_v3_small.html#torchvision.models.mobilenet_v3_small), [MobileNetV3_large](https://pytorch.org/vision/stable/models/generated/torchvision.models.mobilenet_v3_large.html#torchvision.models.mobilenet_v3_large), or [Vitb32](https://pytorch.org/vision/stable/models/generated/torchvision.models.vit_b_32.html#torchvision.models.vit_b_32).",H3,https://outerbounds.com/docs/cv-tutorial-S2E4/#transfer-learning-101,False,8993.0,2364.0
492,Training the Model,"Now we have access to data pipelines and powerful models for transfer learning. How do we use them together?
 The next script shows how to tie together what you have built so far in the following steps:
 * Load data using your custom PyTorch Dataset and DataLoader.
 * Build a model from `MobileNetV3_small`.
 * Run a single epoch of model training.
 
 <CustomCode height=""60vh"">
 
 
 
 ```python",H3,https://outerbounds.com/docs/cv-tutorial-S2E4/#training-the-model,False,396.0,68.0
493,Challenges when Accessing GPUs,"If you ran the code in the previous section, it probably took a little bit of time to finish that single training epoch. 
 It is no secret that hardware accelerators like Nvidia GPUs can help you increase your rate of experimentation. 
 But there is no shortage of difficulties when accessing these machines in a [full stack machine learning](/docs/infra-stack/) environment.
 
 With a bit of restructuring the code you've seen so far in this tutorial, Metaflow makes [GPU access straightforward](https://docs.metaflow.org/scaling/remote-tasks/introduction) for data scientists, data analysts, and operations researchers.
 Moreover, the stack used in next section's Metaflow will demonstrate a way to set up controls you can use to address the following challenges that emerge when using remote GPU instances:
 1. How do you guarantee dependencies on the remote compute instance if you don't have a local GPU? 
     * You can create a Docker image and access it with a Metaflow decorator like `@batch(image = X)` or `@kubernetes(image = X)`.
 2. How to efficiently load data batches so you don't pay for idle GPU instances?
     * With Metaflow's `@batch(memory = X)` or `@kubernetes(memory = X)` decorators you can control the amount of memory your compute instance needs. This works nicely with PyTorch DataLoader features like tuning the `num_workers` that are used to load mini-batches in separate processes to feed the model. 
 3. How to quickly iterate with different transfer learning model heads? 
     * In this section, you will see a flow where you can type the model name into the command as a string `--model <MODEL_NAME> --checkpoint <S3_PATH_TO_MODEL_NAME>.pth`, with the rest of the configuration remaining the same.
 4. How do you include local files such as the configuration at `hagrid/classifier/config/default.yaml`, on the GPU instance? 
     * Metaflow flows can be run with the `--package-suffixes <FILE EXTENSION>` top-level argument. In similar circumstances where you only want to include a single file, you can use Metaflow's [IncludeFile](/docs/load-local-data-with-include/) parameter.",H3,https://outerbounds.com/docs/cv-tutorial-S2E4/#challenges-when-accessing-gpus,False,2115.0,335.0
494,The TrainHandGestureClassifier Flow,Now we have all the ingredients we need to encapsulate data loading and model training logic in a flow.,H3,https://outerbounds.com/docs/cv-tutorial-S2E4/#the-trainhandgestureclassifier-flow,False,103.0,19.0
495,Run the TrainHandGestureClassifier Flow,"Now it is time to run the `TrainHandGestureClassifier`. 
 The model can be run using a command structured like this:
 
 ```bash
 python classifier_flow.py --package-suffixes '.yaml' run --model 'ResNet18' --checkpoint 'best_model.pth'
 ```",H3,https://outerbounds.com/docs/cv-tutorial-S2E4/#run-the-trainhandgestureclassifier-flow,False,239.0,32.0
496,Computer Vision with Metaflow: Intermediate Tutorial,"In this tutorial, you will build a set of workflows to train and evaluate a machine learning model that performs image classification. You will use PyTorch and Metaflow to write computer vision code you can use as a foundation for real-world data science projects.
 
 <CustomCode height=""60vh"" sandboxLink=""https://account.outerbounds.dev/account/?workspace=/home/workspace/workspaces/cv-2/workspace.code-workspace"">
 
 ```python title=""Computer Vision with Metaflow - Intermediate""
 from metaflow import FlowSpec, Parameter, step, batch, environment, S3, metaflow_config, current
 
 class TrainHandGestureClassifier(FlowSpec):
 
     S3_URI = Parameter(
         's3', type=str, 
         default='s3://outerbounds-tutorials/computer-vision/hand-gesture-recognition',
         help = 'The s3 uri to the root of the model objects.'
     )
 
     DATA_ROOT = Parameter(
         'data', type=str, default='data/',
         help = 'The relative location of the training data.'
     )
 
     IMAGES = Parameter(
         'images', type=str,
         default = 'subsample.zip',
         help = 'The path to the images.'
     )
 
     ANNOTATIONS = Parameter(
         'annotations', type=str,
         default = 'subsample-annotations.zip'
     )
 
     PATH_TO_CONFIG = Parameter(
         'config', type=str, 
         default = 'hagrid/classifier/config/default.yaml',
         help = 'The path to classifier training config.'
     )
     
     NUMBER_OF_EPOCHS = Parameter(
         'epochs', type=int, default=100,
         help = 'The number of epochs to train the model from.'
     )
 
     MODEL_NAME = Parameter(
         'model', type=str,
         default = 'MobileNetV3_small',
         help = '''Pick a model from:
             - [ResNet18, ResNext50, ResNet152, MobileNetV3_small, MobileNetV3_large, Vitb32]
         '''
     )
     
     CHECKPOINT_PATH = Parameter(
         'checkpoint', type=str, default = None, 
         help = 'Path to the model state you want to resume. Eithe'
     )
     
     # # If you do not plan to checkpoint models in S3, then you may want
     # # to use Metaflow's IncludeFile here, instead of this parameter to 
     # # the path. Make sure to import IncludeFile :)
     # CHECKPOINT_PATH = IncludeFile(
     #    'best_model.pth',
     #    is_text=False,
     #    help='The path to your local best_model.pth checkpoint',
     #    default='./best_model.pth'
     # )
 
     @step
     def start(self):
         # Configure the (remote) experiment tracking location.
         # In this tutorial, experiment tracking means
             # 1: Storing the best model state checkpoints to S3.
             # 2: Storing parameters as Metaflow artifacts.
             # 3: Storing metrics/logs with Tensorboard. 
         import os
         print(""Training {} in flow {}"".format(self.MODEL_NAME, current.flow_name))
         self.datastore = metaflow_config.METAFLOW_CONFIG['METAFLOW_DATASTORE_SYSROOT_S3']
         self.experiment_storage_prefix = os.path.join(self.datastore, current.flow_name, current.run_id)
         self.next(self.train)
 
     def _download_data_from_s3(self, file, sample : bool = True):
         import zipfile
         import os
         with S3(s3root = self.S3_URI) as s3:
             if sample:
                 path = os.path.join(self.DATA_ROOT, file)
                 result = s3.get(path)
                 with zipfile.ZipFile(result.path, 'r') as zip_ref:
                     zip_ref.extractall(path.split('.zip')[0])
             else: # Full dataset takes too long for the purpose of this tutorial.
                 raise NotImplementedError()
 
     # 🚨🚨🚨 Do you want to ▶️ on ☁️☁️☁️?
     # You need to be configured with a Metaflow AWS deployment to use this decorator.
     # If you want to run locally, you can comment the `@batch` decorator out.
     @batch(
         gpu=1,
         memory=32000,
         image='eddieob/cv-tutorial:gpu-latest',
         shared_memory=8000,
     )
     @step
     def train(self):
         from hagrid.classifier.run import run_train
         from hagrid.classifier.utils import get_device
         import os
         
         # Download the dataset onto the compute instance.
         if not os.path.exists(self.DATA_ROOT):
             os.mkdir(self.DATA_ROOT)
         print(""Downloading images..."")
         self._download_data_from_s3(self.IMAGES, sample=True)
         print(""Done!"")
         print(""Downloading annotations..."")
         self._download_data_from_s3(self.ANNOTATIONS, sample=True)
         print(""Done!"")
 
         # Train a model from available MODEL_NAME options from a checkpoint.
         # There will be errors that happen if CHECKPOINT_PATH doesn't match MODEL_NAME.
         # The user should know which checkpoint paths came from which models.
         self.train_args = dict(
             path_to_config = self.PATH_TO_CONFIG,
             number_of_epochs = self.NUMBER_OF_EPOCHS,
             device = get_device(),
             checkpoint_path = self.CHECKPOINT_PATH,
             model_name = self.MODEL_NAME,
             tensorboard_s3_prefix = self.experiment_storage_prefix,
             always_upload_best_model = True
         )
         _ = run_train(**self.train_args)
 
         # Move the best model checkpoint to S3 if METAFLOW_DATASTORE_SYSROOT_S3 is available. 
         # See the comment in the start step about setting self.experiment_storage_prefix.
         experiment_path = os.path.join(""experiments"", self.MODEL_NAME)
         path_to_best_model = os.path.join(experiment_path, 'best_model.pth')
         self.best_model_location = os.path.join(self.experiment_storage_prefix, path_to_best_model)
         if self.best_model_location.startswith('s3://'):
             with S3(s3root = self.experiment_storage_prefix) as s3:
                 s3.put_files([(path_to_best_model, path_to_best_model)])
                 print(""Best model checkpoint saved at {}"".format(self.best_model_location))
         self.next(self.end)
         
     @step
     def end(self):
         pass # You could do some fancy analytics, post-processing, or write a nice message here too! 
 
 if __name__ == '__main__':
     TrainHandGestureClassifier()
 ```
 </CustomCode>",H1,https://outerbounds.com/docs/cv-tutorial-S2-overview#computer-vision-with-metaflow-intermediate-tutorial,False,6225.0,1584.0
497,Prerequisites,"We assume that you have taken the [introductory tutorials](/docs/intro-tutorial-overview) or know the basics of Metaflow.
 This tutorial is directed toward learners familiar with model training concepts such as experiment tracking and model checkpointing. 
 It is not required but it will be helpful if you are familiar with [using Metaflow with AWS](https://docs.metaflow.org/scaling/introduction) and [PyTorch](https://pytorch.org/get-started/locally/). You will need access to a Metaflow deployment for the lessons that use cloud resources. Reach out in [Slack](http://slack.outerbounds.co) to get set up, or get a feel for what the full-feature set Metaflow can offer by signing up for your [free hosted Sandbox](https://account.outerbounds.dev/account/). Alternatively, you can check out our [beginner CV with Metaflow tutorial](/docs/cv-tutorial-overview/) which requires no cloud deployment.",H2,https://outerbounds.com/docs/cv-tutorial-S2-overview#prerequisites,False,898.0,113.0
498,Tutorial Structure,"The content includes the following:
 * [Episode 1](/docs/cv-tutorial-S2E1): Understand the Data
 * [Episode 2](/docs/cv-tutorial-S2E2): PyTorch Data Fundamentals
 * [Episode 3](/docs/cv-tutorial-S2E3): Store Images in the Cloud
 * [Episode 4](/docs/cv-tutorial-S2E4): Data Modeling in the Cloud
 * [Episode 5](/docs/cv-tutorial-S2E5): Model Checkpoints
 * [Episode 6](/docs/cv-tutorial-S2E6): Tracking with TensorBoard
 
 Each episode contains a Metaflow script to run or a Jupyter notebook. 
 The estimated time to read through the tutorial is 30 minutes to an hour. Running and adapting code will add a few more hours.",H2,https://outerbounds.com/docs/cv-tutorial-S2-overview#tutorial-structure,False,620.0,82.0
499,Computer Vision with Metaflow: Intermediate Tutorial,"In this tutorial, you will build a set of workflows to train and evaluate a machine learning model that performs image classification. You will use PyTorch and Metaflow to write computer vision code you can use as a foundation for real-world data science projects.
 
 <CustomCode height=""60vh"" sandboxLink=""https://account.outerbounds.dev/account/?workspace=/home/workspace/workspaces/cv-2/workspace.code-workspace"">
 
 ```python title=""Computer Vision with Metaflow - Intermediate""
 from metaflow import FlowSpec, Parameter, step, batch, environment, S3, metaflow_config, current
 
 class TrainHandGestureClassifier(FlowSpec):
 
     S3_URI = Parameter(
         's3', type=str, 
         default='s3://outerbounds-tutorials/computer-vision/hand-gesture-recognition',
         help = 'The s3 uri to the root of the model objects.'
     )
 
     DATA_ROOT = Parameter(
         'data', type=str, default='data/',
         help = 'The relative location of the training data.'
     )
 
     IMAGES = Parameter(
         'images', type=str,
         default = 'subsample.zip',
         help = 'The path to the images.'
     )
 
     ANNOTATIONS = Parameter(
         'annotations', type=str,
         default = 'subsample-annotations.zip'
     )
 
     PATH_TO_CONFIG = Parameter(
         'config', type=str, 
         default = 'hagrid/classifier/config/default.yaml',
         help = 'The path to classifier training config.'
     )
     
     NUMBER_OF_EPOCHS = Parameter(
         'epochs', type=int, default=100,
         help = 'The number of epochs to train the model from.'
     )
 
     MODEL_NAME = Parameter(
         'model', type=str,
         default = 'MobileNetV3_small',
         help = '''Pick a model from:
             - [ResNet18, ResNext50, ResNet152, MobileNetV3_small, MobileNetV3_large, Vitb32]
         '''
     )
     
     CHECKPOINT_PATH = Parameter(
         'checkpoint', type=str, default = None, 
         help = 'Path to the model state you want to resume. Eithe'
     )
     
     # # If you do not plan to checkpoint models in S3, then you may want
     # # to use Metaflow's IncludeFile here, instead of this parameter to 
     # # the path. Make sure to import IncludeFile :)
     # CHECKPOINT_PATH = IncludeFile(
     #    'best_model.pth',
     #    is_text=False,
     #    help='The path to your local best_model.pth checkpoint',
     #    default='./best_model.pth'
     # )
 
     @step
     def start(self):
         # Configure the (remote) experiment tracking location.
         # In this tutorial, experiment tracking means
             # 1: Storing the best model state checkpoints to S3.
             # 2: Storing parameters as Metaflow artifacts.
             # 3: Storing metrics/logs with Tensorboard. 
         import os
         print(""Training {} in flow {}"".format(self.MODEL_NAME, current.flow_name))
         self.datastore = metaflow_config.METAFLOW_CONFIG['METAFLOW_DATASTORE_SYSROOT_S3']
         self.experiment_storage_prefix = os.path.join(self.datastore, current.flow_name, current.run_id)
         self.next(self.train)
 
     def _download_data_from_s3(self, file, sample : bool = True):
         import zipfile
         import os
         with S3(s3root = self.S3_URI) as s3:
             if sample:
                 path = os.path.join(self.DATA_ROOT, file)
                 result = s3.get(path)
                 with zipfile.ZipFile(result.path, 'r') as zip_ref:
                     zip_ref.extractall(path.split('.zip')[0])
             else: # Full dataset takes too long for the purpose of this tutorial.
                 raise NotImplementedError()
 
     # 🚨🚨🚨 Do you want to ▶️ on ☁️☁️☁️?
     # You need to be configured with a Metaflow AWS deployment to use this decorator.
     # If you want to run locally, you can comment the `@batch` decorator out.
     @batch(
         gpu=1,
         memory=32000,
         image='eddieob/cv-tutorial:gpu-latest',
         shared_memory=8000,
     )
     @step
     def train(self):
         from hagrid.classifier.run import run_train
         from hagrid.classifier.utils import get_device
         import os
         
         # Download the dataset onto the compute instance.
         if not os.path.exists(self.DATA_ROOT):
             os.mkdir(self.DATA_ROOT)
         print(""Downloading images..."")
         self._download_data_from_s3(self.IMAGES, sample=True)
         print(""Done!"")
         print(""Downloading annotations..."")
         self._download_data_from_s3(self.ANNOTATIONS, sample=True)
         print(""Done!"")
 
         # Train a model from available MODEL_NAME options from a checkpoint.
         # There will be errors that happen if CHECKPOINT_PATH doesn't match MODEL_NAME.
         # The user should know which checkpoint paths came from which models.
         self.train_args = dict(
             path_to_config = self.PATH_TO_CONFIG,
             number_of_epochs = self.NUMBER_OF_EPOCHS,
             device = get_device(),
             checkpoint_path = self.CHECKPOINT_PATH,
             model_name = self.MODEL_NAME,
             tensorboard_s3_prefix = self.experiment_storage_prefix,
             always_upload_best_model = True
         )
         _ = run_train(**self.train_args)
 
         # Move the best model checkpoint to S3 if METAFLOW_DATASTORE_SYSROOT_S3 is available. 
         # See the comment in the start step about setting self.experiment_storage_prefix.
         experiment_path = os.path.join(""experiments"", self.MODEL_NAME)
         path_to_best_model = os.path.join(experiment_path, 'best_model.pth')
         self.best_model_location = os.path.join(self.experiment_storage_prefix, path_to_best_model)
         if self.best_model_location.startswith('s3://'):
             with S3(s3root = self.experiment_storage_prefix) as s3:
                 s3.put_files([(path_to_best_model, path_to_best_model)])
                 print(""Best model checkpoint saved at {}"".format(self.best_model_location))
         self.next(self.end)
         
     @step
     def end(self):
         pass # You could do some fancy analytics, post-processing, or write a nice message here too! 
 
 if __name__ == '__main__':
     TrainHandGestureClassifier()
 ```
 </CustomCode>",H1,https://outerbounds.com/docs/cv-tutorial-S2-overview#computer-vision-with-metaflow-intermediate-tutorial,False,6225.0,1584.0
500,Prerequisites,"We assume that you have taken the [introductory tutorials](/docs/intro-tutorial-overview) or know the basics of Metaflow.
 This tutorial is directed toward learners familiar with model training concepts such as experiment tracking and model checkpointing. 
 It is not required but it will be helpful if you are familiar with [using Metaflow with AWS](https://docs.metaflow.org/scaling/introduction) and [PyTorch](https://pytorch.org/get-started/locally/). You will need access to a Metaflow deployment for the lessons that use cloud resources. Reach out in [Slack](http://slack.outerbounds.co) to get set up, or get a feel for what the full-feature set Metaflow can offer by signing up for your [free hosted Sandbox](https://account.outerbounds.dev/account/). Alternatively, you can check out our [beginner CV with Metaflow tutorial](/docs/cv-tutorial-overview/) which requires no cloud deployment.",H2,https://outerbounds.com/docs/cv-tutorial-S2-overview#prerequisites,False,898.0,113.0
501,Tutorial Structure,"The content includes the following:
 * [Episode 1](/docs/cv-tutorial-S2E1): Understand the Data
 * [Episode 2](/docs/cv-tutorial-S2E2): PyTorch Data Fundamentals
 * [Episode 3](/docs/cv-tutorial-S2E3): Store Images in the Cloud
 * [Episode 4](/docs/cv-tutorial-S2E4): Data Modeling in the Cloud
 * [Episode 5](/docs/cv-tutorial-S2E5): Model Checkpoints
 * [Episode 6](/docs/cv-tutorial-S2E6): Tracking with TensorBoard
 
 Each episode contains a Metaflow script to run or a Jupyter notebook. 
 The estimated time to read through the tutorial is 30 minutes to an hour. Running and adapting code will add a few more hours.",H2,https://outerbounds.com/docs/cv-tutorial-S2-overview#tutorial-structure,False,620.0,82.0
502,Natural Language Processing with Metaflow Tutorial,"In this series of episodes, you will learn how to build, train, test, and deploy a machine learning model that performs text classification.  You will use Tensorflow, Scikit-learn, and Metaflow to operationalize a machine learning product using best practices for evaluating and testing.
 Lastly, you will learn how you can use this model in downstream processes.
 
 <CustomCode height=""60vh"" sandboxLink=""https://account.outerbounds.dev/account/?workspace=/home/workspace/workspaces/nlp/workspace.code-workspace"">
 
 ```python title=""Natural Language Processing with Metaflow""
 from metaflow import FlowSpec, step, Flow, current
 
 class NLPPredictionFlow(FlowSpec):
     
     def get_latest_successful_run(self, flow_nm, tag):
         """"""Gets the latest successful run 
             for a flow with a specific tag.""""""
         for r in Flow(flow_nm).runs(tag):
             if r.successful: return r
         
     @step
     def start(self):
         """"""Get the latest deployment candidate 
             that is from a successfull run""""""
         self.deploy_run = self.get_latest_successful_run(
             'NLPFlow', 'deployment_candidate')
         self.next(self.end)
     
     @step
     def end(self):
         ""Make predictions""
         from model import NbowModel
         import pandas as pd
         import pyarrow as pa
         new_reviews = pd.read_parquet(
             'predict.parquet')['review']
         
         # Make predictions
         model = NbowModel.from_dict(
             self.deploy_run.data.model_dict)
         predictions = model.predict(new_reviews)
         msg = 'Writing predictions to parquet: {} rows'
         print(msg.format(predictions.shape[0]))
         pa_tbl = pa.table({""data"": predictions.squeeze()})
         pa.parquet.write_table(
             pa_tbl, ""sentiment_predictions.parquet"")
         
 if __name__ == '__main__':
     NLPPredictionFlow()
 ```
 </CustomCode>
 
 If you want to code along, you can open up your sandbox, skip the rest of this setup page, and follow Hugo and Hamel Husain in the [Natural Language Processing meets MLOps Live Code Along](https://www.youtube.com/watch?v=wRS2YlOCGVY) on YouTube.
 
 ```mdx-code-block
 import LiteYouTubeEmbed from 'react-lite-youtube-embed';
 import 'react-lite-youtube-embed/dist/LiteYouTubeEmbed.css';
 
 <div className=""video-container"">
 <LiteYouTubeEmbed
         id=""wRS2YlOCGVY""
         title=""Natural Language Processing meets MLOps""
         params=""start=220""
 />
 </div>
 ```",H1,https://outerbounds.com/docs/nlp-tutorial-overview#natural-language-processing-with-metaflow-tutorial,False,2503.0,543.0
503,Prerequisites,We assume that you have taken the [introductory tutorials](/docs/intro-tutorial-overview) or know the basics of Metaflow.,H2,https://outerbounds.com/docs/nlp-tutorial-overview#prerequisites,False,121.0,15.0
504,Tutorial Structure,"The tutorial consists of seven episodes, all centering around a text classification task you will be introduced to in the first episode.
 * [Episode 1](/docs/nlp-tutorial-L1): Understand the Data
 * [Episode 2](/docs/nlp-tutorial-L2): Construct a Model
 * [Episode 3](/docs/nlp-tutorial-L3): Set Up a Baseline Flow
 * [Episode 4](/docs/nlp-tutorial-L4): Train your Model
 * [Episode 5](/docs/nlp-tutorial-L5): Evaluate your Model
 * [Episode 6](/docs/nlp-tutorial-L6): Use your Model in Python
 * [Episode 7](/docs/nlp-tutorial-L7): Make Batch Predictions in a Flow
 * [Episode 8](/docs/nlp-tutorial-L8): Create a Realtime Endpoint
 
 Each episode contains either a Metaflow script to run or a Jupyter notebook. 
 You do not need access to cloud computing or a [Metaflow deployment](/engineering/welcome/) to complete the episodes.
 The estimated time to complete all episodes is 1-2 hours.",H2,https://outerbounds.com/docs/nlp-tutorial-overview#tutorial-structure,False,890.0,119.0
505,Why Metaflow?,"The main benefit of using a data science workflow solution like Metaflow when prototyping is that your code will be built on a strong foundation for deploying to a production environment. 
 Metaflow is most useful when projects have scaling requirements, are mission-critical, and/or have many interacting parts. 
 You can read more at these links: 
 * [Should I Use Metaflow?](https://docs.metaflow.org/introduction/what-is-metaflow#should-i-use-metaflow)
 * [Why Metaflow?](https://docs.metaflow.org/introduction/why-metaflow)
 
 After completing the lessons, you will be able to transfer insights and code from the tutorial to your real-world data science projects.
 It is important to be mindful that this is a beginner tutorial so it will not reflect many important challenges to consider in production ML environments. For example, in production, you may consider using Metaflow features such as the `@conda` decorator for dependency management, `@batch` or `@kubernetes` for remote execution, and `@schedule` to automatically trigger jobs.",H2,https://outerbounds.com/docs/nlp-tutorial-overview#why-metaflow,False,1046.0,144.0
506,Run in your Browser,"<OpenSandbox fileName=""Natural Language Processing with Metaflow"" link=""https://account.outerbounds.dev/account/?workspace=/home/workspace/workspaces/nlp/workspace.code-workspace"">
 </OpenSandbox>",H2,https://outerbounds.com/docs/nlp-tutorial-setup#run-in-your-browser,False,196.0,8.0
507,Run on your own Infrastructure,"To run this tutorial locally or with your own Metaflow deployment, you will need to clone the repository, and then install dependencies using an environment manager of your choice. In the repository you will find the necessary dependencies in a `env.yml` file for conda. You can install them by following these instructions.",H2,https://outerbounds.com/docs/nlp-tutorial-setup#run-on-your-own-infrastructure,False,324.0,52.0
508,Clone the Repository,"```
 git clone https://github.com/outerbounds/tutorials.git
 ```",H3,https://outerbounds.com/docs/nlp-tutorial-setup#clone-the-repository,False,64.0,5.0
509,Install Dependencies with Conda,"To run the tutorial, you need to install the required dependencies via conda. We have included a conda environment in the form of an [env.yml](https://github.com/outerbounds/tutorials/nlp/env.yml) file for you to use.  You can install the environment via the following command:
 
 > We are using [`mamba`](https://mamba.readthedocs.io/en/latest/) instead of `conda` because it is significantly faster. However, you can use `conda` if you want to.
 
 ```
 cd tutorials/nlp
 conda install mamba -n base -c conda-forge
 mamba env create -f env.yml
 conda activate mf-tutorial-nlp
 ```",H3,https://outerbounds.com/docs/nlp-tutorial-setup#install-dependencies-with-conda,False,581.0,83.0
510,Running the Tutorials,Please follow the instructions in each lesson for running either the associated Python script or Jupyter Notebook.,H3,https://outerbounds.com/docs/nlp-tutorial-setup#running-the-tutorials,False,114.0,17.0
511,What is Branching?,"[Branching](https://docs.metaflow.org/metaflow/basics#branch) is a powerful feature in Metaflow that allows you complete steps in parallel instead of in a linear fashion.  To demonstrate this feature, we will construct our `baseline` and `train` steps as two branches that will execute in parallel.  It should be noted that anytime you use branching, you also need a `join` step to disambiguate the branches, which you can [read more about here](https://docs.metaflow.org/metaflow/basics#branch).",H3,https://outerbounds.com/docs/nlp-tutorial-L4/#what-is-branching,False,496.0,68.0
512,Write a Flow,"In this flow, we will modify the `start`  and `join` steps to achieve branching, as well as add a `train` step that will train our model.
 
 Below is a detailed explanation of the changes we are making to our original flow:
 
 1. **Create a branching workflow to create a baseline and candidate model in parallel**  in the `baseline` and `train` steps.
     - When we call `self.next(self.baseline, self.train)`,  this creates a [branching flow](https://docs.metaflow.org/metaflow/basics#branch) that will allow the `baseline` and  `train` steps to run in parallel.
 2. **Add a training step** The `train` step uses a neural-bag-of-words model to train a text classifier.  
     - We import the `NbowModel` module we created in Lesson 1.
     - We save this model in a special way by setting the `model_dict` property of our custom model to `self.model_dict`, which has the effect of storing this data in Metaflow's artifact store, where data is versioned and saved automatically.
 3. **Add a join step**: In this step, we will load our model using `NbowModel.from_dict(self.model_dict)` as well as disambiguate the data in our branches.
     - The join step can disambiguate data by referring to a specific step in the branch. For example, `inputs.train.df` refers to the `train` step, and specifically the `df` artifact stored in that step.
     - We print the performance metrics of our model and the baseline in this join step.
     
 ![](/assets/nlp-tutorial-BranchNLPFlow.png)
 
 <CustomCode height=""75vh"">
 
 
 
 ```py title=""branchflow.py""
 from metaflow import FlowSpec, step, Flow, current
 
 class BranchNLPFlow(FlowSpec):
         
     @step
     def start(self):
         ""Read the data""
         import pandas as pd
         self.df = pd.read_parquet('train.parquet')
         self.valdf = pd.read_parquet('valid.parquet')
         print(f'num of rows: {self.df.shape[0]}')
         self.next(self.baseline, self.train)
 
     @step
     def baseline(self):
         ""Compute the baseline""
         from sklearn.metrics import accuracy_score, roc_auc_score
         baseline_predictions = [1] * self.valdf.shape[0]
         self.base_acc = accuracy_score(
             self.valdf.labels, baseline_predictions)
         self.base_rocauc = roc_auc_score(
             self.valdf.labels, baseline_predictions)
         self.next(self.join)
 
     @step
     def train(self):
         ""Train the model""
         from model import NbowModel
         model = NbowModel(vocab_sz=750)
         model.fit(X=self.df['review'], y=self.df['labels'])
         self.model_dict = model.model_dict #save model
         self.next(self.join)
         
     @step
     def join(self, inputs):
         ""Compare the model results with the baseline.""
         import pandas as pd
         from model import NbowModel
         self.model_dict = inputs.train.model_dict
         self.train_df = inputs.train.df
         self.val_df = inputs.baseline.valdf
         self.base_rocauc = inputs.baseline.base_rocauc
         self.base_acc = inputs.baseline.base_acc
         model = NbowModel.from_dict(self.model_dict)
         
         self.model_acc = model.eval_acc(
             X=self.val_df['review'], labels=self.val_df['labels'])
         self.model_rocauc = model.eval_rocauc(
             X=self.val_df['review'], labels=self.val_df['labels'])
         
         print(f'Baseline Acccuracy: {self.base_acc:.2%}')
         print(f'Baseline AUC: {self.base_rocauc:.2}')
         print(f'Model Acccuracy: {self.model_acc:.2%}')
         print(f'Model AUC: {self.model_rocauc:.2}')
         self.next(self.end)
         
     @step
     def end(self):
         print('Flow is complete')
         
 
 if __name__ == '__main__':
     BranchNLPFlow()
 ```
 
 </CustomCode>",H3,https://outerbounds.com/docs/nlp-tutorial-L4/#write-a-flow,False,3765.0,849.0
513,Run the Flow,"```bash
 python branchflow.py run
 ```
 
 <CodeOutputBlock lang=""bash"">
 
 ```
      Workflow starting (run-id 1666721142756833):
      [1666721142756833/start/1 (pid 53077)] Task is starting.
      [1666721142756833/start/1 (pid 53077)] num of rows: 20377
      [1666721142756833/start/1 (pid 53077)] Task finished successfully.
      [1666721142756833/baseline/2 (pid 53080)] Task is starting.
      [1666721142756833/train/3 (pid 53081)] Task is starting.
      [1666721142756833/baseline/2 (pid 53080)] Task finished successfully.
      [1666721142756833/train/3 (pid 53081)] 319: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz
      [1666721142756833/train/3 (pid 53081)] Epoch 1/10
     510/510 [==============================] - 1s 846us/step - loss: 0.3544 - accuracy: 0.8510 - val_loss: 0.2970 - val_accuracy: 0.8759 - loss: 0.7140 - accuracy: 0.50
      [1666721142756833/train/3 (pid 53081)] Epoch 2/10
     510/510 [==============================] - 0s 668us/step - loss: 0.2967 - accuracy: 0.8779 - val_loss: 0.2954 - val_accuracy: 0.8739 loss: 0.2272 - accuracy: 0.87
      [1666721142756833/train/3 (pid 53081)] Epoch 3/10
     510/510 [==============================] - 0s 667us/step - loss: 0.2853 - accuracy: 0.8857 - val_loss: 0.2966 - val_accuracy: 0.8776 loss: 0.1206 - accuracy: 1.00
      [1666721142756833/train/3 (pid 53081)] Epoch 4/10
     510/510 [==============================] - 0s 660us/step - loss: 0.2757 - accuracy: 0.8899 - val_loss: 0.2972 - val_accuracy: 0.8763 loss: 0.2387 - accuracy: 0.90
      [1666721142756833/train/3 (pid 53081)] Epoch 5/10
     510/510 [==============================] - 0s 660us/step - loss: 0.2692 - accuracy: 0.8945 - val_loss: 0.3018 - val_accuracy: 0.8778 loss: 0.3284 - accuracy: 0.78
      [1666721142756833/train/3 (pid 53081)] Epoch 6/10
     510/510 [==============================] - 0s 663us/step - loss: 0.2622 - accuracy: 0.8998 - val_loss: 0.3032 - val_accuracy: 0.8751 loss: 0.1531 - accuracy: 0.96
      [1666721142756833/train/3 (pid 53081)] Epoch 7/10
     510/510 [==============================] - 0s 669us/step - loss: 0.2536 - accuracy: 0.9037 - val_loss: 0.3063 - val_accuracy: 0.8763 loss: 0.4173 - accuracy: 0.81
      [1666721142756833/train/3 (pid 53081)] Epoch 8/10
     510/510 [==============================] - 0s 663us/step - loss: 0.2408 - accuracy: 0.9120 - val_loss: 0.3117 - val_accuracy: 0.8754 loss: 0.1745 - accuracy: 0.93
      [1666721142756833/train/3 (pid 53081)] Epoch 9/10
     510/510 [==============================] - 0s 659us/step - loss: 0.2309 - accuracy: 0.9176 - val_loss: 0.3267 - val_accuracy: 0.8705 loss: 0.1803 - accuracy: 0.93
      [1666721142756833/train/3 (pid 53081)] Epoch 10/10
     510/510 [==============================] - 0s 668us/step - loss: 0.2260 - accuracy: 0.9223 - val_loss: 0.3341 - val_accuracy: 0.8734 loss: 0.1451 - accuracy: 0.93
      [1666721142756833/train/3 (pid 53081)] WARNING:absl:Function `_wrapped_model` contains input name(s) Input with unsupported characters which will be renamed to input in the SavedModel.
      [1666721142756833/train/3 (pid 53081)] Task finished successfully.
      [1666721142756833/join/4 (pid 53087)] Task is starting.
      [1666721142756833/join/4 (pid 53087)] 334: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz
     71/71 [==============================] - 0s 330us/stepin/4 (pid 53087)] 1/71 [..............................] - ETA: 
     71/71 [==============================] - 0s 304us/stepin/4 (pid 53087)] 1/71 [..............................] - ETA: 
      [1666721142756833/join/4 (pid 53087)] Baseline Acccuracy: 77.30%
      [1666721142756833/join/4 (pid 53087)] WARNING:absl:Function `_wrapped_model` contains input name(s) Input with unsupported characters which will be renamed to input in the SavedModel.
      [1666721142756833/join/4 (pid 53087)] Baseline AUC: 0.5
      [1666721142756833/join/4 (pid 53087)] Model Acccuracy: 87.10%
      [1666721142756833/join/4 (pid 53087)] Model AUC: 0.92
      [1666721142756833/join/4 (pid 53087)] Task finished successfully.
      [1666721142756833/end/5 (pid 53090)] Task is starting.
      [1666721142756833/end/5 (pid 53090)] Flow is complete
      [1666721142756833/end/5 (pid 53090)] Task finished successfully.
      Done!
 ```
 
 </CodeOutputBlock>
 
 We can see from the Metaflow logs that our model looks promising in that it is performing better than the baseline!  However, computing the baseline isn't just meant for the logs!  We should use the baseline alongside other tests to gate which models make it to production.
 
 In the next lesson, you will learn how to test our models and use tagging to manage which models are promoted to production.",H3,https://outerbounds.com/docs/nlp-tutorial-L4/#run-the-flow,False,4793.0,760.0
514,What is Tagging?,"Tags allow you to express opinions about the results of your and your colleagues' work, and, importantly, change those assessments at any time. In contrast to runs and artifacts that represent immutable facts (history shouldn't be rewritten), the way how you interpret those facts may change over time, which is reflected in tags.  This makes tags ideal for managing which models are promoted to the next step in your modeling workflow.
 
 You can add a tag to a flow with only a few lines of code.  Below is a snippet of code we will use to add tags in our flow:
 
 ```python
 from metaflow import Flow, current
 run = Flow(current.flow_name)[current.run_id]
 run.add_tag('deployment_candidate')
 ```",H3,https://outerbounds.com/docs/nlp-tutorial-L5/#what-is-tagging,False,701.0,116.0
515,Write a Flow,"In this flow, we modify our `end` step to apply the tag `deployment_candidate` if our model passes two tests: (1) a baseline (2) and a smoke test.
 
 Concretely, we will add the following to the `end` step:
 
 1. **A smoke test** that tests that the model is performing correctly against very easy examples that it should not be getting wrong.  A smoke test is a lightweight way to catch unexpected behaviors in your model, even if your model is beating the baseline. 
 2. **A comparison of the model with the baseline**. We are going to check if our model's AUC score is better than the baseline.  There are more advanced variations on this technique, including using other models for baselines, or requiring that your model performs better than the baseline by a specific margin.  We leave these variations as an exercise for the reader.
 3. **Add a tag** if our model passes the smoke test and beats the baseline.
 
 ![](/assets/nlp-tutorial-NLPFlow.png)
 
 <CustomCode height=""75vh"">
 
 
 
 ```py title=""nlpflow.py""
 from metaflow import FlowSpec, step, Flow, current
 
 class NLPFlow(FlowSpec):
         
     @step
     def start(self):
         ""Read the data""
         import pandas as pd
         self.df = pd.read_parquet('train.parquet')
         self.valdf = pd.read_parquet('valid.parquet')
         print(f'num of rows: {self.df.shape[0]}')
         self.next(self.baseline, self.train)
 
     @step
     def baseline(self):
         ""Compute the baseline""
         from sklearn.metrics import accuracy_score, roc_auc_score
         baseline_predictions = [1] * self.valdf.shape[0]
         self.base_acc = accuracy_score(
             self.valdf.labels, baseline_predictions)
         self.base_rocauc = roc_auc_score(
             self.valdf.labels, baseline_predictions)
         self.next(self.join)
 
     @step
     def train(self):
         ""Train the model""
         from model import NbowModel
         model = NbowModel(vocab_sz=750)
         model.fit(X=self.df['review'], y=self.df['labels'])
         self.model_dict = model.model_dict #save model
         self.next(self.join)
         
     @step
     def join(self, inputs):
         ""Compare the model results with the baseline.""
         import pandas as pd
         from model import NbowModel
         self.model_dict = inputs.train.model_dict
         self.train_df = inputs.train.df
         self.val_df = inputs.baseline.valdf
         self.base_rocauc = inputs.baseline.base_rocauc
         self.base_acc = inputs.baseline.base_acc
         model = NbowModel.from_dict(self.model_dict)
         
         self.model_acc = model.eval_acc(
             X=self.val_df['review'], labels=self.val_df['labels'])
         self.model_rocauc = model.eval_rocauc(
             X=self.val_df['review'], labels=self.val_df['labels'])
         
         print(f'Baseline Acccuracy: {self.base_acc:.2%}')
         print(f'Baseline AUC: {self.base_rocauc:.2}')
         print(f'Model Acccuracy: {self.model_acc:.2%}')
         print(f'Model AUC: {self.model_rocauc:.2}')
         self.next(self.end)
         
     @step
     def end(self):
         """"""Tags model as a deployment candidate
            if it beats the baseline and passes smoke tests.""""""
         from model import NbowModel
         model = NbowModel.from_dict(self.model_dict)
         
         self.beats_baseline = self.model_rocauc > self.base_rocauc
         print(f'Model beats baseline (T/F): {self.beats_baseline}')
         #smoke test to make sure model does the right thing.
         _tst_reviews = [
             ""poor fit its baggy in places where it isn't supposed to be."",
             ""love it, very high quality and great value""
         ]
         _tst_preds = model.predict(_tst_reviews)
         check_1 = _tst_preds[0][0] < .5
         check_2 = _tst_preds[1][0] > .5
         self.passed_smoke_test = check_1 and check_2
         msg = 'Model passed smoke test (T/F): {}'
         print(msg.format(self.passed_smoke_test))
         
         if self.beats_baseline and self.passed_smoke_test:
             print(""\n\nThis flow is ready for deployment! \U0001f6a2\U0001f6a2\U0001f6a2 \n\n"")
             run = Flow(current.flow_name)[current.run_id]
             run.add_tag('deployment_candidate')
         else:
             print(""\n\nThis flow failed some tests.\n\n"")
             
 
 if __name__ == '__main__':
     NLPFlow()
 ```
 
 </CustomCode>",H3,https://outerbounds.com/docs/nlp-tutorial-L5/#write-a-flow,False,4415.0,1095.0
516,Run the Flow,"```bash
 python nlpflow.py run
 ```
 
 <CodeOutputBlock lang=""bash"">
 
 ```
      Workflow starting (run-id 1680309275323813):
      [1680309275323813/start/1 (pid 34991)] Task is starting.
      [1680309275323813/start/1 (pid 34991)] num of rows: 20377
      [1680309275323813/start/1 (pid 34991)] Task finished successfully.
      [1680309275323813/baseline/2 (pid 34994)] Task is starting.
      [1680309275323813/train/3 (pid 34995)] Task is starting.
      [1680309275323813/baseline/2 (pid 34994)] Task finished successfully.
      [1680309275323813/train/3 (pid 34995)] 663: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz
      [1680309275323813/train/3 (pid 34995)] Epoch 1/10
     510/510 [==============================] - 1s 791us/step - loss: 0.3503 - accuracy: 0.8523 - val_loss: 0.2971 - val_accuracy: 0.8754 - loss: 0.6661 - accuracy: 0.65
      [1680309275323813/train/3 (pid 34995)] Epoch 2/10
     510/510 [==============================] - 0s 629us/step - loss: 0.2945 - accuracy: 0.8819 - val_loss: 0.2957 - val_accuracy: 0.8778 loss: 0.4071 - accuracy: 0.84
      [1680309275323813/train/3 (pid 34995)] Epoch 3/10
     510/510 [==============================] - 0s 629us/step - loss: 0.2814 - accuracy: 0.8869 - val_loss: 0.2991 - val_accuracy: 0.8741 loss: 0.2374 - accuracy: 0.90
      [1680309275323813/train/3 (pid 34995)] Epoch 4/10
     510/510 [==============================] - 0s 624us/step - loss: 0.2744 - accuracy: 0.8941 - val_loss: 0.2971 - val_accuracy: 0.8773 loss: 0.2606 - accuracy: 0.93
      [1680309275323813/train/3 (pid 34995)] Epoch 5/10
     510/510 [==============================] - 0s 622us/step - loss: 0.2664 - accuracy: 0.8971 - val_loss: 0.2977 - val_accuracy: 0.8773 loss: 0.3826 - accuracy: 0.87
      [1680309275323813/train/3 (pid 34995)] Epoch 6/10
     510/510 [==============================] - 0s 629us/step - loss: 0.2550 - accuracy: 0.9033 - val_loss: 0.3015 - val_accuracy: 0.8810 loss: 0.1779 - accuracy: 0.90
      [1680309275323813/train/3 (pid 34995)] Epoch 7/10
     510/510 [==============================] - 0s 623us/step - loss: 0.2455 - accuracy: 0.9100 - val_loss: 0.3039 - val_accuracy: 0.8783 loss: 0.1469 - accuracy: 0.93
      [1680309275323813/train/3 (pid 34995)] Epoch 8/10
     510/510 [==============================] - 0s 631us/step - loss: 0.2381 - accuracy: 0.9145 - val_loss: 0.3116 - val_accuracy: 0.8759 loss: 0.2759 - accuracy: 0.93
      [1680309275323813/train/3 (pid 34995)] Epoch 9/10
     510/510 [==============================] - 0s 630us/step - loss: 0.2282 - accuracy: 0.9182 - val_loss: 0.3175 - val_accuracy: 0.8786 loss: 0.2789 - accuracy: 0.90
      [1680309275323813/train/3 (pid 34995)] Epoch 10/10
     510/510 [==============================] - 0s 641us/step - loss: 0.2221 - accuracy: 0.9229 - val_loss: 0.3270 - val_accuracy: 0.8751 loss: 0.1545 - accuracy: 0.96
      [1680309275323813/train/3 (pid 34995)] 214: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.
      [1680309275323813/train/3 (pid 34995)] Task finished successfully.
      [1680309275323813/join/4 (pid 35000)] Task is starting.
      [1680309275323813/join/4 (pid 35000)] 704: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz
      [1680309275323813/join/4 (pid 35000)] Baseline Acccuracy: 77.30%
      [1680309275323813/join/4 (pid 35000)] 655: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.
      [1680309275323813/join/4 (pid 35000)] Baseline AUC: 0.5
      [1680309275323813/join/4 (pid 35000)] Model Acccuracy: 87.50%
      [1680309275323813/join/4 (pid 35000)] Model AUC: 0.91
      [1680309275323813/join/4 (pid 35000)] Task finished successfully.
      [1680309275323813/end/5 (pid 35003)] Task is starting.
      [1680309275323813/end/5 (pid 35003)] Model beats baseline (T/F): True
      [1680309275323813/end/5 (pid 35003)] 543: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz
      [1680309275323813/end/5 (pid 35003)] Model passed smoke test (T/F): True
      [1680309275323813/end/5 (pid 35003)] 169: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.
      [1680309275323813/end/5 (pid 35003)] 
      [1680309275323813/end/5 (pid 35003)] 
      [1680309275323813/end/5 (pid 35003)] This flow is ready for deployment! 🚢🚢🚢
      [1680309275323813/end/5 (pid 35003)] 
      [1680309275323813/end/5 (pid 35003)] 
      [1680309275323813/end/5 (pid 35003)] Task finished successfully.
      Done!
 ```
 
 </CodeOutputBlock>
 
 Now that we have tagged our model, based on standards, we can confidently use it in downstream workflows.  In the next lesson, we will explore different ways you can utilize the model you have trained.",H3,https://outerbounds.com/docs/nlp-tutorial-L5/#run-the-flow,False,5057.0,811.0
517,Load the Data,"We are going to build a model that classifies customer reviews as positive or negative sentiment, using the [Women's E-Commerce Clothing Reviews Dataset](https://www.kaggle.com/datasets/nicapotato/womens-ecommerce-clothing-reviews). Here is what the data looks like:
 
 
 ```python
 import pandas as pd
 df = pd.read_parquet('train.parquet')
 print(f'num of rows: {df.shape[0]}')
 ```
 
 <CodeOutputBlock lang=""python"">
 
 ```
     num of rows: 20377
 ```
 
 </CodeOutputBlock>
 
 The data is stored in a [parquet file](/docs/tabular-file-formats), which is a framework agnostic way of storing data that you are likely to encounter in the wild.  It works seamlessly with pandas and is a format that is commonly available if your data is already in a database. 
 
 
 ```python
 df.head()
 ```
     
 <HTMLOutputBlock >
 
 
 
 
 ```html
 <div>
 <style scoped>
     .dataframe tbody tr th:only-of-type {
         vertical-align: middle;
     }
 
     .dataframe tbody tr th {
         vertical-align: top;
     }
 
     .dataframe thead th {
         text-align: right;
     }
 </style>
 <table border=""1"" class=""dataframe"">
   <thead>
     <tr style=""text-align: right;"">
       <th></th>
       <th>labels</th>
       <th>review</th>
     </tr>
   </thead>
   <tbody>
     <tr>
       <th>0</th>
       <td>0</td>
       <td>Odd fit: I wanted to love this sweater but the...</td>
     </tr>
     <tr>
       <th>1</th>
       <td>1</td>
       <td>Very comfy dress: The quality and material of ...</td>
     </tr>
     <tr>
       <th>2</th>
       <td>0</td>
       <td>Fits nicely but fabric a bit thin: I ordered t...</td>
     </tr>
     <tr>
       <th>3</th>
       <td>1</td>
       <td>Great fit: Love these jeans, fit and style... ...</td>
     </tr>
     <tr>
       <th>4</th>
       <td>0</td>
       <td>Stretches out, washes poorly. wish i could ret...</td>
     </tr>
   </tbody>
 </table>
 </div>
 ```
 
 
 
 </HTMLOutputBlock>",H3,https://outerbounds.com/docs/nlp-tutorial-L1/#load-the-data,False,1942.0,456.0
518,Fit a Baseline Model,"Before we begin training a model, it is useful to set a baseline.  One such baseline is the majority-class classifier, which measures what happens when we label all of our examples with the majority class.  We can then calculate our performance metrics by using this baseline model, which in this case is [accuracy](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html) and the [area under the ROC curve](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html):
 
 
 ```python
 from sklearn.metrics import accuracy_score, roc_auc_score
 
 valdf = pd.read_parquet('valid.parquet')
 baseline_predictions = [1] * valdf.shape[0]
 base_acc = accuracy_score(valdf.labels, 
                           baseline_predictions)
 base_rocauc = roc_auc_score(valdf.labels, 
                             baseline_predictions)
 
 msg = 'Baseline Accuracy: {}\nBaseline AUC: {}'
 print(msg.format(round(base_acc,3), round(base_rocauc,3)))
 ```
 
 <CodeOutputBlock lang=""python"">
 
 ```
     Baseline Accuracy: 0.773
     Baseline AUC: 0.5
 ```
 
 </CodeOutputBlock>
 
 Now that we understand the dataset and the problem a bit more, we can start building our model.  We will draw upon machine learning techniques from [natural language processing](https://en.wikipedia.org/wiki/Natural_language_processing) to see if we can train an algorithm to predict the sentiment of these fashion reviews.",H3,https://outerbounds.com/docs/nlp-tutorial-L1/#fit-a-baseline-model,False,1438.0,223.0
519,Use the Client API to Fetch the Latest Run,"In addition to manipulating tags as seen in the previous lesson, the Metaflow [client API](https://docs.metaflow.org/api/client) allows you to access data from past runs.  For example, this is how you can retrieve a model tagged as a `deployment candidate` outside of a flow:
 
 
 ```python
 from metaflow import Flow
 
 def get_latest_successful_run(flow_nm, tag):
     ""Gets the latest successful run for a flow with a specific tag.""
     for r in Flow(flow_nm).runs(tag):
         if r.successful: return r
 ```
 
 The above code allows you to retrieve runs for flows matching `flow_nm` and filter them according to whether or not they are tagged. Finally, we check if the run is successful with the `successful` property.",H3,https://outerbounds.com/docs/nlp-tutorial-L6/#use-the-client-api-to-fetch-the-latest-run,False,725.0,127.0
520,Load the Model,"After retrieving the model's data with the client API, we can load the model like this:
 
 
 ```python
 from model import NbowModel
 
 run = get_latest_successful_run('NLPFlow', 'deployment_candidate')
 model = NbowModel.from_dict(run.data.model_dict)
 ```",H3,https://outerbounds.com/docs/nlp-tutorial-L6/#load-the-model,False,256.0,32.0
521,Make Predictions with the Model,"Now that we have retrieved the model using the tag we can use it to make predictions:
 
 
 ```python
 import pandas as pd
 
 predict_df = pd.read_parquet('predict.parquet')
 preds = model.predict(predict_df['review'])
 preds
 ```
 
 <CodeOutputBlock lang=""python"">
 
 ```
     2023-03-31 17:41:26.790229: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz
 
 
 
 
 
     array([[0.9973424 ],
            [0.98123443],
            [0.99737483],
            ...,
            [0.9996966 ],
            [0.9987401 ],
            [0.4805344 ]], dtype=float32)
 ```
 
 </CodeOutputBlock>",H3,https://outerbounds.com/docs/nlp-tutorial-L6/#make-predictions-with-the-model,False,623.0,143.0
522,Save Predictions,"You can write these predictions to a parquet file like so:
 
 
 ```python
 import pyarrow as pa
 pa_tbl = pa.table({""data"": preds.squeeze()})
 pa.parquet.write_table(pa_tbl, ""sentiment_predictions.parquet"")
 ```
 
 In this episode, you saw how to use the results of a completed flow run, in this case accessing a trained model to make predictions on new data. In the next lesson, you will see how to access the model from a different flow.",H3,https://outerbounds.com/docs/nlp-tutorial-L6/#save-predictions,False,439.0,70.0
523,Description of a Custom Model,"Now it’s time to build our ML model.  We are going to define our model in a separate file with a custom class called `Nbow_Model`.  The model contains two subcomponents: the count vectorizer for preprocessing and the model.  The `Nbow_Model` class facilitates combining these two components together so that we don't have to deal with them separately.
 
 Here is an explanation of the various methods in this model:
 
 1. `__init__`: Initialize the count vectorizer, a preprocessor that counts the tokens in the text, and a neural network to do the modeling.
 2. `fit`:  Fit the count vectorizer, followed by the model. 
 3.  `predict`: Transform the data with the count vectorizer before making predictions.
 4. `eval_acc`: Calculate model accuracy given a dataset and labels.
 5. `eval_rocauc`: Calculate the area under the roc curve given a dataset and labels.
 6. `model_dict`: This exposes a dictionary that has two components that form this model, the count vectorizer and the neural network.  We will use this to serialize the model's data into Metaflow. 
 7.  `from_dict`: This allows you to instantiate a `NbowModel` from a `model_dict` which is useful for de-serializing data in Metaflow.",H3,https://outerbounds.com/docs/nlp-tutorial-L2/#description-of-a-custom-model,False,1198.0,200.0
524,How to Serialize Data,"Anytime you create your own model library or define models in custom classes, we recommend explicitly defining how you will serialize and load the model.  This will minimize the chances that things will break as your model code changes. Explicit definitions for serialization processes give you the ability to make sure any new versions of your code are backward compatible on how to load your model or allow you to deal with serialization/de-serialization accordingly in a way that is transparent to you. This is the purpose of the `from_dict` method and `model_dict` property in this example.  
 
 For Metaflow, it is very convenient if you have an interface that allows you to save model information that is [pickleable](https://docs.python.org/3/library/pickle.html), as that is how Metaflow saves data.  That is the purpose of `model_dict` and `from_dict`: they allow saving and retrieving data from a pickleable data structure.
 
 <CustomCode height=""50vh"">
 
 
 
 ```py title=""model.py""
 import tensorflow as tf
 from tensorflow.keras import layers, optimizers, regularizers
 from sklearn.base import BaseEstimator, ClassifierMixin
 from sklearn.metrics import accuracy_score, roc_auc_score
 from sklearn.feature_extraction.text import CountVectorizer
 
 class NbowModel():
     def __init__(self, vocab_sz):
         self.vocab_sz = vocab_sz
         # Instantiate the CountVectorizer
         self.cv = CountVectorizer(
             min_df=.005, max_df = .75, stop_words='english', 
             strip_accents='ascii', max_features=self.vocab_sz
         )
 
         # Define the keras model
         inputs = tf.keras.Input(shape=(self.vocab_sz,), 
                                 name='input')
         x = layers.Dropout(0.10)(inputs)
         x = layers.Dense(
             15, activation=""relu"",
             kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4)
         )(x)
         predictions = layers.Dense(1, activation=""sigmoid"",)(x)
         self.model = tf.keras.Model(inputs, predictions)
         opt = optimizers.Adam(learning_rate=0.002)
         self.model.compile(loss=""binary_crossentropy"", 
                            optimizer=opt, metrics=[""accuracy""])
 
     def fit(self, X, y):
         res = self.cv.fit_transform(X).toarray()
         self.model.fit(x=res, y=y, batch_size=32, 
                        epochs=10, validation_split=.2)
     
     def predict(self, X):
         res = self.cv.transform(X).toarray()
         return self.model.predict(res)
     
     def eval_acc(self, X, labels, threshold=.5):
         return accuracy_score(labels, 
                               self.predict(X) > threshold)
     
     def eval_rocauc(self, X, labels):
         return roc_auc_score(labels,  self.predict(X))
 
     @property
     def model_dict(self): 
         return {'vectorizer':self.cv, 'model': self.model}
 
     @classmethod
     def from_dict(cls, model_dict):
         ""Get Model from dictionary""
         nbow_model = cls(len(
             model_dict['vectorizer'].vocabulary_
         ))
         nbow_model.model = model_dict['model']
         nbow_model.cv = model_dict['vectorizer']
         return nbow_model
 ```
 
 </CustomCode>",H3,https://outerbounds.com/docs/nlp-tutorial-L2/#how-to-serialize-data,False,3191.0,748.0
525,Fit the Custom Model on the Dataset,"Next, let's import the `NbowModel` and train it on this dataset.  The purpose of doing this is to make sure the code works as we expect before using Metaflow.  For this example, we will set our `vocab_sz = 750`.
 
 
 ```python
 from model import NbowModel
 import pandas as pd
 model = NbowModel(vocab_sz=750)
 df = pd.read_parquet('train.parquet')
 model.fit(X=df['review'], y=df['labels'])
 ```
 
 <CodeOutputBlock lang=""python"">
 
 ```
     Epoch 1/10
       1/510 [..............................] - ETA: 1:17 - loss: 0.7379 - accuracy: 0.3438
 
     2023-03-31 17:05:22.277608: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz
 
 
     510/510 [==============================] - 1s 836us/step - loss: 0.3559 - accuracy: 0.8495 - val_loss: 0.2983 - val_accuracy: 0.8756
     Epoch 2/10
     510/510 [==============================] - 0s 628us/step - loss: 0.2934 - accuracy: 0.8823 - val_loss: 0.2939 - val_accuracy: 0.8724
     Epoch 3/10
     510/510 [==============================] - 0s 651us/step - loss: 0.2825 - accuracy: 0.8876 - val_loss: 0.2934 - val_accuracy: 0.8754
     Epoch 4/10
     510/510 [==============================] - 0s 624us/step - loss: 0.2735 - accuracy: 0.8922 - val_loss: 0.2986 - val_accuracy: 0.8781
     Epoch 5/10
     510/510 [==============================] - 0s 626us/step - loss: 0.2654 - accuracy: 0.8963 - val_loss: 0.2967 - val_accuracy: 0.8759
     Epoch 6/10
     510/510 [==============================] - 0s 636us/step - loss: 0.2548 - accuracy: 0.9036 - val_loss: 0.3070 - val_accuracy: 0.8756
     Epoch 7/10
     510/510 [==============================] - 0s 620us/step - loss: 0.2476 - accuracy: 0.9056 - val_loss: 0.3051 - val_accuracy: 0.8783
     Epoch 8/10
     510/510 [==============================] - 0s 627us/step - loss: 0.2400 - accuracy: 0.9106 - val_loss: 0.3136 - val_accuracy: 0.8714
     Epoch 9/10
     510/510 [==============================] - 0s 632us/step - loss: 0.2303 - accuracy: 0.9180 - val_loss: 0.3269 - val_accuracy: 0.8741
     Epoch 10/10
     510/510 [==============================] - 0s 626us/step - loss: 0.2233 - accuracy: 0.9214 - val_loss: 0.3310 - val_accuracy: 0.8719
 ```
 
 </CodeOutputBlock>",H3,https://outerbounds.com/docs/nlp-tutorial-L2/#fit-the-custom-model-on-the-dataset,False,2230.0,374.0
526,Evaluate the Model Performance,"Next, we can evaluate our model on the validation set as well, using the built-in evaluation methods we created:
 
 
 ```python
 valdf = pd.read_parquet('valid.parquet')
 model_acc = model.eval_acc(
     valdf['review'], valdf['labels'])
 model_rocauc = model.eval_rocauc(
     valdf['review'], valdf['labels'])
 
 msg = 'Baseline Accuracy: {}\nBaseline AUC: {}'
 print(msg.format(
     round(model_acc, 3), round(model_rocauc, 3)
 ))
 ```
 
 <CodeOutputBlock lang=""python"">
 
 ```
     Baseline Accuracy: 0.875
     Baseline AUC: 0.912
 ```
 
 </CodeOutputBlock>
 
 Great! This is an improvement upon our baseline!  Now we have set up what we need to start using Metaflow.  In the next lesson, we are going to operationalize the steps we manually performed here by refactoring them as a Metaflow flow.",H3,https://outerbounds.com/docs/nlp-tutorial-L2/#evaluate-the-model-performance,False,802.0,129.0
527,Best Practice: Create a Baseline,"When creating flows, we recommend starting simple: create a flow that reads your data and reports a baseline metric.  This way, you can ensure you have the right foundation to incorporate your model.  Furthermore, starting simple helps with debugging.",H3,https://outerbounds.com/docs/nlp-tutorial-L3/#best-practice-create-a-baseline,False,251.0,41.0
528,Write a Flow,"For our baseline flow, we have three steps including:
 * a `start` step where we read the data, 
 * a `baseline` step, and 
 * an `end` step that will be a placeholder for now.
 
 Below is a detailed explanation of each step:
 
 1. **Read data from a parquet file** in the `start` step.
     - We use pandas to read `train.parquet`.
     - Notice how we are assigning the training data to `self.df` and the validation data to `self.valdf` this stores the data as an artifact in Metaflow, which means it will be versioned and saved in the [artifact store](https://docs.metaflow.org/metaflow/data) for later retrieval.  Furthermore, this allows you to pass data to another step.  The prerequisite for being able to do this is that the data you are trying to store must be pickleable.
     - We log the number of rows in the data.  It is always a good idea to log information about your dataset for debugging. 
 2. **Compute the baseline** in the `baseline` step.
      - The `baseline` step records the performance metrics (accuracy and ROC AUC score) that result from classifying all examples with the majority class.  This will be our baseline against which we evaluate our model.
 3. **Print the baseline metrics** in the `end` step.  
     - This is just a placeholder for now, but also serves to illustrate how you can retrieve artifacts from any step.
     
 ![](/assets/nlp-tutorial-BaselineNLPFlow.png)
 
 
 ```py title=""baselineflow.py""
 from metaflow import FlowSpec, step, Flow, current
 
 class BaselineNLPFlow(FlowSpec):
 
     @step
     def start(self):
         ""Read the data""
         import pandas as pd
         self.df = pd.read_parquet('train.parquet')
         self.valdf = pd.read_parquet('valid.parquet')
         print(f'num of rows: {self.df.shape[0]}')
         self.next(self.baseline)
 
     @step
     def baseline(self):
         ""Compute the baseline""
         from sklearn.metrics import accuracy_score, roc_auc_score
         baseline_predictions = [1] * self.valdf.shape[0]
         self.base_acc = accuracy_score(
             self.valdf.labels, baseline_predictions)
         self.base_rocauc = roc_auc_score(
             self.valdf.labels, baseline_predictions)
         self.next(self.end)
 
     @step
     def end(self):
         msg = 'Baseline Accuracy: {}\nBaseline AUC: {}'
         print(msg.format(
             round(self.base_acc,3), round(self.base_rocauc,3)
         ))
 
 if __name__ == '__main__':
     BaselineNLPFlow()
 ```",H3,https://outerbounds.com/docs/nlp-tutorial-L3/#write-a-flow,False,2478.0,535.0
529,Run the Flow,"```bash
 python baselineflow.py run
 ```
 
 <CodeOutputBlock lang=""bash"">
 
 ```
      Workflow starting (run-id 1680313032202317):
      [1680313032202317/start/1 (pid 36676)] Task is starting.
      [1680313032202317/start/1 (pid 36676)] num of rows: 20377
      [1680313032202317/start/1 (pid 36676)] Task finished successfully.
      [1680313032202317/baseline/2 (pid 36679)] Task is starting.
      [1680313032202317/baseline/2 (pid 36679)] Task finished successfully.
      [1680313032202317/end/3 (pid 36682)] Task is starting.
      [1680313032202317/end/3 (pid 36682)] Baseline Accuracy: 0.773
      [1680313032202317/end/3 (pid 36682)] Baseline AUC: 0.5
      [1680313032202317/end/3 (pid 36682)] Task finished successfully.
      Done!
 ```
 
 </CodeOutputBlock>
 
 In the next lesson, you will learn how to incorporate your model into the flow as well as deal with branching for parallel runs.",H3,https://outerbounds.com/docs/nlp-tutorial-L3/#run-the-flow,False,905.0,153.0
530,Use your Trained Model in a Prediction Flow,"With the Metaflow client API, you can retrieve your artifacts in whatever downstream application you want, or even just use the API for ad-hoc testing.
 
 You can utilize the client API to also retrieve model artifacts within a flow!
 
 This flow contains the following steps:
 
 1. Get the latest deployment candidate using the Metaflow API in the `start` step.  Recall that the name of our previous flow is `NLPFlow`.
 2. Make predictions with our deployment candidate on a new dataset and write that to a parquet file in the `batch_predict` step.
 
 ![](/assets/nlp-tutorial-NLPPredictionFlow.png)
 
 
 ```py title=""predflow.py""
 from metaflow import FlowSpec, step, Flow, current, Parameter
 
 class BatchPredict(FlowSpec):
     
     prediction_storage_uri = Parameter(
         'uri', 
         default=""sentiment_predictions.parquet"",
         help = ""Where to park your predictions for other apps to consume.""
     )
     
     def get_latest_successful_run(self, flow_nm, tag):
         """"""Gets the latest successful run 
             for a flow with a specific tag.""""""
         for r in Flow(flow_nm).runs(tag):
             if r.successful: return r
         
     @step
     def start(self):
         """"""Get the latest deployment candidate 
             that is from a successfull run""""""
         self.deploy_run = self.get_latest_successful_run(
             'NLPFlow', 'deployment_candidate')
         self.next(self.batch_predict)
     
     @step
     def batch_predict(self):
         ""Make predictions""
         from model import NbowModel
         import pandas as pd
         import pyarrow as pa
         new_reviews = pd.read_parquet(
             'predict.parquet')['review']
         
         # Make predictions
         model = NbowModel.from_dict(
             self.deploy_run.data.model_dict)
         predictions = model.predict(new_reviews)
         msg = 'Writing predictions to parquet: {} rows'
         print(msg.format(predictions.shape[0]))
         pa_tbl = pa.table({""data"": predictions.squeeze()})
         pa.parquet.write_table(
             pa_tbl, self.prediction_storage_uri)
         self.next(self.end)
     
     @step
     def end(self):
         print(f""\n\nAll done! Now you can read your predictions at {self.prediction_storage_uri}.\n\n"")
         
 if __name__ == '__main__':
     BatchPredict()
 ```",H3,https://outerbounds.com/docs/nlp-tutorial-L7/#use-your-trained-model-in-a-prediction-flow,False,2353.0,591.0
531,Run the Prediction Flow,"```bash
 python predflow.py run
 ```
 
 <CodeOutputBlock lang=""bash"">
 
 ```
      Workflow starting (run-id 1680367095362550):
      [1680367095362550/start/1 (pid 45395)] Task is starting.
      [1680367095362550/start/1 (pid 45395)] Task finished successfully.
      [1680367095362550/batch_predict/2 (pid 45398)] Task is starting.
      [1680367095362550/batch_predict/2 (pid 45398)] 073: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz
      [1680367095362550/batch_predict/2 (pid 45398)] Writing predictions to parquet: 2264 rows
      [1680367095362550/batch_predict/2 (pid 45398)] Task finished successfully.
      [1680367095362550/end/3 (pid 45401)] Task is starting.
      [1680367095362550/end/3 (pid 45401)] 
      [1680367095362550/end/3 (pid 45401)] 
      [1680367095362550/end/3 (pid 45401)] All done! Now you can read your predictions at sentiment_predictions.parquet.
      [1680367095362550/end/3 (pid 45401)] 
      [1680367095362550/end/3 (pid 45401)] 
      [1680367095362550/end/3 (pid 45401)] Task finished successfully.
      Done!
 ```
 
 </CodeOutputBlock>
 
 The batch prediction pattern is a good way to start deploying a new machine learning system. As a next step to, consider writing these predictions to cloud storage such as [GCP Storage](https://cloud.google.com/storage) or [S3](https://aws.amazon.com/s3/). If you need high availablility and partition tolerance you could write predictions to a system like AWS [DynamoDB](https://aws.amazon.com/dynamodb/) or Azure [Cosmos DB](https://azure.microsoft.com/en-us/products/cosmos-db). 
 
 The batch pattern are desirable in many machine learning contexts, such as when your ML service can update its predictions on a schedule instead of in real-time, and when you expect to support many users with low latency predictions. In other cases, you may want to put your model behind an endpoint, so you can make granular requests to the model in time. In the next lesson, we will put a model behind an endpoint using FastAPI. See you there!",H3,https://outerbounds.com/docs/nlp-tutorial-L7/#run-the-prediction-flow,False,2064.0,319.0
532,Serve Your Model,,H3,https://outerbounds.com/docs/nlp-tutorial-L8/#serve-your-model,False,0.0,1.0
533,Run the Server,"Now open up your terminal, make sure the `mf-tutorial-nlp` environment is active, and run the following command to start your API server:
 
 
 ```bash
 uvicorn model_server:api --reload
 ```
 
 :::note
 The `--reload` argument is a development feature that enables hot reloading of the API server, so you can edit the `model_server.py` file, hit save, and not have to turn off the server. Turn this off in production.
 :::
 
 In production, you will want to deploy this API on another machine, but you are already most of the way to being able to serve live predictions in your apps. This is really powerful! You can find much more information about how to scale operations of your API on the [FastAPI deployment documentation](https://fastapi.tiangolo.com/deployment/).",H3,https://outerbounds.com/docs/nlp-tutorial-L8/#run-the-server,False,770.0,123.0
534,Make an API Request,"Let's make a request to the server! :tada: :sparkles:
 
 
 ```python
 import requests 
 import urllib",H3,https://outerbounds.com/docs/nlp-tutorial-L8/#make-an-api-request,False,101.0,17.0
535,Conclusion,"Congratulations, you have completed Metaflow's introductory tutorial on operationalizing NLP workflows! You have learned how to:
 
 1. Create a baseline flow that reads data and computes a baseline.
 2. Use branching to perform steps in parallel.
 3. Serialize and de-serialize data in Metaflow.
 4. Use tagging to evaluate and gate models for production.
 5. Retrieve your model both outside Metaflow and from another flow.
 6. Set up a basic batch prediction workflow.
 7. Set up a basic real-time serving endpoint.",H3,https://outerbounds.com/docs/nlp-tutorial-L8/#conclusion,False,517.0,81.0
536,Further Discussion,"This tutorial is intended as a simple example to get you utilizing Metaflow in realistic ways on your laptop.  However, for production use cases you may want to use other built-in Metaflow features such as [@conda](https://docs.metaflow.org/metaflow/dependencies#managing-dependencies-with-conda-decorator) for dependency management, [@batch](https://docs.metaflow.org/v/r/metaflow/scaling#using-aws-batch) or [@kubernetes](https://docs.metaflow.org/metaflow/scaling-out-and-up/effortless-scaling-with-kubernetes) for remote execution, and [@schedule](https://docs.metaflow.org/going-to-production-with-metaflow/scheduling-metaflow-flows/scheduling-with-aws-step-functions#scheduling-a-flow) to automatically trigger jobs.  
 
 You can find more tutorials like this with code you can run in the browser at https://outerbounds.com/sandbox.",H3,https://outerbounds.com/docs/nlp-tutorial-L8/#further-discussion,False,838.0,72.0
537,Natural Language Processing with Metaflow Tutorial,"In this series of episodes, you will learn how to build, train, test, and deploy a machine learning model that performs text classification.  You will use Tensorflow, Scikit-learn, and Metaflow to operationalize a machine learning product using best practices for evaluating and testing.
 Lastly, you will learn how you can use this model in downstream processes.
 
 <CustomCode height=""60vh"" sandboxLink=""https://account.outerbounds.dev/account/?workspace=/home/workspace/workspaces/nlp/workspace.code-workspace"">
 
 ```python title=""Natural Language Processing with Metaflow""
 from metaflow import FlowSpec, step, Flow, current
 
 class NLPPredictionFlow(FlowSpec):
     
     def get_latest_successful_run(self, flow_nm, tag):
         """"""Gets the latest successful run 
             for a flow with a specific tag.""""""
         for r in Flow(flow_nm).runs(tag):
             if r.successful: return r
         
     @step
     def start(self):
         """"""Get the latest deployment candidate 
             that is from a successfull run""""""
         self.deploy_run = self.get_latest_successful_run(
             'NLPFlow', 'deployment_candidate')
         self.next(self.end)
     
     @step
     def end(self):
         ""Make predictions""
         from model import NbowModel
         import pandas as pd
         import pyarrow as pa
         new_reviews = pd.read_parquet(
             'predict.parquet')['review']
         
         # Make predictions
         model = NbowModel.from_dict(
             self.deploy_run.data.model_dict)
         predictions = model.predict(new_reviews)
         msg = 'Writing predictions to parquet: {} rows'
         print(msg.format(predictions.shape[0]))
         pa_tbl = pa.table({""data"": predictions.squeeze()})
         pa.parquet.write_table(
             pa_tbl, ""sentiment_predictions.parquet"")
         
 if __name__ == '__main__':
     NLPPredictionFlow()
 ```
 </CustomCode>",H1,https://outerbounds.com/docs/nlp-tutorial-overview#natural-language-processing-with-metaflow-tutorial,False,1929.0,459.0
538,Prerequisites,We assume that you have taken the [introductory tutorials](/docs/intro-tutorial-overview) or know the basics of Metaflow.,H2,https://outerbounds.com/docs/nlp-tutorial-overview#prerequisites,False,121.0,15.0
539,Tutorial Structure,"The tutorial consists of seven episodes, all centering around a text classification task you will be introduced to in the first episode.
 * [Episode 1](/docs/nlp-tutorial-L1): Understand the Data
 * [Episode 2](/docs/nlp-tutorial-L2): Construct a Model
 * [Episode 3](/docs/nlp-tutorial-L3): Set Up a Baseline Flow
 * [Episode 4](/docs/nlp-tutorial-L4): Train your Model
 * [Episode 5](/docs/nlp-tutorial-L5): Evaluate your Model
 * [Episode 6](/docs/nlp-tutorial-L6): Use your Model in Python
 * [Episode 7](/docs/nlp-tutorial-L7): Make Batch Predictions in a Flow
 * [Episode 8](/docs/nlp-tutorial-L8): Create a Realtime Endpoint
 
 Each episode contains either a Metaflow script to run or a Jupyter notebook. 
 You do not need access to cloud computing or a [Metaflow deployment](/engineering/welcome/) to complete the episodes.
 The estimated time to complete all episodes is 1-2 hours.",H2,https://outerbounds.com/docs/nlp-tutorial-overview#tutorial-structure,False,890.0,119.0
540,Why Metaflow?,"The main benefit of using a data science workflow solution like Metaflow when prototyping is that your code will be built on a strong foundation for deploying to a production environment. 
 Metaflow is most useful when projects have scaling requirements, are mission-critical, and/or have many interacting parts. 
 You can read more at these links: 
 * [Should I Use Metaflow?](https://docs.metaflow.org/introduction/what-is-metaflow#should-i-use-metaflow)
 * [Why Metaflow?](https://docs.metaflow.org/introduction/why-metaflow)
 
 After completing the lessons, you will be able to transfer insights and code from the tutorial to your real-world data science projects.
 It is important to be mindful that this is a beginner tutorial so it will not reflect many important challenges to consider in production ML environments. For example, in production, you may consider using Metaflow features such as the `@conda` decorator for dependency management, `@batch` or `@kubernetes` for remote execution, and `@schedule` to automatically trigger jobs.",H2,https://outerbounds.com/docs/nlp-tutorial-overview#why-metaflow,False,1046.0,144.0
541,Run in your Browser,"<OpenSandbox fileName=""Computer Vision with Metaflow - Beginner"" link=""https://account.outerbounds.dev/account/?workspace=/home/workspace/workspaces/cv/workspace.code-workspace"">
 </OpenSandbox>",H2,https://outerbounds.com/docs/cv-tutorial-setup#run-in-your-browser,False,194.0,9.0
542,Run on your own Infrastructure,"To run this tutorial locally or with your own Metaflow deployment, you will need to clone the repository, and then install dependencies using an environment manager of your choice. In the repository you will find the necessary dependencies in a `env.yml` file for conda. You can install them by following these instructions.",H2,https://outerbounds.com/docs/cv-tutorial-setup#run-on-your-own-infrastructure,False,324.0,52.0
543,Clone the Repository,"```
 git clone https://github.com/outerbounds/tutorials.git
 ```",H3,https://outerbounds.com/docs/cv-tutorial-setup#clone-the-repository,False,64.0,5.0
544,Install Dependencies with Conda,"To run the tutorial, you need to install the required dependencies via conda. We have included a conda environment in the [env.yml](https://github.com/outerbounds/tutorials/cv/env.yml) file.  You can install and activate the environment with this command:
 
 > We are using [`mamba`](https://mamba.readthedocs.io/en/latest/) instead of `conda` because it is significantly faster. However, you can use `conda` if you want to.
 
 ```
 cd tutorials/cv
 conda install mamba -n base -c conda-forge
 mamba env create -f env.yml
 conda activate mf-tutorial-cv
 ```",H3,https://outerbounds.com/docs/cv-tutorial-setup#install-dependencies-with-conda,False,557.0,77.0
545,Running the Tutorials,Please follow the instructions in each lesson for running either the associated Python script or Jupyter Notebook.,H3,https://outerbounds.com/docs/cv-tutorial-setup#running-the-tutorials,False,114.0,17.0
546,Write a Tuning Flow,"[This flow](https://github.com/outerbounds/tutorials/blob/main/cv/tuning_flow.py) shows how you can tune the `cnn` model. The flow includes
 * A `start` step that loads data.
 * A `train` step that trains and scores a model for each hyperparameter configuration.
     * Metaflow's [foreach](https://docs.metaflow.org/metaflow/basics#foreach) pattern is used to make this happen in parallel locally and in the cloud. 
 * A `gather_scores` step that joins the results from each modeling step and stores the results in a Metaflow card visual.
 * An `end` step that saves the best model.
 
 ![](/assets/cv-tutorial-TuningFlow.png)
 
 <CustomCode height=""50vh"">
 
 
 
 ```py title=""tuning_flow.py""
 from metaflow import FlowSpec, step, Flow, current, card
 from metaflow.cards import Image, Table
 from tensorflow import keras
 from models import ModelOperations
 
 class TuningFlow(FlowSpec, ModelOperations):
 
     best_model_location = (""best_tuned_model"")
     num_pixels = 28 * 28
     kernel_initializer = 'normal'
     optimizer = 'adam'
     loss = 'categorical_crossentropy'
     metrics = [
         'accuracy',
         'precision at recall'
     ]
     input_shape = (28, 28, 1)
     kernel_size = (3, 3)
     pool_size = (2, 2)
     p_dropout = 0.5
     epochs = 5
     batch_size = 64
     verbose = 2
 
     @step
     def start(self):
         import numpy as np
         self.num_classes = 10
         ((x_train, y_train), 
          (x_test, y_test)) = keras.datasets.mnist.load_data()
         x_train = x_train.astype(""float32"") / 255
         x_test = x_test.astype(""float32"") / 255
         self.x_train = np.expand_dims(x_train, -1)
         self.x_test = np.expand_dims(x_test, -1)
         self.y_train = keras.utils.to_categorical(
             y_train, self.num_classes)
         self.y_test = keras.utils.to_categorical(
             y_test, self.num_classes)
         #highlight-start
         self.param_config = [
             {""hidden_conv_layer_sizes"": [16, 32]},
             {""hidden_conv_layer_sizes"": [16, 64]},
             {""hidden_conv_layer_sizes"": [32, 64]},
             {""hidden_conv_layer_sizes"": [32, 128]},
             {""hidden_conv_layer_sizes"": [64, 128]}
         ]
         self.next(self.train, foreach='param_config')
         #highlight-end
 
     @step
     def train(self):
         from neural_net_utils import plot_learning_curves
         #highlight-start
         self.model = self.make_cnn(
             self.input['hidden_conv_layer_sizes'])
         #highlight-end
         self.history, self.scores = self.fit_and_score(
             self.x_train, self.x_test)
         self._name = 'CNN'
         self.plots = [
             Image.from_matplotlib(p) for p in
             plot_learning_curves(
                 self.history,
                 'Hidden Layers - ' + ', '.join([
                     str(i) for i in
                     self.input['hidden_conv_layer_sizes']
                 ])
             )
         ]
         self.next(self.gather_scores)
 
     @card
     @step
     def gather_scores(self, models):
         import pandas as pd
         self.max_class = models[0].y_train
         results = {
             'hidden conv layer sizes': [],
             'model': [], 
             'test loss': [],
             **{metric: [] for metric in self.metrics}
         }
         max_seen_acc = 0
         rows = []
         for model in models:
             results['model'].append(model._name)
             results['test loss'].append(model.scores[0])
             for i, metric in enumerate(self.metrics):
                 results[metric].append(model.scores[i+1])
             results['hidden conv layer sizes'].append(
                 ','.join([
                     str(i) for i in model.input[
                         'hidden_conv_layer_sizes'
                     ]
                 ])
             )
             # A simple rule for determining the best model.
             # In production flows you need to think carefully
                 # about how this kind of rule maps to your objectives.
             if model.scores[1] > max_seen_acc:
                 self.best_model = model.model
                 max_seen_acc = model.scores[1]
             rows.append(model.plots)
 
         #highlight-next-line
         current.card.append(Table(rows))
         self.results = pd.DataFrame(results)
         self.next(self.end)
 
     @step
     def end(self):
         self.best_model.save(self.best_model_location)
 
 if __name__ == '__main__':
     TuningFlow()
 ```
 
 </CustomCode>",H3,https://outerbounds.com/docs/cv-tutorial-S1E4#write-a-tuning-flow,False,4565.0,1366.0
547,Run the Tuning Flow,"```bash
 python tuning_flow.py run
 ```
 
 <CodeOutputBlock lang=""bash"">
 
 ```
      Workflow starting (run-id 1666721523161525):
      [1666721523161525/start/1 (pid 53367)] Task is starting.
      [1666721523161525/start/1 (pid 53367)] Foreach yields 5 child steps.
      [1666721523161525/start/1 (pid 53367)] Task finished successfully.
      [1666721523161525/train/2 (pid 53375)] Task is starting.
      [1666721523161525/train/3 (pid 53376)] Task is starting.
      [1666721523161525/train/4 (pid 53377)] Task is starting.
      [1666721523161525/train/5 (pid 53378)] Task is starting.
      [1666721523161525/train/6 (pid 53379)] Task is starting.
      [1666721523161525/train/3 (pid 53376)] 742: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz
      [1666721523161525/train/6 (pid 53379)] 279: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz
      [1666721523161525/train/3 (pid 53376)] Epoch 1/5
      [1666721523161525/train/4 (pid 53377)] 553: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz
      [1666721523161525/train/2 (pid 53375)] 816: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz
      [1666721523161525/train/6 (pid 53379)] Epoch 1/5
      [1666721523161525/train/4 (pid 53377)] Epoch 1/5
      [1666721523161525/train/2 (pid 53375)] Epoch 1/5
      [1666721523161525/train/5 (pid 53378)] 924: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz
      [1666721523161525/train/5 (pid 53378)] Epoch 1/5
      [1666721523161525/train/2 (pid 53375)] 938/938 - 14s - loss: 0.3722 - accuracy: 0.8858 - precision_at_recall: 0.6490 - val_loss: 0.0966 - val_accuracy: 0.9721 - val_precision_at_recall: 0.9819 - 14s/epoch - 15ms/step
      [1666721523161525/train/3 (pid 53376)] 938/938 - 17s - loss: 0.2737 - accuracy: 0.9170 - precision_at_recall: 0.7856 - val_loss: 0.0725 - val_accuracy: 0.9771 - val_precision_at_recall: 0.9890 - 17s/epoch - 18ms/step
      [1666721523161525/train/3 (pid 53376)] Epoch 2/5
      [1666721523161525/train/4 (pid 53377)] 938/938 - 25s - loss: 0.2661 - accuracy: 0.9174 - precision_at_recall: 0.7964 - val_loss: 0.0650 - val_accuracy: 0.9802 - val_precision_at_recall: 0.9930 - 25s/epoch - 27ms/step
      [1666721523161525/train/4 (pid 53377)] Epoch 2/5
      [1666721523161525/train/2 (pid 53375)] Epoch 2/5
      [1666721523161525/train/2 (pid 53375)] 938/938 - 13s - loss: 0.1234 - accuracy: 0.9624 - precision_at_recall: 0.9658 - val_loss: 0.0606 - val_accuracy: 0.9821 - val_precision_at_recall: 0.9934 - 13s/epoch - 14ms/step
      [1666721523161525/train/3 (pid 53376)] 938/938 - 16s - loss: 0.0948 - accuracy: 0.9709 - precision_at_recall: 0.9809 - val_loss: 0.0491 - val_accuracy: 0.9853 - val_precision_at_recall: 0.9959 - 16s/epoch - 17ms/step
      [1666721523161525/train/3 (pid 53376)] Epoch 3/5
      [1666721523161525/train/5 (pid 53378)] 938/938 - 34s - loss: 0.2178 - accuracy: 0.9345 - precision_at_recall: 0.8700 - val_loss: 0.0588 - val_accuracy: 0.9814 - val_precision_at_recall: 0.9943 - 34s/epoch - 36ms/step
      [1666721523161525/train/5 (pid 53378)] Epoch 2/5
      [1666721523161525/train/2 (pid 53375)] Epoch 3/5
      [1666721523161525/train/2 (pid 53375)] 938/938 - 12s - loss: 0.0940 - accuracy: 0.9701 - precision_at_recall: 0.9793 - val_loss: 0.0467 - val_accuracy: 0.9846 - val_precision_at_recall: 0.9961 - 12s/epoch - 13ms/step
      [1666721523161525/train/4 (pid 53377)] 938/938 - 22s - loss: 0.0894 - accuracy: 0.9721 - precision_at_recall: 0.9825 - val_loss: 0.0437 - val_accuracy: 0.9854 - val_precision_at_recall: 0.9962 - 22s/epoch - 24ms/step
      [1666721523161525/train/4 (pid 53377)] Epoch 3/5
      [1666721523161525/train/3 (pid 53376)] 938/938 - 15s - loss: 0.0732 - accuracy: 0.9777 - precision_at_recall: 0.9893 - val_loss: 0.0382 - val_accuracy: 0.9873 - val_precision_at_recall: 0.9976 - 15s/epoch - 16ms/step
      [1666721523161525/train/3 (pid 53376)] Epoch 4/5
      [1666721523161525/train/6 (pid 53379)] 938/938 - 51s - loss: 0.1926 - accuracy: 0.9420 - precision_at_recall: 0.8998 - val_loss: 0.0510 - val_accuracy: 0.9833 - val_precision_at_recall: 0.9963 - 51s/epoch - 55ms/step
      [1666721523161525/train/6 (pid 53379)] Epoch 2/5
      [1666721523161525/train/2 (pid 53375)] Epoch 4/5
      [1666721523161525/train/2 (pid 53375)] 938/938 - 12s - loss: 0.0836 - accuracy: 0.9743 - precision_at_recall: 0.9852 - val_loss: 0.0378 - val_accuracy: 0.9869 - val_precision_at_recall: 0.9974 - 12s/epoch - 13ms/step
      [1666721523161525/train/3 (pid 53376)] 938/938 - 14s - loss: 0.0629 - accuracy: 0.9803 - precision_at_recall: 0.9920 - val_loss: 0.0353 - val_accuracy: 0.9877 - val_precision_at_recall: 0.9981 - 14s/epoch - 15ms/step
      [1666721523161525/train/3 (pid 53376)] Epoch 5/5
      [1666721523161525/train/2 (pid 53375)] Epoch 5/5
      [1666721523161525/train/2 (pid 53375)] 938/938 - 11s - loss: 0.0716 - accuracy: 0.9780 - precision_at_recall: 0.9898 - val_loss: 0.0362 - val_accuracy: 0.9878 - val_precision_at_recall: 0.9977 - 11s/epoch - 12ms/step
      [1666721523161525/train/5 (pid 53378)] 938/938 - 29s - loss: 0.0776 - accuracy: 0.9752 - precision_at_recall: 0.9874 - val_loss: 0.0408 - val_accuracy: 0.9861 - val_precision_at_recall: 0.9969 - 29s/epoch - 31ms/step
      [1666721523161525/train/5 (pid 53378)] Epoch 3/5
      [1666721523161525/train/2 (pid 53375)] WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
      [1666721523161525/train/2 (pid 53375)] WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
      [1666721523161525/train/2 (pid 53375)] Task finished successfully.
      [1666721523161525/train/4 (pid 53377)] 938/938 - 21s - loss: 0.0686 - accuracy: 0.9790 - precision_at_recall: 0.9907 - val_loss: 0.0375 - val_accuracy: 0.9875 - val_precision_at_recall: 0.9977 - 21s/epoch - 22ms/step
      [1666721523161525/train/4 (pid 53377)] Epoch 4/5
      [1666721523161525/train/3 (pid 53376)] 938/938 - 12s - loss: 0.0556 - accuracy: 0.9833 - precision_at_recall: 0.9940 - val_loss: 0.0313 - val_accuracy: 0.9896 - val_precision_at_recall: 0.9979 - 12s/epoch - 13ms/step
      [1666721523161525/train/3 (pid 53376)] WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
      [1666721523161525/train/3 (pid 53376)] WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
      [1666721523161525/train/3 (pid 53376)] Task finished successfully.
      [1666721523161525/train/4 (pid 53377)] 938/938 - 16s - loss: 0.0577 - accuracy: 0.9815 - precision_at_recall: 0.9932 - val_loss: 0.0332 - val_accuracy: 0.9889 - val_precision_at_recall: 0.9980 - 16s/epoch - 17ms/step
      [1666721523161525/train/4 (pid 53377)] Epoch 5/5
      [1666721523161525/train/5 (pid 53378)] 938/938 - 21s - loss: 0.0593 - accuracy: 0.9814 - precision_at_recall: 0.9933 - val_loss: 0.0362 - val_accuracy: 0.9880 - val_precision_at_recall: 0.9970 - 21s/epoch - 23ms/step
      [1666721523161525/train/5 (pid 53378)] Epoch 4/5
      [1666721523161525/train/6 (pid 53379)] 938/938 - 36s - loss: 0.0682 - accuracy: 0.9793 - precision_at_recall: 0.9908 - val_loss: 0.0375 - val_accuracy: 0.9882 - val_precision_at_recall: 0.9979 - 36s/epoch - 38ms/step
      [1666721523161525/train/6 (pid 53379)] Epoch 3/5
      [1666721523161525/train/4 (pid 53377)] 938/938 - 18s - loss: 0.0508 - accuracy: 0.9840 - precision_at_recall: 0.9952 - val_loss: 0.0279 - val_accuracy: 0.9908 - val_precision_at_recall: 0.9988 - 18s/epoch - 19ms/step
      [1666721523161525/train/4 (pid 53377)] WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
      [1666721523161525/train/5 (pid 53378)] 938/938 - 20s - loss: 0.0496 - accuracy: 0.9843 - precision_at_recall: 0.9952 - val_loss: 0.0324 - val_accuracy: 0.9893 - val_precision_at_recall: 0.9984 - 20s/epoch - 21ms/step
      [1666721523161525/train/5 (pid 53378)] Epoch 5/5
      [1666721523161525/train/4 (pid 53377)] WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
      [1666721523161525/train/4 (pid 53377)] Task finished successfully.
      [1666721523161525/train/6 (pid 53379)] 938/938 - 25s - loss: 0.0537 - accuracy: 0.9834 - precision_at_recall: 0.9944 - val_loss: 0.0306 - val_accuracy: 0.9900 - val_precision_at_recall: 0.9983 - 25s/epoch - 27ms/step
      [1666721523161525/train/6 (pid 53379)] Epoch 4/5
      [1666721523161525/train/5 (pid 53378)] 938/938 - 15s - loss: 0.0411 - accuracy: 0.9866 - precision_at_recall: 0.9969 - val_loss: 0.0310 - val_accuracy: 0.9899 - val_precision_at_recall: 0.9980 - 15s/epoch - 16ms/step
      [1666721523161525/train/5 (pid 53378)] WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
      [1666721523161525/train/5 (pid 53378)] WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
      [1666721523161525/train/5 (pid 53378)] Task finished successfully.
      [1666721523161525/train/6 (pid 53379)] 938/938 - 17s - loss: 0.0435 - accuracy: 0.9862 - precision_at_recall: 0.9963 - val_loss: 0.0300 - val_accuracy: 0.9906 - val_precision_at_recall: 0.9984 - 17s/epoch - 18ms/step
      [1666721523161525/train/6 (pid 53379)] Epoch 5/5
      [1666721523161525/train/6 (pid 53379)] 938/938 - 14s - loss: 0.0373 - accuracy: 0.9880 - precision_at_recall: 0.9974 - val_loss: 0.0270 - val_accuracy: 0.9910 - val_precision_at_recall: 0.9993 - 14s/epoch - 15ms/step
      [1666721523161525/train/6 (pid 53379)] WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
      [1666721523161525/train/6 (pid 53379)] WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
      [1666721523161525/train/6 (pid 53379)] Task finished successfully.
      [1666721523161525/gather_scores/7 (pid 53481)] Task is starting.
      [1666721523161525/gather_scores/7 (pid 53481)] WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
      [1666721523161525/gather_scores/7 (pid 53481)] Task finished successfully.
      [1666721523161525/end/8 (pid 53487)] Task is starting.
      [1666721523161525/end/8 (pid 53487)] WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
      [1666721523161525/end/8 (pid 53487)] WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
      [1666721523161525/end/8 (pid 53487)] Task finished successfully.
      Done!
 ```
 
 </CodeOutputBlock>",H3,https://outerbounds.com/docs/cv-tutorial-S1E4#run-the-tuning-flow,False,12159.0,1772.0
548,Visualize Results,"```bash
 python tuning_flow.py card view gather_scores
 ```
 
 <CodeOutputBlock lang=""bash"">
 
 ```
     Metaflow 2.7.12 executing TuningFlow for user:eddie
     Resolving card: TuningFlow/1666721523161525/gather_scores/7
 ```
 
 </CodeOutputBlock>
 
 In this lesson, you saw how to extend your model training flows to tune hyperparameters in parallel.
 Whether you are building an ML platform or a workflow targeting a single application, it is important to consider your exploration budget and how you will improve models through processes like hyperparameter tuning. For example, here you can see an example of using Metaflow with more complex tuning algorithms via [Optuna](/docs/use-optuna-with-metaflow/).
 In the next lesson, you will interpret the results of all the models you have trained thus far. 
 See you there!",H3,https://outerbounds.com/docs/cv-tutorial-S1E4#visualize-results,False,825.0,123.0
549,Load the Data,"To begin, let's access the MNIST dataset using Keras:
 
 
 ```python
 from tensorflow import keras
 import numpy as np
 
 num_classes = 10
 ((x_train, y_train), (x_test, y_test)) = keras.datasets.mnist.load_data()
 x_train = x_train.astype(""float32"") / 255
 x_test = x_test.astype(""float32"") / 255
 x_train = np.expand_dims(x_train, -1)
 x_test = np.expand_dims(x_test, -1)
 y_train = keras.utils.to_categorical(y_train, num_classes)
 y_test = keras.utils.to_categorical(y_test, num_classes)
 ```
 
 You will find 60000 and 10000 data instances (images) in the training and test set. 
 Each image has dimensions `28x28x1`. 
 
 
 ```python",H3,https://outerbounds.com/docs/cv-tutorial-S1E1/#load-the-data,False,638.0,83.0
550,Fit a Baseline,"Before training a model, it is useful to set a baseline. A common baseline for classification tasks is the majority-class classifier, which measures what happens when all of the data instances are predicted to be from the majority class. This pattern is demonstrated in our [NLP tutorial](/docs/nlp-tutorial-overview/). However, for the MNIST dataset and the corresponding image classification task described above, the majority class-classifier will lead to a baseline model that predicts correctly just over 10% of the time. This is not a very useful baseline. Instead, let's build a feedforward neural network to compare to a more advanced convolutional neural network you will build later.",H3,https://outerbounds.com/docs/cv-tutorial-S1E1/#fit-a-baseline,False,693.0,105.0
551,Configure Hyperparameters,"These variables represent some training settings and the model's hyperparameters. 
 Don't worry if you are unfamiliar with neural networks or what these words mean.
 If you do know what they are, feel free to experiment!
 
 
 ```python
 from tensorflow.keras import layers, Sequential, Input
 
 num_pixels = 28 * 28
 num_classes = y_test.shape[1]
 
 kernel_initializer = 'normal'
 optimizer = 'adam'
 loss = 'categorical_crossentropy'
 metrics = ['accuracy']
 
 epochs = 3
 batch_size = 32
 verbose = 2
 ```",H3,https://outerbounds.com/docs/cv-tutorial-S1E1/#configure-hyperparameters,False,507.0,78.0
552,Build a Baseline Model,"Next, let's construct a Keras model adding `layers`. 
 The Keras `layers` apply matrix operations to data as it moves toward the output layer of the neural network.
 In this case, we use two `Dense` layers. `Dense` means they are feed-forward, fully-connected layers. 
 
 
 ```python
 model = Sequential()
 model.add(layers.Dense(
     num_pixels, input_dim=num_pixels,
     kernel_initializer=kernel_initializer,
     activation='relu'
 ))
 model.add(layers.Dense(
     num_classes,
     kernel_initializer=kernel_initializer,
     activation='softmax'
 ))
 model.compile(loss=loss, optimizer=optimizer, metrics=metrics)
 ```",H3,https://outerbounds.com/docs/cv-tutorial-S1E1/#build-a-baseline-model,False,626.0,89.0
553,Train Your Model,"To work with the feed-forward network you need to reshape the images. 
 This is done by flattening the matrix representing the images into a one-dimensional vector with length `num_pixels`. 
 Notice that this is the same value as the `input_dim` of the first `layer` in the neural network you defined in the last step. 
 Once the data is ready, we can pass it to the `model.fit` function and train a neural network.
 
 
 ```python
 _x_train = x_train.reshape(x_train.shape[0], num_pixels).astype('float32')
 _x_test = x_test.reshape(x_test.shape[0], num_pixels).astype('float32')
 history = model.fit(
     _x_train, y_train,
     validation_data = (_x_test, y_test),
     epochs = epochs,
     batch_size = batch_size,
     verbose = verbose
 )
 ```
 
 <CodeOutputBlock lang=""python"">
 
 ```
     Epoch 1/3
 
 
     2022-10-25 13:09:45.505250: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz
 
 
     1875/1875 - 5s - loss: 0.1911 - accuracy: 0.9434 - val_loss: 0.0991 - val_accuracy: 0.9687 - 5s/epoch - 3ms/step
     Epoch 2/3
     1875/1875 - 4s - loss: 0.0753 - accuracy: 0.9768 - val_loss: 0.0743 - val_accuracy: 0.9768 - 4s/epoch - 2ms/step
     Epoch 3/3
     1875/1875 - 5s - loss: 0.0480 - accuracy: 0.9843 - val_loss: 0.0809 - val_accuracy: 0.9766 - 5s/epoch - 3ms/step
 ```
 
 </CodeOutputBlock>",H3,https://outerbounds.com/docs/cv-tutorial-S1E1/#train-your-model,False,1352.0,239.0
554,Evaluate the Baseline,"After training the model you will want to evaluate its performance to see if it's ability to generalize is improving:
 
 
 ```python
 scores = model.evaluate(
     x_test.reshape(x_test.shape[0], x_test.shape[1] * x_test.shape[2]).astype('float32'),
     y_test,
     verbose=0
 )
 categorical_cross_entropy = scores[0]
 accuracy = scores[1]
 msg = ""The model predicted correctly {}% of the time on the test set.""
 print(msg.format(round(100*accuracy, 3)))
 ```
 
 <CodeOutputBlock lang=""python"">
 
 ```
     The model predicted correctly 97.66% of the time on the test set.
 ```
 
 </CodeOutputBlock>",H3,https://outerbounds.com/docs/cv-tutorial-S1E1/#evaluate-the-baseline,False,601.0,92.0
555,Save and Load Keras Models,"Like all software development, it is important to create robust processes for checkpointing, saving, and loading.
 This is even more important in computer vision, as model training can be expensive.
 Luckily, Keras provides utilities for saving and loading models. 
 For example, you can save the model architecture, model weights, and the traced TensorFlow subgraphs of call functions with a simple `model.save` API. 
 Later you will see how to incorporate this into your flows which will help you train and make predictions with models in any environment you need to.
 
 
 ```python
 location = 'test_model'
 model.save(location)
 ```
 
 <CodeOutputBlock lang=""python"">
 
 ```
     INFO:tensorflow:Assets written to: test_model/assets
 ```
 
 </CodeOutputBlock>
 
 Using `model.load` with the same location will then reload the same model object state:
 
 
 ```python
 new_model = keras.models.load_model(location)
 scores = model.evaluate(
     x_test.reshape(x_test.shape[0], x_test.shape[1] * x_test.shape[2]).astype('float32'),
     y_test,
     verbose=0
 )
 assert scores[1] > .96, ""Model should be doing better after two epochs.""
 ```
 
 To learn more about your options for saving and loading Keras models please see [this guide](https://www.tensorflow.org/guide/keras/save_and_serialize). It describes cases like how to [save to the Keras H5 format](https://www.tensorflow.org/guide/keras/save_and_serialize#whole-model_saving_loading) instead of the newer SavedModel format and how to [save and load only model weights](https://www.tensorflow.org/guide/keras/save_and_serialize#saving_loading_only_the_models_weights_values).
 
 In this lesson, you explored the MNIST dataset and built a high-accuracy baseline model. In the next lesson, you will build a convolutional neural network model to see how its performance compares to the baseline. See you there!",H3,https://outerbounds.com/docs/cv-tutorial-S1E1/#save-and-load-keras-models,False,1869.0,254.0
556,Load Flow Results,"[Tagging](https://docs.metaflow.org/scaling/tagging#tagging) helps you organize flows. Tags let you apply interpretations to the results of flows. Let's see how they work by loading run data from the `TuningFlow` you built in [episode 4](/docs/cv-tutorial-S1E4). The data can be accessed in any Python environment using Metaflow's Client API:
 
 
 ```python
 from metaflow import Flow
 model_comparison_flow = Flow('ModelComparisonFlow')
 tuning_flow = Flow('TuningFlow')
 ```",H3,https://outerbounds.com/docs/cv-tutorial-S1E5/#load-flow-results,False,476.0,59.0
557,Define How to Aggregate and Compare Results,"Next we define a function to parse the data in the runs. 
 The customizable `get_stats` function will progressively build up a dictionary called `stats`.
 Each new entry in the `stats` dictionary contains hyperparameters, metrics, and metadata corresponding to a model trained in a `TuningFlow`.
 
 
 ```python
 import numpy as np 
 
 def get_stats(stats, run, metrics):
     if run.successful and hasattr(run.data, 'results'):
         results = run.data.results
         if not np.all(_m in results.columns for _m in metrics):
             return stats
         best_run = results.iloc[results[metrics[0]].idxmax()]
         stats['flow id'].append(run.id)
         stats['flow name'].append(run.parent.pathspec)
         stats['model name'].append(best_run['model'])
         for _m in metrics:
             stats[_m].append(best_run[_m])
         stats['test loss'].append(best_run['test loss'])
     return stats
 ```
 
 Next we loop through runs of `TuningFlow` and `ModelComparisonFlow` and aggregate `stats`:
 
 
 ```python
 metrics = tuning_flow.latest_run.data.metrics
 
 stats = {
     'flow id': [],
     'flow name': [],
     'model name': [],
     'test loss': [],
     **{metric: [] for metric in metrics}
 }
 
 for run in tuning_flow.runs():
     stats = get_stats(stats, run, metrics)
     
 for run in model_comparison_flow.runs():
     stats = get_stats(stats, run, metrics)
 ```
 
 
 ```python
 import pandas as pd
 
 best_models = pd.DataFrame(stats)
 best_models
 ```
     
 <HTMLOutputBlock >
 
 
 
 
 ```html
 <div>
 <style scoped>
     .dataframe tbody tr th:only-of-type {
         vertical-align: middle;
     }
 
     .dataframe tbody tr th {
         vertical-align: top;
     }
 
     .dataframe thead th {
         text-align: right;
     }
 </style>
 <table border=""1"" class=""dataframe"">
   <thead>
     <tr style=""text-align: right;"">
       <th></th>
       <th>flow id</th>
       <th>flow name</th>
       <th>model name</th>
       <th>test loss</th>
       <th>accuracy</th>
       <th>precision at recall</th>
     </tr>
   </thead>
   <tbody>
     <tr>
       <th>0</th>
       <td>1666721523161525</td>
       <td>TuningFlow</td>
       <td>CNN</td>
       <td>0.026965</td>
       <td>0.9910</td>
       <td>0.999272</td>
     </tr>
     <tr>
       <th>1</th>
       <td>1665967558891569</td>
       <td>TuningFlow</td>
       <td>CNN</td>
       <td>0.027228</td>
       <td>0.9907</td>
       <td>0.999168</td>
     </tr>
     <tr>
       <th>2</th>
       <td>1666721393687341</td>
       <td>ModelComparisonFlow</td>
       <td>CNN</td>
       <td>0.026307</td>
       <td>0.9910</td>
       <td>0.999272</td>
     </tr>
     <tr>
       <th>3</th>
       <td>1665967344088184</td>
       <td>ModelComparisonFlow</td>
       <td>CNN</td>
       <td>0.030421</td>
       <td>0.9892</td>
       <td>0.998545</td>
     </tr>
   </tbody>
 </table>
 </div>
 ```
 
 
 
 </HTMLOutputBlock>",H3,https://outerbounds.com/docs/cv-tutorial-S1E5/#define-how-to-aggregate-and-compare-results,False,2929.0,715.0
558,Access the Best Model,"With the list of `best_models`, we can sort by `test accuracy` performance and find the run containing the best model.
 
 
 ```python
 from metaflow import Run
 sorted_models = best_models.sort_values(by=metrics[0], ascending=False).iloc[0]
 run = Run(""{}/{}"".format(sorted_models['flow name'], sorted_models['flow id']))
 run
 ```
 
 <CodeOutputBlock lang=""python"">
 
 ```
     Run('TuningFlow/1666721523161525')
 ```
 
 </CodeOutputBlock>
 
 Next, the model can be used to make predictions that we can check make sense when compared with the true targets:
 
 
 ```python
 from tensorflow import keras
 import numpy as np",H3,https://outerbounds.com/docs/cv-tutorial-S1E5/#access-the-best-model,False,622.0,85.0
559,Interpret Results with Tags,"In the last section, you saw how to access and use the best model by filtering Metaflow runs.
 What if you want to add a property to runs so you can filter by that property later? 
 Then it is time to leverage tagging. 
 You can use `.add_tag` on runs that meet any condition.
 
 In this case, we consider models that have a `test accuracy > threshold`. 
 Runs that have models meeting this threshold are tagged as `production`.
 
 
 ```python
 def tag_runs(flow, metric = 'accuracy', threshold = 0.99):
     for run in flow:
         if run.successful and hasattr(run.data, 'results'):
             if run.data.results[metric].max() > threshold:
                 run.add_tag('production')
 
 tag_runs(tuning_flow)
 tag_runs(model_comparison_flow)
 ```
 
 Now runs can be accessed by filtering on this tag:
 
 
 ```python
 from metaflow import Flow
 production_runs = Flow('TuningFlow').runs('production')
 ```
 
 In this lesson, you saw how to load and analyze results of your flows. 
 You added tags to runs that met your requirements for production quality.
 In the next lesson, you will see how to use models, filtered by the `production` tag, in a prediction flow.",H3,https://outerbounds.com/docs/cv-tutorial-S1E5/#interpret-results-with-tags,False,1169.0,220.0
560,Load the Data,"We start by loading the data in the same way as the [the previous episode](/docs/cv-tutorial-S1E1/):
 
 
 ```python
 from tensorflow import keras
 import numpy as np
 
 num_classes = 10
 ((x_train, y_train), (x_test, y_test)) = keras.datasets.mnist.load_data()
 x_train = x_train.astype(""float32"") / 255
 x_test = x_test.astype(""float32"") / 255
 x_train = np.expand_dims(x_train, -1)
 x_test = np.expand_dims(x_test, -1)
 y_train = keras.utils.to_categorical(y_train, num_classes)
 y_test = keras.utils.to_categorical(y_test, num_classes)
 ```",H3,https://outerbounds.com/docs/cv-tutorial-S1E2/#load-the-data,False,543.0,63.0
561,Configure Hyperparameters,"The model has several hidden layers defined by the `hidden_conv_layer_sizes` hyperparameter. 
 If you are new to machine learning you don't need to know about setting these values for now.
 For more experienced users, notice how these values appear in the Metaflow code you will write starting in the next episode. 
 We will use Metaflow as an experiment tracker for hyperparameter values and corresponding metric scores for models they define. 
 
 
 ```python
 hidden_conv_layer_sizes = [32, 64]
 input_shape = (28, 28, 1)
 kernel_size = (3, 3)
 pool_size = (2, 2)
 p_dropout = 0.5
 
 num_classes = y_test.shape[1]
 
 epochs = 5
 batch_size = 32
 verbose = 2 
 
 optimizer = 'adam'
 loss = 'categorical_crossentropy'
 metrics = ['accuracy']
 ```",H3,https://outerbounds.com/docs/cv-tutorial-S1E2/#configure-hyperparameters,False,746.0,121.0
562,Build a Model,"In this section you, will build a neural network using the `keras.Sequential` API. 
 The loop constructs a list of convolutional and pooling layers. 
 The list is then extended with a fully-connected layer.
 Finally, the Keras model is compiled so it is ready to train.
 
 
 ```python
 from tensorflow.keras import layers, Sequential, Input
 
 _layers = [Input(shape=input_shape)]",H3,https://outerbounds.com/docs/cv-tutorial-S1E2/#build-a-model,False,380.0,59.0
563,Train Your Model,"Keras models like the one you made in the previous step have a `.fit` function following the convention of the [sklearn Estimator API](https://scikit-learn.org/stable/developers/develop.html). One benefit of this API is that you can pass data in NumPy arrays directly to `model.fit`. 
 
 
 ```python
 history = model.fit(
     x_train, y_train,
     validation_data = (x_test, y_test),
     epochs = epochs,
     batch_size = batch_size,
     verbose = verbose
 )
 ```
 
 <CodeOutputBlock lang=""python"">
 
 ```
     Epoch 1/5
 
 
     2022-10-25 13:09:47.543570: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz
 
 
     1875/1875 - 15s - loss: 0.2047 - accuracy: 0.9368 - val_loss: 0.0522 - val_accuracy: 0.9831 - 15s/epoch - 8ms/step
     Epoch 2/5
     1875/1875 - 15s - loss: 0.0789 - accuracy: 0.9753 - val_loss: 0.0412 - val_accuracy: 0.9865 - 15s/epoch - 8ms/step
     Epoch 3/5
     1875/1875 - 13s - loss: 0.0616 - accuracy: 0.9803 - val_loss: 0.0360 - val_accuracy: 0.9878 - 13s/epoch - 7ms/step
     Epoch 4/5
     1875/1875 - 13s - loss: 0.0523 - accuracy: 0.9838 - val_loss: 0.0346 - val_accuracy: 0.9882 - 13s/epoch - 7ms/step
     Epoch 5/5
     1875/1875 - 14s - loss: 0.0469 - accuracy: 0.9857 - val_loss: 0.0278 - val_accuracy: 0.9902 - 14s/epoch - 7ms/step
 ```
 
 </CodeOutputBlock>",H3,https://outerbounds.com/docs/cv-tutorial-S1E2/#train-your-model,False,1346.0,256.0
564,Evaluate Performance,"Keras models also have a `.evaluate` function you can use to retrieve accuracy and loss scores.
 
 
 ```python
 scores = model.evaluate(x_test, y_test, verbose=0)
 categorical_cross_entropy = scores[0]
 accuracy = scores[1]
 msg = ""The CNN model predicted correctly {}% of the time on the test set.""
 print(msg.format(round(100*accuracy, 3)))
 ```
 
 <CodeOutputBlock lang=""python"">
 
 ```
     The CNN model predicted correctly 99.02% of the time on the test set.
 ```
 
 </CodeOutputBlock>
 
 In the last two episodes, you have developed and evaluated two different models. 
 Comparing models against each other is an important part of machine learning workflows. 
 For example, you may want to compare changes in hyperparameters and effects on performance metrics.
 You can also compare more complex models like the `CNN` from this episode to a simpler model like the `baseline` from [episode 1](/docs/cv-tutorial-S1E1/). Among other benefits, this comparison can help you avoid promoting unnecessarily complex models to a production environment.
 
 In any case, it is likely that at some point you will want to train models in parallel processes or on cloud machines. 
 In the next episode, you will learn how to package the `baseline` and `CNN` models into a flow that trains each model in parallel processes on your laptop. Then one of the primary Metaflow benefits kicks in, allowing you to run these flows on your cloud infrastructure seamlessly.",H3,https://outerbounds.com/docs/cv-tutorial-S1E2/#evaluate-performance,False,1454.0,230.0
565,Write a Prediction Flow,"[This flow](https://github.com/outerbounds/tutorials/blob/main/cv/prediction_flow.py) shows how you can:
 * Access the latest `TuningFlow` that has been tagged with `production`. 
 * Use the `best_model` from that flow run to make a prediction on a test image.
 * Produce a card showing the true label, the predicted label, and the probabilities for each target class. 
 
 ![](/assets/cv-tutorial-SinglePredictionFlow.png)
 
 <CustomCode height=""50vh"">
 
 
 
 ```py title=""prediction_flow.py""
 from metaflow import FlowSpec, step, Flow, Parameter, current, card
 import numpy as np
 
 class SinglePredictionFlow(FlowSpec):
 
     upstream_flow = Parameter('flow', default = 'TuningFlow')
     image_location = Parameter('im', default = './mnist_random_img.npy')
 
     @step
     def start(self):
         from tensorflow import keras
         softmax = keras.layers.Softmax()
         run = list(Flow(self.upstream_flow).runs('production'))[-1]
         self.model = run.data.best_model 
         with open(self.image_location, 'rb') as f:
             self.image = np.load(f)
         self.logits = self.model.predict(x = np.array([self.image]))
         self.probs = softmax(self.logits).numpy()
         if np.isclose(1, np.sum(self.probs)):
             self.pred = self.probs.argmax()
         else:
             self.fallback_model = ""Random Guess""
             self.pred = np.random.randint(low=0, high=9)
             print(""{}/{} probabilities not adding to 1"".format(
                 self.__class__.__name__, current.run_id))
             print(""Returning random fall back prediction"")
         self.next(self.end)
 
     @card
     @step
     def end(self):
         import matplotlib.pyplot as plt
         from metaflow.cards import Table, Markdown, Image
         self.im_fig, self.im_ax = plt.subplots()
         self.im_ax.imshow(self.image, cmap='gray')
         im1 = Image.from_matplotlib(self.im_fig)
         md = Markdown(""# Prediction: {}"".format(self.pred))
         self.dist_fig, self.dist_ax = plt.subplots()
         self.dist_ax.barh(y=np.arange(
             self.probs[0].shape[0]), width=self.probs[0])
         self.dist_ax.set_yticks(
             np.arange(self.probs[0].shape[0]), 
             labels=np.arange(self.probs[0].shape[0])
         )
         self.dist_ax.set_ylabel('Probability', fontsize=18)
         im2 = Image.from_matplotlib(self.dist_fig)
         current.card.append(Table([[im1, md, im2]]))
 
 if __name__ == '__main__': 
     SinglePredictionFlow()
 ```
 
 </CustomCode>",H3,https://outerbounds.com/docs/cv-tutorial-S1E6/#write-a-prediction-flow,False,2532.0,554.0
566,Choose a Test Data Sample,"To test the `SinglePredictionFlow`, you can save an image with NumPy's `.npy` file extension like `mnist-random-img.npy`. 
 Then, pass the filename to a `SinglePredictionFlow` run and get a prediction from the latest `TuningFlow` model tagged with `production`. 
 
 
 ```python
 from tensorflow import keras
 import numpy as np
 ((x_train, y_train), (x_test, y_test)) = keras.datasets.mnist.load_data()
 random_idx = np.random.randint(x_test.shape[0])
 with open('./mnist_random_img.npy', 'wb') as f:
     np.save(f, x_test[random_idx])
 ```",H3,https://outerbounds.com/docs/cv-tutorial-S1E6/#choose-a-test-data-sample,False,541.0,70.0
567,Run the Prediction Flow,"```bash
 python prediction_flow.py run --im './mnist_random_img.npy'
 ```
 
 <CodeOutputBlock lang=""bash"">
 
 ```
      Workflow starting (run-id 1666725084887884):
      [1666725084887884/start/1 (pid 54513)] Task is starting.
      [1666725084887884/start/1 (pid 54513)] 782: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz
     1/1 [==============================] - 0s 40ms/step/start/1 (pid 54513)] 1/1 [==============================] - ETA: 
      [1666725084887884/start/1 (pid 54513)] WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
      [1666725084887884/start/1 (pid 54513)] Task finished successfully.
      [1666725084887884/end/2 (pid 54516)] Task is starting.
      [1666725084887884/end/2 (pid 54516)] Task finished successfully.
      Done!
 ```
 
 </CodeOutputBlock>",H3,https://outerbounds.com/docs/cv-tutorial-S1E6/#run-the-prediction-flow,False,979.0,138.0
568,Monitor Your Model,"Now you can look at the results of the prediction. You could use the client API as shown in the [previous episode](/docs/cv-tutorial-S1E5/).
 In this case, you appended a Metaflow `card` to the `end` step of `SinglePredictionFlow`, so you can view the resulting visualization with this command:
 
 
 ```bash
 python prediction_flow.py card view end
 ```
 
 <CodeOutputBlock lang=""bash"">
 
 ```
     Metaflow 2.7.12 executing SinglePredictionFlow for user:eddie
     Resolving card: SinglePredictionFlow/1666725084887884/end/2
 ```
 
 </CodeOutputBlock>",H3,https://outerbounds.com/docs/cv-tutorial-S1E6/#monitor-your-model,False,552.0,80.0
569,Conclusion,"Congratulations, you have completed Metaflow's introductory tutorial on computer vision workflows! You have learned how to:
 * Create a flow that tracks and enables a comparison between machine learning models.
 * Visualize results of model training and prediction.
 * Use branching to perform steps in parallel.
 * Use tagging to evaluate and gate models for production.
 * Retrieve your model from another flow for prediction.
 
 To keep progressing in your Metaflow journey you can:
 * Check out the open-source [repository](https://github.com/Netflix/metaflow).
 * Join our [Slack community](http://slack.outerbounds.co/) and learn with us in #ask-metaflow.",H3,https://outerbounds.com/docs/cv-tutorial-S1E6/#conclusion,False,661.0,92.0
570,Best Practice: Reusable Code,"To integrate the Keras models you have built in a Metaflow flow it is convenient to put them in functions in a Python file such as [models.py](https://github.com/outerbounds/tutorials/blob/main/cv/models.py). In this example we implement the following functions:
 * `make_baseline`: Constructs a Keras model like our baseline architecture from episode 1.
 * `make_cnn`: Construct a Keras model like the CNN architecture from episode 2.
 * `fit_and_score`: Fit the model and return scores on a validation set.
 
 The functions are grouped together in the `ModelOperations` class. This class isn't necessary but is shown here to demonstrate a useful pattern to make functions easier to reuse and test. In the next section, you will be introduced to Metaflow by defining the `ModelComparisonFlow` object, which will use functionality defined by the `ModelOperations` class.
 
 <CustomCode height=""30vh"">
 
 
 
 ```py title=""models.py""
 from tensorflow import keras
 
 class ModelOperations:
 
     recall = 0.96
     precision = 0.96
 
     def make_baseline(self):
         model = keras.Sequential()
         model.add(keras.layers.Dense(
             self.num_pixels, input_dim=self.num_pixels,
             kernel_initializer=self.kernel_initializer,
             activation='relu'
         ))
         model.add(keras.layers.Dense(
             self.num_classes,
             kernel_initializer=self.kernel_initializer,
             activation='softmax'
         ))
         model.compile(
             loss=self.loss, 
             optimizer=self.optimizer, 
             metrics=self._keras_metrics()
         )
         return model
 
     def make_cnn(
         self,
         hidden_conv_layer_sizes = None
     ):
         model = keras.Sequential()
         if hidden_conv_layer_sizes is None:
             hidden_conv_layer_sizes = self.hidden_conv_layer_sizes
         _layers = [keras.Input(shape=self.input_shape)]
         for conv_layer_size in hidden_conv_layer_sizes:
             _layers.append(keras.layers.Conv2D(
                 conv_layer_size, 
                 kernel_size=self.kernel_size, 
                 activation=""relu""
             ))
             _layers.append(keras.layers.MaxPooling2D(
                 pool_size=self.pool_size
             ))
         _layers.extend([
             keras.layers.Flatten(),
             keras.layers.Dropout(self.p_dropout),
             keras.layers.Dense(self.num_classes, activation=""softmax"")
         ])
         model = keras.Sequential(_layers)
         model.compile(
             loss=self.loss, 
             optimizer=self.optimizer, 
             metrics=self._keras_metrics()
         )
         return model
 
     def fit_and_score(self, x_train, x_test):
         history = self.model.fit(
             x_train, self.y_train,
             validation_data = (x_test, self.y_test),
             epochs = self.epochs,
             batch_size = self.batch_size,
             verbose = self.verbose
         )
         scores = self.model.evaluate(x_test, self.y_test, verbose = 0)
         return history, scores
 
     def _keras_metrics(self):
         keras_metrics = []
         for _m in self.metrics:
             if _m == 'precision at recall':
                 keras_metrics.append(
                     keras.metrics.PrecisionAtRecall(recall=self.recall)
                 )
             elif _m == 'recall at precision':
                 keras.metrics.append(
                     keras.metrics.RecallAtPrecision(precision=self.precision)
                 )
             else:
                 keras_metrics.append(_m)
         return keras_metrics
 ```
 
 </CustomCode>",H3,https://outerbounds.com/docs/cv-tutorial-S1E3/#best-practice-reusable-code,False,3661.0,1062.0
571,Write a Flow,"[This flow](https://github.com/outerbounds/tutorials/blob/main/cv/model_comparison_flow.py) is designed to compare the baseline model against the CNN model. It includes:
 * All of the functions defined in the `ModelOperations` class.
 * A `start` step where we read the data.
 * A `baseline` step where the baseline model is fit to the data and scored on the test set.
 * A `cnn` step where the CNN model is fit to the data and scored on the test set.
 * A `gather_scores` step that joins the results from each modeling step and stores the results in a Metaflow card.
 * An `end` step that saves the best model.
 
 ![](/assets/cv-tutorial-ModelComparisonFlow.png)
 
 <CustomCode height=""50vh"">
 
 
 
 ```py title=""model_comparison_flow.py""
 from metaflow import FlowSpec, step, Flow, current, card
 from metaflow.cards import Image, Table
 from tensorflow import keras
 from models import ModelOperations
 
 class ModelComparisonFlow(FlowSpec, ModelOperations):
 
     best_model_location = (""latest_image_classifier"")
     num_pixels = 28 * 28
     kernel_initializer = 'normal'
     optimizer = 'adam'
     loss = 'categorical_crossentropy'
     metrics = ['accuracy', 'precision at recall']
     hidden_conv_layer_sizes = [32, 64]
     input_shape = (28, 28, 1)
     kernel_size = (3, 3)
     pool_size = (2, 2)
     p_dropout = 0.5
     epochs = 6
     batch_size = 64
     verbose = 2 
 
     @step
     def start(self):
         import numpy as np
         self.num_classes = 10
         ((x_train, y_train), 
          (x_test, y_test)) = keras.datasets.mnist.load_data()
         x_train = x_train.astype(""float32"") / 255
         x_test = x_test.astype(""float32"") / 255
         self.x_train = np.expand_dims(x_train, -1)
         self.x_test = np.expand_dims(x_test, -1)
         self.y_train = keras.utils.to_categorical(
             y_train, self.num_classes)
         self.y_test = keras.utils.to_categorical(
             y_test, self.num_classes)
         self.num_classes = self.y_test.shape[1]
         self.next(self.baseline, self.cnn)
 
     @step
     def baseline(self):
         from neural_net_utils import plot_learning_curves
         self.model = self.make_baseline()
         _x_train = self.x_train.reshape(
             self.x_train.shape[0], self.num_pixels
         ).astype('float32')
         _x_test = self.x_test.reshape(
             self.x_test.shape[0], self.num_pixels
         ).astype('float32')
         self.history, self.scores = self.fit_and_score(
             _x_train, _x_test)
         self._name = ""Baseline FFN""
         self.plots = [
             Image.from_matplotlib(p) for p in
             plot_learning_curves(self.history, self._name)
         ]
         self.next(self.gather_scores)
     
     @step
     def cnn(self):
         from neural_net_utils import plot_learning_curves
         self.model = self.make_cnn()
         #highlight-start
         self.history, self.scores = self.fit_and_score(
             self.x_train, self.x_test)
         #highlight-end
         self._name = ""CNN""
         self.plots = [
             Image.from_matplotlib(p) for p in
             plot_learning_curves(self.history, self._name)
         ]
         self.next(self.gather_scores)
 
     
     @card
     @step
     def gather_scores(self, models):
         import pandas as pd
         results = {
             'model': [], 'test loss': [],
             **{metric: [] for metric in self.metrics}
         }
         max_seen_acc = 0
         rows = []
         for model in models:
             #highlight-start
             results['model'].append(model._name)
             results['test loss'].append(model.scores[0])
             for i, metric in enumerate(self.metrics):
                 results[metric].append(model.scores[i+1])
             #highlight-end
             rows.append(model.plots)
             if model.scores[1] > max_seen_acc:
                 self.best_model = model.model
                 max_seen_acc = model.scores[1]
         #highlight-next-line
         current.card.append(Table(rows))
         self.results = pd.DataFrame(results)
         self.next(self.end)
 
     @step
     def end(self):
         self.best_model.save(self.best_model_location)
 
 if __name__ == '__main__':
     ModelComparisonFlow()
 ```
 
 </CustomCode>",H3,https://outerbounds.com/docs/cv-tutorial-S1E3/#write-a-flow,False,4313.0,1129.0
572,Run the Flow,"```bash
 python model_comparison_flow.py run
 ```
 
 <CodeOutputBlock lang=""bash"">
 
 ```
      Workflow starting (run-id 1666721393687341):
      [1666721393687341/start/1 (pid 53275)] Task is starting.
      [1666721393687341/start/1 (pid 53275)] Task finished successfully.
      [1666721393687341/baseline/2 (pid 53280)] Task is starting.
      [1666721393687341/cnn/3 (pid 53281)] Task is starting.
      [1666721393687341/cnn/3 (pid 53281)] 767: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz
      [1666721393687341/cnn/3 (pid 53281)] Epoch 1/6
      [1666721393687341/baseline/2 (pid 53280)] 753: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz
      [1666721393687341/baseline/2 (pid 53280)] Epoch 1/6
      [1666721393687341/baseline/2 (pid 53280)] 938/938 - 4s - loss: 0.2096 - accuracy: 0.9391 - precision_at_recall: 0.8943 - val_loss: 0.1061 - val_accuracy: 0.9670 - val_precision_at_recall: 0.9726 - 4s/epoch - 4ms/step
      [1666721393687341/baseline/2 (pid 53280)] Epoch 2/6
      [1666721393687341/baseline/2 (pid 53280)] 938/938 - 3s - loss: 0.0831 - accuracy: 0.9745 - precision_at_recall: 0.9860 - val_loss: 0.0776 - val_accuracy: 0.9758 - val_precision_at_recall: 0.9894 - 3s/epoch - 3ms/step
      [1666721393687341/baseline/2 (pid 53280)] Epoch 3/6
      [1666721393687341/baseline/2 (pid 53280)] 938/938 - 3s - loss: 0.0514 - accuracy: 0.9841 - precision_at_recall: 0.9952 - val_loss: 0.0663 - val_accuracy: 0.9796 - val_precision_at_recall: 0.9914 - 3s/epoch - 3ms/step
      [1666721393687341/baseline/2 (pid 53280)] Epoch 4/6
      [1666721393687341/baseline/2 (pid 53280)] 938/938 - 3s - loss: 0.0346 - accuracy: 0.9890 - precision_at_recall: 0.9980 - val_loss: 0.0653 - val_accuracy: 0.9807 - val_precision_at_recall: 0.9931 - 3s/epoch - 3ms/step
      [1666721393687341/baseline/2 (pid 53280)] Epoch 5/6
      [1666721393687341/cnn/3 (pid 53281)] 938/938 - 12s - loss: 0.2617 - accuracy: 0.9197 - precision_at_recall: 0.8069 - val_loss: 0.0631 - val_accuracy: 0.9814 - val_precision_at_recall: 0.9929 - 12s/epoch - 13ms/step
      [1666721393687341/cnn/3 (pid 53281)] Epoch 2/6
      [1666721393687341/baseline/2 (pid 53280)] 938/938 - 3s - loss: 0.0258 - accuracy: 0.9917 - precision_at_recall: 0.9990 - val_loss: 0.0699 - val_accuracy: 0.9792 - val_precision_at_recall: 0.9912 - 3s/epoch - 3ms/step
      [1666721393687341/baseline/2 (pid 53280)] Epoch 6/6
      [1666721393687341/baseline/2 (pid 53280)] 938/938 - 3s - loss: 0.0202 - accuracy: 0.9936 - precision_at_recall: 0.9994 - val_loss: 0.0659 - val_accuracy: 0.9801 - val_precision_at_recall: 0.9913 - 3s/epoch - 3ms/step
      [1666721393687341/baseline/2 (pid 53280)] Task finished successfully.
      [1666721393687341/cnn/3 (pid 53281)] 938/938 - 11s - loss: 0.0888 - accuracy: 0.9726 - precision_at_recall: 0.9831 - val_loss: 0.0437 - val_accuracy: 0.9858 - val_precision_at_recall: 0.9965 - 11s/epoch - 11ms/step
      [1666721393687341/cnn/3 (pid 53281)] Epoch 3/6
      [1666721393687341/cnn/3 (pid 53281)] 938/938 - 9s - loss: 0.0660 - accuracy: 0.9801 - precision_at_recall: 0.9911 - val_loss: 0.0372 - val_accuracy: 0.9884 - val_precision_at_recall: 0.9978 - 9s/epoch - 10ms/step
      [1666721393687341/cnn/3 (pid 53281)] Epoch 4/6
      [1666721393687341/cnn/3 (pid 53281)] 938/938 - 9s - loss: 0.0575 - accuracy: 0.9820 - precision_at_recall: 0.9937 - val_loss: 0.0341 - val_accuracy: 0.9886 - val_precision_at_recall: 0.9978 - 9s/epoch - 10ms/step
      [1666721393687341/cnn/3 (pid 53281)] Epoch 5/6
      [1666721393687341/cnn/3 (pid 53281)] 938/938 - 10s - loss: 0.0504 - accuracy: 0.9844 - precision_at_recall: 0.9951 - val_loss: 0.0306 - val_accuracy: 0.9895 - val_precision_at_recall: 0.9982 - 10s/epoch - 11ms/step
      [1666721393687341/cnn/3 (pid 53281)] Epoch 6/6
      [1666721393687341/cnn/3 (pid 53281)] 938/938 - 9s - loss: 0.0466 - accuracy: 0.9856 - precision_at_recall: 0.9956 - val_loss: 0.0263 - val_accuracy: 0.9910 - val_precision_at_recall: 0.9993 - 9s/epoch - 10ms/step
      [1666721393687341/cnn/3 (pid 53281)] WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
      [1666721393687341/cnn/3 (pid 53281)] WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
      [1666721393687341/cnn/3 (pid 53281)] Task finished successfully.
      [1666721393687341/gather_scores/4 (pid 53301)] Task is starting.
      [1666721393687341/gather_scores/4 (pid 53301)] WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
      [1666721393687341/gather_scores/4 (pid 53301)] Task finished successfully.
      [1666721393687341/end/5 (pid 53307)] Task is starting.
      [1666721393687341/end/5 (pid 53307)] WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
      [1666721393687341/end/5 (pid 53307)] WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.
      [1666721393687341/end/5 (pid 53307)] Task finished successfully.
      Done!
 ```
 
 </CodeOutputBlock>",H3,https://outerbounds.com/docs/cv-tutorial-S1E3/#run-the-flow,False,5694.0,840.0
573,Visualize Training Results in a Card,"When training many models it is important to visualize training characteristics and performance. 
 One way to do this is to use [Metaflow cards](https://docs.metaflow.org/metaflow/visualizing-results/easy-custom-reports-with-card-components). 
 You can use cards to inspect artifacts produced by a task or visualize the structure of the flow.
 They offer you a quick way to generate dashboards consisting of markdown, tables, images, and Python objects from your flows.
 You can read more about the available card components [here](https://docs.metaflow.org/api/cards#card-components). 
 
 The card in this flow is in the `gather_scores` step. 
 The function shows how to use Metaflow's `card` and `current` objects to add a component to the card. 
 In this instance, you will also see a `Table` constructed. 
 You don't need to remember much about the `Table` object or cards at this point, just that they are used to compose dashboards of flow results.
 For example, each row in this `Table` contains plots of the loss and accuracy curves for the model trained in the previous step.
 
 You can visualize the `card` for the `gather_scores` step with the following command after running the flow: 
 
 
 ```bash
 python model_comparison_flow.py card view gather_scores
 ```
 
 <CodeOutputBlock lang=""bash"">
 
 ```
     Metaflow 2.7.12 executing ModelComparisonFlow for user:eddie
     Resolving card: ModelComparisonFlow/1666721393687341/gather_scores/4
 ```
 
 </CodeOutputBlock>",H3,https://outerbounds.com/docs/cv-tutorial-S1E3/#visualize-training-results-in-a-card,False,1479.0,218.0
574,Access Results with the Client API,"In addition to storing results in cards, the Metaflow [client API](https://docs.metaflow.org/api/client) allows you to access data from past runs. If you are interested in a more robust Metaflow monitoring solution, you may want to check out the [Metaflow UI](https://github.com/Netflix/metaflow-ui).",H3,https://outerbounds.com/docs/cv-tutorial-S1E3/#access-results-with-the-client-api,False,300.0,39.0
575,Access the Metaflow Run Object,"```python
 from metaflow import Flow
 run = Flow('ModelComparisonFlow').latest_successful_run
 ```",H4,https://outerbounds.com/docs/cv-tutorial-S1E3/#access-the-metaflow-run-object,False,98.0,9.0
576,Fetch Versioned Run Data,"Once you have the run object, you are able to access any data stored to `.self` in the flow. For example, in the `gather_scores` function of `ModelComparisonFlow`, we save a dataframe to `self.results`. This is accessible after the run has completed as `run.data.results`. 
 
 
 ```python
 results = run.data.results
 results
 ```
     
 <HTMLOutputBlock >
 
 
 
 
 ```html
 <div>
 <style scoped>
     .dataframe tbody tr th:only-of-type {
         vertical-align: middle;
     }
 
     .dataframe tbody tr th {
         vertical-align: top;
     }
 
     .dataframe thead th {
         text-align: right;
     }
 </style>
 <table border=""1"" class=""dataframe"">
   <thead>
     <tr style=""text-align: right;"">
       <th></th>
       <th>model</th>
       <th>test loss</th>
       <th>accuracy</th>
       <th>precision at recall</th>
     </tr>
   </thead>
   <tbody>
     <tr>
       <th>0</th>
       <td>Baseline FFN</td>
       <td>0.065856</td>
       <td>0.9801</td>
       <td>0.991326</td>
     </tr>
     <tr>
       <th>1</th>
       <td>CNN</td>
       <td>0.026307</td>
       <td>0.9910</td>
       <td>0.999272</td>
     </tr>
   </tbody>
 </table>
 </div>
 ```
 
 
 
 </HTMLOutputBlock>",H4,https://outerbounds.com/docs/cv-tutorial-S1E3/#fetch-versioned-run-data,False,1202.0,304.0
577,Access the Best Model,"The `gather_scores` function of `ModelComparisonFlow` was also used to save the best model. You can access and use this model using the flow artifacts stored in `run.data`.
 
 
 ```python
 model = run.data.best_model
 model.summary()
 ```
 
 <CodeOutputBlock lang=""python"">
 
 ```
     Model: ""sequential_1""
     _________________________________________________________________
      Layer (type)                Output Shape              Param #   
     =================================================================
      conv2d (Conv2D)             (None, 26, 26, 32)        320       
                                                                      
      max_pooling2d (MaxPooling2D  (None, 13, 13, 32)       0         
      )                                                               
                                                                      
      conv2d_1 (Conv2D)           (None, 11, 11, 64)        18496     
                                                                      
      max_pooling2d_1 (MaxPooling  (None, 5, 5, 64)         0         
      2D)                                                             
                                                                      
      flatten (Flatten)           (None, 1600)              0         
                                                                      
      dropout (Dropout)           (None, 1600)              0         
                                                                      
      dense (Dense)               (None, 10)                16010     
                                                                      
     =================================================================
     Total params: 34,826
     Trainable params: 34,826
     Non-trainable params: 0
     _________________________________________________________________
 ```
 
 </CodeOutputBlock>",H4,https://outerbounds.com/docs/cv-tutorial-S1E3/#access-the-best-model,False,1910.0,1016.0
578,Make a Prediction,"Now you can make a prediction using the model. 
 The following code will load a data sample so we can see what the network predicts.
 
 
 ```python
 import numpy as np
 from tensorflow import keras
 ((x_train, y_train), (x_test, y_test)) = keras.datasets.mnist.load_data()
 x_test = x_test.astype(""float32"") / 255
 x_test = np.expand_dims(x_test, -1)
 
 idx = 147 # index of a data instance
 ```
 
 
 ```python
 import matplotlib.pyplot as plt
 plt.ioff();
 ```
 
 
 ```python
 fig, ax = plt.subplots()
 ax.imshow(x_test[idx], cmap='gray')
 ax.set_title('Label: {}'.format(y_test[idx], fontsize=18))
 fig.savefig('test_img.png');
 ```
 
 ![](./test_img.png)
 
 
 ```python
 logits = model.predict(x = np.array([x_test[idx]]))
 softmax = keras.layers.Softmax()
 probs = softmax(logits).numpy()
 pred = probs.argmax()
 print(""Model predicts {}"".format(pred))
 ```
 
 <CodeOutputBlock lang=""python"">
 
 ```
     1/1 [==============================] - 0s 43ms/step
     Model predicts 2
 
 
     2022-10-25 13:11:19.067485: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz
 ```
 
 </CodeOutputBlock>
 
 In this lesson you have seen a lot! 
 You used the baseline and CNN models in one flow.
 You tracked all the hyperparameters and model performance scores.
 Then you used Metaflow cards and the Metaflow client API to analyze results and make a prediction with your trained model.
 Stay tuned for the next lesson where we start thinking more about building a system to help you find high-performing models.",H4,https://outerbounds.com/docs/cv-tutorial-S1E3/#make-a-prediction,False,1547.0,221.0
579,Computer Vision with Metaflow: Beginner Tutorial,"In this tutorial, you will build a set of workflows to train and evaluate a machine learning model that performs image classification. You will use Keras and Metaflow to write computer vision code you can use as a foundation for real-world data science projects.
 
 <CustomCode height=""60vh"" sandboxLink=""https://account.outerbounds.dev/account/?workspace=/home/workspace/workspaces/cv/workspace.code-workspace"">
 
 ```python title=""Computer Vision with Metaflow - Beginner""
 from metaflow import FlowSpec, step, Flow, current, card
 from metaflow.cards import Image, Table
 from tensorflow import keras
 from models import ModelOperations
 
 class TuningFlow(FlowSpec, ModelOperations):
 
     best_model_location = (""best_tuned_model"")
     num_pixels = 28 * 28
     kernel_initializer = 'normal'
     optimizer = 'adam'
     loss = 'categorical_crossentropy'
     metrics = [
         'accuracy',
         'precision at recall'
     ]
     input_shape = (28, 28, 1)
     kernel_size = (3, 3)
     pool_size = (2, 2)
     p_dropout = 0.5
     epochs = 5
     batch_size = 64
     verbose = 2
 
     @step
     def start(self):
         import numpy as np
         self.num_classes = 10
         ((x_train, y_train), 
          (x_test, y_test)) = keras.datasets.mnist.load_data()
         x_train = x_train.astype(""float32"") / 255
         x_test = x_test.astype(""float32"") / 255
         self.x_train = np.expand_dims(x_train, -1)
         self.x_test = np.expand_dims(x_test, -1)
         self.y_train = keras.utils.to_categorical(
             y_train, self.num_classes)
         self.y_test = keras.utils.to_categorical(
             y_test, self.num_classes)
         self.param_config = [
             {""hidden_conv_layer_sizes"": [16, 32]},
             {""hidden_conv_layer_sizes"": [16, 64]},
             {""hidden_conv_layer_sizes"": [32, 64]},
             {""hidden_conv_layer_sizes"": [32, 128]},
             {""hidden_conv_layer_sizes"": [64, 128]}
         ]
         self.next(self.train, foreach='param_config')
 
     @step
     def train(self):
         from neural_net_utils import plot_learning_curves
         self.model = self.make_cnn(
             self.input['hidden_conv_layer_sizes'])
         self.history, self.scores = self.fit_and_score(
             self.x_train, self.x_test)
         self._name = 'CNN'
         self.plots = [
             Image.from_matplotlib(p) for p in
             plot_learning_curves(
                 self.history,
                 'Hidden Layers - ' + ', '.join([
                     str(i) for i in
                     self.input['hidden_conv_layer_sizes']
                 ])
             )
         ]
         self.next(self.gather_scores)
 
     @card
     @step
     def gather_scores(self, models):
         import pandas as pd
         self.max_class = models[0].y_train
         results = {
             'hidden conv layer sizes': [],
             'model': [], 
             'test loss': [],
             **{metric: [] for metric in self.metrics}
         }
         max_seen_acc = 0
         rows = []
         for model in models:
             results['model'].append(model._name)
             results['test loss'].append(model.scores[0])
             for i, metric in enumerate(self.metrics):
                 results[metric].append(model.scores[i+1])
             results['hidden conv layer sizes'].append(
                 ','.join([
                     str(i) for i in model.input[
                         'hidden_conv_layer_sizes'
                     ]
                 ])
             )
             # A simple rule for determining the best model.
             # In production flows you need to think carefully
                 # about how this kind of rule maps to your objectives.
             if model.scores[1] > max_seen_acc:
                 self.best_model = model.model
                 max_seen_acc = model.scores[1]
             rows.append(model.plots)
 
         current.card.append(Table(rows))
         self.results = pd.DataFrame(results)
         self.next(self.end)
 
     @step
     def end(self):
         self.best_model.save(self.best_model_location)
 
 if __name__ == '__main__':
     TuningFlow()
 ```
 </CustomCode>",H1,https://outerbounds.com/docs/cv-tutorial-overview#computer-vision-with-metaflow-beginner-tutorial,False,4215.0,1280.0
580,Prerequisites,We assume that you have taken the [introductory tutorials](/docs/intro-tutorial-overview) or know the basics of Metaflow.,H2,https://outerbounds.com/docs/cv-tutorial-overview#prerequisites,False,121.0,15.0
581,Tutorial Structure,"The content includes the following:
 * [Episode 1](/docs/cv-tutorial-S1E1): Exploratory Data Analysis
 * [Episode 2](/docs/cv-tutorial-S1E2): Build a CNN Model
 * [Episode 3](/docs/cv-tutorial-S1E3): Train Models in Parallel
 * [Episode 4](/docs/cv-tutorial-S1E4): Tune Hyperparameters
 * [Episode 5](/docs/cv-tutorial-S1E5): Analyze Results in a Notebook
 * [Episode 6](/docs/cv-tutorial-S1E6): Predict with the Best Model
 
 Each episode contains either a Metaflow script to run and/or a Jupyter notebook. 
 You do not need access to cloud computing or a [Metaflow deployment](/engineering/welcome/) to complete the episodes.
 The estimated time to complete all episodes is 1-2 hours.",H2,https://outerbounds.com/docs/cv-tutorial-overview#tutorial-structure,False,686.0,87.0
582,Why Metaflow?,"The main benefit of using a data science workflow solution like Metaflow when prototyping is that your code will be built on a strong foundation for deploying to a production environment. Metaflow is most useful when projects have scaling requirements, are mission-critical, and/or have many interacting parts. You can read more at these links:
 * [Should I Use Metaflow?](https://docs.metaflow.org/introduction/what-is-metaflow#should-i-use-metaflow)
 * [Why Metaflow?](https://docs.metaflow.org/introduction/why-metaflow)
 
 After completing the lessons, you will be able to transfer insights and code from the tutorial to your real-world data science projects.
 It is important to be mindful that this is a beginner tutorial so it will not reflect many important challenges to consider in production ML environments. For example, in production, you may consider using Metaflow features such as the `@conda` decorator for dependency management, `@batch` or `@kubernetes` for remote execution, and `@schedule` to automatically trigger jobs.",H2,https://outerbounds.com/docs/cv-tutorial-overview#why-metaflow,False,1041.0,141.0
583,Computer Vision with Metaflow: Beginner Tutorial,"In this tutorial, you will build a set of workflows to train and evaluate a machine learning model that performs image classification. You will use Keras and Metaflow to write computer vision code you can use as a foundation for real-world data science projects.
 
 <CustomCode height=""60vh"" sandboxLink=""https://account.outerbounds.dev/account/?workspace=/home/workspace/workspaces/cv/workspace.code-workspace"">
 
 ```python title=""Computer Vision with Metaflow - Beginner""
 from metaflow import FlowSpec, step, Flow, current, card
 from metaflow.cards import Image, Table
 from tensorflow import keras
 from models import ModelOperations
 
 class TuningFlow(FlowSpec, ModelOperations):
 
     best_model_location = (""best_tuned_model"")
     num_pixels = 28 * 28
     kernel_initializer = 'normal'
     optimizer = 'adam'
     loss = 'categorical_crossentropy'
     metrics = [
         'accuracy',
         'precision at recall'
     ]
     input_shape = (28, 28, 1)
     kernel_size = (3, 3)
     pool_size = (2, 2)
     p_dropout = 0.5
     epochs = 5
     batch_size = 64
     verbose = 2
 
     @step
     def start(self):
         import numpy as np
         self.num_classes = 10
         ((x_train, y_train), 
          (x_test, y_test)) = keras.datasets.mnist.load_data()
         x_train = x_train.astype(""float32"") / 255
         x_test = x_test.astype(""float32"") / 255
         self.x_train = np.expand_dims(x_train, -1)
         self.x_test = np.expand_dims(x_test, -1)
         self.y_train = keras.utils.to_categorical(
             y_train, self.num_classes)
         self.y_test = keras.utils.to_categorical(
             y_test, self.num_classes)
         self.param_config = [
             {""hidden_conv_layer_sizes"": [16, 32]},
             {""hidden_conv_layer_sizes"": [16, 64]},
             {""hidden_conv_layer_sizes"": [32, 64]},
             {""hidden_conv_layer_sizes"": [32, 128]},
             {""hidden_conv_layer_sizes"": [64, 128]}
         ]
         self.next(self.train, foreach='param_config')
 
     @step
     def train(self):
         from neural_net_utils import plot_learning_curves
         self.model = self.make_cnn(
             self.input['hidden_conv_layer_sizes'])
         self.history, self.scores = self.fit_and_score(
             self.x_train, self.x_test)
         self._name = 'CNN'
         self.plots = [
             Image.from_matplotlib(p) for p in
             plot_learning_curves(
                 self.history,
                 'Hidden Layers - ' + ', '.join([
                     str(i) for i in
                     self.input['hidden_conv_layer_sizes']
                 ])
             )
         ]
         self.next(self.gather_scores)
 
     @card
     @step
     def gather_scores(self, models):
         import pandas as pd
         self.max_class = models[0].y_train
         results = {
             'hidden conv layer sizes': [],
             'model': [], 
             'test loss': [],
             **{metric: [] for metric in self.metrics}
         }
         max_seen_acc = 0
         rows = []
         for model in models:
             results['model'].append(model._name)
             results['test loss'].append(model.scores[0])
             for i, metric in enumerate(self.metrics):
                 results[metric].append(model.scores[i+1])
             results['hidden conv layer sizes'].append(
                 ','.join([
                     str(i) for i in model.input[
                         'hidden_conv_layer_sizes'
                     ]
                 ])
             )
             # A simple rule for determining the best model.
             # In production flows you need to think carefully
                 # about how this kind of rule maps to your objectives.
             if model.scores[1] > max_seen_acc:
                 self.best_model = model.model
                 max_seen_acc = model.scores[1]
             rows.append(model.plots)
 
         current.card.append(Table(rows))
         self.results = pd.DataFrame(results)
         self.next(self.end)
 
     @step
     def end(self):
         self.best_model.save(self.best_model_location)
 
 if __name__ == '__main__':
     TuningFlow()
 ```
 </CustomCode>",H1,https://outerbounds.com/docs/cv-tutorial-overview#computer-vision-with-metaflow-beginner-tutorial,False,4215.0,1280.0
584,Prerequisites,We assume that you have taken the [introductory tutorials](/docs/intro-tutorial-overview) or know the basics of Metaflow.,H2,https://outerbounds.com/docs/cv-tutorial-overview#prerequisites,False,121.0,15.0
585,Tutorial Structure,"The content includes the following:
 * [Episode 1](/docs/cv-tutorial-S1E1): Exploratory Data Analysis
 * [Episode 2](/docs/cv-tutorial-S1E2): Build a CNN Model
 * [Episode 3](/docs/cv-tutorial-S1E3): Train Models in Parallel
 * [Episode 4](/docs/cv-tutorial-S1E4): Tune Hyperparameters
 * [Episode 5](/docs/cv-tutorial-S1E5): Analyze Results in a Notebook
 * [Episode 6](/docs/cv-tutorial-S1E6): Predict with the Best Model
 
 Each episode contains either a Metaflow script to run and/or a Jupyter notebook. 
 You do not need access to cloud computing or a [Metaflow deployment](/engineering/welcome/) to complete the episodes.
 The estimated time to complete all episodes is 1-2 hours.",H2,https://outerbounds.com/docs/cv-tutorial-overview#tutorial-structure,False,686.0,87.0
586,Why Metaflow?,"The main benefit of using a data science workflow solution like Metaflow when prototyping is that your code will be built on a strong foundation for deploying to a production environment. Metaflow is most useful when projects have scaling requirements, are mission-critical, and/or have many interacting parts. You can read more at these links:
 * [Should I Use Metaflow?](https://docs.metaflow.org/introduction/what-is-metaflow#should-i-use-metaflow)
 * [Why Metaflow?](https://docs.metaflow.org/introduction/why-metaflow)
 
 After completing the lessons, you will be able to transfer insights and code from the tutorial to your real-world data science projects.
 It is important to be mindful that this is a beginner tutorial so it will not reflect many important challenges to consider in production ML environments. For example, in production, you may consider using Metaflow features such as the `@conda` decorator for dependency management, `@batch` or `@kubernetes` for remote execution, and `@schedule` to automatically trigger jobs.",H2,https://outerbounds.com/docs/cv-tutorial-overview#why-metaflow,False,1041.0,141.0
587,Shared Mode Architecture,"The diagram below shows an overview of services used by Metaflow in the shared mode. The services outlined in yellow are required: Development Environment, Datastore, and Metaflow Service and its database. The services outlined with dashed lines are optional.
 
 ![](/assets/service-arch-01.png)
 <Caption> Communication channels between Metaflow services </Caption>
 
 Before we go into details about each of the services, the following diagram highlights their role in the Metaflow stack.
 
 ![](/assets/service-arch-02.png)
 <Caption> The roles Metaflow services play in the stack </Caption>
 
 Metaflow treats both prototyping and production as equally important activities, which is why both Development Environment and Production Scheduler are supported as first-class execution environments. Both of these environments can rely on a separate Compute Cluster, typically a container platform, to provision compute resources on the fly. Much of the value of Metaflow relates to the fact that all executions are tracked by a central metadata service and their results are persisted in a common datastore.",H2,https://outerbounds.com/engineering/service-architecture/#shared-mode-architecture,False,1107.0,158.0
588,Required Services,"At the minimum, you need to set up the following three services to leverage Metaflow in the shared mode.",H2,https://outerbounds.com/engineering/service-architecture/#required-services,False,104.0,19.0
589,Development Environment,"The client-side of Metaflow consists of a command-line tool, `metaflow`, and a Python package, also named `metaflow`. The user needs a development environment where they can develop and test their Metaflow workflows.
 
 The development environment has the following requirements:
 
 1. [Metaflow needs to be installed](https://docs.metaflow.org/getting-started/install). Note that there isn’t a strict requirement to use the latest version of the library. Users can upgrade their library at their own cadence.
 2. [Metaflow needs to be configured](/engineering/operations/configure-metaflow/) to access the services provided. At the minimum, the configuration needs to include the metadata service URL and information related to the datastore.
 3. The development environment needs to have the right cloud credentials to make API calls to the provided cloud services \(e.g. `~/.aws/credentials` or IAM instance profile needs to be set up\).
 
 The user’s laptop can work as a development environment but we recommend providing a sandbox instance or a container in the cloud, if possible. A cloud instance has multiple benefits:",H3,https://outerbounds.com/engineering/service-architecture/#development-environment,False,1127.0,162.0
590,Metadata,"[The Metaflow Service](https://github.com/Netflix/metaflow-service) tracks all details about Metaflow executions, including pointers to the artifacts in Datastore. The main consumer of the service is the [Client API](https://docs.metaflow.org/metaflow/client). Metaflow executions write to the service but they don't read from it internally as they access Datastore directly. This ensures that the Metaflow Service doesn't easily become a bottleneck, even when tens of thousands of tasks are being executed concurrently.
 
 No bulk data is stored in the service, so just a small instance backed by a medium-size database can keep track of millions of executions. As of today, we support Postgres as the backend database.
 
 The service is provided as [a pre-baked Docker container](https://hub.docker.com/r/netflixoss/metaflow_metadata_service) which you can deploy to a microservice platform of your choice. The service is a simple Flask app communicating with the Metaflow library over HTTP, so you can deploy it like any other basic web application.
 
 The service doesn’t have any built-in support for authentication. We assume that the service is typically deployed inside a \(virtual\) private network that provides a secure operating environment.
 
 Optionally, you can deploy multiple instances of the service behind a load balancer. The AWS CloudFormation deployment path does this for you automatically.",H3,https://outerbounds.com/engineering/service-architecture/#metadata,False,1413.0,201.0
591,Metadata Migrations,"We recommend that you always deploy the latest version of the service image. The latest image is guaranteed to be compatible with all previous versions of the client. However, occasionally new features of Metaflow require the database schema to be changed, which will require manual action from the administrator.
 
 To make the administrator’s life easy, the Metaflow Service comes with a built-in migration service. When you deploy or restart the latest version of the image, it will detect the schema version of the database, and launch a corresponding version of the service. It will not upgrade the database automatically.
 
 If a new feature in the client requires a newer version of the service, a clear error message is shown. In this situation, the administrator should decide if and when they want to run the migration, which will incur some downtime - up to a few minutes. As a best practice, it is advisable to take a backup of the database prior to the migration which allows you to roll back the migration in case something goes wrong.
 
 The migration itself is just a matter of running a single command and restarting the service afterwards. How to do this exactly depends on your deployment strategy: See a separate section about [running migrations on AWS](/engineering/operations/migration/).",H4,https://outerbounds.com/engineering/service-architecture/#metadata-migrations,False,1311.0,216.0
592,Datastore,"While Metaflow uses local disk as a datastore in the local mode, all serious use cases of Metaflow are expected to use a cloud-based object store such as AWS S3 as the persistence layer. Object stores provide excellent scalability in terms of space, massive throughput for concurrent requests, high durability, and desirable cost-effectiveness, which means that Metaflow can afford storing all artifacts automatically without having to worry about overloading the storage backend. Internally, Metaflow uses the datastore as a [Content Addressable Storage](https://en.wikipedia.org/wiki/Content-addressable_storage), which means that no duplicate artifacts are stored.
 
 In the administrator’s point of view, an object store like S3 is effectively maintenance-free. You may configure access policies and lifecycle rules for your object store as required by your organization.",H3,https://outerbounds.com/engineering/service-architecture/#datastore,False,875.0,122.0
593,Optional Services,"The following two services are optional. They provide a way to scale out Metaflow executions and deploy Metaflow workflows in a highly available production scheduler. If your organization doesn’t require elastic scalability and occasional downtime for scheduled workflow executions is acceptable, you may ignore these services.",H2,https://outerbounds.com/engineering/service-architecture/#optional-services,False,327.0,46.0
594,Compute Cluster,"Similarly as a cloud-based object store provides a virtually infinite datastore, a cloud-based, elastically scalable compute cluster provides an infinitely scalable compute backend. Conceptually, Metaflow uses the compute cluster as a function-as-a-service platform: The user defines a function to be computed, a Metaflow step, optionally [specifying resources it requires](https://docs.metaflow.org/metaflow/scaling-out-and-up/effortless-scaling-with-kubernetes#requesting-resources-with-resources-decorator). The function is either executed locally in the Development Environment as a subprocess or it is shipped out to a compute cluster which provisions resources for it on the fly.
 
 Metaflow packages and snapshots the user's code automatically in Datastore for remote execution. In addition, Metaflow provides built-in support for [defining dependencies](https://docs.metaflow.org/metaflow/dependencies) required by the user's code. The dependencies are also snapshot in Datastore, to make sure that unexpected changes in the package repository won't affect Metaflow executions.
 
 Since Metaflow packages both the user's code and dependencies automatically, executions on Compute Cluster can use a single, off-the-shelf Docker image for all executions, which removes a major source of operational complexity as the administrator doesn't need to maintain a CI/CD pipeline, custom Dockerfiles, and potentially hundreds of separate images.
 
 The compute cluster can support two dimensions of scalability: vertical \(bigger instances\) and horizontal \(more instances\). For instance, vertical scalability can a good way to scale the training step of a large model that runs most efficiently on a large instance with tens of CPU cores, multiple GPUs, and hundreds of GBs of RAM. Horizontal scalability through [Metaflow’s foreach construct](https://docs.metaflow.org/metaflow/basics#foreach) can be an efficient way to handle e.g. sharded datasets in an embarrassingly parallel fashion. The user can choose any combination of the two dimensions which best matches to their workload.
 
 Currently, Metaflow provides an integration to Kubernetes and AWS Batch as a Compute Cluster backend. The administrator can configure namespace in Kubernetes or compute environments in AWS Batch depending on the types of workloads that they want to support in Metaflow.",H3,https://outerbounds.com/engineering/service-architecture/#compute-cluster,False,2360.0,313.0
595,Production Scheduler,"Metaflow comes with a built-in local scheduler which makes it easy to develop and test workflows in Development Environment. It is a great solution for use cases where quick, manual iterations are preferred over high availability and unattended execution.
 
 Data science workflows that need to run automatically without any human intervention have a different set of requirements. Most importantly, Production Scheduler needs to be highly available: it needs to be backed by a cluster of instances so that a failure of any single instance won’t cause downtime. Preferably, the scheduler should be highly scalable, both in terms of the size of a single workflow, which may spawn tens of thousands of tasks, as well as in terms of the number of concurrent workflows, which can grow to hundreds of thousands in a large deployment like at Netflix. In addition, the scheduler should provide flexible ways to trigger executions, and it should provide a plethora of tools for operational monitoring and alerting.
 
 The user can deploy their Metaflow workflow to Production Scheduler with a single command - no changes in the code are required. We recognize that “deploying to production” is not a linear process. Rather, we expect the user to use both the local scheduler and the production scheduler in parallel. For instance, after the initial deployment, the data scientist typically wants to continue working on the project locally. Eventually, they might want to deploy a new, experimental version on the production scheduler to run in parallel with the production version as an A/B test. Also, things fail in production. Metaflow allows the user to reproduce issues that occur on the production scheduler locally, simply by using [the resume command](https://docs.metaflow.org/metaflow/debugging#how-to-use-the-resume-command) to continue the execution on their local machine.
 
 Currently, Metaflow supports three workflow orchestrators for scheduling flows in production:
 
  - [Argo Workflows](https://docs.metaflow.org/going-to-production-with-metaflow/scheduling-metaflow-flows/scheduling-with-argo-workflows)
  - [AWS Step Functions](https://docs.metaflow.org/going-to-production-with-metaflow/scheduling-metaflow-flows/scheduling-with-aws-step-functions)
  - [Apache Airflow](https://docs.metaflow.org/going-to-production-with-metaflow/scheduling-metaflow-flows/scheduling-with-airflow)
 
 For more background about production orchestration, see [the release blog post for Step Functions integration](https://medium.com/@NetflixTechBlog/unbundling-data-science-workflows-with-metaflow-and-aws-step-functions-d454780c6280) and [a post about Metaflow-Airflow integration](https://outerbounds.com/blog/better-airflow-with-metaflow/).",H3,https://outerbounds.com/engineering/service-architecture/#production-scheduler,False,2738.0,338.0
596,Security Considerations,"Metaflow relies on the security mechanisms and policies provided by the deployment environment, for instance, VPCs, Security Groups and IAM on AWS. These mechanisms allow you to define as fine-grained security policies as required by your organization.
 
 In the simplest setup, you can deploy all Metaflow services inside a common security perimeter, e.g. within a single virtual subnet. A more tightly controlled deployment is provided by our CloudFormation template which creates tightly scoped IAM roles and VPCs for you automatically.
 
 Metaflow allows all users that have access to a shared Datastore and Metadata to access any data from past executions. If your organization requires more control over data governance, you can create multiple isolated Metaflow deployments.
 
 For maximum security and control, you can consider a similar setup as what we provide with [Metaflow Sandboxes](https://docs.metaflow.org/metaflow-on-aws/metaflow-sandbox). Sandboxes disallow all network ingress and egress within Compute Cluster. The user is able to execute arbitrary code, even using arbitrary library dependencies defined using [@conda](https://docs.metaflow.org/metaflow/dependencies), and they are able to process arbitrary data that is imported in Datastore either using standard S3 tools or [the IncludeFile construct](https://docs.metaflow.org/metaflow/data#data-in-local-files). They can deploy workflows to Production Scheduler and analyze results in the provided notebook environment. In effect, they can perform any data science operations as usual.
 
 However, no data can leave the environment since all communications with the external world is blocked. Technically, Sandboxes are implemented using extremely tightly scoped IAM roles, VPCs, and security groups. If you are interested in providing a similar sandbox environment for your users, [please reach out to us](/docs/get-in-touch).",H2,https://outerbounds.com/engineering/service-architecture/#security-considerations,False,1904.0,260.0
597,Learn more,"See these resources to learn more about the internals of Metaflow:
 
  - [Overview of Metaflow Stacks](https://docs.metaflow.org/getting-started/infrastructure)
  - [Service Architecture](/engineering/service-architecture/)
  - [Internals of Metaflow](https://docs.metaflow.org/internals/technical-overview)",H2,https://outerbounds.com/engineering/motivation/#learn-more,False,307.0,27.0
598,Metaflow 2.1.+,"In Metaflow 2.1.0, we introduced [a new AWS service integration with AWS Step Functions](https://docs.metaflow.org/going-to-production-with-metaflow/scheduling-metaflow-flows). Now, users of Metaflow can easily deploy their flows to AWS Step Functions. If this is a functionality that you would like to use, depending on if/when you deployed the [metaflow service](https://app.gitbook.com/@hawkins/s/metaflow-admin/~/drafts/-MDQ9c_b9eEtHKMgoQni/metaflow-on-aws/metaflow-on-aws#metadata), you might have to take some actions to upgrade your service. If while trying to [schedule your flows](https://docs.metaflow.org/going-to-production-with-metaflow/scheduling-metaflow-flows) on AWS Step Functions via :
 
 ```text
 python myflow.py step-functions create
 ```
 
 you ran into the following error :
 
 ![](/assets/screenshot-2020-07-29-at-8.27.22-am.png)
 
 then you would need to upgrade the deployed version of your metaflow service. This upgrade requires migration of the backing RDS instance.
 
 In this situation, the administrator should decide if and when they want to run the migration, which will incur some downtime - up to a few minutes. As a best practice, it is advisable to take a backup of the database prior to the migration which allows you to roll back the migration in case something goes wrong.
 
 :::info
 We highly recommend [taking a backup of your RDS instance](https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_CommonTasks.BackupRestore.html) before attempting this upgrade. This will allow you to restore the service from the backup in case there are any issues with the migration.
 :::
 
 :::info
 This migration will result in a short downtime of the metaflow service. Metaflow users can guard against this downtime by using the [@retry](https://docs.metaflow.org/metaflow/failures) decorator in their flows.
 :::
 
 To make this database migration easy, metaflow service comes with a built-in migration service. When you deploy or restart the [latest version of the metaflow service image](https://hub.docker.com/repository/docker/netflixoss/metaflow_metadata_service), the migration service will detect the schema version of the backing database, and launch the latest version of the metaflow service that is compatible with the database schema. The migration service provides hooks to upgrade the database schema to the latest version so that you can upgrade the metaflow service to the latest version.
 
 There are two paths to upgrading your service, depending on how you first deployed the service - using our AWS CloudFormation template or manually through the AWS console.
 
 In addition to migrating the service, if you intend to use AWS Step Functions, you would need to update a few IAM roles and set up an Amazon DynamoDB table. The instructions below will walk you through those as well.",H2,https://outerbounds.com/engineering/operations/migration/#metaflow-21,False,2838.0,388.0
599,AWS CloudFormation Deployment,"If you originally deployed the AWS resources needed for Metaflow using our AWS CloudFormation template, then you can use AWS CloudFormation to spin up the necessary resources for this service migration. The latest version of the AWS CloudFormation template pulls the [latest version of the metaflow service image](https://hub.docker.com/repository/docker/netflixoss/metaflow_metadata_service) which comes bundled with a migration service as well as an AWS Lambda function which you can execute manually to trigger the database migration.
 
 :::info
 We highly recommend [taking a backup of your RDS instance](https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_CommonTasks.BackupRestore.html) before attempting this upgrade. This will allow you to restore the service from the backup in case there are any issues with the migration
 :::
 
 1. Open the [AWS CloudFormation console](https://console.aws.amazon.com/cloudformation) and choose the stack corresponding to your existing deployment.
 2. Choose _Update_ and choose _Replace current template_ under _Prerequisite - Prepare template._
 3. Choose _Upload a template file_ under _Specify template_.
 4. Choose _Choose file_ and upload [this template](https://github.com/Netflix/metaflow-tools/blob/master/aws/cloudformation/metaflow-cfn-template.yml). You will have to copy the template to your laptop before you can upload it. Choose _Next._
 5. Select your parameters for your deployment under _Parameters_ and choose _Next._
 6. Feel free to tag your stack in whatever way best fits your organization. When finished, choose _Next._
 7. The _Change set preview_ will log all the changes that this update to your CloudFormation stack will cause. If you were not already on the latest version, you will notice that there are a few additions that this update will result in -
    1. _StepFunctionsRole -_ IAM role for AWS Step Functions
    2. _EventBridgeRole -_ IAM role for Amazon EventBridge
    3. _StepFunctionsStateDDB -_ Amazon DynamoDB table
    4. _ExecuteDBMigration_ - AWS Lamdba function for upgrading the RDS schema
    5. Updates to _BatchS3TaskRole_ and _ECSFargateService_ to allow for migration and AWS Step Functions integration.
 8. Choose _I acknowledge that AWS CloudFormation might create IAM resources_ and choose _Update stack._
 9. Wait for the stack to finish updating itself. This might take ~10 minutes.
 10. Once the stack has updated, you would notice a new key _MigrationFunctionName_ which points to the AWS Lambda function that will upgrade your database schema. Note the name of this function.
 11. Open the [Amazon ECS console](https://console.aws.amazon.com/ecs) and navigate to your AWS Fargate cluster in _Clusters_ tab.
 12. Under the _Tasks_ tab, choose _Stop All._ This will stop all your tasks causing your service to reboot. Once the service has rebooted, your tasks will fetch the latest image of the service from docker hub and launch the migration service on port 8082.
 13. Using either the [AWS Lambda console](https://console.aws.amazon.com/lambda) or AWS CLI, trigger the lambda function from Step 10. -
     1. AWS Lambda console
        1. Choose the function that you just deployed in Step 10.
        2. In the dropdown for _Select a test event_, choose _Configure test events._
        3. In the resulting dialog, give a name to your event in _Event name_. The actual contents don't matter in this case. Choose _Create._
        4. Make sure you have [taken a backup of your RDS instance](https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_CommonTasks.BackupRestore.html) before proceeding with the next step.
        5. Choose _Test._
        6. Check the execution result. In the resulting JSON blob, you should see `upgrade-result` set to `upgrade success` and `is_up_to_date` in `final-status` set to `true`. Congratulations! You have upgraded your database schema successfully. You can skip Step 7. and now let's upgrade the version of the metaflow service.
        7. If you saw a failure, you might need to [restore your RDS instance using the backup](https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_RestoreFromSnapshot.html) that you had generated before if your service fails to behave correctly. Please [get in touch](/docs/get-in-touch) with us so that we can figure out what went wrong.
     2. AWS CLI
        1. Make sure you have appropriate credentials to execute the AWS Lambda function on your laptop.
        2. Make sure you have taken a backup of your RDS instance before proceeding with the next step.
        3. Execute the command `aws lambda invoke --function-name <lambda-function-name> output.log`
        4. Check the execution result. In the resulting JSON blob, you should see `upgrade-result` set to `upgrade success` and `is_up_to_date` in `final-status` set to `true`. Congratulations! You have upgraded your database schema successfully. You can skip Step 5. and now let's upgrade the version of the metaflow service.
        5. If you saw a failure, you might need to [restore your RDS instance using the backup](https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_RestoreFromSnapshot.html) that you had generated before if your service fails to behave correctly. Please [get in touch](/docs/get-in-touch) with us so that we can figure out what went wrong.
 14. Open the [Amazon ECS console](https://console.aws.amazon.com/ecs) and navigate to your AWS Fargate cluster in _Clusters_ tab.
 15. Under the _Tasks_ tab, choose _Stop All._ This will stop all your tasks causing your service to reboot.
 16. Once the tasks have rebooted and entered the _RUNNING_ state, choose any task and select the public IP. Curl this public IP on port 8080 with the _version_ endpoint. curl xxx.xxx.xxx.xxx:8080/version. The response will be the version of your metaflow service and it should be &gt;= 2.0.2. Congratulations! You have successfully upgraded the service!
 17. Because you used the latest [CloudFormation template](https://github.com/Netflix/metaflow-tools/blob/master/aws/cloudformation/metaflow-cfn-template.yml), all the necessary IAM roles and permissions for AWS Step Functions for scheduling Metaflow flows are already configured for you. You can now [configure your Metaflow installation](/engineering/operations/configure-metaflow/) with these additional resources.
 
 In case of any issues, please [get in touch](/docs/get-in-touch) with us.
 
 ![AWS Lambda execution response from AWS console](/assets/screen-shot-2020-07-14-at-1.24.45-am.png)
 
 <Caption>AWS Lambda execution response from AWS console</Caption>",H3,https://outerbounds.com/engineering/operations/migration/#aws-cloudformation-deployment,False,6599.0,1003.0
600,Manual Deployment,"If you originally deployed the AWS resources needed for Metaflow manually, then there are a few steps you would need to take for this service migration. You would also need to manually set up the IAM roles and permissions to be able to schedule Metaflow flows on AWS Step Functions.",H3,https://outerbounds.com/engineering/operations/migration/#manual-deployment,False,282.0,50.0
601,Upgrading Metaflow Service manually,"The metaflow service repository has a [migration tool](https://github.com/Netflix/metaflow-service/blob/master/migration_tools.py) that will perform the update on your behalf once you have upgraded your metaflow service [to use the latest version of the docker image](https://hub.docker.com/repository/docker/netflixoss/metaflow_metadata_service).
 
 :::info
 We highly recommend [taking a backup of your RDS instance](https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_CommonTasks.BackupRestore.html) before attempting this upgrade. This will allow you to restore the service from the backup in case there are any issues with the migration
 :::
 
 1. Open the [Amazon ECS console](https://console.aws.amazon.com/ecs) and navigate to your AWS Fargate cluster in _Clusters_ tab.
 2. Resize your cluster to 1 task \(if you have more than 1 task, otherwise move to Step 4.\) by choosing your service under _Services_ and choosing _Update._
 3. Choose _Number of task_ to 1, choose _Next step, Next step, Next step, Update service._
 4. After the service has updated, under the _Tasks_ tab, choose _Stop All._ This will stop all your tasks causing your service to reboot. Once the service has rebooted, your tasks will fetch the latest image of the service from docker hub and launch the migration service on port 8082. Note the public IP of this task.
    1. Please note that this will only happen if you were using the latest tag \(or no tag\) for the image in the ECS Task Definition. See Step 6.4.2.
    2. If you had pinned the version of the image, you would need to update the task definition to pull the latest version and then stop all your tasks.
 5. Open the [EC2 console](https://console.aws.amazon.com/ec2/) and from the navigation bar, select the region to use.
 6. Choose _Security Groups_ under _Resources_ and choose the security group that you created for the AWS Fargate cluster previously.
 7. For _Inbound rules_,
    1. Choose _Add rule_ and select _Custom TCP_ for _Type._
    2. Use _8082_ for _Port range._ This is needed for the migration service to work.
    3. Select _Anywhere_ for _Source type._
 8. Now we are ready to migrate the service.
 9. Clone [this file](https://github.com/Netflix/metaflow-service/blob/master/migration_tools.py) in your local workstation and name it _migration_tools.py._
 10. Using the IP in Step 2. run `python3 migration_tools.py db-status --base-url http://xxx.xxx.xxx.xxx:8082`
 11. If you need to upgrade your database schema, the flag `is_up_to_date` will be set to `False` in the response. If you need to upgrade the database schema, before proceeding to the next step, make sure you have [taken a backup of your database](https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_CommonTasks.BackupRestore.html).
 12. To upgrade your database schema, execute `python3 migration_tools.py upgrade --base-url http://xxx.xxx.xxx.xxx:8082`
 13. You should see a response `upgrade successful`. If you see an error, you might need to [restore your RDS instance using the backup](https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_RestoreFromSnapshot.html) that you had generated before if your service fails to behave correctly. Please [get in touch](/docs/get-in-touch) with us so that we can figure out what went wrong.
 14. You can verify that the upgrade succeeded by executing `python3 migration_tools.py db-status --base-url http://xxx.xxx.xxx.xxx:8082`
 15. This time the flag `is_up_to_date` should be set to `True`
 16. Next we will upgrade the metaflow service. Open the [Amazon ECS console](https://console.aws.amazon.com/ecs) and navigate to your AWS Fargate cluster in _Clusters_ tab.
 17. Under the _Tasks_ tab, choose _Stop All._ This will stop all your tasks causing your service to reboot.
 18. Once the tasks have rebooted and entered the _RUNNING_ state, choose any task and select the public IP. Curl this public IP on port 8080 with the _version_ endpoint. curl xxx.xxx.xxx.xxx:8080/version. The response will be the version of your metaflow service and it should be &gt;= 2.0.2. Congratulations! You have successfully upgraded the service! This IP \(http://xxx.xxx.xxx.xxx:8080\) is your new METAFLOW_SERVICE_URL when you reconfigure the service after setting up IAM roles for AWS Step Functions in the next section if you are not using a load balancer or an API gateway \(in which case the original address of the load balancer/API gateway is still your METAFLOW_SERVICE_URL\). If you originally had more than 1 task \(Step 2.\), follow the next step to resize your cluster back up, otherwise, you can move to the next section.
 19. Resize your cluster back to the original number of tasks by choosing your service under _Services_ and choosing _Update._
 20. Choose _Number of task_ to a number you want, choose _Next step, Next step, Next step, Update service._ If you are running multiple tasks, most likely you have an API Gateway/Load Balancer in front of these tasks in which case the original address of the load balancer/API gateway is your METAFLOW_SERVICE_URL.",H4,https://outerbounds.com/engineering/operations/migration/#upgrading-metaflow-service-manually,False,5068.0,746.0
602,Setting up IAM roles for scheduling Metaflow flows on AWS Step Functions,"1. Follow the steps listed [here](#scheduling) for setting up an IAM role for AWS Step Functions, Amazon EventBridge and an Amazon DynamoDB table.
 2. Modify the IAM role of AWS Batch so that your AWS Batch instance can access Amazin DynamoDB table. 
 3. That's it! You can now configure your Metaflow installation with these additional resources.",H4,https://outerbounds.com/engineering/operations/migration/#setting-up-iam-roles-for-scheduling-metaflow-flows-on-aws-step-functions,False,347.0,57.0
603,Publishing events,"To [trigger workflows based on external events](https://docs.metaflow.org/production/event-triggering/external-events) (in contrast to
 using internal events to [connect workflows via `@trigger_on_finish`](https://docs.metaflow.org/production/event-triggering/flow-events)), you
 need to call [`ArgoEvent.publish`](https://docs.metaflow.org/api/argoevent) in systems outside Metaflow.
 
 To make publishing work, two preconditions must be met:
 
  1. Metaflow configuration needs to be available in the environment that uses `ArgoEvent`.
 
  2.  `ArgoEvent` tries to call the webhook deployed in the Kubernetes cluster based on the address
 set in a config variable `ARGO_EVENTS_WEBHOOK_URL` (you can override it with the `url` argument in `ArgoEvent`). Hence
 this URL must be accessible from the calling environment.
 
   One approach is to use [our port forwarding script](https://github.com/outerbounds/metaflow-tools/blob/master/scripts/forward_metaflow_ports.py) which piggybacks on the `kubectl` authentication to create a secure connection to the cluster. If you don't want to use port forwarding,
   you must ensure that the URL is accessible to the caller otherwise.",H2,https://outerbounds.com/engineering/operations/event-triggering/#publishing-events,False,1176.0,145.0
604,Named Profiles,"The `configure` command supports named profiles. You can create multiple configurations, each of them pointing to a different set of AWS resources by using the `--profile` flag. For example, the following command creates a named profile `my-profile` :
 
 ```text
 metaflow configure aws --profile my-profile
 ```
 
 You can enable this profile by simply exporting `METAFLOW_PROFILE=my-profile` to your environment.",H2,https://outerbounds.com/engineering/operations/configure-metaflow/#named-profiles,False,414.0,59.0
605,How to deploy Airflow for Metaflow?,"You can use your existing Airflow server, assuming it is configured to work with Kubernetes. To deploy a new Airflow server from scratch, follow [our cloud-specific
 deployment guides](/engineering/welcome/).
 
 You can also use the integration with many managed Airflow providers, such as [Amazon MWAA](https://aws.amazon.com/managed-workflows-for-apache-airflow/) which has been
 confirmed to work with this integration.",H2,https://outerbounds.com/engineering/operations/airflow/#how-to-deploy-airflow-for-metaflow,False,422.0,53.0
606,How does the integration work?,"Metaflow flow can be compiled into a native Airflow DAG simply by executing
 ```
 python myflow.py airflow create myflow_airflow_dag.py
 ```
 
 Behind the scenes, this command packages the user code in the current working directory and stores them in the chosen datastore in S3, Azure Blob store, or Google Cloud Storage. It then compiles the code into an Airflow DAG, stored in the specified output file. The generated DAG reflects the currently active Metaflow configuration,
 which you can change with [the `METAFLOW_PROFILE` environment variable](../configure-metaflow).
 
 To schedule the DAG on Airflow, you need to move the generated file, like `myflow_airflow_dag.py` above, to the Airflow scheduler's `dags` folder. Once placed in the folder, the scheduler will run the workflow as any other native Airflow workflow.
 
 Airflow DAGs created by Metaflow run tasks as Kubernetes pods using Airflow's `KubernetesPodOperator`.",H2,https://outerbounds.com/engineering/operations/airflow/#how-does-the-integration-work,False,931.0,138.0
607,Command-line options,"Besides the usual Metaflow-specific options, the `airflow create` command includes some Airflow-specific options:",H2,https://outerbounds.com/engineering/operations/airflow/#command-line-options,False,113.0,13.0
608,Configuring Metaflow for Airflow,"As all Metaflow tasks orchestrated by Airflow run on Kubernetes using `KubernetesPodOperator`, Metaflow needs to be aware of Airflow's Kubernetes setup at the time
 when `airflow create` is executed.
 
 There are three alternative ways to define the Airflow's Kubernetes configuration in Metaflow's configuration file or as an environment variable.
 
 1. `METAFLOW_AIRFLOW_KUBERNETES_KUBECONFIG_CONTEXT` - sets the K8s context specified in `~/.kube/config` to use.
 
 2. `METAFLOW_AIRFLOW_KUBERNETES_KUBECONFIG_FILE` - sets the path of the Kube config on the Airflow installation. Airflow tasks will use this configuration file.
 
 3. `METAFLOW_AIRFLOW_KUBERNETES_CONN_ID` - sets Metaflow to use an existing [Kubernetes cluster connection in Airflow](https://airflow.apache.org/docs/apache-airflow-providers-cncf-kubernetes/stable/connections/kubernetes.html#). You need to configure the connection in the Airflow UI or the configuration file.
 
 For instance, you can deploy a flow as follows:
 
 ```bash
 export METAFLOW_AIRFLOW_KUBERNETES_KUBECONFIG_CONTEXT=arn:aws:eks:us-west-2:00612345:cluster/mycluster-j345e 
 python foreach.py airflow create foreachdag.py
 ```",H2,https://outerbounds.com/engineering/operations/airflow/#configuring-metaflow-for-airflow,False,1169.0,135.0
609,FAQ,,H2,https://outerbounds.com/engineering/operations/airflow/#faq,False,0.0,1.0
610,What is the minimum version of Airflow supported?,"At least Airflow 2.2.0 is required to support basic Metaflow decorators and flows with nested branches. Metaflow doesn’t support foreaches with Airflow version 2.2.0.
 
 Airflow introduced [dynamic task mapping](https://airflow.apache.org/docs/apache-airflow/2.3.0/concepts/dynamic-task-mapping.html) in version 2.3.0, which is
 required if you want to use Metaflow's foreaches with Metaflow. Nested foreaches are not supported by Airflow.
 
 In addition, foreach support will require the Airflow installation to have `apache-airflow-providers-cncf-kubernetes` provider installed at a minimum of version 4.2.0.",H3,https://outerbounds.com/engineering/operations/airflow/#what-is-the-minimum-version-of-airflow-supported,False,610.0,73.0
611,What compute layers are supported?,At this time only `@kubernetes` is supported - not `@batch`.,H3,https://outerbounds.com/engineering/operations/airflow/#what-compute-layers-are-supported,False,60.0,10.0
612,What Airflow executors are supported?,"Metaflow compiled DAGs can run on any Airflow Executors such as `LocalExecutor`, `KubernetesExecutor`, or `CeleryKubernetesExecutor`.",H3,https://outerbounds.com/engineering/operations/airflow/#what-airflow-executors-are-supported,False,133.0,15.0
613,Can I create multiple DAGs of the same flow?,"When Metaflow creates an Airflow DAG file, it sets the name of the flow as [`dag_id`](https://airflow.apache.org/docs/apache-airflow/stable/_api/airflow/models/dag/index.html#airflow.models.dag.DAG). The name of the flow corresponds to the class name of the class inheriting the `FlowSpec` class. When these files are moved into the Airflow scheduler ensure that only one file for that `dag_id` exists. 
 
 Use [the @project decorator](https://docs.metaflow.org/production/coordinating-larger-metaflow-projects) to deploy multiple variants of the same Flow. Setting the `@project` decorator will ensure that the `dag_id` is set based on the project name and branch that you specify.
 
 :::caution
 
 Ensure that only one dag file exists per `dag_id`. Having multiple DAGs deployed with the same `dag_id` can lead to anomalous behavior during task execution. 
 
 :::",H3,https://outerbounds.com/engineering/operations/airflow/#can-i-create-multiple-dags-of-the-same-flow,False,865.0,117.0
614,Can I use KubernetesExecutor?,"Our Terraform templates deploy Airflow with a `LocalExecutor`. [The deployment scheme recommended by the `KubernetesExecutor`](https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/executor/kubernetes.html) indicates that each worker pod requires access to DAG files. Our Terraform templates provided for AWS, GCP, and Azure do not set this up.
 
 If you want to use `KubernetesExecutor`, you can set up your own deployment scheme and ways to manage DAG files as [described here](https://airflow.apache.org/docs/helm-chart/1.2.0/manage-dags-files.html).",H3,https://outerbounds.com/engineering/operations/airflow/#can-i-use-kubernetesexecutor,False,567.0,63.0
615,Can I use Metaflow with Airflow on Minikube?,Yes. [Follow these instructions to run Airflow on Minikube](https://github.com/outerbounds/airflow-on-minikube).,H3,https://outerbounds.com/engineering/operations/airflow/#can-i-use-metaflow-with-airflow-on-minikube,False,112.0,9.0
616,How to configure K8s permissions for Metaflow-Airflow?,There are two different sets of permissions included:,H3,https://outerbounds.com/engineering/operations/airflow/#how-to-configure-k8s-permissions-for-metaflow-airflow,False,53.0,8.0
617,Kubernetes Related Permissions,"Airflow components require certain permissions on Kubernetes to launch jobs. If you are deploying Airflow on Kubernetes, ensure that the Airflow worker or scheduler’s service account has the right [role bindings](https://kubernetes.io/docs/reference/access-authn-authz/rbac/) to deploy pods in the required Kubernetes namespace. You can find [the permissions for the role over here](https://github.com/apache/airflow/blob/06acf40a4337759797f666d5bb27a5a393b74fed/chart/templates/rbac/pod-launcher-role.yaml#L41). 
 
 If Airflow is deployed using the Helm chart provided by Apache, it will automatically ensure that the scheduler and the workers can launch pods on Kubernetes.",H4,https://outerbounds.com/engineering/operations/airflow/#kubernetes-related-permissions,False,675.0,77.0
618,Cloud Storage Access Permissions,"Metaflow-related pods on Kubernetes require some permissions to access the cloud datastore (S3, Azure Blob Store, or Google Cloud Storage).
 On AWS, you can set the `METAFLOW_KUBERNETES_SERVICE_ACOUNT` configuration variable or `@kubernetes(service_account='myserviceaccount')`
 to ensure that the pods get the correct permissions automatically.",H4,https://outerbounds.com/engineering/operations/airflow/#cloud-storage-access-permissions,False,345.0,41.0
619,Steps for AWS CloudFormation Deployment,"1. Navigate to _Services_ and select _CloudFormation_ under the _Management and Governance_ heading (or search for it in the search bar) in your AWS console.
 2. Click _Create stack_ and select _With new resources (standard)_.
 3. Download the template from [this location](https://github.com/outerbounds/metaflow-tools/blob/master/aws/cloudformation/metaflow-cfn-template.yml) and save it locally.
 4. Ensure _Template is ready_ remains selected, choose _Upload a template file_, and click _Choose file_ and upload the file saved in previous step.
 5. Name your stack, select your parameters, and click _Next_, noting that if you enable _APIBasicAuth_ and/or _CustomRole_, further configuration will be required after deployment.&#x20;
 6. If desired, feel free to tag your stack in whatever way best fits your organization. When finished, click _Next_.
 7. Ensure you select the check box next to _I acknowledge that AWS CloudFormation might create IAM resources._ and click _Create stack_.
 8. Wait roughly 10-15 minutes for deployment to complete. The Stack status will eventually change to _CREATE_COMPLETE_.
 
 
 ![](/assets/screencapture-us-west-2-console-aws-amazon-cloudformation-home-2020-07-27-14_22_27.png)
     
 Once complete, you'll find an _Outputs_ tab that contains values for the components generated by this CloudFormation template. Those values correlate to respective environment variables (listed next to the outputs) you'll set to enable cloud features within Metaflow.
 
 ![](/assets/screencapture-us-west-2-console-aws-amazon-cloudformation-home-2020-07-27-14_28_17.png)",H2,https://outerbounds.com/engineering/deployment/aws-managed/cloudformation/#steps-for-aws-cloudformation-deployment,False,1596.0,202.0
620,Additional Configuration,Did you choose to enable _APIBasicAuth_ and/or _CustomRole_ and are wondering how they work? Below are some details on what happens when those features are enabled and how to make use of them.,H3,https://outerbounds.com/engineering/deployment/aws-managed/cloudformation/#additional-configuration,False,192.0,33.0
621,Option to deploy Metaflow User Interface (`EnableUI`),"Please note: This section can be ignored if `EnableUI` parameter is set to false (this is the default value).
 
 This template deploys the UI with authentication using Amazon Cognito. For Cognito to work, you'll need to provide a DNS name and SSL certificate from AWS ACM. That means you'll need a few additional steps if using the UI:
 
 1. Figure out what DNS name to use, that you have control of. You can either register a new domain name, or create a subdomain. 
 2. Generate and verify a SSL certificate valid for that name using AWS ACM. Follow [the instructions from AWS](https://docs.aws.amazon.com/acm/latest/userguide/gs-acm-request-public.html) for this.
 3. Deploy this Cloudformation template. You'll need to set `EnableUI` to ""true"", and in addition to this:
     - set `PublicDomainName` to the domain name you chose
     - set `CertificateArn` to the certificate ARN from step 2 above
 4. After Cloudformation template is deployed, make note of `LoadBalancerUIDNSName` output value. You'll need to modify  DNS settings to point your domain name to that name.
     * If you're using Route53, create an A record that is an Alias and choose the load balancer from the drop down. 
     * If using a different DNS management tool/registrar, create a CNAME record that points to `LoadBalancerUIDNSName`
 5. After DNS changes propagate, you should be able to navigate to the DNS name in your browser and see a login prompt. To create a user, go to AWS Console -> Cognito -> User Pools, find the pool that corresponds to this stack and create a new user under ""Users and Groups"".",H3,https://outerbounds.com/engineering/deployment/aws-managed/cloudformation/#option-to-deploy-metaflow-user-interface-enableui,False,1588.0,279.0
622,Architecture Diagram,![](/assets/azure-architecture-diagram.png),H2,https://outerbounds.com/engineering/deployment/azure-k8s/details/#architecture-diagram,False,43.0,1.0
623,Azure Resource List,"| Category       | Resources   | Purpose                                |
 | -------------- | ----------- | -------------------------------------- |
 | Resource Group | n/a         | This will contain all resources directly created by the Terraform template.|
 | Access Control | Azure Active Directory Application       | This represents Metaflow as an ""app"" that will access various Azure resources. More [info](https://docs.microsoft.com/en-us/azure/active-directory/develop/app-objects-and-service-principals). |
 | Access Control | Azure Active Directory Service Principal | This is an identity, linked to the AAD application above that will be used by the Metaflow application. More [info](https://docs.microsoft.com/en-us/azure/active-directory/develop/app-objects-and-service-principals). |
 | Access Control | Azure Active Directory Service Principal Password | This will be used by Metaflow to authenticate as the service principal above. |
 | Access Control | Role Assignments | Grant the service principal above sufficient access to Azure Blob Storage and AKS cluster. For specific details and conditions tied to these role assignments, please refer to the [source code](https://github.com/outerbounds/metaflow-tools/blob/f7ff07d49563dc8217f7fe49150b2d50a14d999f/azure/terraform/infra/credentials.tf#L21). |
 | Networking     | Virtual network  | Top-level private virtual network to house all Metaflow-related Azure resources. |
 | Networking     | Subnets          | There are two of these. One to house the PostgreSQL DB. One to house the AKS cluster. Both subnets live within the single virtual network above. |
 | Storage        | Azure Storage Account | Dedicated storage account for use with Metaflow |
 | Storage        | Azure Blob Storage container | Metaflow artifacts will be stored here. This resides within the storage account above. |
 | Kubernetes     | AKS cluster | There are two purposes. One is that Metaflow services run on this cluster. The other is that compute tasks from running flows will be run as pods in this cluster. |
 | Kubernetes     | AKS cluster node pool | A dedicated, autoscaling node pool for running services and tasks, distinct from AKS's Kubernetes control plane pods. |
 | Database       | Azure PostgreSQL Flexible Server | This is a PostgreSQL DB instance for indexing Metaflow run metadata. |",H2,https://outerbounds.com/engineering/deployment/azure-k8s/details/#azure-resource-list,False,2350.0,417.0
624,Required Azure Permissions for Deployment,,H2,https://outerbounds.com/engineering/deployment/azure-k8s/details/#required-azure-permissions-for-deployment,False,0.0,1.0
625,In Azure Active Directory,"Application Administrator role is required (Terraform [doc](https://registry.terraform.io/providers/hashicorp/azuread/latest/docs/resources/application)). We will be creating an Active Directory Application and a related Service Principal in the relevant Active Directory (AKA ""tenant"").
 
 ![](/assets/azure-admin-roles.png)",H3,https://outerbounds.com/engineering/deployment/azure-k8s/details/#in-azure-active-directory,False,325.0,29.0
626,In Azure IAM,"Here is a custom role definition (JSON) containing all required permissions to manage the full lifecycle of a Metaflow-on-Azure stack using the Terraform templates. Note: ""delete"" type permissions are only needed for tearing down the stack (""terraform destroy"").
 
 ```
 {
   ""id"": ""<REDACTED>"",
   ""properties"": {
     ""assignableScopes"": [
       ""/subscriptions/<YOUR_SUBSCRIPTION_ID>""
     ],
     ""description"": """",
     ""permissions"": [
       {
         ""actions"": [
           ""Microsoft.Resources/subscriptions/resourceGroups/read"",
           ""Microsoft.Resources/subscriptions/resourceGroups/write"",
           ""Microsoft.Network/privateDnsZones/read"",
           ""Microsoft.Network/privateDnsZones/write"",
           ""Microsoft.Network/privateDnsZones/SOA/read"",
           ""Microsoft.Storage/storageAccounts/read"",
           ""Microsoft.Network/virtualNetworks/read"",
           ""Microsoft.Network/virtualNetworks/write"",
           ""Microsoft.Storage/storageAccounts/write"",
           ""Microsoft.Network/virtualNetworks/subnets/read"",
           ""Microsoft.Network/virtualNetworks/subnets/write"",
           ""Microsoft.Storage/storageAccounts/listkeys/action"",
           ""Microsoft.Storage/storageAccounts/blobServices/read"",
           ""Microsoft.Storage/storageAccounts/blobServices/write"",
           ""Microsoft.Storage/storageAccounts/fileServices/read"",
           ""Microsoft.ContainerService/managedClusters/read"",
           ""Microsoft.ContainerService/managedClusters/write"",
           ""Microsoft.Network/virtualNetworks/subnets/join/action"",
           ""Microsoft.ContainerService/managedClusters/accessProfiles/listCredential/action"",
           ""Microsoft.Network/privateDnsZones/virtualNetworkLinks/read"",
           ""Microsoft.Authorization/roleAssignments/read"",
           ""Microsoft.ContainerService/managedClusters/agentPools/read"",
           ""Microsoft.ContainerService/managedClusters/agentPools/write"",
           ""Microsoft.Network/privateDnsZones/virtualNetworkLinks/write"",
           ""Microsoft.Authorization/roleAssignments/write"",
           ""Microsoft.Network/virtualNetworks/join/action"",
           ""Microsoft.DBforPostgreSQL/flexibleServers/read"",
           ""Microsoft.DBforPostgreSQL/flexibleServers/write"",
           ""Microsoft.DBforPostgreSQL/flexibleServers/databases/read"",
           ""Microsoft.DBforPostgreSQL/flexibleServers/databases/write"",
           ""Microsoft.DBforPostgreSQL/flexibleServers/configurations/read"",
           ""Microsoft.DBforPostgreSQL/flexibleServers/configurations/write"",
           ""Microsoft.ContainerService/managedClusters/listClusterUserCredential/action"",
           ""Microsoft.Authorization/roleAssignments/delete"",
           ""Microsoft.DBforPostgreSQL/flexibleServers/databases/delete"",
           ""Microsoft.ContainerService/managedClusters/agentPools/delete"",
           ""Microsoft.Storage/storageAccounts/delete"",
           ""Microsoft.ContainerService/managedClusters/delete"",
           ""Microsoft.DBforPostgreSQL/flexibleServers/delete"",
           ""Microsoft.Network/virtualNetworks/subnets/delete"",
           ""Microsoft.Network/privateDnsZones/virtualNetworkLinks/delete"",
           ""Microsoft.Network/virtualNetworks/delete"",
           ""Microsoft.Network/privateDnsZones/delete"",
           ""Microsoft.Resources/subscriptions/resourceGroups/delete""
         ],
         ""dataActions"": [],
         ""notActions"": [],
         ""notDataActions"": []
       }
     ],
     ""roleName"": ""Metaflow admin""
   }
 }
 ```
 
 You can create a custom role as follows. From [Azure Portal](https://portal.azure.com), Go to Subscriptions => select the right subscription ⇒ Access Control (IAM), then choose the ""Create custom role"" panel on RHS. Paste in the role definition JSON.
 
 ![](/assets/azure-iam.png)
 
 ![](/assets/azure-custom-role.png)",H3,https://outerbounds.com/engineering/deployment/azure-k8s/details/#in-azure-iam,False,3834.0,686.0
627,Required Azure Permissions for Running Flows,,H2,https://outerbounds.com/engineering/deployment/azure-k8s/details/#required-azure-permissions-for-running-flows,False,0.0,1.0
628,Storage Access,"In the Azure portal, navigate to the relevant storage account/storage container. E.g. from this Terraform output:
 
 ```
 METAFLOW_AZURE_STORAGE_BLOB_SERVICE_ENDPOINT=https://stobmetaflowminion.blob.core.windows.net/
 METAFLOW_DATASTORE_SYSROOT_AZURE=metaflow-storage-container/tf-full-stack-sysroot
 ```
 
 `stobmetaflowminion` is the storage account, `metaflow-storage-container` is the storage container.
 
 From the container page, go to ""Access Control (IAM)"" to assign the role *Storage Blob Data Contributor*). Note this role assignment can take several minutes to propagate in our experience.",H3,https://outerbounds.com/engineering/deployment/azure-k8s/details/#storage-access,False,600.0,63.0
629,AKS Cluster Access,"In Azure portal, navigate to the relevant AKS cluster. E.g. from this Terraform output:
 
 ```
 az aks get-credentials --resource-group rg-metaflow-minion-westus --name aks-ob-metaflow-minion
 ```
 
 `aks-ob-metaflow-minion` is the AKS cluster name.
 
 Navigate to the management for this cluster, and go to ""Access Control (IAM)"" to assign the roles:
 * *Azure Kubernetes Service Contributor Role*
 * *Azure Kubernetes Service Cluster User Role*",H3,https://outerbounds.com/engineering/deployment/azure-k8s/details/#aks-cluster-access,False,446.0,62.0
630,Azure Services List,We deploy these services in the AKS cluster:,H2,https://outerbounds.com/engineering/deployment/azure-k8s/details/#azure-services-list,False,44.0,8.0
631,Metaflow,"* Metadata service - this supports read/write of metadata. Supports features such as:
     * When a flow is running, it POST's metadata here.
     * The Metaflow Client library calls this service to read metadata.
 * The UI static service serves the web UI frontend bundle.
 * The UI backend supports UI's data needs.",H3,https://outerbounds.com/engineering/deployment/azure-k8s/details/#metaflow,False,317.0,61.0
632,Argo Workflows,"The [quickstart](https://argoproj.github.io/argo-workflows/quick-start/) k8s manifest published by Argo Workflows spins up the following services:
 
 ```
 kubectl get services -n argo
 NAME                          TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)             AGE
 argo-server                   ClusterIP   10.0.26.126    <none>        2746/TCP            32m
 httpbin                       ClusterIP   10.0.66.229    <none>        9100/TCP            32m
 minio                         ClusterIP   10.0.173.242   <none>        9000/TCP,9001/TCP   32m
 postgres                      ClusterIP   10.0.51.199    <none>        5432/TCP            32m
 workflow-controller-metrics   ClusterIP   10.0.139.237   <none>        9090/TCP            32m
 ```",H3,https://outerbounds.com/engineering/deployment/azure-k8s/details/#argo-workflows,False,764.0,298.0
633,What's with all the port forwards?,See [here](/engineering/deployment/azure-k8s/advanced/#authenticated-public-endpoints-for-metaflow-services). We publish a short utility script to open up the port forwards and keep them open by preventing inactivity timeouts [here](https://github.com/outerbounds/metaflow-tools/blob/master/scripts/forward_metaflow_ports.py).,H3,https://outerbounds.com/engineering/deployment/azure-k8s/faq/#what-s-with-all-the-port-forwards,False,326.0,23.0
634,"I want to use an existing storage account/container, an existing AKS, or DB. What to do?","The terraform templates provide an all-in experience, spinning up all fresh resources. The templates need to be adapted to accommodate existing resources. E.g. to bring your own storage account and storage container, here are some possible approaches:
 
 *Full manual reference replacement*
 * Look for all references to the terraform resources:
     * `metaflow_storage_account`
     * `metaflow_storage_container`
 * Replace references with known attributes of the account and container you are bringing. E.g. storage_account_name, or container_name, etc.
 * Remove terraform resources for `metaflow_storage_account` and `metaflow_storage_container` from templates.
 
 *Terraform ""data source"" approach*
 * Replace the resources `metaflow_storage_account` and `metaflow_storage_container` with ""[data sources](https://www.terraform.io/language/data-sources)"". I.e. [storage_container](https://registry.terraform.io/providers/hashicorp/azurerm/latest/docs/data-sources/storage_container), [storage_account](https://registry.terraform.io/providers/hashicorp/azurerm/latest/docs/data-sources/storage_account).
 * Update references to these resources, so that the callers refer to the ""data source"" object instead. I.e. `data.storage_account.<name>`.",H3,https://outerbounds.com/engineering/deployment/azure-k8s/faq/#i-want-to-use-an-existing-storage-account-container-an-existing-aks-or-db-what-to-do,False,1248.0,129.0
635,How do I use GPU nodes in my AKS cluster?,Our quickstart terraform templates will not support that out of the box. They may be extended in the future. The approach would likely implement [this](https://docs.microsoft.com/en-us/azure/aks/gpu-cluster) in an automated manner within the templates.,H3,https://outerbounds.com/engineering/deployment/azure-k8s/faq/#how-do-i-use-gpu-nodes-in-my-aks-cluster,False,252.0,32.0
636,How do I change the VM instance types for the AKS control plane as well as for Metaflow task runner nodes?,"Change these lines ([control plane](https://github.com/outerbounds/metaflow-tools/blob/3048eb1e4ac1b0d32c4ad9e73249385b813ef772/azure/terraform/infra/kubernetes.tf#L9), [tasks](https://github.com/outerbounds/metaflow-tools/blob/3048eb1e4ac1b0d32c4ad9e73249385b813ef772/azure/terraform/infra/kubernetes.tf#L40)) and reapply the Terraform template (""`terraform apply -target=module.infra`"")",H3,https://outerbounds.com/engineering/deployment/azure-k8s/faq/#how-do-i-change-the-vm-instance-types-for-the-aks-control-plane-as-well-as-for-metaflow-task-runner-nodes,False,388.0,14.0
637,I want finer grain auth on running flows. What to do?,See [here](/engineering/deployment/azure-k8s/advanced/#aks-workload-identities).,H3,https://outerbounds.com/engineering/deployment/azure-k8s/faq/#i-want-finer-grain-auth-on-running-flows-what-to-do,False,80.0,2.0
638,Why is my Metaflow flow stuck on k8s pending state forever?,"When Metaflow submits tasks to Kubernetes for execution, there are two scenarios:
 * There are sufficient spare resources to immediately run the task.
 * There are NOT sufficient spare resources right now. AKS autoscaler provisions additional compute nodes to satisfy the requirements of the new task. Once provisioning is complete, the task is run.
 
 The AKS autoscaler is configured with a specific instance type (defaults to Standard_D8_v5) to provision if needed. If a task's resource requirements exceed the VM size, upscaling can never satisfy the new task and the task will be stuck in pending forever.
 
 When this is suspected, double-check your resource requirements vs the VM instance type used in the *taskworker* node pool.",H3,https://outerbounds.com/engineering/deployment/azure-k8s/faq/#why-is-my-metaflow-flow-stuck-on-k8s-pending-state-forever,False,737.0,117.0
639,Can I do the deployment quickstart on Windows?,"Yes, all the CLI tools involved work on Windows natively.",H3,https://outerbounds.com/engineering/deployment/azure-k8s/faq/#can-i-do-the-deployment-quickstart-on-windows,False,57.0,10.0
640,Need Help?,The quickest way to get help is our public Slack channel [#ask-metaflow](http://slack.outerbounds.co). We look forward to your questions and feedback.,H2,https://outerbounds.com/engineering/deployment/azure-k8s/faq/#need-help,False,150.0,20.0
641,1. Preparation,,H2,https://outerbounds.com/engineering/deployment/azure-k8s/deployment/#1-preparation,False,0.0,1.0
642,Terraform Tooling,"[Terraform](https://www.terraform.io/) is a popular infrastructure-as-code tool for managing cloud resources. We have published a set of Terraform templates [here](https://github.com/outerbounds/metaflow-tools/tree/master/azure/terraform) for setting up Metaflow on Microsoft Azure. Terraform needs to be installed on your system in order to use these templates.
 
 1. Install Terraform by following [these instructions](https://learn.hashicorp.com/tutorials/terraform/install-cli).
 2. Download [Metaflow on Azure terraform templates](https://github.com/outerbounds/metaflow-tools/tree/master/azure/terraform):
 
 ```
 git clone git@github.com:outerbounds/metaflow-tools.git
 ```",H3,https://outerbounds.com/engineering/deployment/azure-k8s/deployment/#terraform-tooling,False,680.0,61.0
643,Azure Command Line Interface,"This is the [official CLI tool (""az"")](https://docs.microsoft.com/en-us/cli/azure/) published by Microsoft for working with Azure. It will be used by Terraform when applying our templates (e.g. for authentication with Azure). Please install it by following [these instructions](https://docs.microsoft.com/en-us/cli/azure/install-azure-cli).",H3,https://outerbounds.com/engineering/deployment/azure-k8s/deployment/#azure-command-line-interface,False,340.0,36.0
644,kubectl Command Line Interface,[kubectl](https://kubernetes.io/docs/reference/kubectl/) is a standard CLI tool for working with [Kubernetes](https://kubernetes.io/) clusters. It will be used by Terraform when applying our templates (e.g. for deploying some services to your [Azure Kubernetes Service](https://azure.microsoft.com/en-us/services/kubernetes-service/) cluster). Please install it by following [these instructions](https://kubernetes.io/docs/tasks/tools/#kubectl).,H3,https://outerbounds.com/engineering/deployment/azure-k8s/deployment/#kubectl-command-line-interface,False,445.0,39.0
645,2. Provision Azure Resources,"See here for the exact set of resources to be provisioned. Also, note the [permissions](/engineering/deployment/azure-k8s/details/#required-azure-permissions-for-running-flows) that are needed.",H2,https://outerbounds.com/engineering/deployment/azure-k8s/deployment/#2-provision-azure-resources,False,193.0,18.0
646,Login to Azure,"You must be logged onto Azure as an account with [sufficient permissions](/engineering/deployment/azure-k8s/details/#required-azure-permissions-for-running-flows) to provision the required resources. Use the Azure CLI (`az`):
 
 ```
 az login
 ```",H3,https://outerbounds.com/engineering/deployment/azure-k8s/deployment/#login-to-azure,False,247.0,27.0
647,Initialize your Terraform Workspace,"From your `metaflow-tools/azure/terraform` directory, run:
 
 ```
 terraform init
 ```",H3,https://outerbounds.com/engineering/deployment/azure-k8s/deployment/#initialize-your-terraform-workspace,False,86.0,10.0
648,Set org_prefix,"Create a TF vars file `FILE.tfvars` (`FILE` could be something else), with this content.
 
     org_prefix = ""yourorg""  # use something short and distinctive
 
 Some Azure resources must have globally unique names:",H3,https://outerbounds.com/engineering/deployment/azure-k8s/deployment/#set-org-prefix,False,214.0,38.0
649,Optional: Enable Argo Events,"To enable [event triggering](https://docs.metaflow.org/production/event-triggering) for Metaflow, add the following line in `FILE.tfvars`:
 
 ```
 enable_argo=true
 ```
 
 For more technical context, [see this page about event triggering](/engineering/operations/event-triggering/).",H4,https://outerbounds.com/engineering/deployment/azure-k8s/deployment/#optional-enable-argo-events,False,282.0,27.0
650,Optional: Enable Airflow,"Optionally, you can include [Apache Airflow as the production orchestrator for Metaflow](https://docs.metaflow.org/production/scheduling-metaflow-flows/scheduling-with-airflow) in your deployment by including the following lines in `FILE.tfvars`:
 
 ```
 deploy_airflow=true
 ```
 
 Setting `deploy_airflow=true` will create a storage blob-container named `airflow-logs`, provide blob-container read and write permissions to the service principal and deploy Airflow in the AKS cluster with a `LocalExecutor`. The Airflow installation will store the logs in the `airflow-logs` blob container.",H4,https://outerbounds.com/engineering/deployment/azure-k8s/deployment/#optional-enable-airflow,False,591.0,68.0
651,Apply Terraform Template to Provision Azure Infrastructure,"From your local `metaflow-tools/azure/terraform directory`, run:
 
 ```
 terraform apply -target=""module.infra"" -var-file=""FILE.tfvars""
 ```
 
 A plan of action will be printed to the terminal. You should review it before accepting. See [details](/engineering/deployment/azure-k8s/details/) for what to expect.",H3,https://outerbounds.com/engineering/deployment/azure-k8s/deployment/#apply-terraform-template-to-provision-azure-infrastructure,False,310.0,36.0
652,Common Resource Provisioning Hiccups,,H3,https://outerbounds.com/engineering/deployment/azure-k8s/deployment/#common-resource-provisioning-hiccups,False,0.0,1.0
653,PostgreSQL Provisioning API Errors (on Azure Side),"If you do not create Azure PostgreSQL Flexible Server instances often, Azure API may be flaky for you initially:
 
 ```
 | Error: waiting for creation of the Postgresql Flexible Server ""metaflow-database-server-xyz"" (Resource Group ""rg-db-metaflow-xyz""):
 | Code=""InternalServerError"" Message=""An unexpected error occured while processing the request. Tracking ID: 'xyz'""
 |
 |   with module.infra.azurerm_postgresql_flexible_server.metaflow_database_server,
 |   on infra/database.tf line 20, in resource ""azurerm_postgresql_flexible_server"" ""metaflow_database_server"":
 |   20: resource ""azurerm_postgresql_flexible_server"" ""metaflow_database_server"" {
 ```
 
 In our experience, waiting 20 mins and trying again resolves this issue. This appears to be a one-time phenomenon - future stack spin-ups do not encounter such `InternalServerError`s.",H4,https://outerbounds.com/engineering/deployment/azure-k8s/deployment/#postgresql-provisioning-api-errors-on-azure-side,False,846.0,103.0
654,Node Pool Provisioning,"We have hard-coded some default instance types to be used for Kubernetes nodes as well as worker pools (taskworkers). Depending on the real-time availability of such instances in your region or availability zone, you may [consider choosing alternate instance types](/engineering/deployment/azure-k8s/faq/#how-do-i-change-the-vm-instance-types-for-the-aks-control-plane-as-well-as-for-metaflow-task-runner-nodes).
 
 VM Availability issues might look something like this:
 
 ```
 | Error: waiting for creation of Node Pool: (Agent Pool Name ""taskworkers"" / Managed Cluster Name ""metaflow-kubernetes-xyz"" /
 | Resource Group ""rg-k8s-metaflow-xyz""): Code=""ReconcileVMSSAgentPoolFailed"" Message=""Code=\""AllocationFailed\"" Message=\""Allocation failed.
 | We do not have sufficient capacity for the requested VM size in this region. Read more about improving likelihood of allocation success
 | at http://aka.ms/allocation-guidance\""""
 
 ```
 
 VM quotas may also cause provisioning to fail - we recommend working with your Azure admin to raise quotas, and/or pick other instance types:
 
 ```
 | Error: creating Node Pool: (Agent Pool Name ""taskworkers"" / Managed Cluster Name ""metaflow-kubernetes-default"" / Resource Group ""rg-k8s-metaflow-default""):
 | containerservice.AgentPoolsClient#CreateOrUpdate: Failure sending request: StatusCode=400 -- Original Error: Code=""PreconditionFailed""
 | Message=""Provisioning of resource(s) for Agent Pool taskworkers failed. Error: {\n  \""code\"": \""InvalidTemplateDeployment\"",\n
 | \""message\"": \""The template deployment '8b1a99f1-e35e-44be-a8ac-0f82009b7149' is not valid according to the validation procedure.
 | The tracking id is 'xyz'. See inner errors for details.\"",\n  \""details\"":
 | [\n   {\n    \""code\"": \""QuotaExceeded\"",\n    \""message\"": \""Operation could not be completed as it results in exceeding approved standardDv5Family Cores quota.
 | Additional details - Deployment Model: Resource Manager, Location: westeurope, Current Limit: 0, Current Usage: 0,
 | Additional Required: 4, (Minimum) New Limit Required: 4.
 | Submit a request for Quota increase at https://<AZURE_LINK> by specifying parameters listed in the ‘Details’ section for deployment to succeed.
 | Please read more about quota limits at https://docs.microsoft.com/en-us/azure/azure-supportability/per-vm-quota-requests\""\n   }\n  ]\n }""
 ```",H4,https://outerbounds.com/engineering/deployment/azure-k8s/deployment/#node-pool-provisioning,False,2362.0,291.0
655,3. Deploy Metaflow Services to AKS Cluster,,H2,https://outerbounds.com/engineering/deployment/azure-k8s/deployment/#3-deploy-metaflow-services-to-aks-cluster,False,0.0,1.0
656,Apply Terraform Template to Deploy Services,"From your local `metaflow-tools/azure/terraform` directory, run:
 
 ```
 terraform apply -target=""module.services"" -var-file=""FILE.tfvars""
 ```",H3,https://outerbounds.com/engineering/deployment/azure-k8s/deployment/#apply-terraform-template-to-deploy-services,False,143.0,13.0
657,4. End User Setup Instructions,"When the command above completes, it will print a set of setup instructions for Metaflow end users (folks who will be writing and running flows). These instructions are meant to get end users started on running flows quickly.
 
 You can access the Terraform instruction output at any time by running (from `metaflow-tools/azure/terraform` directory):
 
 ```
 terraform output -raw END_USER_SETUP_INSTRUCTIONS
 ```",H2,https://outerbounds.com/engineering/deployment/azure-k8s/deployment/#4-end-user-setup-instructions,False,413.0,61.0
658,Sample Output,```,H3,https://outerbounds.com/engineering/deployment/azure-k8s/deployment/#sample-output,False,3.0,1.0
659,Remote state backends for Terraform,"Terraform manages the state of Azure resources in tfstate files locally by default.
 
 If you plan to maintain the minimal stack for any significant period of time, it is highly recommended that these state files be stored in cloud storage (e.g. Azure Blob Storage) instead.
 
 Some reasons include:
 * More than one person needs to administer the stack (using Terraform). Everyone should work off a single copy of tfstate.
 * You wish to mitigate the risk of data loss on your local disk.
 
 For more details, see Terraform docs.",H2,https://outerbounds.com/engineering/deployment/azure-k8s/advanced/#remote-state-backends-for-terraform,False,530.0,92.0
660,Deploying Multiple Metaflow Stacks,"If you want to run more than one instance of this stack, you can use [Terraform workspaces](https://www.terraform.io/language/state/workspaces).",H2,https://outerbounds.com/engineering/deployment/azure-k8s/advanced/#deploying-multiple-metaflow-stacks,False,144.0,17.0
661,Authenticated Public Endpoints for Metaflow Services,"The deployment approach taken by the terraform templates minimizes publicly accessible surface area. Only the AKS Kubernetes API is available publicly. This allows authorized users (through the secure Kubernetes API) to:
 * Inspect cluster's workloads
 * CRUD Kubernetes objects (e.g. submit job pods).
 
 However, this deployment style does not include publicly accessible endpoints for the [web services](/engineering/deployment/azure-k8s/details/#azure-services-list) running within the AKS cluster. For the purpose of this sample deployment template, users must use the Kubernetes API to set up port-forwarding in order to access these services from their workstations.
 
 For a more friendly user experience, publicly accessible endpoints can be authenticated and authorized using technologies like:
 * [OIDC](https://openid.net/connect/) (OpenID Connect)
 * [JWT](https://jwt.io/) tokens
 * Identity-as-a-service providers (e.g. [Auth0](https://auth0.com/)).
 
 Please [talk to us](http://slack.outerbounds.co) for more information about this topic.",H2,https://outerbounds.com/engineering/deployment/azure-k8s/advanced/#authenticated-public-endpoints-for-metaflow-services,False,1055.0,131.0
662,AKS Workload Identities,"In the Metaflow stack generated by these terraform templates, all Metaflow workloads running within AKS access Azure resources as a specific Service Principal identity. For finer grain control, it is possible to map separate identities to each workload and Metaflow task. The strategy currently recommended by Azure is [Azure AD Workload Identity](https://azure.github.io/azure-workload-identity/docs/).",H2,https://outerbounds.com/engineering/deployment/azure-k8s/advanced/#aks-workload-identities,False,403.0,52.0
663,Architecture Diagram,![](/assets/gke-architecture-diagram.png),H2,https://outerbounds.com/engineering/deployment/gcp-k8s/details/#architecture-diagram,False,41.0,1.0
664,GCP Resource List,"| Category       | Resources   | Purpose                                |
 | -------------- | ----------- | -------------------------------------- |
 | Access Control | Service account | This is an identity that has all required permissions to run Metaflow workloads, either locally vs Google Cloud Storage, or all the way running in the GKE cluster. More [info](https://cloud.google.com/kubernetes-engine/docs/how-to/workload-identity). |
 | Access Control | Service account key | This will be used by Metaflow to authenticate as the service account above. Note: This is needed for local runs as well as for Metaflow logic prior to workload tasks to GKE. For all GCP accesses from within a GKE pod, this credential is not required.|
 | Access Control | Role Assignments | Grants the service account above sufficient access to: Google Cloud Storage, GKE, and Cloud SQL (PostgreSQL). For specific details and conditions tied to these role assignments, please refer to the [source code](https://github.com/outerbounds/metaflow-tools/blob/master/gcp/terraform/infra/google_service_account.tf). |
 | Networking     | Virtual network  | Top-level private virtual network to house all Metaflow-related GCP resources. |
 | Networking     | Subnet        | To house the PostgreSQL DB |
 | Storage        | Google Cloud Storage bucket | Metaflow artifacts will be stored here. This resides within the storage account above. |
 | Kubernetes     | GKE cluster |This has built-in compute node autoscaling. There are two purposes. First, Metaflow services run on this cluster. Second, compute tasks from running flows will be run as pods in this cluster. |
 | Database       | Cloud SQL instance | This is a PostgreSQL DB instance for indexing Metaflow run metadata. |",H2,https://outerbounds.com/engineering/deployment/gcp-k8s/details/#gcp-resource-list,False,1755.0,324.0
665,Required GCP Permissions for Deployment,"The permissions required can be described by the following custom role (`gcloud iam roles describe` output):
 
 ```
 description: <DESCRIPTION>",H2,https://outerbounds.com/engineering/deployment/gcp-k8s/details/#required-gcp-permissions-for-deployment,False,143.0,20.0
666,Required GCP Permissions for Running Flows,,H2,https://outerbounds.com/engineering/deployment/gcp-k8s/details/#required-gcp-permissions-for-running-flows,False,0.0,1.0
667,Kubernetes Engine Developer Role,"Note: as of Q3, 2022, there is no direct way to scope this to a specific GKE cluster.",H3,https://outerbounds.com/engineering/deployment/gcp-k8s/details/#kubernetes-engine-developer-role,False,85.0,18.0
668,Storage Object Admin Role,"This should be granted under this IAM condition:
 ```
 resource.name.startsWith(""projects/_/buckets/<BUCKET_NAME>"")
 ```
 
 The bucket name can be found from the end user output from Terraform run. For example,
 ```
 …
 ""METAFLOW_DATASTORE_SYSROOT_GS"": ""gs://ob-metaflow-storage-bucket-ci/tf-full-stack-sysroot"",
 …
 ```",H3,https://outerbounds.com/engineering/deployment/gcp-k8s/details/#storage-object-admin-role,False,320.0,34.0
669,GKE services list,We deploy these services in the GKE cluster:,H2,https://outerbounds.com/engineering/deployment/gcp-k8s/details/#gke-services-list,False,44.0,8.0
670,Metaflow,"* Metadata service - this supports read/write of metadata. Supports features such as:
     * When flow is running, it POST's metadata here.
     * Metaflow Client library calls this service to read metadata.
 * The UI static service serves the web UI frontend bundle.
 * The UI backend supports UI's data needs.",H3,https://outerbounds.com/engineering/deployment/gcp-k8s/details/#metaflow,False,311.0,59.0
671,Argo Workflows,"The [quickstart](https://argoproj.github.io/argo-workflows/quick-start/) Kubernetes manifest published by Argo Workflows spins up the following services:
 ```
 kubectl get services -n argo
 NAME                          TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)             AGE
 argo-server                   ClusterIP   10.0.26.126    <none>        2746/TCP            32m
 httpbin                       ClusterIP   10.0.66.229    <none>        9100/TCP            32m
 minio                         ClusterIP   10.0.173.242   <none>        9000/TCP,9001/TCP   32m
 postgres                      ClusterIP   10.0.51.199    <none>        5432/TCP            32m
 workflow-controller-metrics   ClusterIP   10.0.139.237   <none>        9090/TCP            32m
 ```",H3,https://outerbounds.com/engineering/deployment/gcp-k8s/details/#argo-workflows,False,769.0,297.0
672,What's with all the port forwards?,See [here](https://docs.google.com/document/d/19kQc0fpC7PWUiJoyb-vS41wTG_T1_p61gagGmnbM45I/edit#heading=h.o2w8nhj962u3). We publish a short utility script to open up the port forwards and keep them open by preventing inactivity timeouts [here](https://github.com/outerbounds/metaflow-tools/blob/master/scripts/forward_metaflow_ports.py).,H3,https://outerbounds.com/engineering/deployment/gcp-k8s/faq/#what-s-with-all-the-port-forwards,False,337.0,23.0
673,"I want to use an existing storage account/container, an existing GKE, or DB. What to do?","The terraform templates provide an all-in experience, spinning up all fresh resources. The templates need to be adapted to accommodate existing resources. E.g. to bring your own storage account and storage container, here are some possible approaches:
 
 *Full manual reference replacement:*
 * Look for all references to `metaflow_storage_bucket`
 * Replace references with known attributes of the account and container you are bringing. E.g. storage_account_name, or container_name, etc.
 * Remove terraform resources for `metaflow_storage_bucket` from templates.
 
 * Terraform ""data source"" approach:*
 * Replace the resources `metaflow_storage_bucket` with ""data sources"". i.e. [storage_bucket](https://registry.terraform.io/providers/hashicorp/google/latest/docs/data-sources/storage_bucket)
 * Update references to these resources, so that the callers refer to the ""data source"" object instead. I.e. `data.storage_bucket.<name>`",H3,https://outerbounds.com/engineering/deployment/gcp-k8s/faq/#i-want-to-use-an-existing-storage-account-container-an-existing-gke-or-db-what-to-do,False,935.0,111.0
674,How do I use GPU nodes in my GKE cluster?,Our quickstart terraform templates will not support that out of box. They may be extended in the future. The approach would likely implement [this](https://cloud.google.com/kubernetes-engine/docs/how-to/gpus) in an automated manner within the templates.,H3,https://outerbounds.com/engineering/deployment/gcp-k8s/faq/#how-do-i-use-gpu-nodes-in-my-gke-cluster,False,253.0,31.0
675,How do I change the VM instance types for Metaflow task runner nodes?,"The quickstart terraform templates provided use GKE's [node auto-provisioning](https://cloud.google.com/kubernetes-engine/docs/how-to/node-auto-provisioning), out of the box. Node auto provisioning should be aware of the resource requirements of incoming pods, which means it will spin up appropriately sized instances as needed.",H3,https://outerbounds.com/engineering/deployment/gcp-k8s/faq/#how-do-i-change-the-vm-instance-types-for-metaflow-task-runner-nodes,False,329.0,37.0
676,I want finer grain auth on running flows. What to do?,See [here](https://docs.google.com/document/d/19kQc0fpC7PWUiJoyb-vS41wTG_T1_p61gagGmnbM45I/edit#heading=h.4serr1lx4hxo).,H3,https://outerbounds.com/engineering/deployment/gcp-k8s/faq/#i-want-finer-grain-auth-on-running-flows-what-to-do,False,120.0,2.0
677,Why is my Metaflow flow stuck on k8s pending state forever?,"When Metaflow submits tasks to Kubernetes for execution, there are two scenarios:
 * There are sufficient spare resources to immediately run the task.
 * There are NOT sufficient spare resources right now. GKE autoscaler provisions additional compute nodes to satisfy the requirements of the new task. Once provisioning is complete, the task is run.
 
 If GKE autoscaler (running [node auto-provisioning](https://cloud.google.com/kubernetes-engine/docs/how-to/node-auto-provisioning)) can never satisfy the new task and the task will be stuck in pending forever. E.g. if the aggregate hard CPU or memory limit has been reached on the GKE cluster. To resolve the aggregate limit issue, up the limit in the [templates](https://github.com/outerbounds/metaflow-tools/blob/26c338860b9725cf5c8689036e4581781e12fbda/gcp/terraform/infra/kubernetes.tf#L29).",H3,https://outerbounds.com/engineering/deployment/gcp-k8s/faq/#why-is-my-metaflow-flow-stuck-on-k8s-pending-state-forever,False,848.0,104.0
678,Need Help?,The quickest way to get help is our public Slack channel [#ask-metaflow](http://slack.outerbounds.co). We look forward to your questions and feedback.,H3,https://outerbounds.com/engineering/deployment/gcp-k8s/faq/#need-help,False,150.0,20.0
679,Remote State Backends for Terraform,"Terraform manages the state of GCP resources in [tfstate](https://www.terraform.io/language/state) files locally by default.
 
 If you plan to maintain the minimal stack for any significant period of time, it is highly recommended that these state files be stored in cloud storage (e.g. Google Cloud Storage) instead.
 
 Some reasons include
 * More than one person needs to administer the stack (using terraform). Everyone should work off a single copy of tfstate.
 * You wish to mitigate the risk of data loss on your local disk.
 
 For more details, see [Terraform docs](https://www.terraform.io/language/settings/backends/configuration).",H2,https://outerbounds.com/engineering/deployment/gcp-k8s/advanced/#remote-state-backends-for-terraform,False,641.0,92.0
680,Deploying Multiple Metaflow Stacks,"If you want to run more than one instance of this stack, you can use [Terraform workspaces](https://www.terraform.io/language/state/workspaces).",H2,https://outerbounds.com/engineering/deployment/gcp-k8s/advanced/#deploying-multiple-metaflow-stacks,False,144.0,17.0
681,Authenticated Public Endpoints for Metaflow Services,"The deployment approach taken by the terraform templates minimizes publicly accessible surface area. Only the GKE Kubernetes API is available publicly. This allows authorized users (through the secure Kubernetes API) to
 * Inspect cluster's workloads
 * CRUD Kubernetes objects (e.g. submit job pods)
 
 However, this deployment style does not include publicly accessible endpoints for the [web services](https://docs.google.com/document/d/19kQc0fpC7PWUiJoyb-vS41wTG_T1_p61gagGmnbM45I/edit#heading=h.lziyyitnsxax) running within the GKE cluster. For the purpose of this sample deployment template, users must use the Kubernetes API to set up port-forwarding in order to access these services from their workstations.
 
 For a more friendly user experience, publicly accessible endpoints can be authenticated and authorized using technologies like:
 * [OIDC](https://openid.net/connect/) (OpenID Connect)
 * [JWT](https://jwt.io/) tokens
 * Identity-as-a-service providers (e.g. [Auth0](https://auth0.com/)).
 
 Please [talk to us](http://slack.outerbounds.co) for more information about this topic.",H2,https://outerbounds.com/engineering/deployment/gcp-k8s/advanced/#authenticated-public-endpoints-for-metaflow-services,False,1098.0,131.0
682,GKE Workload Identities,"In the Metaflow stack generated by these terraform templates, all Metaflow workloads running within GKE access GCP resources as a specific service account identity. We use [GKE Workload Identity](https://cloud.google.com/kubernetes-engine/docs/how-to/workload-identity) (in short), which does the following:
 * Metaflow tasks pods run as a certain Kubernetes Service Account (KSA)
 * KSA is annotated with a link to the Google Service Account (GSA)
 * The pod running as KSA assumes the identity of GSA when access GCP resources
 
  For finer grain control, it is possible to map separate identities to each workload and Metaflow task. It is possible to extend this further. E.g. user X's Metaflow runs may assume a certain KSA_1 (mapping to GSA_1), whilst user Y's Metaflow runs may assume a certain KSA_2 (mapping to GSA_2).",H2,https://outerbounds.com/engineering/deployment/gcp-k8s/advanced/#gke-workload-identities,False,826.0,127.0
683,1. Preparation,,H2,https://outerbounds.com/engineering/deployment/gcp-k8s/deployment/#1-preparation,False,0.0,1.0
684,Terraform Tooling,"[Terraform](https://www.terraform.io/) is a popular infrastructure-as-code tool for managing cloud resources. We have published a set of terraform templates [here](https://github.com/outerbounds/metaflow-tools/tree/master/azure/terraform) for setting up Metaflow on GCP. Terraform needs to be installed on your system in order to use these templates.
 
 1. Install terraform by following [these instructions](https://learn.hashicorp.com/tutorials/terraform/install-cli).
 2. Download [Metaflow on GCP terraform templates](https://github.com/outerbounds/metaflow-tools/tree/master/gcp/terraform): `git clone git@github.com:outerbounds/metaflow-tools.git`",H3,https://outerbounds.com/engineering/deployment/gcp-k8s/deployment/#terraform-tooling,False,653.0,57.0
685,GCloud Command Line Interface,"This is the [official CLI tool (""gcloud"")](https://cloud.google.com/sdk/gcloud) published by Google for working with GCP. It will be used by Terraform when applying our templates (e.g. for authentication vs GCP). Please install it by following [these instructions](https://cloud.google.com/sdk/docs/install-sdk).",H3,https://outerbounds.com/engineering/deployment/gcp-k8s/deployment/#gcloud-command-line-interface,False,312.0,36.0
686,kubectl Command Line Interface,[kubectl](https://kubernetes.io/docs/reference/kubectl/) is a standard CLI tool for working with [Kubernetes](https://kubernetes.io/) clusters. It will be used by Terraform when applying our templates (e.g. for deploying some services to your [Google Kubernetes Engine (GKE)](https://cloud.google.com/kubernetes-engine) cluster). Please install it by following [these instructions](https://kubernetes.io/docs/tasks/tools/#kubectl).,H3,https://outerbounds.com/engineering/deployment/gcp-k8s/deployment/#kubectl-command-line-interface,False,431.0,40.0
687,2. Provision GCP Resources,"See [here](/engineering/deployment/gcp-k8s/details/#gcp-resource-list) for the exact set of resources to be provisioned. Also, note the [permissions](/engineering/deployment/gcp-k8s/details/#required-gcp-permissions-for-deployment) that are needed.",H2,https://outerbounds.com/engineering/deployment/gcp-k8s/deployment/#2-provision-gcp-resources,False,248.0,18.0
688,Enable Google Cloud APIs,"You need to manually enable APIs used by the Metaflow stack on the Google Cloud console. Make sure that the following APIs are enabled:
 
  - Cloud Resource Manager
  - Compute Engine API
  - Service Networking
  - Cloud SQL Admin API
  - Kubernetes Engine API
 
 If you have used the account/project for other deployments in the past, it is possible that these APIs are already enabled. Also note that enabling these APIs automatically enables a bunch of other required APIs.",H3,https://outerbounds.com/engineering/deployment/gcp-k8s/deployment/#enable-google-cloud-apis,False,476.0,86.0
689,Login to GCP,"You must be logged onto GCP as an account with [sufficient permissions](/engineering/deployment/gcp-k8s/details/#required-gcp-permissions-for-deployment) to provision the required resources. Use the GCloud CLI (`gcloud`)
 ```
 gcloud auth application-default login
 ```",H3,https://outerbounds.com/engineering/deployment/gcp-k8s/deployment/#login-to-gcp,False,269.0,28.0
690,Initialize your Terraform Workspace,"From your `metaflow-tools/gcp/terraform directory`, run:
 ```
 terraform init
 ```",H3,https://outerbounds.com/engineering/deployment/gcp-k8s/deployment/#initialize-your-terraform-workspace,False,82.0,9.0
691,Set Terraform Variables,"Create a `FILE.tfvars` file with the following content (updating relevant values):
 ```
 org_prefix = ""<ORG_PREFIX>""
 project = ""<GCP_PROJECT_ID>""
 ```
 
 For `org_prefix`, choose a short and memorable alphanumeric string. It will be used for naming the Google Cloud Storage bucket, whose name must be globally unique across GCP.
 
 For `GCP_PROJECT_ID`, set the GCP project ID you wish to use.
 
 You may rename `FILE.tfvars` to a more friendly name appropriate for your project. E.g. `metaflow.poc.tfvars`.
 
 The variable assignments defined in this file will be passed to `terraform` CLI.",H3,https://outerbounds.com/engineering/deployment/gcp-k8s/deployment/#set-terraform-variables,False,592.0,90.0
692,Optional: Enable Argo Events,"To enable [event triggering](https://docs.metaflow.org/production/event-triggering) for Metaflow, add the following line in `FILE.tfvars`:
 
 ```
 enable_argo=true
 ```
 
 For more technical context, [see this page about event triggering](/engineering/operations/event-triggering/).",H4,https://outerbounds.com/engineering/deployment/gcp-k8s/deployment/#optional-enable-argo-events,False,282.0,27.0
693,Optional: Enable Airflow,"Optionally, you can include [Apache Airflow as the production orchestrator for Metaflow](https://docs.metaflow.org/production/scheduling-metaflow-flows/scheduling-with-airflow) in your deployment by including the following lines in `FILE.tfvars`:
 
 ```
 deploy_airflow=true
 ```
 
 Setting `deploy_airflow=true` will deploy Airflow in the GKE cluster with a `LocalExecutor`.",H4,https://outerbounds.com/engineering/deployment/gcp-k8s/deployment/#optional-enable-airflow,False,375.0,39.0
694,Apply Terraform Template to Provision GCP Infrastructure,"From your local `metaflow-tools/gcp/terraform` directory, run:
 ```
 terraform apply -target=""module.infra"" -var-file=FILE.tfvars
 ```
 
 A plan of action will be printed to the terminal. You should review it before accepting. See [details](/engineering/deployment/gcp-k8s/details/) for what to expect.",H3,https://outerbounds.com/engineering/deployment/gcp-k8s/deployment/#apply-terraform-template-to-provision-gcp-infrastructure,False,302.0,35.0
695,Common Resource Provisioning Hiccups,,H3,https://outerbounds.com/engineering/deployment/gcp-k8s/deployment/#common-resource-provisioning-hiccups,False,0.0,1.0
696,Cloud SQL instance name conflicts,"Cloud SQL instance (the ""PostgreSQL DB"") names must be unique within your GCP project - including instances that have been deleted within the last 7 days. It means that if you should want to reprovision the entire set of GCP resources within that time window, a fresh name must be chosen. In this scenario, please update the DB generation variable [here](https://github.com/outerbounds/metaflow-tools/blob/26c338860b9725cf5c8689036e4581781e12fbda/gcp/terraform/variables.tf#L9).",H4,https://outerbounds.com/engineering/deployment/gcp-k8s/deployment/#cloud-sql-instance-name-conflicts,False,478.0,61.0
697,3. Deploy Metaflow Services to GKE cluster,,H2,https://outerbounds.com/engineering/deployment/gcp-k8s/deployment/#3-deploy-metaflow-services-to-gke-cluster,False,0.0,1.0
698,Apply Terraform Template to Deploy Services,"From your local `metaflow-tools/gcp/terraform` directory, run:
 ```
 terraform apply -target=""module.services"" -var-file=FILE.tfvars
 ```",H3,https://outerbounds.com/engineering/deployment/gcp-k8s/deployment/#apply-terraform-template-to-deploy-services,False,137.0,12.0
699,4. End User Setup Instructions,"When the command above completes, it will print a set of setup instructions for Metaflow end users (folks who will be writing and running flows). These instructions are meant to get end users started on running flows quickly.
 
 You can access the terraform instruction output at any time by running (from `metaflow-tools/gcp/terraform directory`):
 ```
 terraform output -raw END_USER_SETUP_INSTRUCTIONS
 ```
 
 If the output is not available, run
 ```
 terraform apply -var-file=FILE.tfvars
 ```
 and try the `output` command again.",H2,https://outerbounds.com/engineering/deployment/gcp-k8s/deployment/#4-end-user-setup-instructions,False,534.0,79.0
700,Sample Output,```,H3,https://outerbounds.com/engineering/deployment/gcp-k8s/deployment/#sample-output,False,3.0,1.0
701,AWS Services List,The terraform template will deploy these services in your AWS account:,H2,https://outerbounds.com/engineering/deployment/aws-k8s/details/#aws-services-list,False,70.0,11.0
702,Argo Workflows,"In addition to this, you'll want to install Argo Workflows for scheduling of production runs. The [quickstart](https://argoproj.github.io/argo-workflows/quick-start/) Kubernetes manifest published by Argo Workflows spins up the following services inside the EKS cluster:
 
 ```
 kubectl get services -n argo
 NAME                          TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)             AGE
 argo-server                   ClusterIP   10.0.26.126    <none>        2746/TCP            32m
 httpbin                       ClusterIP   10.0.66.229    <none>        9100/TCP            32m
 minio                         ClusterIP   10.0.173.242   <none>        9000/TCP,9001/TCP   32m
 postgres                      ClusterIP   10.0.51.199    <none>        5432/TCP            32m
 workflow-controller-metrics   ClusterIP   10.0.139.237   <none>        9090/TCP            32m
 ```",H3,https://outerbounds.com/engineering/deployment/aws-k8s/details/#argo-workflows,False,888.0,317.0
703,"I want to use an existing storage account/container, an existing EKS, or DB. What to do?","The terraform templates provide an all-in experience, spinning up all fresh resources. These templates can be adapted to accommodate existing resources. Please [talk to us](http://slack.outerbounds.co) if you would like assistance in modifying these templates.",H3,https://outerbounds.com/engineering/deployment/aws-k8s/faq/#i-want-to-use-an-existing-storage-account-container-an-existing-eks-or-db-what-to-do,False,260.0,34.0
704,How do I use GPU nodes in my EKS cluster?,Our quickstart terraform templates do support that out of box but can be easily adapted to accommodate [GPU-enabled EC2 instances](https://aws.amazon.com/about-aws/whats-new/2018/08/amazon-eks-supports-gpu-enabled-ec2-instances/).,H3,https://outerbounds.com/engineering/deployment/aws-k8s/faq/#how-do-i-use-gpu-nodes-in-my-eks-cluster,False,230.0,20.0
705,How do I change the VM instance types for Metaflow task runner nodes?,"The quickstart terraform templates provided use EKS's autoscaler out of the box. Node auto provisioning should be aware of the resource requirements of incoming pods, which means it will spin up appropriately sized instances as needed. You can override the instance types by [modifying](https://github.com/outerbounds/terraform-aws-metaflow/blob/master/examples/eks_argo/eks.tf#L21) the template.",H3,https://outerbounds.com/engineering/deployment/aws-k8s/faq/#how-do-i-change-the-vm-instance-types-for-metaflow-task-runner-nodes,False,396.0,46.0
706,I want finer grain auth on running flows. What to do?,See [here](/engineering/deployment/aws-k8s/advanced/#eks-workload-identities).,H3,https://outerbounds.com/engineering/deployment/aws-k8s/faq/#i-want-finer-grain-auth-on-running-flows-what-to-do,False,78.0,2.0
707,Why is my Metaflow flow stuck on k8s pending state forever?,"When Metaflow submits tasks to Kubernetes for execution, there are two scenarios:
 * There are sufficient spare resources to immediately run the task.
 * There are NOT sufficient spare resources right now. EKS autoscaler provisions additional compute nodes to satisfy the requirements of the new task. Once provisioning is complete, the task is run.
 
 If EKS autoscaler can never satisfy the new task and the task will be stuck in pending forever. E.g. if the aggregate hard CPU or memory limit has been reached on the EKSE cluster. To resolve the aggregate limit issue, up the limit in the [templates](https://github.com/outerbounds/terraform-aws-metaflow/blob/master/examples/eks_argo/eks.tf#L21).",H3,https://outerbounds.com/engineering/deployment/aws-k8s/faq/#why-is-my-metaflow-flow-stuck-on-k8s-pending-state-forever,False,700.0,101.0
708,How do I access Argo Workflows UI?,"Argo Workflows UI is not accessible from outside the cluster in these templates, but you can use port forwarding to access it.
 
 ```
 kubectl port-forward -n argo service/argo-argo-workflows-server 2746:2746
 ```",H3,https://outerbounds.com/engineering/deployment/aws-k8s/faq/#how-do-i-access-argo-workflows-ui,False,213.0,31.0
709,How do I deploy the Metaflow UI?,Please [talk to us](http://slack.outerbounds.co) for more information about this topic.,H3,https://outerbounds.com/engineering/deployment/aws-k8s/faq/#how-do-i-deploy-the-metaflow-ui,False,87.0,10.0
710,Need help?,The quickest way to get help is our public Slack channel [#ask-metaflow](http://slack.outerbounds.co). We look forward to your questions and feedback.,H3,https://outerbounds.com/engineering/deployment/aws-k8s/faq/#need-help,False,150.0,20.0
711,1. Preparation,,H2,https://outerbounds.com/engineering/deployment/aws-k8s/deployment/#1-preparation,False,0.0,1.0
712,Install Terraform,"[Terraform](https://www.terraform.io/) is a popular infrastructure-as-code tool for managing cloud resources. We have published a set of terraform templates [here](https://github.com/outerbounds/terraform-aws-metaflow) for setting up Metaflow on AWS. Terraform needs to be installed on your system in order to use these templates.
 
 1. Install terraform by following [these instructions](https://learn.hashicorp.com/tutorials/terraform/install-cli).
 2. Download [Metaflow on AWS terraform templates](https://github.com/outerbounds/terraform-aws-metaflow):
 ```
 git clone git@github.com:outerbounds/terraform-aws-metaflow.git
 ```",H3,https://outerbounds.com/engineering/deployment/aws-k8s/deployment/#install-terraform,False,632.0,59.0
713,Install aws CLI,"This is the [official CLI tool (""aws"")](https://aws.amazon.com/cli/) published by Amazon for working with AWS. It will be used by Terraform when applying our templates (e.g. for authentication vs AWS). Please install it by following [these instructions](https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html).",H3,https://outerbounds.com/engineering/deployment/aws-k8s/deployment/#install-aws-cli,False,333.0,36.0
714,Install kubectl,[kubectl](https://kubernetes.io/docs/reference/kubectl/) is a standard CLI tool for working with [Kubernetes](https://kubernetes.io/) clusters. It will be used by Terraform when applying our templates (e.g. for deploying some services to your [Amazon Elastic Kubernetes Service (EKS)](https://aws.amazon.com/eks/) cluster). Please install it by following [these instructions](https://kubernetes.io/docs/tasks/tools/#kubectl).,H3,https://outerbounds.com/engineering/deployment/aws-k8s/deployment/#install-kubectl,False,425.0,41.0
715,2. Provision AWS Resources,See [here](/engineering/deployment/aws-k8s/details/) for the exact set of resources to be provisioned.,H2,https://outerbounds.com/engineering/deployment/aws-k8s/deployment/#2-provision-aws-resources,False,102.0,11.0
716,Login to AWS,"You must be [logged onto AWS](https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-quickstart.html) as an account with sufficient permissions to provision the required resources. Use the AWS CLI (aws)
 ```
 aws configure
 ```",H3,https://outerbounds.com/engineering/deployment/aws-k8s/deployment/#login-to-aws,False,235.0,26.0
717,Initialize your Terraform Workspace,"First, choose which workflow orchestrator you want to use for [Metaflow's production deployments](https://docs.metaflow.org/production/introduction):
 
 1. `cd terraform-aws-metaflow/examples/eks_argo` to use [Argo Workflows](https://docs.metaflow.org/production/scheduling-metaflow-flows/scheduling-with-argo-workflows) or
 2. `cd terraform-aws-metaflow/examples/eks_airflow` to use [Apache Airflow](https://docs.metaflow.org/production/scheduling-metaflow-flows/scheduling-with-airflow)
 
 If in doubt, choose `eks_argo` as it comes with fewer limitations. In particular, the `eks_argo` option comes with [Metaflow's event triggering](https://docs.metaflow.org/production/event-triggering) enabled automatically ([more technical details here](/engineering/operations/event-triggering/)).
 
 Next, in your chosen directory run
 ```
 terraform init
 ```",H3,https://outerbounds.com/engineering/deployment/aws-k8s/deployment/#initialize-your-terraform-workspace,False,853.0,68.0
718,Apply terraform Template to Provision AWS Infrastructure,"In the same directory, run
 ```
 terraform apply
 ```
 
 A plan of action will be printed to the terminal. You should review it before accepting. See details for what to expect. This command typically takes ~20 minutes to execute.",H3,https://outerbounds.com/engineering/deployment/aws-k8s/deployment/#apply-terraform-template-to-provision-aws-infrastructure,False,230.0,40.0
719,3. End User Setup Instructions,"When the command above completes, make note of the EKS cluster name (it is a short string that starts with mf-). Use the AWS CLI (aws) to generate cluster configuration
 
 ```
 aws eks update-kubeconfig --name <cluster name> configure
 ```
 
 Create the file `~/.metaflowconfig/config.json` with the contents of `config.json`. If this file already exists, keep a backup of it and move it aside first.",H2,https://outerbounds.com/engineering/deployment/aws-k8s/deployment/#3-end-user-setup-instructions,False,400.0,65.0
720,Remote State Backends for Terraform,"Terraform manages the state of AWS resources in [tfstate](https://www.terraform.io/language/state) files locally by default.
 
 If you plan to maintain the minimal stack for any significant period of time, it is highly recommended that these state files be stored in cloud storage (e.g. Amazon S3) instead.
 
 Some reasons include:
 * More than one person needs to administer the stack (using terraform). Everyone should work off a single copy of tfstate.
 * You wish to mitigate the risk of data loss on your local disk.
 
 For more details, see [Terraform documentation](https://www.terraform.io/language/settings/backends/configuration).",H2,https://outerbounds.com/engineering/deployment/aws-k8s/advanced/#remote-state-backends-for-terraform,False,640.0,91.0
721,Deploying Multiple Metaflow Stacks,"If you want to run more than one instance of this stack, you can use [Terraform workspaces](https://www.terraform.io/language/state/workspaces).",H2,https://outerbounds.com/engineering/deployment/aws-k8s/advanced/#deploying-multiple-metaflow-stacks,False,144.0,17.0
722,Authenticated Public Endpoints for Metaflow Services,"The deployment approach taken by the terraform templates minimizes publicly accessible surface area. Only the EKS Kubernetes API is available publicly. This allows authorized users (through the secure Kubernetes API) to:
 * Inspect cluster's workloads.
 * CRUD Kubernetes objects (e.g. submit job pods).
 
 However, this deployment style does not include publicly accessible endpoints for the [web services](/engineering/deployment/aws-k8s/details/#aws-services-list) running within the EKS cluster. For the purpose of this sample deployment template, users must use the Kubernetes API to set up port-forwarding in order to access these services from their workstations.
 
 For a more friendly user experience, publicly accessible endpoints can be authenticated and authorized using technologies like:
 * [OIDC (OpenID Connect)](https://openid.net/connect/)
 * [JWT](https://jwt.io/) tokens
 * Identity-as-a-service providers (e.g. [Auth0](https://auth0.com/)).
 
 Please [talk to us](http://slack.outerbounds.co) for more information about this topic.",H2,https://outerbounds.com/engineering/deployment/aws-k8s/advanced/#authenticated-public-endpoints-for-metaflow-services,False,1052.0,131.0
723,EKS Workload Identities,"In the Metaflow stack generated by these terraform templates, all Metaflow workloads running within EKS access AWS resources as a specific service account identity. We use [IRSA](https://docs.aws.amazon.com/eks/latest/userguide/iam-roles-for-service-accounts.html) which does the following:
 * Metaflow tasks pods run as a certain Kubernetes Service Account (KSA).
 * KSA is annotated with a link to the IAM role.
 * The pod running as KSA assumes the IAM role when accessing AWS resources.
 
 For finer grain control and end-to-end identity, please [talk to us](http://slack.outerbounds.co).",H2,https://outerbounds.com/engineering/deployment/aws-k8s/advanced/#eks-workload-identities,False,592.0,80.0
724,The 4 main tasks in the production ML lifecycle,"**Hugo:** What are the main tasks you discovered that people do in the production machine learning lifecycle?
 
 
 **Shreya:** I think a good answer to this is what the textbook says and then what we found that is different from what the textbook says. And what the textbook will tell you for machine learning is first you collect data (step one), and then you train a model (step two). Step three is to validate that model on a holdout data set and ensure there’s no overfitting. And then step four is you deploy. And what we found is that we can still categorize it into four steps. And maybe the data collection part is similar except that it’s more of a loop like every, I don’t know, a week or so we want to collect new data. 
 
 
 But the rest of the steps last three steps are totally different. The second step, what I said before model training is actually experimentation. In general, whether it be training new models, whether it be trying to source new data, or adding new features, there are a lot of ways you can think about improving a model. And a lot of the participants actually preferred to look into finding new data that gave new signal or making features more fresh instead of stale features that they had before. So that’s kind of step two, stage three in the process we call evaluation and deployment. So evaluation is not a one-and-done thing. What happens is, evaluation is done maybe on a holdout data set first, and then it’s deployed to a small fraction of users. And then when the model shows a little promise, then increasingly it’s deployed to more and more users. As we learn more about what it can do, what it can’t do, what failure modes exist, how do we go and catch problems until we’ve kind of gotten to the full population? 
 
 
 So the key takeaway is evaluation is not a one-time thing. It is a loop on evaluation and deployment of multistage deployment. And then the latest step four, which we found was this overall monitoring and response stage, which was when you do have these models in production. What is their live performance? If you see the performance dropping, what are the bugs? Where are the bugs? How do we respond to them quickly, whether that be actually trying to go do root cause analysis, or simply retraining the model, there is a stage around making sure that there’s little downtime for these services. So we do have those four stages shown in the first figure in the paper. And it was interesting to several of us authors, that they don’t match the textbook, I think that’s like kind of our tip that we want people to take away.
 
 
 ![](_shreya-fs-chat-recap_data/0_img)
 <Caption>Source: https://arxiv.org/abs/2209.09125</Caption>
 
 ```mdx-code-block
 <div className=""video-container"">
 <LiteYouTubeEmbed
         id=""hv5CCOAH7og""
         title=""The 4 main tasks in the production ML lifecycle""
 />
 </div>
 ```",H2,https://outerbounds.com/blog/production-machine-learning-data-centric#the-4-main-tasks-in-the-production-ml-lifecycle,False,2880.0,515.0
725,The 3 factors that determine the success of ML projects,"**Hugo:** So something I’m really excited about is you’ve identified three key properties of ML workloads and infrastructure that dictate how successful deployments are. What are they and why are they?
 
 
 **Shreya:** We have these three V’s they’re not even related, or we didn’t even know about the big data three V’s: these three V’s are *velocity*, *validating*,  and *versioning*. And why did we come up with these three V’s? We wanted some way to explain kind of best practices and pain points, as we were looking for patterns in what our interviewees said. And since we asked such open-ended questions, they’re in the appendix. But we ask open-ended questions like tell me about a bug you had last week, something that caused you things like that are so open-ended, they’re so hard to extract patterns from. So it helped us to come up with these variables like velocity, people kept mentioning that they needed to iterate quickly on experiments, because they had a large frontier of ideas to try, and they wanted to see something that would give a production.
 
 
 At the end of the day, we were like, oh, when people are doing experimentation, they care about velocity. And it really resonated with us, when we started thinking about MLOps tools. What makes an ML ops tool successful? Well, experiment tracking is a nice space because it really 10xes your experimentation velocity. Now I don’t have to go copy and paste into Google Sheets. And maybe that works if I’m the only person working on my model. But at the moment that multiple people are working on an ML pipeline or model system, then it’s super nice to centralize all of the experimentation we do so we can share the knowledge that we’ve had. So we had velocity for that. For *validating early*, a lot of people complained about the fact that in their organization, either too many bad models made it to production, or so that was like it was validating too late, or models were validated way too early, and that they couldn’t get anything to production. So for one example, in an autonomous vehicle company, the cost of deploying a bad model is so high. So they incorporated all these checks, they made evaluation take much longer, they decrease the velocity, and engineers were grumpy. 
 
 
 But at the end of the day, there’s a quote in the paper that says you know, we’d much rather get the velocity if it means that we don’t get failures on the road. So again, different tasks, they have different priorities where and I think that’s also why people keep talking about how like machine learning, like, you know, it’s not even generalizable, it’s so different for different tasks. And when you think about it through the lens of these V’s, it’s not that it makes total sense for different tasks, they just have different priorities. Some people prioritize velocity over validation when the stakes aren’t so bad if there’s a failure, for sure. So in that sense, we really liked this kind of framework of evaluating tools, evaluating what people cared about. And as people who like to build tools ourselves. There are some cool ideas that I’ve had that now I can confidently say that, Oh, this is really not 10x improvement in people’s workflows, it doesn’t really help their velocity doesn’t validate better. And it doesn’t help people manage any more versions. So why bother? And I really liked that way of thinking about it.
 
 
 ```mdx-code-block
 <div className=""video-container"">
 <LiteYouTubeEmbed
         id=""HPU8ttaZc6U""
         title=""The 3 factors that Determine the success of ML projects
 ""
 />
 </div>
 ```",H2,https://outerbounds.com/blog/production-machine-learning-data-centric#the-3-factors-that-determine-the-success-of-ml-projects,False,3595.0,632.0
726,Why data scientists love and hate notebooks: velocity and validation,"**Hugo:** This is incredibly useful. I mean, this framework. So what I’m hearing is in this framework of velocity, validation, and versioning, we can look at people who prefer Jupyter Notebooks. And in this framework, they’re essentially prioritizing velocity. Whereas people who are strongly opinionated against Jupyter, notebooks, prioritizing validation and versioning, or mostly validation?
 
 
 **Shreya:** I think of it as validation more, because it’s like, how do you make sure that development and production environments are as similar as possible? So you can remove the need to validate a lot when promoting from dev to prod, if there is no real environment change, from dev to prod. One great example is like sometimes people will iterate locally, and then deploy to the prod service on the cloud. That is a huge environment mismatch! So you need to do some sort of big validation. I don’t even know I don’t think people have solved this problem of like, making sure there aren’t bugs, this mismatch of environments. 
 
 
 ```mdx-code-block
 <div className=""video-container"">
 <LiteYouTubeEmbed
         id=""8rncasQuZqE""
         title=""Why data scientists love and hate notebooks: velocity and validation""
 />
 </div>
 ```",H2,https://outerbounds.com/blog/production-machine-learning-data-centric#why-data-scientists-love-and-hate-notebooks-velocity-and-validation,False,1235.0,205.0
727,Is the premise of data-centric AI flawed?,"**Hugo:** Could you speak to what you’ve seen with respect to the differences between the data-centric machine learning paradigm, and the model-centric, and how this comes back to your point about what’s taught in textbooks and courses?
 
 
 **Shreya:** Courses are now talking about data-centric AI. But the premise is flawed. It’s the same premise that people talk about model-centric AI and that by hook or by crook, we will edit model hyperparameters. Until we get something that works on a small validation set, people are doing the same ethos when it comes to data-centric AI. By hook or by crook, we will add three or four examples or remove six of these labels, or clean 12 of these labels. And we will get 1% or 5% better performance on the validation set. This is the same ethos like maybe this is easier to do in the data-centric sets. But it doesn’t, it’s in the model-centric sense. But we found in the interview study that this is not at all the way to get validation, right, you want to get a win that lasts beyond the initial validation. And we talked about this in the section around experimentation, where you want to find ideas that lead to huge gains in the first offline validation because there are diminishing returns in successive stages of deployments. Like in the offline validation stage, if you get a 15% booster I don’t that’s kind of odd. But if you get like 5% in the third stage of deployment, later on, it’s only going to be half a percent. So account for these diminishing returns as you go down all of a sudden, now that changes the way you think about your experiments, what can I do to bring long-term gains, right? It’s not about editing the view of data that I’m training my model on, I want to add a new signal, I want to go find a new data set that will add a new signal to the model. I want to fix engineering problems. I want to add data validation. So I don’t train or retrain on corrupted data. Like these are the big wins that give you the long-term boost right over though, I hesitate to like preach about data-centric AI in the way that it’s taught.
 
 ```mdx-code-block
 <div className=""video-container"">
 <LiteYouTubeEmbed
         id=""tMHWDpJo_8o""
         title=""Is the premise of data-centric AI flawed?""
 />
 </div>
 ```",H2,https://outerbounds.com/blog/production-machine-learning-data-centric#is-the-premise-of-data-centric-ai-flawed,False,2274.0,419.0
728,ML engineering vs traditional software engineering: similarities and differences,"**Hugo**: Something we’ve been dancing around is software engineering, classical, traditional software engineering, and machine learning engineering. Are they the same or different?
 
 
 **Shreya:** Okay, so I guess this is a nice preview for my normconf talk: it’s going to be on all of my machine learning problems or data management problems? I think so maybe that’s how I feel about any kind of engineering. And I don’t think it’s like software engineering, per se, that is really like the skill set that an ML engineer needs to have if they want to be a 10x to use the stupid term 10x. But I think it is an understanding of how data works. What is a data pipeline? What is a table? What is a relation? Okay? And what is a view this, this is a nice one, a lot of people don’t even know about views. A view is kind of, I run some query on a data set, I just store that as a view that will either be materialized before I query the view, or it will be materialized as I query the view. Okay, so there’s like a question of like, when do I materialized to view, this is the same problem in machine learning, if you think of a machine learning model, as the view over the underlying trade underlying training data, okay, when I train the model, that’s when the view is materialized. So all of the problems around view staleness are the same thing. There’s a model staleness, okay, we don’t want to train, we don’t want to compute the view on wrong data. We don’t want to train the model on incorrect data. These are all the problems that we had talking about time and time again, and databases that are showing up in the ML world. So in that sense, I think ML engineering really, really is just like recast data problems.
 
 
 ```mdx-code-block
 <div className=""video-container"">
 <LiteYouTubeEmbed
         id=""3KVRHHoRS9U""
         title=""ML engineering vs traditional software engineering: similarities and differences""
 />
 </div>
 ```",H2,https://outerbounds.com/blog/production-machine-learning-data-centric#ml-engineering-vs-traditional-software-engineering-similarities-and-differences,False,1938.0,358.0
729,"If you can reason about data, you can reason about machine learning","After our fireside chats, we have async AMAs with our guests on [our community slack](http://slack.outerbounds.co/). A lot tends to happen there so join us if you’re interested in such conversations!
 
 
 This week, for example, Hugo asked 
 
 
 > At [this point in our chat](https://www.youtube.com/watch?v=7zB6ESFto_U&t=3210s), you said something I haven’t stopped thinking about since: “my argument is that you definitely know how to reason about machine learning if you know how to reason about data.” What does reasoning about data and ML actually look like and what are the top 3-5 things to keep in mind when reasoning about both?”
 
 
 And Shreya responded as follows:
 
 
 > Love this & am still working on refining this argument. Here’s the outline:
 
 
 > * end users interact with production ML systems in the form of queries over some existing relation(s), e.g., is this transaction fraudulent
 * in database speak, a view is a query over existing relation(s) that you want to materialize, so it’s easier to return results to the end user
 * so ML models can be thought of as views, accessible and/or compressed representations of a relation for easy querying in the future
 * Classic problems in view maintenance involve staleness and correctness. How do I efficiently update my view when the base relation(s) update? How do I choose which views to materialize? How do I rewrite queries to use the views? All these problems apply to production ML.
 * The most immediate analogy between data engineering and ML engineering is around correctness. We have many checks in place & SLAs to make sure the results of recurring data pipeline results are fresh and correct. On-call rotations ensure timely response to failures. I’d argue that we should treat the maintenance of prod ML models similarly.
 * Top things to keep in mind: establish SLAs on model staleness and correctness & human-centric processes to ensure them. A bug can usually be traced back to incorrectness or staleness, where the fixes are ensuring well-formed features or retraining.
 
 
 [Join us on slack](http://slack.outerbounds.co/) for more such conversations and also join us for our next fireside chat:  How to Build an Enterprise Machine Learning Platform from Scratch with Russell Brooks (Realtor.com). We’ll be discussing what building an enterprise ML platform from scratch looks like in practice, including the journeys experienced at both OpCity and Realtor.com, where he took both organizations from a bus factor of 1 to reproducible and automated ML-powered software. [You can sign up here](https://www.eventbrite.com/e/how-to-build-an-enterprise-machine-learning-platform-from-scratch-tickets-452414773767)!",H2,https://outerbounds.com/blog/production-machine-learning-data-centric#if-you-can-reason-about-data-you-can-reason-about-machine-learning,False,2700.0,421.0
730,"Four new features to support serious data, ML, and AI teams","Today, we are releasing four new features in the Outerbounds Platform to support companies that treat data, ML, and AI as a core component of their business. The features have been developed in collaboration with a number of our customers to address
 their immediate needs, as well as anticipate the needs of tomorrow.
 
 For years, we have been developing Metaflow with the goal of making it [the most user-friendly and productive way](https://outerbounds.com/features/friendly-api/) to develop data, AI, and ML apps, and deploy them to production quickly. Today's features align with this story:
 
 ![](_workstation_announcement/feature_set.png)
 
 Let's take tour of the new features:
 
 1. [Built-in cloud workstations](#built-in-cloud-workstations).
 2. [Develop custom Generative AI and large language models with flexible GPU resources](#develop-custom-genai-and-llms-with-flexible-gpu-resources).
 3. [Support for building reactive ML and AI systems](#building-reactive-event-based-systems).
 4. [Bank-grade security and compliance](#bank-grade-security-and-compliance).",H2,https://outerbounds.com/blog/obp-release-july-2023#four-new-features-to-support-serious-data-ml-and-ai-teams,False,1078.0,134.0
731,Built-in cloud workstations,<ReactPlayer playing controls muted loop url='/assets/obp-demo-vscode.mp4' width='100%' height='100%'/>,H2,https://outerbounds.com/blog/obp-release-july-2023#built-in-cloud-workstations,False,103.0,8.0
732,Develop custom GenAI and LLMs with flexible GPU resources,"We have been actively [working with the research community](https://outerbounds.com/blog/llm-value-and-risk/) to make sure that Metaflow and Outerbounds Platform work well for state-of-the-art generative AI and LLM use cases. We believe that over the coming years, many companies will want to [retain control over their data and models](https://outerbounds.com/blog/llm-infrastructure-stack/), differentiating their offerings from unrefined foundation models and generic AI APIs.
 
 To support them in this rapidly evolving landscape, we have been testing popular models with Metaflow and publishing popular recipes for many open-source foundation models, such as [Stable Diffusion](https://outerbounds.com/blog/parallelizing-stable-diffusion-production-use-cases/), [Whisper](https://outerbounds.com/blog/mlops-whisper-and-metaflow/), [Dolly](https://outerbounds.com/blog/train-dolly-metaflow/), and [LLaMA](https://outerbounds.com/blog/llm-tuning-metaflow/).
 
 ![](../static/assets/coreweave-instruction-tuning.png)
 
 In addition to the modeling layer, we have worked actively on the compute layer, making sure GPUs are easily and cost-effectively accessible to all users of the Outerbounds Platform. Besides supporting GPUs offered by AWS, GCP, and Azure, we have partnered with [CoreWeave, one of the largest dedicated GPU clouds](https://www.coreweave.com/), to expand the pool of available GPU resources that are available to our customers easily. Also, we are happy to help you
 to leverage local GPU clusters.
 
 Imagine executing your code on an instance with four A100 GPUs simply by writing a line of code, `@resources(gpu=4)`, paying a low price only for the seconds the code executes. Behind the scenes, the Outerbounds Platform takes care of the heavy lifting of allocating the hardware resources, shipping the code and its dependencies for execution securely, and tracking progress and results.
 
 Expect to see many more announcements on this front during the course of the year! Meanwhile, you can read more
 at our new [product page for LLM, GenAI, and GPUs](https://outerbounds.com/features/use-gpus-for-ai/).",H2,https://outerbounds.com/blog/obp-release-july-2023#develop-custom-genai-and-llms-with-flexible-gpu-resources,False,2130.0,264.0
733,"Building reactive, event-based systems","While the new workstations allow you to develop and experiment effectively, the new built-in support for reactive workflows allows you to build sophisticated production systems powered by ML, integrating workflows to data warehouses like Snowflake, Databricks, or [open-source data lakes (e.g. based on Apache Iceberg)](https://outerbounds.com/blog/oss-modern-data-stack-recap/), as well as your other systems downstream, similar to the system that powers all business-critical data processing and data science at Netflix.
 
 <ReactPlayer playing controls muted loop url='/assets/et-basic-event.mp4' width='100%' height='100%'/>
 
 The Outerbounds Platform covers the end-to-end process of developing ML and data systems, from the simplest models to the state-of-the-art systems composed of tens of separate flows, connected together via events. You can
 
 1. **Develop** and test projects rapidly with secure Outerbounds Workstations, even testing events locally.
 2. **Deploy** projects as A/B experiments or to production through CI/CD systems, running in a highly-available, scalable environment.
 3. **Connect** the projects to surrounding systems through real-time events, allowing them to benefit from ML and AI seamlessly.
  
 The event-triggering feature that enables this has been available in open-source Metaflow since [the recent 2.9 release](https://outerbounds.com/blog/metaflow-event-triggering/), and now the Outerbounds Platform makes the feature enterprise-ready:",H2,https://outerbounds.com/blog/obp-release-july-2023#building-reactive-event-based-systems,False,1482.0,190.0
734,Bank-grade security and compliance,"Thanks to our roots in large business-critical use cases, we have wanted to make sure that ML and data science teams working in highly regulated industries can benefit from the Outerbounds Platform. We have been working with a number of fintechs to ensure that their requirements are being met. A great example of this is a major European fintech, [Trade Republic](https://traderepublic.com/):
 
 > We are a bank, everything we do needs to be auditable. This means we need to be able to reproduce what was in production and the Outerbounds Platform gives us that for free, as all models and metadata are versioned. I sleep much more comfortably knowing this. <br></br> &emsp; - Thanasis Noulas, VP of Data Science, Trade Republic
 
 
 ![](_workstation_announcement/security_compliance.png)
 
 
 To support the compliance requirements of customers in highly regulated industries, the Outerbounds Platform comes with a number of critical security features:",H2,https://outerbounds.com/blog/obp-release-july-2023#bank-grade-security-and-compliance,False,954.0,148.0
735,Not forgetting our human-centric focus,"![](_workstation_announcement/humans.png)
 
 Ever since the initial open-sourcing of Metaflow in 2019, its key differentiator has been [a delightful user experience](https://outerbounds.com/features/friendly-api/). All APIs are carefully crafted to fit in the hands of data-centric developers and engineers supporting them, making sure developers with diverse domain expertise can focus on actual business problems, not on infrastructure.
 
 In addition to the product design, we invest heavily in support and education. This is particularly important now that there is a lot of excitement, but also confusion and doubt, about the future of ML and AI. Over the past months, we have greatly expanded our available learning paths:",H2,https://outerbounds.com/blog/obp-release-july-2023#not-forgetting-our-human-centric-focus,False,728.0,103.0
736,Start your free trial in 15 minutes,"We know that data and ML teams, as well as engineering teams supporting them, are short-staffed and busy with
 existing responsibilities. Having this in mind, we have made it extremely easy to get started with the platform. It
 takes about 15 minutes of engineering time.
 
 If you are curious to see how the Outerbounds Platform could work for you, [schedule a quick call](/schedule) and we can
 get you started with a free trial today!
 
 <a href=""https://outerbounds.com/schedule"">
 
 ![](_workstation_announcement/get_started.png)
 
 </a>",H2,https://outerbounds.com/blog/obp-release-july-2023#start-your-free-trial-in-15-minutes,False,542.0,82.0
737,Metaflow: A Human-Centric Way to Develop ML Apps,"When we started developing [open-source Metaflow at Netflix](https://netflixtechblog.com/open-sourcing-metaflow-a-human-centric-framework-for-data-science-fa72e04a5d9) in 2017, we focused on the first two challenges: To tame the inherent complexity of these systems, we wanted to make it easy for data scientists to express their ideas in straightforward Python. We didn't believe in silver bullets, cute-but-simplistic gimmicks, or novel paradigms with a steep learning curve.
 
 In our experience, the best ML applications are tailor-made by humans with domain expertise, using pragmatic tools that are purpose-built for [developing data-intensive applications](https://www.oreilly.com/radar/mlops-and-devops-why-data-makes-it-different/). The tool should keep track of all changes automatically – including changes in data, code, models, and people – so systems can grow over time through continuous, steady improvements.
 
 In particular, Metaflow acknowledges that the journey from prototype to production is a gradual process, and the concept of “production” is not a binary one. Production can mean anything from an internal dashboard or a limited-scale A/B experiment to a Netflix-scale, low-latency, business-critical, customer-facing product. We wanted to make it possible to evolve the former to the latter without imposing too much overhead upfront, and without requiring drastic – and costly – re-platforming along the way.
 
 
 ![](_announcing-outerbounds-platform_data/1_img)
 Prior to Outerbounds, we didn’t have to focus on the infrastructure behind Metaflow. Netflix, similar to other tech giants, employs a large organization of engineers who build and operate a jungle of infrastructure from multiple data platforms and [workflow orchestrators](https://netflixtechblog.com/orchestrating-data-ml-workflows-at-scale-with-netflix-maestro-aaa2b41b800c) to [a massive scale compute platform](https://netflixtechblog.com/titus-the-netflix-container-management-platform-is-now-open-source-f868c9fb5436) and myriads of surrounding systems. Under the hood, Metaflow was able to leverage these production systems, operated and approved by engineers, to enable data scientists to develop production-ready ML applications without having to worry about low-level technical details.",H2,https://outerbounds.com/blog/announcing-outerbounds-platform#metaflow-a-human-centric-way-to-develop-ml-apps,False,2288.0,275.0
738,Metaflow Success Stories,"![](_announcing-outerbounds-platform_data/2_img)
 The result of this division of responsibilities – engineers providing robust and scalable infrastructure that is readily available to data scientists through a human-friendly interface – empowered everyone to focus on their areas of strength.
 
 Thanks to increased productivity unlocked by Metaflow, Netflix was able to expand to whole new areas of ML-driven use cases, allow data scientists to conduct real-life experiments autonomously, and have a clear path for promoting the most promising ideas to production quickly. To give an idea of the scale, [by 2021 Netflix’s Metaflow deployment](https://netflixtechblog.com/open-sourcing-a-monitoring-gui-for-metaflow-75ff465f0d60) included thousands of projects and millions of executions and the numbers are growing quickly.
 
 Since open-sourcing of Metaflow in 2019, we have seen many other well-known organizations, such as [CNN](https://medium.com/cnn-digital/accelerating-ml-within-cnn-983f6b7bd2eb), [23andMe](https://medium.com/23andme-engineering/machine-learning-eeee69d40736), [Realtor.com](https://medium.com/realtor-com-innovation-blog/improving-data-science-processes-to-speed-innovation-at-realtor-com-b6b90fa530dc), and [MoneyLion](https://outerbounds.com/blog/mlops-fin-tech/), repeat this success story. With support from the friendly Metaflow community, their engineering teams have been able to leverage Metaflow’s open-source templates to set up and operate the infrastructure to support their data scientists.
 
 
 ![](_announcing-outerbounds-platform_data/3_img)
 We founded Outerbounds in 2021 to continue developing Metaflow together with Netflix and the wider community. At Outerbounds, we have been able to expand our reach to organizations across industries, from biotech and fintech to e-commerce and bleeding-edge AI use cases – and learn from the data science and platform teams powering these companies.
 
 
 Hearing your thoughts and feedback on [our 2000+ member community](http://slack.outerbounds.co) has been immensely helpful in improving Metaflow further and understanding its gaps – thank you all! Keep your questions, thoughts, and feedback coming.
 
 
 Besides the open-source community, the company got started thanks to support from Apoorva Pandhi (now at Zetta Ventures) and Steve Vassallo at [Foundation Capital](https://foundationcapital.com/) (also an early investor in Netflix):
 
 
 > ""Netflix will spend $17 billion on content this year,” said Steve Vassallo, general partner at Foundation Capital. “And while real-life humans are still responsible for creating the bingeworthy scenes, how that gigantic budget is allocated rides on machine learning models that run on Metaflow, a powerful open-source framework created by the brilliant founding team of Outerbounds. We are absolutely thrilled to back them and add to Foundation’s deep portfolio of AI/ML infrastructure investments focused on empowering modern data teams with better, more scalable, more integrated, human-centered tools.""",H2,https://outerbounds.com/blog/announcing-outerbounds-platform#metaflow-success-stories,False,3039.0,374.0
739,Introducing the Outerbounds Platform,"As we supported hundreds of organizations on their Metaflow journey, we started to see patterns emerge. While the open-source Metaflow is common for all organizations, a great deal of variation exists on the infrastructure side.
 
 Some variation is expected and desirable due to genuine differences in organizations and the businesses behind them, but much of it is accidental. Ultimately, the *outer bounds* of common engineering concerns - security, scalability, cost-efficiency, high-availability, and integrations to surrounding systems - look similar. At the high level, all happy infrastructures are alike.
 
 Based on lessons we had learned from organizations we had worked with - small, large, and every size in between - we set out to bake all infrastructural concerns and best practices into one platform, Outerbounds Platform, which
 [we are finally happy to make publicly available today](/platform/).
 
 ![](_announcing-outerbounds-platform_data/4_img)
 
 We hope that Outerbounds Platform will help organizations produce ML-powered value faster and apply ML to new business domains, following the success stories of Netflix and other early adopters. The platform allows them to shortcut years of time and effort that they would otherwise incur.
 
 Outerbounds Platform is a fully managed service that runs securely on your AWS account (crucially, data and code never leave your premises) providing an optimized backend for projects developed with Metaflow. The platform addresses the third challenge in ML projects: Instead of having a village of engineers providing infrastructure for ML, they can focus on solving business-specific challenges.
 
 ![](_announcing-outerbounds-platform_data/5_img)
 
 ML and data applications can’t live on an island or a walled garden, so a truly effective platform must adapt to its environment - your microservices, policies, and processes - with minimal friction. Hence, by design, Outerbounds Platform is not a black box. It adapts to the bespoke and changing requirements of your engineers and organization by providing a set of battle-hardened patterns and guardrails that can be composed to fit your needs. Better still, data scientists get to rely on [the well-documented](/docs/data-science-welcome/) tried-and-true, Metaflow that will always remain open-source.
 
 
 You can [read more about Outerbounds Platform on our new product page](/platform/). If you are interested in deploying it in your environment, which requires minimal engineering effort on your side, [schedule a call with us](https://meetings.hubspot.com/ville-tuulos).
 
 
 In the words of another of our insightful investors, [Greenoaks Capital](https://greenoaks.com/):
 
 
 > ""Metaflow is the way that the world’s best data scientists and engineers work,” said Sreyas Misra of Greenoaks. “We’re excited about Outerbounds because they abstract away complex engineering and infrastructure challenges, letting data scientists focus on what matters most: building new products and uncovering new insights. We think that few people know the challenges and opportunities of data science better than the team at Outerbounds, and we are thrilled to partner them as they accelerate their customers’ adoption of ML and AI.”",H2,https://outerbounds.com/blog/announcing-outerbounds-platform#introducing-the-outerbounds-platform,False,3242.0,472.0
740,Onwards to an ML-driven future,"We believe that we have seen only the early days of ML/AI adoption. Major technological shifts like ML, AI, and data-driven development will take decades to spread across industries – not only because of technical hurdles but also because of the pace of human change. Accounting for this is a part of our human-centric design: Metaflow and Outerbounds Platform can meet your organization where it is today, inviting your existing teams and systems to a long-term journey to an ML-driven future.
 
 
 ![](_announcing-outerbounds-platform_data/6_img)
 
 To support our long-term vision, we have partnered with top-tier investors, [Foundation Capital](https://foundationcapital.com/), [Amplify Partners](https://www.amplifypartners.com/), and [Greenoaks Capital](https://greenoaks.com/), who provide unique experience at the intersection of human-centric product design, developer tooling, ML/AI, enterprise infrastructure, and open-source. Thanks to them and a group of angel investors with deep domain expertise, we have raised over $24M in Seed and Series A funding.
 
 
 In the words of Sarah Catanzaro at Amplify Partners:
 
 
 > Amplify is committed to backing companies that empower technical practitioners to build better, safer, models and applications. As such, we were thrilled to support Outerbounds as they dramatically improve the agility of ML teams and the reliability of the products they build.
 
 > For over a decade, Amplify has supported founders as they set new standards for model and application development and management. Based on this experience, we strongly believe that Outerbounds will redefine the process whereby data science projects are developed and deployed.
 
 > Amplify backs founders building technical tools and platforms that change how developers work. We’re so excited by how Outerbounds helps data scientists run more experiments and move from local development to production faster.",H2,https://outerbounds.com/blog/announcing-outerbounds-platform#onwards-to-an-ml-driven-future,False,1924.0,279.0
741,Get started today,"![](_announcing-outerbounds-platform_data/7_img)
 You can get started with Metaflow and Outerbounds Platform right away:
 
 
 * Get a feel of [Metaflow](/metaflow) in the browser by [signing up for a free Metaflow sandbox](/sandbox).
 
 * And, to learn more about Outerbounds Platform, [read the product page](/platform/) and [schedule a call with us](https://meetings.hubspot.com/ville-tuulos/outerbounds-discovery).
 
 
 Also, be sure to [join our community of over 2000 data scientists and engineers](http://slack.outerbounds.co/)!
 
 
 PS. If our approach that combines a clear vision, deep tech, and a human-centric product experience resonates with you, [we are hiring](/work-with-us/).",H2,https://outerbounds.com/blog/announcing-outerbounds-platform#get-started-today,False,692.0,90.0
742,Recap: From the Modern Data Stack to MLOPs,"The modern data stack (MDS) has been consolidated as a series of best practices around data collection, storage, and transformation. In particular, the MDS encompasses three pillars: 
 
 
 * A scalable ingestion mechanism, either through tools (e.g. Fivetran, Airbyte) or custom [infrastructure](https://github.com/jacopotagliabue/paas-data-ingestion);
 * A data warehouse (e.g. Snowflake) storing all data sources together;
 * A transformation tool (e.g. dbt), ensuring versioned, DAG-like operations over raw data using SQL.
 
 
 A lot has been said already about the MDS as such, but the situation is more “scattered” on the other side of the fence: *once data is properly transformed, how is that consumed downstream to produce business value*? 
 
 
 Our solution is to accept the fact that not every company requires elaborate and infinitely scalable infrastructure like those deployed by Googles and Metas of the world, and [that is totally ok](https://www.youtube.com/watch?v=SAal0hqqE74): [doing ML at “reasonable scale”](https://towardsdatascience.com/ml-and-mlops-at-a-reasonable-scale-31d2c0782d9c) is more rewarding and effective than ever, thanks to a great ecosystem of vendors and open source solutions.
 
 
 The backbone for this work is provided by Metaflow, our open-source framework which (among other things) lowers the barrier to entry for data scientists to take machine learning from prototype to production and the general stack looks like this, although Metaflow will allow you to switch in and out any other component parts:
 
 
 ![](_dataops-and-mlops-for-reasonable-organizations_data/0_img)
 How does this stack translate good data culture into working software at scale? A useful way to isolate (and reduce) complexity is by understanding *where computation happens*. In our pipeline, we have four computing steps, and two providers:
 
 
 * Data is stored and transformed in Snowflake, which provides the underlying compute for SQL, including data transformations managed by a tool like dbt;
 * Training happens on AWS Batch, leveraging the abstractions provided by Metaflow;
 * Serving is on SageMaker, leveraging the PaaS offering by AWS;
 * Scheduling is on AWS Step Functions, leveraging once again Metaflow (not shown in the repo, but [straightforward to achieve](https://docs.metaflow.org/going-to-production-with-metaflow/scheduling-metaflow-flows)).",H2,https://outerbounds.com/blog/dataops-mlops-reasonable-organizations#recap-from-the-modern-data-stack-to-mlops,False,2387.0,331.0
743,A tale of two cultures,"This stack is built for productivity: provisioning, scaling, versioning, and documentation all come for free. But it is also built for *collaboration*: part of it is easy to spot, as training models on Metaflow and Comet is indeed a team sport. You need both Mario *and* Luigi! Part of it is subtler, and we want to discuss it through our own experience at [Coveo](https://www.coveo.com/en), a B2B SaaS company specializing in AI services, moving from an older stack to (a version of) this stack.  
 
 When Jacopo joined Coveo, the data team was in charge of safely storing application data and behavioral signals. From there, two teams transformed that data in modeling and insights, *but they would do so in parallel*:the BI team would perform their own transformation leveraging Redshift and SQL to power dashboards for Jacopo and his clients; the ML team would run a Spark-based stack, where transformations are Scala jobs moving data from s3 to s3, and finally into EMR for the training step.
 
 
 ![](_dataops-and-mlops-for-reasonable-organizations_data/1_img)",H2,https://outerbounds.com/blog/dataops-mlops-reasonable-organizations#a-tale-of-two-cultures,False,1066.0,171.0
744,"Concerns for data semantics, pipeline duplication, and siloed intelligence","Before even discussing tools, we have three *organizational* problems to solve:
 
 
 * Lack of ownership in data *semantics*: raw data is, unsurprisingly, raw. Understanding the nuances of *any* ingestion protocol is no easy feat: if data engineers own just persistence, and not normalization, we are now asking ML and BI folks to go into the subtleties of the ingestion pipeline. This is error-prone as they may not know what is the correct definition of, say, a shopping session, given our cookie strategy, and an unwanted dependency: we want data teams to work with well-understood entities in our business domain (cart, products, recommendations) not to reinvent the wheel every time they run a query.
 * Duplication of data pipelines and storage, and proliferation of tools: we maintain two parallel pipelines, send data to S3 and Redshift (duplicating security, governance, etc.) and start a “my data / your data” dynamics: when a KPI for BI and ML is different, *how do we get to the truth*? Moreover, we now have Scala, SQL, Python, Spark, and an orchestrator (not shown) to master: if the BI team wants to help out the ML team, it’s going to be hard to even set up an environment for them.
 * Siloed intelligence: not only do bad things happen in *both pipelines* but good things cannot be shared; if the BI team has a strategy to calculate conversion rate, the ML team would be completely oblivious to that work, as it sits on a different code base and a different data source.",H2,https://outerbounds.com/blog/dataops-mlops-reasonable-organizations#concerns-for-data-semantics-pipeline-duplication-and-siloed-intelligence,False,1487.0,257.0
745,"The Solution: SQL, Python, and a Single Source of Truth","Our stack provides a much better blueprint for collaboration at reasonable scale:
 
 
 ![](_dataops-and-mlops-for-reasonable-organizations_data/og.png)
 First, we empower the Data Team to own raw data *and the first layer of transformation,* from JSON to basic entities in our business. Second, we all operate out of a Single Source of Truth (SSoT, i.e. Snowflake), so all insights come from the same place and lineage is easy. Third, we drastically cut down the number of tools to use: SQL and Python will get you all the way. Finally, we promote a culture of sharing, which is perfectly embodied by dbt Cloud: a lot of intermediate features may be in common between ML and BI, and they can collaborate effectively by building on each other’s work. If you look at the repository, for example, you’ll notice that we are using [dbt+Snowflake basically as the offline component](https://towardsdatascience.com/the-modern-data-pattern-d34d42216c81) of a feature store.
 
 
 Most of all, we found through everyday practice that this stack is the perfect playground for the [end-to-end data scientist](https://multithreaded.stitchfix.com/blog/2019/03/11/FullStackDS-Generalists/): the person that doesn’t [need to know kubernetes](https://outerbounds.com/blog/kubernetes-to-metaflow/), or the Google Ingestion protocol, but who is responsible for transforming data into models by iterating quickly and independently, going all the way from tables to endpoint and back. This stack makes her happy, as **it’s heavy on open-source and light on people’s time**: by encouraging data ownership, fostering collaboration, and abstracting away computation, our scientists can focus on the high-value, high-margin logic they want to write, to solve a business problem from start to finish. 
 
 
 In this post, we’ve shown how good tools provide a better way to think about the division of work and productivity, thus providing an organizational template for managers and data leaders. This was based upon the stack of tools used in [our previous post](https://outerbounds.com/blog/modern-data-stack-mlops), where we also introduced a technical template for teams starting up and wondering how to join DataOps and MLOps efficiently. We’ve also shared [an open-source repository](https://github.com/jacopotagliabue/post-modern-stack) embodying in working code our principles for data collaboration: in our experience, DataOps and MLOps are better done under the same principles, instead of “handing over” artifacts to the team on the other side of the fence.
 
 
 If these topics are of interest, come chat with us on [our community slack here](http://slack.outerbounds.co/).",H2,https://outerbounds.com/blog/dataops-mlops-reasonable-organizations#the-solution-sql-python-and-a-single-source-of-truth,False,2658.0,379.0
746,Using Stable Diffusion Locally and on Colab and their limitations,"There are several ways you can use Stable Diffusion via the Hugging Face Diffusers library. Two of the most straightforward are:
 
 
 * If you have GPUs on your laptop, you can use your local system to follow the instructions in [this Github repository README](https://github.com/huggingface/diffusers);
 * Using [Google Colab](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable_diffusion.ipynb) to leverage cloud-based GPUs for free with notebooks [such as this](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable_diffusion.ipynb).
 
 
 The 7 lines of code used to generate the “painting of Nicolas Cage as the Godfather” above in a Colab notebook were:
 
 
 ![](_parallelizing-stable-diffusion-production-use-cases_data/2_img.png)
 
 
 These two methods are both wonderful approaches to getting up and running and they allow you to generate images, but they are relatively slow. For example, with Colab, you’re rate-limited by Google’s free GPU provisioning to around 3 images per minute. This then begs the question: if you want to scale to use more GPUs, how do you do that trivially? On top of this, Colab notebooks and local computation are great for experimentation and exploration, but if you want to embed your Stable Diffusion model in a larger application, it isn’t clear how to use these tools to do so. 
 
 
 Moreover, when scaling to generate potentially orders of magnitude more images, versioning your models, runs, and images become increasingly important. This is not to cast shade on your local workstation or on Colab notebooks: they were never intended to achieve these goals and they do their jobs very well!
 
 
 But the question remains: how can we massively scale our Stable Diffusion image generation, version our models and images, and create a machine learning workflow that can be embedded in larger production applications? Metaflow to the rescue!",H2,https://outerbounds.com/blog/parallelizing-stable-diffusion-production-use-cases/#using-stable-diffusion-locally-and-on-colab-and-their-limitations,False,1955.0,281.0
747,Using Metaflow to Massively Parallelize Image Generation with Stable Diffusion,"Metaflow allows us to solve these challenges by providing an API that affords:
 
 
 * Massive parallelization via branching,
 * Versioning,
 * Production orchestration of data science and machine learning workflows, and
 * Visualization.
 
 
 The Metaflow API allows you to develop, test, and deploy machine learning workflows using the increasingly common abstraction of [directed acyclic graphs (DAGs)](https://outerbounds.com/docs/dags-in-data-science/), in which you define your workflow as a set of steps: the basic idea here is that, when it comes to generating images using Stable Diffusion, you have a branching workflow, in which
 
 
 * each branch is executed on different GPUs and
 * the branches are brought together in a join step.
 
 
 As an example, let’s say that we wanted to generate a large number of subject-style pairs: given a large number of subjects, it would make sense to parallelize the computation over the prompts. You can see how such branching works in the following schematic of a flow:
 
 ![](_parallelizing-stable-diffusion-production-use-cases_data/3_img.png)
 <Caption>Visualization of the flow that generates images</Caption>
 
 The key elements of the `generate_images` step are as follows (you can see the whole step in the repository [here](https://github.com/outerbounds/diffusion-metaflow/blob/master/meta_dynamic_prompts.py#L98)):
 
 ![](_parallelizing-stable-diffusion-production-use-cases_data/4_img)
 <Caption>Key elements of the generate_images step </Caption>
 
 To understand what’s happening in this code, first note that, when executing the Metaflow flow from the command line, the user has included the subjects and styles. For example:
 
 
 ![](_parallelizing-stable-diffusion-production-use-cases_data/5_img)
 <Caption> Command to run the image generation flow </Caption>
 
 The styles, subjects, and seeds (for reproducibility) are stored as a special type of Metaflow object called [Parameters](https://docs.metaflow.org/metaflow/basics#how-to-define-parameters-for-flows), which we can access throughout our flow using `self.styles`, `self.subjects`, and `self.seed`, respectively. More generally, instance variables such as `self.X` can be used in any flow step to create and access objects that can be passed between steps. For example, in our `start` step, we pack our random seeds and styles into the instance variable `self.style_rand_seeds` as follows:
 
 
 ![](_parallelizing-stable-diffusion-production-use-cases_data/6_img)
 As noted in the comments in the `generate_images` step, what we are doing is
 
 
 * extracting the seed and style that were passed from the `start` step in the instance variable `self.input` (the reason it’s `self.input` is due the branching from the `start`: for more technical details, check out [Metaflow’s `foreach`](https://docs.metaflow.org/metaflow/basics#foreach)),
 * combining subject and style to create a single sentence for each prompt, such as  subject “mahatma gandhi” and style ”van gogh” create the prompt “mahatma gandhi by van gogh”, and
 * downloading the model into the container, run the model on the prompt and save the images created by the model.
 
 
 Note that, in order to send compute to the cloud, all we needed to do is add the `@batch` decorator to the step `generate_images`. This affordance of Metaflow allows data scientists to rapidly switch between prototyping code and models and sending them to production, which closes the iteration loop between prototyping and productionizing models. In this case, we’re using AWS batch but you can use whichever cloud provider suits your organizational needs best.
 
 
 With respect to the entire flow, the devil is in the (computational) detail so let’s now have a look at what is happening in our Metaflow flow more generally, noting that we download the Stable Diffusion model from Hugging Face (HF) to our local workstation before running the flow. 
 
 
 1. `start` : [Local execution] We cache the HF model to a common data source (in our case, S3). Once cached, parallelly run the next steps based on the number of subjects/images;
 2. `generate_images`: [Remote execution] For each style, we run a docker container on a unique GPU that creates images for the subject+style combination;
 3. `paint_cards`: [Local Execution] For each style, we split the images generated into batches and generate visualizations with [Metaflow cards](https://docs.metaflow.org/metaflow/visualizing-results/easy-custom-reports-with-card-components) for each batch;
 4. `join_cards`: [Local Execution] We join the parallelized branches for the cards we generate;
 5. `join_styles`: [Local Execution] We join the parallelized branches for all styles;
 6. `end`: [Local Execution]: END step.
 
 ```mdx-code-block
 import ReactPlayer from 'react-player';
 ```
 
 
 After the `paint_cards` step has finished execution, the user can visit the Metaflow UI to inspect the images created by the model. The user can monitor the status of different tasks and their timelines:
 
 ```mdx-code-block
 <ReactPlayer playing muted loop url='/assets/stable-diff-mf-ui-demo.mp4' width=""800"" />
 ```
 <Caption> Exploring the Metaflow UI </Caption>
 
 You can also [explore results](https://demo.public.outerbounds.xyz/api/flows/DynamicPromptsToImages/runs/23/steps/end/tasks/1135/cards/f63bcdd867c3e6ab271e3f8ff9ac11a92801c401) in the Metaflow UI yourself and see the images we generated when we executed the code.
 
 Once the flow completes running, the user can use a Jupyter notebook to search and filter the images based on prompts and/or styles (we’ve provided [such a notebook](https://github.com/outerbounds/diffusion-metaflow/blob/master/analyze.ipynb) in the companion repository). As Metaflow versions all artifacts from different runs, the user can compare and contrast the results from multiple runs. This is key when scaling to generate 1,000s, if not 10s of 1000s, of images as versioning your models, runs, and images become increasingly important:
 
 ```mdx-code-block
 <ReactPlayer playing muted loop url='/assets/stable-diff-notebook.mp4' width=""800"" />
 ```",H2,https://outerbounds.com/blog/parallelizing-stable-diffusion-production-use-cases/#using-metaflow-to-massively-parallelize-image-generation-with-stable-diffusion,False,6109.0,835.0
748,Conclusion,"Stable Diffusion is a new and popular way of generating images from text prompts. In this post, we’ve seen how there are several ways to produce images using Stable Diffusion, such as Colab notebooks, that are great for exploration and experimentation, but that these do have limitations and that using Metaflow has the following affordances:
 
 
 * Parallelism, in that you can scale out your machine learning workflows to any cloud;
 * All MLOps building blocks are wrapped in a single convenient Pythonic interface (such as versioning, experiment tracking, workflows, and so on);
 * Most importantly, you can actually build a production-grade, highly-available, SLA-satisfying system or application using these building blocks. Hundreds of companies have done it before using Metaflow, so the solution is battle-hardened too.
 
 
 If these topics are of interest, you can come chat with us on [our community slack here](http://slack.outerbounds.co/).",H2,https://outerbounds.com/blog/parallelizing-stable-diffusion-production-use-cases/#conclusion,False,953.0,146.0
749,The Full Stack Experience,"Metaflow Sandboxes include [the full stack of data science infrastructure.](https://outerbounds.com/docs/infra-stack/) Each sandbox comes with:
 
 
 * **An auto-scaling Kubernetes cluster**, which allows you to [test scalable compute with Metaflow’s `@kubernetes`](https://docs.metaflow.org/metaflow/scaling-out-and-up/effortless-scaling-with-kubernetes) decorator.
 * **A private S3-based datastore**, built-in Metaflow.
 * **A metadata service for experiment tracking**, which keeps track of all Metaflow executions happening inside the sandbox. You can query any past results using [the Client API](https://docs.metaflow.org/metaflow/client).
 * **Metaflow GUI**, which allows you to monitor executions.
 * **In-browser VSCode editor** with an embedded tutorial, so you can learn Metaflow and create flows without leaving the browser.
 
 
 We will add new features in the sandboxes, like [Argo Workflows for workflow orchestration](https://docs.metaflow.org/going-to-production-with-metaflow/scheduling-metaflow-flows/scheduling-with-argo-workflows), over time. [Let us know on Slack](http://slack.outerbounds.co) what features you would like to see!
 
 
 You can run any Metaflow flows in the sandbox, [go through tutorials](https://outerbounds.com/docs/intro-tutorial-overview/), and even access publicly available datasets in the sandbox. By default, you have a few days to use the sandbox. If you need more time, just [let us know on Slack](http://slack.outerbounds.co).
 
 
 We hope that the sandbox convinces you to move to the Metaflow neighborhood and join hundreds of other forward-looking companies! Once you are done with your evaluation, you can deploy the same stack that’s available in the sandbox in your own cloud environment using [our CloudFormation and Terraform templates](https://outerbounds.com/docs/engineering-welcome/). Or, if you want to equip your organization with sandbox-like data science environments with enterprise-level scale, security, and policy guarantees, [schedule a call with us – we are happy to tell you more](https://meetings.hubspot.com/ville-tuulos)!",H2,https://outerbounds.com/blog/free-data-science-infrastructure-in-the-browser#the-full-stack-experience,False,2098.0,255.0
750,Hands-on experience with the full stack of ML infrastructure - no installation required,"Through our work with open-source [Metaflow](https://metaflow.org/), which was started at Netflix in 2017, we have had an opportunity to work and talk with hundreds of companies regarding machine learning infrastructure. A common theme across these organizations is that they want to make it quick and easy for their data scientists and ML engineers to promote their work from notebook-based prototypes to production-grade ML systems.
 
 Doing this is not trivial. Comprehensive tooling like that provided by Metaflow helps, but as with any sophisticated real-world activity, there’s no substitute for hands-on practice. However, practicing the whole infrastructure stack from scalable compute, event-based orchestration, versioning, and A/B testing is not easy for individuals who rarely have unhindered access to production-grade infrastructure.
 
 To address this gap, we have partnered with [CoRise](https://corise.com/), an e-learning platform targeting professionals. Our [new course](https://corise.com/course/full-stack-machine-learning-with-metaflow), which we taught for the first time in April 2023, covers the full-stack of ML infrastructure, allowing you to experience and experiment with modern ML and the infrastructure behind it directly from your browser, powered by open-source Metaflow, without having to install anything locally. The next course will start in August, so read on to learn more!
 
 <a href=""https://corise.com/course/full-stack-machine-learning-with-metaflow"">
 
 ![](/assets/corise_social_image.png)
 
 </a>
 
 ![](/assets/corise-logos.png)
 
 > “One standout feature of the class was the efficient use of Metaflow sandboxes to streamline the environment setup. This approach allowed participants to focus on learning rather than getting bogged down in complex configuration processes."" –Emily Ekdahl, Senior ML Engineer at Vouchsafe Insurance
 
 > “The Metaflow sandbox made it very easy for students to review content, run their projects, and take what they learned back to their colleagues. We had so many learners ask about how they could introduce Metaflow at work!” –Shreya Vora, course coordinator at CoRise
 
 Attending this course were data scientists, machine learning engineers, software engineers, data science managers, solutions architects, engineering managers, heads of data science, directors of ML, and ML consultants from Amazon, Heineken, YNAB, Sequoia, Fidelity Investments, Wadhwani AI, and more. Given such an array of participants and verticals, we thought it important to teach both the technical and business sides of things through real-world, hands-on projects. So how did we do this?",H2,https://outerbounds.com/blog/learn-full-stack-ml-corise#hands-on-experience-with-the-full-stack-of-ml-infrastructure-no-installation-required,False,2648.0,366.0
751,Get your hands dirty with real-world ML projects,"![](/assets/full-stack-stages.png)
 
 Moving between ML prototype and production is a spectrum, as opposed to an on-off switch. In the real-world projects you work on in this course, you get to experience many different elements of production ML, from using cloud workstations, interacting with updating data streams, and versioning your code, models, and data to scheduling execution, freezing dependencies, and A/B testing champion and challengers ML models.
 
 We made sure to focus on real-life scenarios and common failure patterns, derived from experiences from hundreds of real-life data science projects. _These projects involved a real production system with updating data and experiment tracking, leveraging state-of-the-art cloud infrastructure and schedulers, along with building custom and shareable dashboard to document results._
 
 ![](/assets/flow-code-stack-viz.png)
 
 As we are so excited about the projects, we wanted to share with you some of the first cohort’s submissions, along with additional context.",H2,https://outerbounds.com/blog/learn-full-stack-ml-corise#get-your-hands-dirty-with-real-world-ml-projects,False,1027.0,145.0
752,ML workflows and human-readable reports,"In week 1, participants build several machine learning workflows and create a custom report using Metaflow cards, with a view to thinking about how essential human-digestible reports are for production ML. Here’s an example of such a report, which includes exploratory data analysis and some model diagnostics:
 
 <iframe src=""https://cards.outerbounds.dev/index.html?id=pw-363809672%2Fd051424d56c84c98"" width={1000} height={800}></iframe>
 <Caption><br></br>A report of model prediction metrics produced by a student in the Full Stack Machine Learning Corise course. <br></br>Sign up to join the next cohort to make and share your own ML reports <a href=""https://corise.com/course/full-stack-machine-learning-with-metaflow"">here</a>!</Caption>",H3,https://outerbounds.com/blog/learn-full-stack-ml-corise#ml-workflows-and-human-readable-reports,False,744.0,87.0
753,"Production deep learning, versioning, parallel training, and the cloud","In week 2, participants dive deeper into modeling techniques in workflows, leveraging state-of-the-art deep learning, versioning with Metaflow, parallel training, and sending specific workflow steps to the cloud (those requiring significant compute, such as parallel training steps). These are all foundational aspects of moving ML workloads from prototype to production and the ability to send particular steps to cloud compute is becoming more and more important (just look at [large language models](https://outerbounds.com/blog/train-dolly-metaflow/) and [generative AI](https://outerbounds.com/blog/whisper-kubernetes/)!). Here’s an example of a custom report that a participant submitted:
 
 <iframe src=""https://cards.outerbounds.dev/index.html?id=pw-1437394648%2F89d287d72523468f"" width={1000} height={800}></iframe>
 
 <br></br>
 <br></br>",H3,https://outerbounds.com/blog/learn-full-stack-ml-corise#production-deep-learning-versioning-parallel-training-and-the-cloud,False,848.0,94.0
754,Model deployment and handling failures,"In week 3, things really heat up when we teach everyone how to deploy ML models to production using Metaflow, Argo, and Kubernetes. In their projects, participants deploy their ML models such that they would update hourly when new data became available! On top of this, you’ll need to deal with a set of commonly occurring failure modes when data is updated. Failure modes aren’t spoken about enough but dealing with them takes up so much of the real work!
 
 <iframe src=""https://cards.outerbounds.dev/index.html?id=pw-549130914%2F8803308530a74b3f"" width={1000} height={800}></iframe>
 
 <br></br>
 <br></br>
 
 Eagle eyes may observe that the model's performance on the R^2 metric got worse over time. Students are encouraged to think about this type of drift and how you use Metaflow and other tools from the ecosystem to identify and rectify this. If that sounds like a lot to cover in one week, well, it is, but it was aided by [Metaflow’s `trigger` decorator that allows you to trigger flows given arbitrary events in your products and ML systems](https://outerbounds.com/blog/metaflow-event-triggering/):
 
 ```mdx-code-block
 import ReactPlayer from 'react-player';
 ```
 
 <ReactPlayer playing controls muted loop url='/assets/et-basic-event.mp4' width='100%' height='100%'/>",H3,https://outerbounds.com/blog/learn-full-stack-ml-corise#model-deployment-and-handling-failures,False,1284.0,184.0
755,A/B-testing machine learning models,"In week 4, we teach participants how to run machine learning experiments while having models already in production. To do this, we demonstrate how to think about and deploy several types of predictions: batch, streaming, and real-time predictions. In their projects, participants deploy both champion and challenger models and A/B-tested them with respect to performance! This is an essential part of ML model maintenance as the real world is in constant flux, resulting in model and data drift. You can see here two custom reports, one for the champion model, the other for the challenger:
 
 <iframe src=""https://cards.outerbounds.dev/index.html?id=pw-535851483%2F63ce6f5b7cf14915"" width={1000} height={800}></iframe>
 
 <br></br>
 <br></br>
 
 <iframe src=""https://cards.outerbounds.dev/index.html?id=pw-535851483%2Fdfb770d9946f4860"" width={1000} height={800}></iframe>
 
 <br></br>
 <br></br>
 
 This is a subset of all the things you need to build ML-powered software that both (1) runs reliably without human supervision and (2) produces correct results.
 
 ![](/assets/example-stack-setup.png)
 
 Although we clearly couldn’t be comprehensive in this post with respect to the contents of the course, we hope that it gave a sense of what we cover.",H3,https://outerbounds.com/blog/learn-full-stack-ml-corise#a-b-testing-machine-learning-models,False,1253.0,170.0
756,Take a course and get in touch!,"We’re excited to be offering this course again in August and would love for you to [join us](https://corise.com/course/full-stack-machine-learning-with-metaflow) for it. As a faithful reader of our blog, you can use the code HUGOB10 for a 10% discount!
 
 As stated, we’ve already taught data scientists, machine learning engineers, and many others from companies such as Amazon, Heineken, YNAB, Sequoia, Fidelity Investments, and Wadhwani AI. Add this to the hundreds of companies we’ve worked with since open-sourcing Metaflow at Netflix in 2017 and the Metaflow sandboxes, sometimes we wish we could take the course, in addition to teaching it!
 
 ![](/blog/og/learn-full-stack-ml-corise.gif)
 
 <a href=""https://corise.com/course/full-stack-machine-learning-with-metaflow"">
 
 ![](/assets/corise_social_image.png)
 
 </a>
 
 If you’d like to get in touch or chat about full-stack ML, come say hi to thousands of data scientists on our community [Slack](http://slack.outerbounds.co/)! 👋",H2,https://outerbounds.com/blog/learn-full-stack-ml-corise#take-a-course-and-get-in-touch,False,989.0,133.0
757,How: Wrapping the Stack for Maximum Usability,"Imagine choosing a production-grade solution for each layer of the stack: for instance, Snowflake for data, Kubernetes for compute (container orchestration), and Argo for workflow orchestration. While each system does a good job in its own domain, it is not trivial to build a data-intensive application that has cross-cutting concerns touching all the foundational layers. In addition, you have to layer the higher-level concerns from versioning to model development on top of the already complex stack. It is not realistic to ask a data scientist to prototype quickly and deploy to production with confidence using such a contraption. Adding more YAML to cover cracks in the stack is not an adequate solution.
 
 
 Many data-centric environments of the previous generation, such as Excel and RStudio, really shine at maximizing usability and developer productivity. Optimally, we could wrap the production-grade infrastructure stack inside a developer-oriented user interface. Such an interface should allow the data scientist to focus on concerns that are most relevant for them, namely the topmost layers of the stack while abstracting away the foundational layers.
 
 
 The combination of a production-grade core and a user-friendly shell makes sure that ML applications can be prototyped rapidly, deployed to production, and brought back to the prototyping environment for continuous improvement. The iteration cycles should be measured in hours or days, not in months.
 
 
 ![](_practical-ml-infra-stack_data/1_img)
 [**Metaflow**](https://docs.metaflow.org/) is an open-source framework, originally developed at Netflix and now supported here at Outerbounds, specifically designed to address this concern: How can we wrap robust production infrastructure in a single coherent, easy-to-use interface for data scientists? Under the hood, Metaflow integrates with best-of-breed production infrastructure, such as Kubernetes and AWS Step Functions, while providing a development experience that draws inspiration from data-centric programming, that is, by treating local prototyping as the first-class citizen.
 
 
 When evaluating solutions, consider focusing on the three key dimensions covered in this series of posts:
 
 
 1. **Does the solution provide a delightful user experience for data scientists and ML engineers?** There is no fundamental reason why data scientists should accept a worse level of productivity than is achievable with existing data-centric tools.
 2. **Does the solution provide first-class support for rapid iterative development and frictionless A/B testing?** It should be easy to take projects quickly from prototype to production and back, so production issues can be reproduced and debugged locally.
 3. **Does the solution integrate with your existing infrastructure, in particular to the foundational data, compute, and orchestration layers?** It is not productive to operate ML as an island. When it comes to operating ML in production, it is beneficial to be able to leverage existing production tooling for observability and deployments, for example, as much as possible.
 
 
 It is safe to say that all existing solutions still have room for improvement. Yet it seems inevitable that over the next five years the whole stack will mature, and the user experience will converge towards and eventually beyond the best data-centric IDEs.  Businesses will learn how to create value with ML similar to traditional software engineering and empirical, data-driven development will take its place amongst other ubiquitous software development paradigms.
 
 
 If you’d like to get in touch or discuss such issues, [come say hi on our community Slack](http://slack.outerbounds.co/)! 👋",H2,https://outerbounds.com/blog/practical-ml-infra-stack/#how-wrapping-the-stack-for-maximum-usability,False,3717.0,549.0
758,Why write a book about data science infrastructure?,"When [Manning, a well known publisher of technical books](https://manning.com), approached me in April 2020 about writing a new book about “doing ML with Python”, I had two immediate reactions (having written a book [once before](https://www.amazon.com/Mobile-Python-prototyping-applications-platform/dp/0470515058)): First, writing a book takes a crazy amount of work, so rationally my answer should be *no way*. Second, do technical books even matter these days? Much of the world’s knowledge is available online for free.
 
 I had given a number of presentations about how to make data scientists more productive and more effective at developing and deploying machine learning applications in general, and our [open-source project Metaflow](https://metaflow.org) in particular. After every such presentation, I had a nagging feeling that I had only scratched the surface. Is the audience really able to apply the ideas at home based on a few slides? While general principles and pretty high-level figures are useful, the devil really is in the details.
 
 I had seen that ML systems come in many shapes and sizes. The most successful systems are developed by autonomous teams who have a deep understanding of their problem domain, as well as a good enough grasp of the technical stack that powers the system.
 
 There’s a plethora of excellent books and papers available about various domains of ML, say, recommender systems, computer vision, and forecasting, but it was – and still is – clear that [the world needs more education about the infrastructure stack](https://outerbounds.com/docs/data-science-welcome) which helps turn these ideas from academic papers into living and evolving systems.
 
 ![](_book-launch-effective-data-science-infrastructure_data/0_img)
 
 Hence over the locked-down summer of 2020 the idea of a long-form treatise of the full stack of data science infrastructure started feeling enticing. The material wouldn’t be freely available online, which is clearly a downside, but on the other hand a book is a time-tested, effective medium for developing a deep understanding of a new field, which is not easily achieved by browsing random web pages.
 
 The book that grew over the next two years, Effective Data Science Infrastructure, distills a decade of experience by me and tens of data scientists and platform engineers from Netflix and other companies who have built and operated business-critical, large-scale ML systems. Compressing all the concepts and learnings into a readable book took a number of iterations but the result – and the journey of getting there – was made infinitely better thanks to my experienced editorial team at Manning, Doug Rudder and Nick Watts in particular.",H2,https://outerbounds.com/blog/book-launch-effective-data-science-infrastructure#why-write-a-book-about-data-science-infrastructure,False,2721.0,412.0
759,What do you learn by reading this book?,"**The book provides a systematic walk-through of everything you need to know to design and deploy a modern infrastructure stack for machine learning and data science applications.**
 
 The name has an important qualifier, *effective*, signaling that the end goal is to produce positive business impact with the infrastructure, not to build it for its own sake. The tagline, *how to make data scientists productive*, signals that ultimately the impact is produced by humans, data scientists, not by the infrastructure itself, so usability, ergonomics, and human-centricity are overarching topics in the book.
 
 
 The book advocates for the idea that data-intensive applications require a whole stack of infrastructure, as depicted in this figure:
 
 
 ![](_book-launch-effective-data-science-infrastructure_data/1_img)
 You need to consider *data*, how and where to process the data exactly (*compute resources*), and how to orchestrate processing that consists of multiple stages (*job scheduler*), while acknowledging the fact that it will take multiple versions and iterations to produce the desired outcome (*versioning*). Fortunately [you don’t need a Google-scale solution](https://outerbounds.com/blog/dataops-mlops-reasonable-organizations) from the get-go, but as the book shows, you can develop a solution that grows with your organization over time.
 
 
 These concerns provide just the technical foundations – the machinery. On top of this, we need people, the data scientists who are often not software engineers by training, to *architect* the applications. Many organizations have realized that in order to increase the speed of iteration and improvement, it is convenient to give the same people tools to *operate* the applications to a large degree as well. Finally, we need to tap into the core expertise of data scientists when it comes to ML-specific issues like *feature engineering* and *model development*.
 
 
 As shown by the triangles in the figure, an effective infrastructure needs to strike a balance between the needs of humans and the machines. All these topics are covered by the book, as it is hard to operate an effective data science organization without addressing all these concerns systematically. Unsurprisingly given my background the book uses [Metaflow](https://metaflow.org) to illustrate the ideas, but you can apply the concepts to other frameworks as well.",H2,https://outerbounds.com/blog/book-launch-effective-data-science-infrastructure#what-do-you-learn-by-reading-this-book,False,2402.0,355.0
760,Learn by doing,"Although the book doesn’t teach you machine learning (many great books do that already), I wanted to ground the discussion into realistic examples. In the course of reading the book, you get to build tiny applications that utilize unsupervised, supervised, and deep learning, natural language processing, visualizations, time-series forecasting. And, of course, you get to build a fully functional movie recommendation system!
 
 Thanks to Metaflow, the examples are rather concise and readable, like this one that performs a hyperparameter sweep for K-means clustering, optionally leveraging parallel compute in the cloud:
 
 
 ```python
 from metaflow import FlowSpec, step, Parameter, resources, conda_base, profile
 
 @conda_base(python='3.8.3', libraries={'scikit-learn': '0.24.1'})
 class ManyKmeansFlow(FlowSpec):
 
     num_docs = Parameter('num-docs', help='Number of documents', default=1000000)
     
     @resources(memory=4000)
     @step
     def start(self):
         import scale_data
         docs = scale_data.load_yelp_reviews(self.num_docs)
         self.mtx, self.cols = scale_data.make_matrix(docs)
         self.k_params = list(range(5, 55, 5))
         self.next(self.train_kmeans, foreach='k_params')
 
     @resources(cpu=4, memory=4000)
     @step
     def train_kmeans(self):
         from sklearn.cluster import KMeans
         self.k = self.input
         with profile('k-means'):
             kmeans = KMeans(n_clusters=self.k, verbose=1, n_init=1)
             kmeans.fit(self.mtx)
         self.clusters = kmeans.labels_
         self.next(self.analyze)
         
     @step
     def analyze(self):
         from analyze_kmeans import top_words
         self.top = top_words(self.k, self.clusters, self.mtx, self.cols)
         self.next(self.join)
     
     @step
     def join(self, inputs):
         self.top = {inp.k: inp.top for inp in inputs}
         self.next(self.end)
         
     @step
     def end(self):
         pass
 
 if __name__ == '__main__':
     ManyKmeansFlow()
 ```
 
 If you are curious, you can find all code snippets featured in the book in [this GitHub repository](https://github.com/outerbounds/dsbook/).",H2,https://outerbounds.com/blog/book-launch-effective-data-science-infrastructure#learn-by-doing,False,2167.0,455.0
761,Is this book for me?,"Designing, building, operating, and using effective data science infrastructure is a team sport. The book is written from the perspective of two protagonists: Alex the data scientist and Bowie the platform engineer. If you identify with either or both of these characters, you should read the book.
 
 
 To make sure that you can actually learn practical, real-life skills and knowledge from the book, all sections are prefaced with a real-world scenario which often involve both Alex and Bowie. Here are a few examples to give you an idea:
 
 
 ![](_book-launch-effective-data-science-infrastructure_data/2_img)
 This figure is from Chapter 5, which introduces cloud-based compute layers like Kubernetes and AWS Batch. You will learn how to set up these systems in such a way that Alex the data scientist doesn’t have to worry about them too much – they can just benefit from the cloud compute that allows them to experiment freely – while the operational burden is minimized for Bowie the engineer.
 
 
 Similarly converting raw data to features in a reliable manner, at scale, takes some coordination, as described in Chapter 8 and 9:
 
 
 ![](_book-launch-effective-data-science-infrastructure_data/3_img)
 Even when all the foundational infrastructure is humming reliably, data scientists need tooling and support for version control and experiment tracking. Reproducibility in a business environment is not just a noble goal but a requirement for effective teamwork, as discussed in Chapter 6:
 
 
 ![](_book-launch-effective-data-science-infrastructure_data/4_img)
 Drawing these illustrations and over 100 others was one of the most fun parts of my authorship.",H2,https://outerbounds.com/blog/book-launch-effective-data-science-infrastructure#is-this-book-for-me,False,1668.0,251.0
762,What next?,"Go and buy the book at [the Manning web site](https://www.manning.com/books/effective-data-science-infrastructure) (or [Amazon](https://www.amazon.com/Effective-Data-Science-Infrastructure-scientists/dp/1617299197))! Besides supporting the hard-working folks at Manning, your money will go to support underrepresented groups in data science, so even if you don’t like the book, your money won’t be wasted! If you feel extra generous, please leave a review on either or both of those sites 🙏.
 
 
 I would love to hear your feedback, thoughts, and questions. [Join our Slack](http://slack.outerbounds.co) to chat with me (@ville – DM me on Slack for a discount code) and over 1500 other Alexes and Bowies who design and use effective data science infrastructure of various kinds daily 🐶. The book is just a blueprint. Our whole community is there to help you during your learning journey and when you get to operate your stack in practice.
 
 
 Finally, we are organizing an exciting live-streamed event that will feature myself as well as another amazing author, [Chip Huyen, who also recently published a book](https://www.amazon.com/Designing-Machine-Learning-Systems-Production-Ready/dp/1098107969) about these topics. We will discuss questions related to writing technical books, educating the world about ML, and data science infrastructure in general – as well as share an exciting product announcement!  [Register today so you won’t miss it!](https://www.eventbrite.com/e/machine-learning-infrastructure-for-humans-tickets-410635119697) 😊",H2,https://outerbounds.com/blog/book-launch-effective-data-science-infrastructure#what-next,False,1545.0,205.0
763,Engineering systems with Metaflow,"There is a tried and true way to engineer maintainable, complex systems: Build them from robust, composable components iteratively, adding complexity only when necessary. Each component is a functional unit with a clearly defined boundary, so they can be developed and tested independently, and finally, assembled together in creative ways.
 
 Metaflow supports three levels of composition:
 
 ![](_event-triggering/et-system.png)",H2,https://outerbounds.com/blog/metaflow-event-triggering#engineering-systems-with-metaflow,False,430.0,59.0
764,New in Metaflow 2.9: Event Triggering,"[The new event triggering feature](https://docs.metaflow.org/production/event-triggering), now available in Metaflow 2.9, enables the third pattern of decoupled, system-level composition. For years, this pattern has been [used at Netflix to power essentially all Metaflow flows](https://netflixtechblog.com/orchestrating-data-ml-workflows-at-scale-with-netflix-maestro-aaa2b41b800c) in production, so we are very excited to support it in open-source finally!
 
 Following the human-centric philosophy of Metaflow, powerful features come in a simple package. You don’t need to be an expert in systems engineering to be able to leverage event triggering, as demonstrated by the examples below. For a more comprehensive overview, see [our new documentation for event-triggering](https://docs.metaflow.org/production/event-triggering).",H2,https://outerbounds.com/blog/metaflow-event-triggering#new-in-metaflow-29-event-triggering,False,831.0,94.0
765,Trigger based on an external event,"Let’s start with a common scenario: Triggering a workflow whenever new data appears in the data warehouse:
 
 ```mdx-code-block
 import ReactPlayer from 'react-player';
 ```
 
 <ReactPlayer playing controls muted loop url='/assets/et-basic-event.mp4' width='100%' height='100%'/>
 
 To trigger a workflow based on an external event, just add a `@trigger` decorator to your flow:
 
 ```python
 @trigger(event='data_updated')
 class FreshDataFlow(FlowSpec):
 	...
 ```
 
 This decorator instructs Metaflow to start the workflow whenever an instance of the event `data_updated` appears in the event queue. To push an event in the queue, say, in an ETL pipeline, use the following two lines:
 	
 ```python
 from metaflow.integrations import ArgoEvent
 ArgoEvent(name='data_updated').publish()
 ```
 
 For instance, you can call this line at the end of an ETL pipeline, bridging data engineering and data science.",H3,https://outerbounds.com/blog/metaflow-event-triggering#trigger-based-on-an-external-event,False,908.0,125.0
766,Flows of flows,"The recommendation example above highlighted a core pattern: Chaining workflows together so that the completion of one workflow triggers another automatically:
 
 <ReactPlayer playing controls muted loop url='/assets/et-flows.mp4' width='100%' height='100%'/>
 
 Given the importance of this pattern, Metaflow provides dedicated support for it in the form a decorator,
 `@trigger_on_finish`. Here is how to make SecondFlow trigger automatically upon completion of FirstFlow:
 
 ```python
 @trigger_on_finish(flow='FirstFlow')
 class SecondFlow(FlowSpec):
 	...
 ```
 
 You don’t need to create events explicitly to chain workflows this way, as Metaflow creates completion events automatically.
 
 Crucially, the owner of FirstFlow doesn’t need to know that SecondFlow depends on it. In other words, a workflow, like the embedding flow mentioned earlier in this article, can have multiple consumers that are fully decoupled from it.
 Workflow owners may develop and iterate on their workflows autonomously without excessive coordination and communication overhead.",H3,https://outerbounds.com/blog/metaflow-event-triggering#flows-of-flows,False,1063.0,142.0
767,Passing data across flows,"Besides triggering execution, you need to be able to pass data and results fluidly across flows. Consider this scenario: You want to refresh a model with a snapshot of historical data whenever new data is available. After this, you want to use the fresh model to produce predictions for the latest data.
 
 It is natural to split the training and inference part into two separate flows. Organizationally, it allows the flows
 to be developed by separate people. Also, it provides a clean separation between outside-facing inference flows (like in the recsys example above) and internal training flows, which may run at their own cadence.
 
 To produce predictions, InferenceModel needs to access the latest model version produced by TrainingFlow:
 
 <ReactPlayer playing controls muted loop url='/assets/et-combo.mp4' width='100%' height='100%'/>
 
 As shown above, TrainingFlow can use `@trigger` to trigger execution when new data is available, and SecondFlow can use `@trigger_on_finish` to trigger execution upon completion of TrainingFlow.
 
 Metaflow makes it easy to access data across flows. In this case, SecondFlow can access the model trained by TrainingFlow simply by referring to it as follows:
 
 ```python
 self.model = current.trigger.run.data.model
 ```",H3,https://outerbounds.com/blog/metaflow-event-triggering#passing-data-across-flows,False,1270.0,191.0
768,Experiment with confidence,"Imagine that you set up a chain of TrainingFlow and InferenceFlow as outlined above. The chain of flows works well in production but eventually you may want to experiment with a new model architecture.
 
 If this was a business critical application, surely you would not dare to deploy the new challenger model in production without a comprehensive evaluation. Optimally, you would like to run the new model side-by-side
 with the existing model, conducting A/B testing, using real production data to get an accurate read.
 
 Managing this level of complexity causes headaches with many platforms, as you need to support multiple concurrent, isolated variants of chains of workflows.
 
 <ReactPlayer playing controls muted loop url='/assets/et-variants.mp4' width='100%' height='100%'/>
 
 Metaflow supports this in a straightforward manner. Simply add the `@project` decorator in every flow:
 
 ```python
 @trigger_on_finish(flow='FirstFlow')
 @project(name='variant_demo')
 class SecondFlow(FlowSpec):
 	...
 ```
 
 and deploy the challenger chain in a separate branch:
 
 ```
 python secondflow.py --branch new_model argo-workflows deploy
 ```
 
 The `@project` decorator sets up namespaces so that data, models, and triggering events are fully isolated in each variant, making sure that the experimental version can't interfere with the production environment.",H3,https://outerbounds.com/blog/metaflow-event-triggering#experiment-with-confidence,False,1364.0,193.0
769,Summary,"As demonstrated above, event triggering provides a principled, human-friendly way to compose advanced ML and data systems from multiple interrelated workflows that can react to events in real-time. The feature allows you to manage complexity, providing a well-defined way to divide work across teams and to compose systems from independent components.
 
 The end result is a fully functional system with a clear operational runbook. You can deploy and test variants of workflows concurrently, each variant being safely managed by Metaflow. You can operate the system with a high-level of observability, thanks to end-to-end lineage of artifacts and events which are automatically tracked by Metaflow.",H2,https://outerbounds.com/blog/metaflow-event-triggering#summary,False,700.0,105.0
770,Next steps,"You can start using event triggering right away! Here’s what to do next:
 
 1. [Sign up for a Metaflow sandbox](https://outerbounds.com/sandbox) to see and test event triggering in action without having to install anything locally. We have updated the onboarding lessons to include a new section that showcases event-triggering in action.
 2. Learn more in our new [documentation for event triggering](https://docs.metaflow.org/production/event-triggering).
 3. Once you are ready to start using the feature in production in your own environment, you can get going with [our standard deployment templates for Kubernetes](https://outerbounds.com/engineering/welcome/).
 
   For business-critical use cases, consider adopting [the Outerbounds Platform that provides the complete Metaflow stack as a managed service](https://outerbounds.com/platform/), including a highly available event backend.
 
 If you have questions or need help with anything, you can [join us and thousands of other data scientists and engineers on Metaflow Slack](http://slack.outerbounds.co)!",H2,https://outerbounds.com/blog/metaflow-event-triggering#next-steps,False,1065.0,141.0
771,What even are Large Language Models?,"**Hugo**: What are large language models and what are they capable of?
 
 
 **Federico**: There are different kinds of language models: we have large and even larger language models. The easiest way to explain what a language model is is a tool that does next-word prediction as a task. So given a sentence, or even a question, what the model is going to do, is going to add novel words that are predicted on the basis of the sentence you started with. So the model is going to output the most likely most probable sentence… even questions. So this is a very simple kind of process but you can do many things with this kind of next word prediction task.
 
 You can think, if you ever tried one of these Chat-GPTs, for example, the idea behind it is that when you ask a question, the model is going out with new words that make sense. And people have built tons of cool things. And I think that our thoughts of application of this kind of technologies… question answering is, indeed, a very cool application, you can use this kind of tool to ask questions and get an answer on many different topics, you can use them as, sparring partners, to learn about new things. 
 
 So you can use them to prepare for interviews or to discover new topics. But also different kinds of applications, for example, one large language model that I use a lot is Github Copilot which does next-word prediction for, let’s say, Python code, at least for me, so it helps you to write code. It actually increases my productivity a lot. 
 
 
 **Hugo**: It was nice that you talked through why we can get things like Chat-GPT because, if you have next-word prediction, it isn't initially obvious why you can then have Question and Answer pairs, right?
 
 
 **Federico**: Yeah. It requires a bit of thinking to understand why this next word prediction thing is the foundation part of this question and answer pipeline, but even also, like of asking a model to write emails or asking a model to summarize documents, is still this kind of next-word prediction task. There is obviously light modification to the training pipeline of these models, but the basic component is still this kind of next-word prediction.
 
 **Hugo**: Digging a bit deeper, maybe you could say a bit about what's happening in the backend. I mean, we have the neural networks and deep learning of a particular form, we hear the term transformer kind of thrown around…
 
 
 **Federico**: Of course, so the transformer is an architecture that was introduced in 2017 originally for machine translation.. This was a broad shift in the NLP community. And the general idea is that the transformer is an architecture that takes input sentences and through the use of self-attention, that is a component that helps the model to look at all the different words in a sentence through many layers of like that will be reflected, the model can build up kind of an understanding of how all the words relate to each other. 
 
 Through this kind of pipeline, the model looks at the different words in the sentence to understand what comes next. It's a very efficient computational unit that is very useful for this kind of task. I think one of the biggest contributions of the transformer architecture is its efficiency in doing computation. At least this is the most valuable contribution to the field as we have LSTMs before towards the late teens. But they had the limitation of a rolling kind of characteristic of the network, because they were sequential, and they could not be processed in touch. So transformers bring an efficient aspect to the computation of sentences that completely changed the way we do NLP.
 
 ```mdx-code-block
 <div className=""video-container"">
 <LiteYouTubeEmbed
         id=""MZIFkkwSt8g""
         title=""What even are Large Language Models?""
 />
 </div>
 ```",H2,https://outerbounds.com/blog/llm-value-and-risk#what-even-are-large-language-models,False,3821.0,680.0
772,Managing Expectations of LLMs,"**Hugo**: What aren't LLMs capable of?
 
 **Federico**: They cannot be trusted, in general, for the answers they give. So there is a concern regarding how factual the information they generate is. For example, the first time people asked Chat-GPT for references about topics they were interested in and Chat-GPT started making up references about things. And that's a huge problem because it gives a sense of confidence in the answer by saying you can look up this reference to trust what I just said. But that reference does not exist. So there are issues with that way like the factuality of what the model says. And that's kind of a big issue for many use cases. 
 
 Because if you don't, if you can't be sure about the output of the model, you cannot trust it in most of the settings… or complex reasoning tasks might still be out of the domain, we will need to see a couple of more things when the entire GPT-4 is going to be released, for example, the one that also combined the multimodal part that is not yet out. And also like unclear at least for GPT. There has been some concern about the actual evaluation. So GPT has been published with some evaluation regarding, for example, the ability to solve coding problems. But there have been some concerns that the model has been trained on the test set that was actually used to evaluate it. 
 
 So it's sometimes not clear how much we can trust the motivation. There might be some concerns about how well it doesn't do specific tasks. So the biggest question I'm getting is how do we undo the failure with failure cases? And the other kind of like, limits or is not really an issue with capability, but something that we are probably going to talk about is that there are cases in which you can not consider the language model safe. Because people have found ways to hop into the language model and make it out or like figure out two things that the original developer tried to prevent in the model, such as toxicity.
 
 
 **Hugo**: So just to recap, the generating factual information is incredibly difficult. Complex reasoning is tough. How to handle failure cases, also that we shouldn't consider it safe. … there's the serious issue of toxicity.",H2,https://outerbounds.com/blog/llm-value-and-risk#managing-expectations-of-llms,False,2207.0,399.0
773,"How to think about the space of LLMs, given there are so many!","**Hugo**: We've been talking about Chat-GPT but there are so many other LLMs out there. Can you give us a rundown of the space of LLMs and how to think about it?
 
  
 **Federico**: The first thing to say is that most of these things came out in the last three months.
 
 **Hugo**: That's so that's one of the wildest parts of it, right?
 
 **Federico**: I'm also not able to keep up with all these things. It's very difficult to keep track. And that is why I think we need to wait a couple of months, probably to understand what is gonna stick? And what are different ways to think about the entire space? One is in terms of open and closed models. Chat-GPT, for example, is a closed model. And you cannot access it, you need to call it by using Open API's API. And that might be something you don't want to do, but it’s high quality. So that's definitely an advantage of the model. We have right now open source alternatives. 
 
 So things charge up that was like around December 2022. There has been kind of like a wave. Very strong way to generate competitors to Chat-GPT that are indeed open source. There has been a lot of work in generating alternative language models. For example, one is Llama, that is the language model that was trained by Meta a couple of months ago. I don't remember when it came out. But people have started building Chat-GPT-like on Llama. And right now we have like, I think I've been Facebook chat is based on that. We have GPT.-J, that is a bit older, I think it's a couple of years older, I guess, that has been built by Eleuther AI… and again, it's open source. And the good thing about this model is that is also it has a commercial license. 
 
 So yes, it can be used by people, Dolly, that is a very recent model has been trained using GPT-J, and has been fine-tuned on instructions. And what does that mean? It means that, when you train a large language model and just the next word prediction tasks, you end up with something that is not as useful as it could be, what you want is a model that kind of behaves well when you give it instruction. So a model like Dolly, or a fine-tuned instruction model is fine-tuned using instructions that are like questions and answer, or, for example, training a sample for this model, or summarize this text in the text. And the output is the summary. Given some example of this kind of task, the model learns to do task in a much better way. So when you actually deploy it, it works very well to do.",H2,https://outerbounds.com/blog/llm-value-and-risk#how-to-think-about-the-space-of-llms-given-there-are-so-many,False,2480.0,473.0
774,How to Get Started with LLMs Today,"```mdx-code-block
 <div className=""video-container"">
 <LiteYouTubeEmbed
         id=""vMuWK9BcbJ8""
         title=""How to get started with LLMs today?""
 />
 </div>
 ```
 <br></br>
 
 **Hugo**: So I presume people know how to go to OpenAI and start interacting with chat-GPT. Right. But I'm wondering for people, you know, programmers and scientists and data scientists and ml people who maybe haven't had a lot of experience with it, how can they get started with playing around with large language models today?
 
 
 **Federico**: There are all kinds of different levels of abstraction in terms of how much code you're willing to write and how much time you have to put into understanding what's going on behind the scenes, I guess because then I will see like different methods to interact with a language model. There are different kinds of pipelines to train a language model if you have data. So I think like the main reserves, if you want to understand a bit what's going on without like, too deep into the kind of entire difficulties check out HuggingFace in which you can go through the fine-tuning of a supervised language model, for example, classification tasks, that's going to give you like a very good intro on how to actually add your own making small, large language models, on your own laptop doing things. 
 
 And that could be already good enough to do many different tasks. It's very controllable, you can do whatever you want with it. And it's usually run on consumer hardware. Most of the time, it might be slow, but at least it's enough to start playing with it. 
 
 Right now, with all these huge developments that are a lot of packages, that wrap APIs for you that are coming out, for example, there is Langchain that is kind of an abstraction that you can use to seamlessly let me use multiple language models, without having to delve into different kinds of setups, different kind of systems. So Langchain is another thing that I think it's like, very interesting to look at. It's empirical. Yeah, like, it's a good engineering effort to make language models accessible to the broader audience. And finally, if you what to maybe write some code that is mostly API, I think that using Cohere’s, or like OpenAI’s APIs are still an effective way to work with language models. 
 
 As you can prompt for questions asked for clarification, there is an entire field that is called prompt engineering which is the art of asking questions to language models, that is becoming more and more relevant. There are courses coming up that are that have been like, advertise everywhere on most social media platforms. And this is the direction of like, talking directly with the model using, for example, API's.",H2,https://outerbounds.com/blog/llm-value-and-risk#how-to-get-started-with-llms-today,False,2721.0,481.0
775,Productionizing LLMs and incorporating them into pre-existing software stacks,"```mdx-code-block
 <div className=""video-container"">
 <LiteYouTubeEmbed
         id=""ctdaxfZOTcE""
         title=""LLMs in production""
 />
 </div>
 ```
 <br></br>
 
 **Hugo**: How do you actually productionize LLMs? And how do you incorporate the wisdom of LLMs into existing software stacks? 
 
 **Federico**: Yeah, I think currently, the best way to think about this is not to use LLMs as core components of departments as the field is moving so fast. We just cannot always trust these pipelines. And even as a researcher, we are struggling to understand this, in terms of like how they work, what I would, what I think it's it's really useful is to make use them as kind of like analytics tools, for maybe your data, or even classification tools in which you can use them. Like I don't know, if you have a conversational agent, you could use one of these larger language models for intent classification. And you can do predictive analytics, you can do many things, the things I think, like you can replace many steps of a pipeline without language model… than you can use them for control flow, you can use them to do many things. 
 
 One thing I think, like we are probably going to talk about next that I think it's very, very important that I want to mention it now is that we should treat LLMs as we treat software in the sense that we have QA to test software, or like to stress test, and we should do the same language model. So there should be someone in the team that tries to break the part of the pipeline we built with LLMs, because we know that there might be an issue. And while software is often rule-based, so we can come up with reasonable flows or things that can happen, this is not true with language models that are by nature stochastic so they can come up with kind of different replies. 
 
 And this kind of product evolves over time. So the original Chat-GPT is not the Chat-GPT we have today, if you use the original Chat-GPT API, and you update it, you might not have the same result. So that could be like an app-breaking update in your current stack.",H2,https://outerbounds.com/blog/llm-value-and-risk#productionizing-llms-and-incorporating-them-into-pre-existing-software-stacks,False,2080.0,389.0
776,Generating Business Value with LLMs,"**Hugo**: I am interested in thinking through… at least initial thoughts on how LLMs can go beyond proof of concept and be used to create sustainable business value. Do you have any thoughts on that?
 
 
 **Federico**: I think we need to wait a bit to see what’s going to come up in the next couple of months. What I see are cool applications, obviously. But then we don't know how well they are going to generalize. I still think that like, some of these things are already useful, like a large language model for classification. It's already very possible that they can improve a lot of aspects in your conversational pipeline, for example, that's definitely something when I see that I'm not sure what's going to come up next is more like the front end, start with a conversational AI model by itself backed by Chat-GPT. that is something I still have some restraint on saying that it's gonna be the next application, because of the fact that there is no regulation… there are a lot of issues with respect to safety. 
 
 We are also building low-scale models that everybody can now use and implement. So the problem a couple of months ago was that the only thing that was out there was OpenAI. And it was big, you have to pay for it, and you could not use it on your own platform. What we are seeing in the last couple of weeks and months is that a lot of models can run on kind of consumer hardware and be used and loaded into your own software…here is obviously an analysis of the costs of having this kind of platform running. But still, I think we are moving also in a possible direction in which you can build your own small LMMs-based business platform that you can use to generate value.",H2,https://outerbounds.com/blog/llm-value-and-risk#generating-business-value-with-llms,False,1697.0,316.0
777,Join us for more chats!,"After our fireside chats, we have async AMAs with our guests on [our community slack](http://slack.outerbounds.co/). A lot tends to happen there so join us if you’re interested in such conversations!
 
 You can join us on [Slack](http://slack.outerbounds.co/) for more such conversations and can view the other fireside chats [here](https://www.youtube.com/playlist?list=PLUsOvkBBnJBeL8tsNGMdDKQjjOdU29sME).",H2,https://outerbounds.com/blog/llm-value-and-risk#join-us-for-more-chats,False,407.0,49.0
778,A brief history of open-source foundation models,"Over the past six months, a diverse garden of variously fine-tuned LLMs have emerged based on a few foundation models. This rapid evolution is driven by the fact that training a new foundation model from scratch can cost anywhere between $300k-$1M using cloud TPUs or GPUs. In contrast, fine-tuning a specialized version of a foundation model is many orders of magnitude cheaper. For instance, the Dolly model used in this article can be trained in a few hours, costing around $500-$1k.
 
 
 The following diagram illustrates the sprawling lineage of these models:
 
 
 ![](_train-dolly-metaflow_data/llm-family.png)
 <Caption> 
 Source: <a href=""https://github.com/michaelthwan/llm_family_chart/"">github.com/michaelthwan/llm_family_chart</a>
 </Caption>
 
 
 Since the original Dolly, Databricks has already followed with [Dolly 2.0](https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm), which is based on a different model and makes Dolly 2.0 commercially usable by using an internally curated fine-tuning dataset. Both Dolly versions are derived from a source model built by the team at [Eleuther AI](https://www.eleuther.ai/). In the case of the first Dolly, the 6 billion parameter model is called GPT-J, where Dolly 2.0 is derived from a 12 billion parameter model called [pythia](https://huggingface.co/EleutherAI/pythia-12b).
 
 
 Even more recently, [Stability.ai](https://stability.ai) released [StableLM](https://stability.ai/blog/stability-ai-launches-the-first-of-its-stablelm-suite-of-language-models), another foundational language model which fits a similar mold as the GPT-J model that Dolly leverages. There are already [several models](https://github.com/stability-AI/stableLM/#models) you can get started with today that finetune StableLM for more specific tasks, as Dolly trains GPT-J.
 
 
 These examples highlight the latest waves in LLM workflows, which have revolved around the [instruction tuning](https://openai.com/research/instruction-following) approach to make the models increasingly suitable for prompting, as popularized by ChatGPT. Naturally this is not the only use case for LLMs but it is showing to be useful for applications based on question-answering.
 
 
 Improvements in instruction tuning models are in a wild-west era, as people explore new techniques and approaches for scaling these workflows. In any case, the high turnover rate of these models points to the importance of understanding where these models come from and what infrastructure choices organizations can make now that will support any of these new modeling approaches.",H2,https://outerbounds.com/blog/train-dolly-metaflow#a-brief-history-of-open-source-foundation-models,False,2628.0,339.0
779,Dolly’s supply chain,"The user experience of Dolly demonstrates how far the machine learning community, in particular services like HuggingFace, have come in making sophisticated modeling APIs accessible to broad swaths of developers. We start by loading the pretrained source model:
 
 ```python
 model = transformers.AutoModelForCausalLM.
         from_pretrained(“EleutherAI/gpt-j-6B”)
 ```
 
 Then, Dolly uses the [HuggingFace Trainer](https://huggingface.co/docs/transformers/main_classes/trainer) API to train on an instruction tuning [dataset](https://raw.githubusercontent.com/tatsu-lab/stanford_alpaca/main/alpaca_data.json) called Alpaca, which was curated by [a team at Stanford’s tatsu lab](https://crfm.stanford.edu/2023/03/13/alpaca.html). The most important thing to realize for this post is that the dataset was generated using OpenAI APIs, by prompting the `text-davinci-003` model. This means that we are training the model from the original state of the GPT-J model and teaching it to [distill](https://en.wikipedia.org/wiki/Knowledge_distillation) the behavior of the larger `text-davinci-003` model from OpenAI and mimic its instruction following-capabilities in our smaller Dolly model. The final dataset contains 52K instruction-tuning examples. 
 
 
 ![](_train-dolly-metaflow_data/alpaca.jpeg)
 <Caption>
 Source: <a href=""https://crfm.stanford.edu/2023/03/13/alpaca.html"">Alpaca: A Strong, Replicable Instruction-Following Model</a>
 </Caption>
 
 Note that due the dependency on the Alpaca dataset, which has a [Creative Commons NonCommercial (CC BY-NC 4.0)](https://creativecommons.org/licenses/by-nc/4.0/legalcode) license, **you cannot use this example or Dolly version 1 as-is in commercial applications** and will need to replace the Alpaca data dependency with your own instruction-tuning dataset or use something that has a free for commercial use license.",H2,https://outerbounds.com/blog/train-dolly-metaflow#dollys-supply-chain,False,1868.0,221.0
780,Training Dolly with Metaflow,"To reproduce the Databricks’ team’s implementation, and to demonstrate Metaflow’s applicability to LLM use cases, we trained Dolly in a variety of contexts. You can find [the source code in this repository](https://github.com/outerbounds/dolly-metaflow).
 
 
 We created a simple Metaflow flow that consists of three steps, `start`, `train`, and `end`. One of the superpowers of Metaflow is that we were able to [rapidly iterate on the flow locally](https://docs.metaflow.org/metaflow/introduction) on a personal GPU workstation. We followed the existing Dolly scripts and used Microsoft’s [deepspeed](https://www.deepspeed.ai/) library to distribute training across many GPUs. Deepspeed integrates tightly with PyTorch and HuggingFace code, so this is a general pattern you can use to train large deep learning models.
 
 
 Our local workstation didn’t have multiple GPUs, so to test distributed training, we annotated the step with`@kubernetes(gpu=4)` to [execute the train step on a cloud instance](https://docs.metaflow.org/scaling/introduction) with multiple GPUs. Conceptually, the flow looks like this:
 
 
 ![](_train-dolly-metaflow_data/dollyflow.png)
 In the source code, the core parts of the yellow box look like this:
 
 ```python
     ...
     @kubernetes(cpu=32, gpu=4, memory=128000)
     @step
     def train(self):
        ...
        subprocess.run(
             [
                 “deepspeed”, 
                 “–num_gpus=%d” % N_GPU, 
                 “–module”, MODEL_TRAINING_SCRIPT, 
                 “–deepspeed”, ds_config_file.name,
                 “–epochs”, “1”,
                 “–local-output-dir”, self.local_output_dir,
                 “–per-device-train-batch-size”, self.batch_size,
                 “–per-device-eval-batch-size”, self.batch_size,
                 “–lr”, self.learning_rate
             ],
             check=True
         )
         # push model to S3 
 ```
 
 
 When it comes to the code, this is all it takes to train the model!",H2,https://outerbounds.com/blog/train-dolly-metaflow#training-dolly-with-metaflow,False,1986.0,236.0
781,But does it actually work?,"When you start running the flow, it will happily start crunching data on a large GPU instance – and go silent for hours as the training is in progress.
 
 
 Especially with a new experimental project like this, we weren’t sure if the deepspeed setup is working correctly, utilizing all GPUs efficiently. To address common situations like this, recently we created [a Metaflow card](https://docs.metaflow.org/metaflow/visualizing-results) for [profiling GPU usage](https://github.com/outerbounds/metaflow-gpu-profile). To use it, drop `gpu_profile.py` next to your flow file and add the following line in your code:
 
 ```python
 ...
     @kubernetes(cpu=X, gpu=Y, memory=Z)
     @gpu_profile(interval=1)
     @step
     def train(self):
         ...
 ``` 
 
 The Metaflow task GPU profiler shows us things such as which NVIDIA devices are visible on the instance, how they are connected, and most importantly how they are being utilized throughout the lifecycle of the model training job.
 
 
 The GPU profiler automatically logs results to a [Metaflow card](https://docs.metaflow.org/api/cards), so you can organize and version these results with modeling metrics and other experiment tracking data in reports that are conveniently viewable in the Metaflow UI ([or in a notebook](https://docs.metaflow.org/metaflow/visualizing-results/effortless-task-inspection-with-default-cards#accessing-cards-via-an-api)). Cards are particularly useful during experimentation, as they are permanently attached to the run that produced them, so you can easily observe past performance in the context of automatically versioned code and data associated with the experiment.
 
 
 Here is an example card showing GPU processor and memory utilization from an epoch of training Dolly:
 ![](_train-dolly-metaflow_data/mfgui-gpu.png)
 
 
 Seeing these charts gave us enough confidence that the GPUs were performing useful work.",H2,https://outerbounds.com/blog/train-dolly-metaflow#but-does-it-actually-work,False,1908.0,262.0
782,The usual infrastructure headaches,"You can run the above Metaflow flow to train Dolly by yourself, assuming that you have [infrastructure setup for Metafow](https://docs.metaflow.org/getting-started/infrastructure). You can reproduce the above results either by using [AWS Batch or Kubernetes](https://outerbounds.com/engineering/welcome/) that conveniently [works with all major clouds](https://outerbounds.com/blog/metaflow-on-all-major-clouds/). Or, if you would rather avoid infrastructure headaches altogether, you can rely on the [Outerbounds Platform](https://outerbounds.com/platform).
 
 
 In our initial tests, we used AWS a `p3dn.24xlarge` EC2 instance to train Dolly for one epoch. The instance this process runs on has 8 [V100 GPUs](https://www.nvidia.com/en-us/data-center/v100/) each with 32GB memory. We used AWS Deep Learning AMI, and were able to run the model for one epoch in 3 hours and 15 minutes. This cost roughly $100. Due to [unavailability of p4 instances on AWS](https://www.theinformation.com/articles/ai-developers-stymied-by-server-shortage-at-aws-microsoft-google), we also ran the same software setup on a [Coreweave](https://www.coreweave.com/) node with 3 [A100 GPUs](https://www.nvidia.com/en-us/data-center/a100/) each with 80GB memory. We trained this model for 10 epochs, which took roughly 30 hours. This cost roughly $200.
 
 
 Besides headaches related to finding large enough GPU instances to run the code, it is crucial to [manage dependencies carefully](https://docs.metaflow.org/scaling/dependencies), which is a silent productivity killer in deep learning. For example, when using NVIDIA GPUs, misalignments between CUDA drivers, and CUDA toolkit components such as the NVCC compiler can lead to flow-breaking debugging cycles when working at the data science layer. We found that Dolly worked with the following combinations:
 
 
 * Ubuntu 20.04 or 22.04
 * NVIDIA driver version 515.105.01
 * CUDA Version >= 11.1 and <= 11.8
 
 
 Thankfully, both the open-source Metaflow and the Outerbounds Platform make it easy to version environments, whether the task requiring the environment runs locally on your workstation or remotely in your compute cluster.",H2,https://outerbounds.com/blog/train-dolly-metaflow#the-usual-infrastructure-headaches,False,2166.0,284.0
783,Evaluation,"Finally, with all our infrastructure in place, we were able to start exploring the bounds of Dolly’s language and reasoning abilities. Dolly can perform a surprisingly broad set of tasks from text generation, brainstorming and open Q&A.
 
 
 As Databricks notes in the [Dolly v1 HuggingFace model card](https://huggingface.co/databricks/dolly-v1-6b#benchmark-metrics), “the qualitative behavior of Dolly is materially different from the underlying model” despite “Dolly [scoring] only marginally better compared to its base model GPT-J-6B [on common LLM evaluation benchmarks]”. It was instructive to validate Databricks claim above as we observed how its responses tended to change as we asked the model to generate responses to a common set of instructions at various stages in its training.
 
 
 For example, we asked the model to “Write a story about a person who cannot see the random forest because they are caught up in the decision trees” to see how it handles creative storytelling-style prompts. Notice how the model seems to get better at interpreting a somewhat open-ended and vague prompt, as it starts to emphasize the “Write a story” part of the instruction in its responses as it trains more and more. 
 
 
 
 
 | **Epoch** | **Sample Response** |
 | :-------------: |---|
 | 1 | When people think of random forests, they often think of the trees being randomly selected from a dataset and being used to make predictions. What they may not know is that when a random forest is built, each tree is created with a rule to determine the best split for a given feature. |
 | 5 | Anil was lost in a thick forest of decision trees. Every step he took felt like he was forcing his way through an endless series of obstacles. He had been walking for hours, and he was still no closer to finding the trail he had been following. His only hope was that eventually he would stumble upon the clearing that had caused his vision to fail him earlier. |
 | 10 | John was an engineer who couldn’t get past the first layer of decision trees. He had worked so hard to understand the data, but something was just wrong with the random forest he was seeing. He worked his way deeper and deeper into the forest, but there were just too many branches and too many combinations of variables that he didn’t understand. John was completely lost, unable to progress any further and in need of a boost to get him out. That’s when he discovered the random forest again, and it gave him the inspiration he needed to understand the data. |
 
 
 This example illustrates one of our main qualitative observations, that the evolution in responses throughout training shows that with more training the model begins to write more verbose and detailed responses, more similar to the GPT models many are familiar with in products like [ChatGPT](https://openai.com/blog/chatgpt). This should come as no surprise, as we already discussed that the Alpaca dataset we are training with was generated by querying a GPT model from OpenAI! This is the distillation process in action.",H2,https://outerbounds.com/blog/train-dolly-metaflow#evaluation,False,3054.0,519.0
784,What next,"To train Dolly with Metaflow, you can find our repository [here](https://github.com/outerbounds/dolly-metaflow). Also, you can use [the new @gpu_profile decorator](https://github.com/outerbounds/metaflow-gpu-profile) in any GPU workflows. We would love to [hear your feedback](http://slack.outerbounds.co) for it, so we can develop the feature further.
 
 
 If you liked this article, you may enjoy our previous posts related to foundation models: [Image generation using Stable Diffusion](/blog/parallelizing-stable-diffusion-production-use-cases/) and [text-to-speech translation using Whisper](/blog/mlops-whisper-and-metaflow/), as well as [infrastructure headaches that you may experience with them](/blog/whisper-kubernetes/).
 
 
 If you need help in getting started with Metaflow and/or experimenting with foundation models and LLMs, join thousands of other ML engineers, data scientists, and platform engineers in [the Metaflow Slack](http://slack.outerbounds.co). If you want to experiment with Metaflow and test fun ML use cases without having to install anything locally, sign up for a free [Metaflow Sandbox](https://outerbounds.com/sandbox).
 
 
 
 PS. To learn more about LLMs in the context of business use cases, [join our next fireside chat](https://www.eventbrite.com/e/large-language-models-beyond-proofs-of-concept-tickets-617461492947) on May 4th!
 
 <a href=""https://www.eventbrite.com/e/large-language-models-beyond-proofs-of-concept-tickets-617461492947"">
 
 ![](_train-dolly-metaflow_data/5_img)
 
 </a>",H2,https://outerbounds.com/blog/train-dolly-metaflow#what-next,False,1529.0,167.0
785,Helping scientists do better science,"*The immense impact of getting the right tools in front of the right people and building an educational (or wisdom) layer around them.*
 
 
 I’ve devoted a large part of my career so far to helping scientists do better science and I want to double down on that. This was initially a result of working in basic scientific research at the intersection of biology, physics, and mathematical modeling. At both the [Max Planck Institute (MPI) for Cell Biology](https://www.mpi-cbg.de/home/) in Dresden, Germany (2011-2013), and [Yale University](https://mbb.yale.edu/) (2013-2016), I worked in biology labs with experimentalists, who were generating increasingly large and complex datasets. I worked with world-class scientists who were ill-equipped to analyze their data and experiments, both in terms of a lack of statistical knowledge and a lack of good tooling (not to mention documentation and education!). At this time, 
 
 
 I started working in what was then known as the IPython notebook and realized the big wins that this type of literate programming could bring to research science. So while continuing research, I began teaching workshops on *Practical statistical Inference for Research Scientists* at both the MPI and Yale and discovered the immense impact you can have by getting the right tools in front of the right people and building an educational (or wisdom) layer around them.
 
 
 Sufficiently jaded by academia (a conversation for another time), I joined a then early-stage startup called [DataCamp](https://www.datacamp.com/), where we built online data science and machine learning courses. Here we worked to bring many data science tools, including the PyData stack and Tidyverse tools, to the masses, and I was fortunate enough to get experience in industrial data science, curriculum building, product management, marketing, and evangelism (such exposure is one pro of joining an early-stage startup). 
 
 
 I then joined [Coiled](https://coiled.io/) as Head of Marketing and Evangelism, where we built a SaaS platform that helps data scientists scale their workflows to the cloud using the PyData stack and open source tools. In this job, I had many exciting opportunities, including the experience of building a business to help other organizations use OSS tools. This is worth taking a beat on. As Brian Granger, co-founder of Project Jupyter, [once said to me](https://www.datacamp.com/community/podcast/project-jupyter-interactive-computing):
 
 
 Open-source software is undergoing a phase transition from having individual users in organizations to having large-scale institutional adoption.
 
 
 At Coiled, I discovered first-hand how OSS is great at meeting the needs of individual scientists but also that institutions have more needs than OSS solves for and we need companies to support the OSS ecosystem so that it can meet these needs. So how important actually are tools? And why?",H3,https://outerbounds.com/blog/hba-excited-to-join-metaflow-and-outerbounds#helping-scientists-do-better-science,False,2919.0,446.0
786,Data tools and wisdom layers,"*Practitioners need not only tools but access to education about the tools and how they can help them do their job.*
 
 
 There’s a paradox of tooling in that, although they’re just means to an end, tools are not only indispensable but need to be molded to both the needs and characteristics of their users. Take a coffee mug, for example: 
 
 
 * It needs to be able to hold coffee in it,
 * It needs a handle or to be insulated enough so that the outside isn’t too hot, and
 * Any handle needs to be comfortably grippable by a human hand.
 
 
 Similarly, data science tools
 
 
 * exist to serve a purpose, for example, pandas and dplyr are for data wrangling, and
 * need to meet users where they are.
 
 
 ![](_hba-excited-to-join-metaflow-and-outerbounds_data/0_img)
 Designing infrastructure for data science is a tad more complicated than coffee mugs:   Meeting users where they are here isn’t as simple as saying a “handle needs to be grippable by a human hand”. The modern toolchain includes many moving parts, such as
 
 
 * Interoperating with other tools in the user’s stack,
 * Reflecting the user’s mental model of the underlying computational and statistical processes (this is why some data scientists prefer the Tidyverse to parts of the PyData stack), and
 * Being something your IT team will sign off on!
 
 
 We require ergonomic tools that solve challenges we have when working with data. I’m convinced that one of the first big wins of pandas was allowing Pythonistas to convert CSVs into DataFrames. Moreover, a large part of the success of the PyData and Tidyverse tooling landscapes is due to the fact that it wasn’t built by computer scientists or software engineers, but by research scientists who needed tools yesterday to do their work.
 
 
 But tooling itself isn’t enough, as I discovered in my time in research science: practitioners need not only tools but access to education about the tools and how they can help them do their job. As the tooling landscape grows and does so increasingly faster, this need becomes more pressing: good tooling requires a robust wisdom layer, including API references, tutorials, comprehensive examples, and a community of users excited to educate and help each other. One of the reasons scikit-learn has garnered such wide adoption is [their fantastic documentation](https://scikit-learn.org/stable/user_guide.html) and it’s no surprise that they practice [Documentation-Driven Development](https://gist.github.com/zsup/9434452): the wisdom layer is a first-call citizen in their development process.",H3,https://outerbounds.com/blog/hba-excited-to-join-metaflow-and-outerbounds#data-tools-and-wisdom-layers,False,2567.0,420.0
787,Why Metaflow? Why Outerbounds?,"*Building human-centric tools for data scientists and developers.*
 
 
 Having taught frameworks and built products that help (data) scientists with a lot of different aspects of their work, it became clear that, although there are wonderful tools for many parts of the pipeline, not only is the deployment story woefully incomplete (this is partially due by the tools being built by academic researchers), but we don’t even necessarily have shared mental models and canonical tools of what the statistical inference and ML deployment story should look like. To be clear, what we need is a shared paradigm (including tooling, workflows, division of labour, education, and more) for building and integrating data-powered software into business and decision-making systems. 
 
 
 To state the problem another way, many of the tools needed to build data-powered software that is integrated correctly into business and decision-making systems (while staying isolated from business logic so that errors don’t propagate) do exist but the last mile is missing. All the moving parts are there: you can work with your tabular data in pandas, .fit() and .predict() with scikit-learn, containerize with Docker and/or Kubernetes, and deploy a REST API on {insert cloud provider of choice here}. But all too often, the solution is developed ad hoc for each case in a brittle manner that won’t generalize. We’re finding ourselves at an historical stage analogous to the craft method of assembling cars by bringing all the parts to a single location, before the advent of interchangeable parts and the production line. There is a huge opportunity here to define not only what tooling and wisdom look like here, but also what division of labour looks like: what does an ML engineer do? What about an infrastructure engineer? What layers of the stack should a data scientist need to care about?
 
 
 These are some of the things I’m really excited to work on, think about, and contribute to, which aligns well with the vision and worldview of open-source Metaflow that we are continuing to develop at [Outerbounds](https://outerbounds.com/about/). At Outerbounds, we think deeply about which layers of the stack data scientists should (and, for the most part, do) care about, so that they can do their most productive work:
 
 
 ![](_hba-excited-to-join-metaflow-and-outerbounds_data/1_img)
 On top of this, I’m once again excited to be building a business to bring OSS tools to the world and to be doing so this time with an empathetic and uniquely experienced group of engineers who are passionate about building delightfully usable, human-centric tooling for data scientists. Joining an early-stage startup is also an opportunity to build a culture and to touch many parts of a business, from product and engineering to sales and business development, in a word, to problem solve across a variety of contexts, which excites me very much!
 
 
 As always, my mission remains to help scientists do better science and Outerbounds is a place where I feel I can achieve this through a combination of OSS tool building, product development, education, and, in particular, interaction with the community and customers.  
 These are a few thoughts about where we’re at as a discipline, what I’m excited about, and why I joined Outerbounds as Head of Developer Relations. If these ideas resonate with you, I’d love to hear from you! Take a look at [our open positions](https://outerbounds.com/workwithus/) or, if you just want to chat, join me and over 1000 data scientists and engineers on the [Metaflow Community Slack](http://slack.outerbounds.co/) 👋",H3,https://outerbounds.com/blog/hba-excited-to-join-metaflow-and-outerbounds#why-metaflow-why-outerbounds,False,3629.0,581.0
788,What Does a Machine Learning Engineer Do?,"**Hugo:** I want to learn more about your journey to being an AI engineering manager. But we’ve used the term ML engineering to talk about what it actually means: what does a machine learning engineer do in your experience?
 
 **Ethan:** Yeah, so good question, I would say that maybe one definition is somebody who builds machine learning models, it could be, for example, a statistical model. And the boundaries between statistics and machine learning are pretty fuzzy. But I would say that they build this model, and it gets used in some sort of an automated fashion is probably the best definition that I have. 
 
 So you might have a statistician who is building a model, and then they, you know, run inference on this model, they interrogate the model in order to try to understand some behavior that exists. But this is more of an offline process that they’re doing. But I would say that a machine learning engineer ends up… they build the model. And maybe the model serves predictions behind an API, maybe the model runs once a day, and that generates some predictions that get run into a database. And then somebody else ends up using those predictions. Maybe, yeah, but I think the model gets used in some sort of an automated fashion, that’s probably the best definition that I have.
 
 **Hugo:** So then, in some ways, it is a data scientist, who specifically builds models that are deployed to production or something along those lines. Yeah, yeah. Yep. But they’re not necessarily engineers per se because I think maybe the term engineer is so overloaded. We have ML engineers, data engineers, and platform engineers, and it’s more on the kind of scientific side focusing on the top levels of the stack that we’ll get to.
 
 **Ethan:** Yes, I think so. At Square, we have the term machine learning engineer and I think that what I’ve said largely matches what those people do. There might be, you know, at a bigger company like Square, there is a platform team, where they are maybe responsible for building out a platform to allow for serving of models with low latency and high concurrency and everything else that you might care about at scale. 
 
 But the machine learning engineer, they’re the ones who are building the actual model, and probably the ones who are responsible for how the model impacts the ecosystem that it operates in. So your model generates predictions, and maybe somebody’s making decisions on those predictions. And so how do you track that process? That’s, that’s probably the job of the person who built the model.",H2,https://outerbounds.com/blog/full-stack-machine-learning-skills#what-does-a-machine-learning-engineer-do,False,2557.0,448.0
789,Software Engineering Skills for Early Career Data Scientists,"**Hugo:** In your first job as a data scientist, what was the focus? And what did other people around you do, maybe with more platform engineering skills and that type of stuff?
 
 **Ethan:** Yeah, so that first job, even when I was trying to decide where to go, which ended up being an easy decision because I got one job offer from my first job. So I took that job. But yeah, it was high on my list originally. 
 
 But like, you know, we, when I was doing that boot camp, we had a bunch of companies come in and tell us what data science like what data science meant, and even back then there was kind of a split between people that are doing more analytics, and then people that are doing more, kind of what we would now call machine learning engineering. And I don’t feel like that was a very popular term back in 2015. 
 
 Back then I found myself gravitating more towards like, putting models into production. And I was interested in that. And this role seemed to have some of that. And so I would say that at my first job, for the recommendation systems, there were some things that I could do like I was definitely allowed to train models. For example, I kind of had free range over like a big cluster that existed in a data center for me to train models and things like that. But the actual model deployment process was sometimes a bit difficult and definitely outside of my wheelhouse. 
 
 I would also say that back then it depends on what you were working on in terms of what is the kind of production requirements required of somebody to push their code out. So, you know, for something like recommendation systems, it’s not. And if you’re at a reasonable scale, it’s not the end of the world, if somebody gets served a bad recommendation, I mean, if Amazon puts out a bad algorithm, you know, because they’re at such a scale, then maybe that is even going to cut into their margins. But at our scale, it was kind of lower stakes. 
 
 So I was allowed to kind of play in that world a bit even like, even though I did not have like a background in software engineering. Like any code that you look at, that you wrote, In the past, you know, you should shudder whenever you look at your old code, because ideally, you’ve learned something since then. And if I look back at the code that I wrote, then it was probably not production-quality code, I didn’t write tests, you know, any of these other kinds of hallmarks of modern software development. But that was okay at the time. One: because it was kind of low stakes that I was operating in. And two: because I think that nobody really expected that kind of production software engineering out of machine learning people back at that point in time, it was hard enough to find anybody who could just kind of program up some of these algorithms, to begin with. And so production level, software engineering would have just been very nice to have.",H2,https://outerbounds.com/blog/full-stack-machine-learning-skills#software-engineering-skills-for-early-career-data-scientists,False,2906.0,547.0
790,Essential ML Libraries for Data Scientists and MLEs,"**Hugo:** What are the essential ML libraries that we need to know and use to help land a full-time data science job?
 
 **Ethan:** I think they’re the same ones that have existed for a while. So like, if you’re on the Python stack, Scikit-learn is still very, very popular. We asked interview questions here where you’re allowed to use Scikit-learn and so if you know how to use that, then it’s a lot better than having to program logistic regression from scratch. And so I feel like basically the PyData stack Scikit-learn, pandas, NumPy, I think continue to be the workhorses of a lot of this If you need to know, deep learning, then TensorFlow or PyTorch are probably fine. And yeah, I don’t know, I, I think those are it. Spark is starting to get popular, I still don’t know Spark. So I’ve had to use it a tiny bit at my first job is really painful. And I’ve somehow managed to avoid it this entire time. And so I hope that some of you can as well.
 
 **Hugo:** Yeah, I don’t know about landing jobs per se. But something which allows you to do gradient-boosted trees, XGBoost is one example. There are others as well. But yeah, I think if you’ve got, I mean, I always say that if you can build Random Forests, boosted trees, and do a bit of deep learning, that will help you add some most machine learning questions, right? So if you do some Scikit-learn some deep learning of some sort, and some boosted trees, you’re doing pretty well.
 
 **Ethan:** 100% We’re big fans of XGBoost at Square.
 
 **Hugo:** Awesome. And yeah, but the stack I mean, NumPy, pandas, and matplotlib, and a few of these other very like foundational PyData, SciPy packages are incredibly, incredibly useful.
 
 **Ethan:** And I think just being comfortable with using those libraries. I know, pandas can be kind of complicated, but I still get confused by the API and everything else. But the more comfortable you are and the quicker you can kind of slice and dice your data, then it all that’s just going to pay dividends and definitely be helpful during interviews to move through the interview fast.
 
 **Hugo:** Absolutely. And related, knowing your way around the Jupyter ecosystem, notebooks, and lab can be incredibly useful. And yeah, a bunch of basic bash and terminal stuff as well.
 
 **Ethan:** Yeah. My team, they’re big fans of bash and make files. Yeah, I’m not going to tell anybody to go out and start building their own make files. But yeah, I think a lot of a lot of terminal stuff you will come across and it’s, it can only be helpful to get fast and nimble with that.
 
 **Hugo:** Absolutely. And then we’re gonna get to this later, but some basic software engineering skills. I mean, I would, I would say maybe top two or three... version control... refactoring...
 
 **Ethan:** Writing tests.
 
 **Hugo:** Yeah! Testing, testing code. Also, data testing can be incredibly useful. Using something like pytest or something like that can be really cool.
 
 
 **Ethan:** Our testing world is much harder than the software engineers testing world. 
 
 
 **Hugo:** Because it involves a lot of the real world.
 
 
 **Ethan:** Yeah... data changes.",H2,https://outerbounds.com/blog/full-stack-machine-learning-skills#essential-ml-libraries-for-data-scientists-and-mles,False,3147.0,563.0
791,Measuring the Success of Machine Learning Projects,"**Hugo:** How do you think about measuring the success of machine learning projects?
 
 
 **Ethan:** So I wanted to join the risk team when I came here because I liked that their definition of success was very clear. So, you know, I mentioned that I used to work in recommendation systems. And you can run some AB tests with recommendation systems to convince yourself that you have proved valuable for the company, that you’ve made the company money and things like that. But sometimes, especially when you’re at a reasonable scale, you might need very long time periods to convince yourself that the improvements that you’ve made to an algorithm have given a large impact on the company. And so I was very interested in the risk team here because it’s at a large scale. And we’re dealing with money. And like the best way to kind of measure success is with money, usually, yeah. 
 
 
 So on the risk team, we could do things like that. So we could run AB tests where maybe you start sending some payments to one model and other payments, go to another model. And then you can measure, how much loss did we incur due to fraud from either model. And so you know, that ends up just being a fairly straightforward way. I mean, it’s not perfectly straight, because, with fraud, your losses are technically unbounded. So if you have a bad actor who figures out that they can steal your money, they’re gonna steal as much money as possible. And so it’s, it ends up being a bit of like a causal inference problem to actually do it, right? But that’s, that ends up being a very nice way to do this. On the chatbot side of things, we, the chatbot, can kind of back out and ask the business owner for help if it’s not able to solve a problem for somebody. And so we can measure, basically, how often it completes the goals that it has. And so that ends up being kind of a very clear measure of at least that model success.",H2,https://outerbounds.com/blog/full-stack-machine-learning-skills#measuring-the-success-of-machine-learning-projects,False,1913.0,358.0
792,What Exactly is the Full Stack of Machine Learning?,"![](_full-stack-machine-learning-skills_data/0_img)
 **Hugo:** Scientists really want to be thinking a lot more about the top layers of this stack, but have easy access to the bottom layers of the stack. That’s one that’s how we tend to think about it. I’m wondering how this resonates with you what you’d add to it and how you think about the full stack more generally.
 
 
 **Ethan:** Yeah, I think that data scientists are largely like that here as well, where you know, you want to focus on the modeling, but you want access to all of the superpowers that the cloud gives you, right? So I want to be able to, I don’t want the scale of my data to matter. I don’t want to have to wait for what I’m doing. So I want, I want things to be fast, which might mean that I want things to work in parallel, I want to be able to store my data as much data as I want, you know. 
 
 
 Yeah, you want to focus on the modeling. I do think that, unfortunately, nowadays, well, unfortunately, right now. And to be clear, I haven’t used metaphor. So maybe this, this solves everything. But the more you try to avoid the lower parts of that stack, the more you try to avoid touching the cloud touching compute, and things like that, the harder time you’re just gonna have, because inevitably, like our abstraction layers are not very good right now. And so like I, you know, my team is on AWS. Right now, when I was on the fraud team here, they were on Google Cloud. And so I’ve like, worked across both clouds, we do have some platform-level tools that start to do a good job of abstracting away the compute layers, and things like that. But still, inevitably, you kind of run up into these, like, nasty edges to all of this. And so when you start to think of things like permissions, especially at a big company, where security and networking are very important things, then you start to bump up and into permissions, which ends up like pulling you down into that those lower layer layers of the stack. 
 
 
 But anyway, in terms of what like what I like, how I think about it, I do think that that orchestration layer that you had, I think that that’s like one highly important part, I think the like a big part of machine learning work is the fact that you have to work locally like you want to kind of write your code locally, but you can’t really do your work unless you’re in the cloud. And so, you know, software engineers, like a lot of times they can, they can write their application code locally, they can spin it up locally, they can test it, run all of their tests locally. And then they can kind of deploy it up into the cloud. But in our world, like training your model on test data locally, is, you know, you can find some bugs, and you can write some tests and things like that. But you can’t actually train your model often. Unless you’re doing it somewhere up in the cloud. Yeah, and so that’s a bit like, it’s kind of difficult to work in the cloud. And so you want to work locally, but then you need to deploy everything up into the cloud. And I think minimizing the cost of switching from local to the cloud is important, and is very difficult right now. 
 
 
 But that orchestration layer really, really is it, it’s like, Alright, I want to do all these things. But I want to do them up in the cloud in like, some agnostic way where maybe this code runs on this computer, this code runs on this other computer, and everything else. And so that, that ends up being kind of a huge, huge part of it.",H2,https://outerbounds.com/blog/full-stack-machine-learning-skills#what-exactly-is-the-full-stack-of-machine-learning,False,3505.0,670.0
793,Summary,"After our fireside chats, we have async AMAs with our guests on [our community slack](http://slack.outerbounds.co/). A lot tends to happen there so join us if you’re interested in such conversations! This week, for example, Michael Ward asked
 
 
 > On the topic of tests – often in traditional software dev, the test suite can be more important for ensuring consistent behavior of a system for future refactorings and feature additions.
 
 
 Is it the same with ML codebases? Or perhaps, because system behavior can’t be as easily encoded into the type system, does the test suite serve more as an exploration of model edge cases?
 
 
 And Ethan replied as follows:
 
 
 > I think you still need all of the testings of traditional software dev. [This talk](https://www.usenix.org/conference/opml20/presentation/papasian) from Google looks at failure modes for one ML system of theirs’, and non-ML failure modes were still the most common. That said, it’s also helpful to perform testing of models beyond traditional software testing. For me, this is often simple behavioral testing, such as “the model’s loss should decrease on each epoch with a simple set of training data”.
 
 
 Beyond all this, what really matters at the end of the day is how the models perform in the wild, and for that, we need good monitoring and analytics.
 
 
 [You can join us on slack](http://slack.outerbounds.co/) for more such conversations and can view the other fireside chats [here](https://www.youtube.com/playlist?list=PLUsOvkBBnJBeL8tsNGMdDKQjjOdU29sME).",H2,https://outerbounds.com/blog/full-stack-machine-learning-skills#summary,False,1542.0,243.0
794,A fast path for tabular data,"All modern data warehousing solutions either store data natively as Parquet-encoded files (like various versions of [the cloud-based data lake](https://towardsdatascience.com/an-aws-data-lake-with-s3-explained-c67c5f161db3) e.g. powered by [AWS Athena](https://aws.amazon.com/athena/) or [Spark](https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-spark.html)), or they allow exporting/unloading data as Parquet files (e.g. [Snowflake](https://docs.snowflake.com/en/user-guide/script-data-load-transform-parquet) or [Redshift](https://docs.aws.amazon.com/redshift/latest/dg/r_UNLOAD.html)).
 
 Conveniently, these systems allow you to create materialized views which can be processed outside the warehouse. You can use SQL and [the Create-Table-As-Select (CTAS) pattern](https://docs.aws.amazon.com/athena/latest/ug/ctas.html) to query even petabytes of data, extracting the results for downstream processing in Metaflow.
 
 Once you have Parquet files in S3, you can hit the fast path:
 
 ![](_metaflow_fast_data/arrow-diagram.png)
 
 1. You can load data from S3 directly to memory *very quickly*, at tens of gigabits per second or more, using [Metaflow’s optimized S3 client, `metaflow.S3`](https://docs.metaflow.org/scaling/data).
 
 2. Once in memory, Parquet data can be decoded efficiently using [Apache Arrow](https://arrow.apache.org/).
 
 3. The in-memory tables produced by Arrow are interoperable with various modern data tools, so you can use the data in various ways without making additional copies, which speeds up processing and avoids unnecessary memory overhead.
 
 This pattern is becoming increasingly popular, thanks to the maturation of Apache Arrow itself and versatile tools around it. The excitement is further motivated by the realization that a single medium-size EC2 instance can handle an amount of data that used to require a Hadoop or Spark cluster just a decade ago - this time with much fewer operational headaches, and at a much lower cost.
 
 The pattern is still new, so not everyone is fully aware of how well and fast it works - under suitable conditions. To showcase this, we first focus on the left side of the diagram, addressing a common myth that loading data from S3 is slow. After this, we focus on the right side, demonstrating how to use the data with various high-performance tools efficiently.",H2,https://outerbounds.com/blog/metaflow-fast-data#a-fast-path-for-tabular-data,False,2347.0,306.0
795,S3 is fast (when used correctly),"Occasionally, we are asked how to cache data from S3 on local disk, instance volume, or a distributed file system like EFS to make loading and processing data faster. Counterintuitively, loading data from S3 can be much faster than local disk, so there's no need to add extra layers of complexity.
 
 The question likely stems from first-hand experiences of S3 being slow, which can easily happen if you don’t have a setup that leverages its strengths. This benchmark illustrates the point:
 
 ![](_metaflow_fast_data/load_s3_ec2_chart.png)
 
 The bars refer to a Metaflow task that uses `metaflow.S3` to download the dataset, running on EC2 with AWS Batch [using the @batch decorator](https://docs.metaflow.org/scaling/remote-tasks/aws-batch), comparing four different configurations:",H2,https://outerbounds.com/blog/metaflow-fast-data#s3-is-fast-when-used-correctly,False,785.0,113.0
796,Maximizing the S3 performance,"A key observation is that the download throughput of S3 is a combination of multiple factors. In order to achieve the maximum throughput, pay attention to the following dimensions:",H3,https://outerbounds.com/blog/metaflow-fast-data#maximizing-the-s3-performance,False,180.0,29.0
797,New feature: tmpfs support for Batch and Kubernetes,"We have been using the above fast data pattern successfully for years to power Metaflow workflows handling terabytes of data. However, this far, there has been a small gotcha.
 
 While allocating enough memory through `@resources` should guarantee that data can be downloaded quickly, we still rely on the local filesystem to back the files nominally, although the files never hit the disk thanks to caching. Sadly, it is not possible to allocate disk space for tasks on the fly while using AWS Batch. Doing it requires a bit more tedious change in [the instance’s launch template](https://docs.aws.amazon.com/batch/latest/userguide/launch-templates.html).
 
 To address this issue, recently we implemented support for memory-based `tmpfs` filesystem on Batch and Kubernetes in Metaflow. You can use the feature to create an ephemeral filesystem backed by memory on the fly, without having to change anything on the infrastructure side.
 
 We made sure that the `metaflow.S3` client is aware of the `tmpfs` volume, so it will automatically use it to speed up downloads when you enable it. To benefit from the new feature, simply add
 
 ```python
 @batch(use_tmpfs=True)
 ```
 
 for your AWS Batch workloads or
 
 ```python
 @kubernetes(use_tmpfs=True)
 ```
 for Kubernetes. There are a few additional features related to `tmpfs` which you can [read more about in the Metaflow documentation](https://docs.metaflow.org/scaling/data#using-metaflows3-for-in-memory-processing).",H2,https://outerbounds.com/blog/metaflow-fast-data#new-feature-tmpfs-support-for-batch-and-kubernetes,False,1473.0,213.0
798,From cloud to table,"As shown above, we can load bytes to memory very quickly. To actually use the data, we must also decode the Parquet files to [an in-memory `pyarrow.Table` object](https://arrow.apache.org/docs/python/generated/pyarrow.Table.html). When taking the decoding time into account, the differences are even more striking:
 
 ![](_metaflow_fast_data/decode_parquet_chart.png)
 
 The configurations are the same as in the first chart above. We added a top-of-the-line M2 Macbook Pro (the purple bar) as a comparison point. The Macbook loads data from a local SSD, not from S3 as the instances do.
 
 As shown above, a powerful Macbook beats small and medium-size instances when the instances are using a local disk, but even a medium-size instance beats the Macbook when using in-memory processing. In this case, a large instance can load a table about 8x faster than a Macbook.
 
 This result is a combination of multiple factors: The larger instances have more CPU cores, more memory, and more IO bandwidth available than a Macbook. Naturally, you must utilize the resources efficiently to gain the maximum performance: Use [metaflow.S3.get_many](https://docs.metaflow.org/api/S3#S3.get_many) to maximize network throughput and multithreaded decoding to benefit from all the available CPU cores, as exemplified by [the `table_loader.py` utility module](https://github.com/outerbounds/fast-data-blog/blob/main/table_loader.py).",H2,https://outerbounds.com/blog/metaflow-fast-data#from-cloud-to-table,False,1419.0,194.0
799,From table to results,"With [a `pyarrow.Table` object](https://arrow.apache.org/docs/python/generated/pyarrow.Table.html) in memory, the possibilities are many! We highlight three common use cases below.
 
 Crucially, all the libraries listed below support zero-copy instantiation from a `pyarrow.Table`, so you don’t have to reserve a large amount of extra `@resources(memory=)` just to handle extra copies or inefficient internal representations. Ballooning memory overhead used to be a problem in the past with older Pandas, prior to Arrow.
 
 
 You can code along with the following sections using [this repository](https://github.com/outerbounds/fast-data-blog/).",H2,https://outerbounds.com/blog/metaflow-fast-data#from-table-to-results,False,645.0,81.0
800,Dataframes,"Moving to Pandas is trivial and fast:
 
 ```python
 df = table.to_pandas()
 ```
 
 You can add a `zero_copy_only=True` flag to make sure that you will get warned if the conversion requires extra copies of data. Since [the Pandas 2.0 release](https://datapythonista.me/blog/pandas-20-and-the-arrow-revolution-part-i) that came out just a month ago, Pandas is fully compatible with Arrow data types, so working with Arrow and Pandas should be smooth sailing.
 
 While Pandas provides a familiar API, its operations are not always the fastest. You can often enjoy a higher performance by using [Polars - a high-performance dataframe library](https://pola.rs):
 
 ```python
 import polars as pl
 df = pl.from_arrow(table)
 ```",H3,https://outerbounds.com/blog/metaflow-fast-data#dataframes,False,722.0,105.0
801,SQL,"Sometimes it is more convenient to use SQL for data processing than dataframe APIs. For instance, you can follow this
 pattern to enable clear division of responsibilities between data engineers and data scientists:
 
 ![](_metaflow_fast_data/data-flow.png)
 <Caption>
 Diagram adapted from the book, <a href=""https://www.amazon.com/Effective-Data-Science-Infrastructure-scientists/dp/1617299197/"">Effective Data Science Infrastructure</a> 
 </Caption>
 
 In this scenario, data engineers are responsible for maintaining company-wide, authoritative fact tables that individual projects can rely on. Projects may create their own interpretations of data through their own lightweight ETL process e.g. for feature transformations.
 
 The project-specific ETL can run within their Metaflow workflows using [a small but surprisingly powerful in-process database, DuckDB](https://duckdb.org/). A benefit of this approach is that data scientists can own and iterate on their specific ETL very quickly, even testing it locally on their workstations.
 
 Using this pattern is not harder than using a dataframe:
 
 ```python
 import duckdb
 query = ""SELECT * FROM arrow_table""
 relation = duckdb.arrow(arrow_table)
 df = relation.query('arrow_table', query).to_df()
 ```
 
 To familiarize with using DuckDB and Metaflow together in a machine learning context, you can follow along with this [recommender systems tutorial](https://outerbounds.com/docs/recsys-tutorial-L2/).",H3,https://outerbounds.com/blog/metaflow-fast-data#sql,False,1463.0,183.0
802,Interfacing with ML libraries,"Many modern ML libraries are well optimized when it comes to utilizing CPU and GPU resources. However, to benefit
 from high-performance training and inferencing, you need to be able to feed data to the model fast.
 
 Luckily, Arrow data is [readily convertible to Pandas and NumPy](https://arrow.apache.org/docs/python/numpy.html) which are supported by all major ML libraries. [Through NumPy, you can also convert data to various tensor objects](https://outerbounds.com/docs/python-tabular-data-structures/).
 
 To illustrate the pattern, [we included a simple ML example](https://github.com/outerbounds/fast-data-blog/blob/main/fast_data_modeling.py) that shows how to convert an Arrow table to a Pandas dataframe, and feed this object to [a LightGBM model](https://lightgbm.readthedocs.io/). The flow trains a model to predict a target variable based on 300 finance instruments.
 
 You can run the code easily by yourself and [visualize the results through a Metaflow card](https://docs.metaflow.org/metaflow/visualizing-results), [like this one](https://cards.outerbounds.dev/index.html?id=pw-1347686565%2F0e5a2797fce749c5):
 
 <iframe src=""https://cards.outerbounds.dev/index.html?id=pw-1347686565%2F0e5a2797fce749c5"" width={1000} height={800}></iframe>",H3,https://outerbounds.com/blog/metaflow-fast-data#interfacing-with-ml-libraries,False,1259.0,137.0
803,Summary,"This post outlined patterns that you can use to load tabular data from S3 to your Metaflow workflows quickly and process it in various ways using Arrow-compatible libraries.
 
 In future posts, we will dive deeper into related topics around data: How to use data engineering tools like `dbt` together with Metaflow, how to handle large datasets in a horizontally scalable manner using the patterns highlighted here, and how to make sure that the pattern comply with your data governance policies.
 
 You can test patterns highlighted here in [the Metaflow sandbox](https://outerbounds.com/sandbox). If you have any
 feedback, questions, or other thoughts around data, [join us and thousands of other data scientists and engineers on Metaflow Slack](http://slack.outerbounds.co)!
 
 
 PS. If open-source data topics pique your interest, join [our upcoming event on The Open-Source Modern Data Stack on June 7th.](https://www.eventbrite.com/e/the-open-source-modern-data-stack-tickets-630584333717)
 
 <a href=""https://www.eventbrite.com/e/the-open-source-modern-data-stack-tickets-630584333717"">
 
 ![](../static/assets/tabular-fs-chat-event.png)
 
 </a>",H2,https://outerbounds.com/blog/metaflow-fast-data#summary,False,1153.0,146.0
804,MLOps Landscape,"**From a DevOps perspective, what would be the new techniques/frameworks/programming languages we have to use/learn to switch to MLOps?**
 
 
 When it comes to languages, Python is pretty unavoidable with today’s ML frameworks. It’s pretty helpful with data processing too. From the DevOps perspective, the three biggest systems you have to consider are:
 
 
 * A data platform/warehouse such as Snowflake or Databricks.
 * A compute layer for processing data and training models such as AWS Batch or Kubernetes.
 * A workflow orchestrator for running workflows reliably such as Argo Workflows.
 
 
 There are other components too like Experiment Tracking, but from the DevOps perspective, their footprint is smaller. If you need real-time data, you need to choose your data platform and compute the layer accordingly (e.g. Flink). 
 
 
 **Ten years from now, what might the landscape of MLOps or Infra look like?**
 
 
 You can listen to Ville’s in-depth take in this [video](https://www.youtube.com/watch?v=gZnhSHvhuFQ).
 
 
 **​​Big players like Databricks and Amazon are using Jupyter-like interfaces as code editors. How can this affect (positively /negatively) the CI/CD of the ML pipeline?**
 
 
 For starters, Sagemaker uses a customized version of JupyterLab as their IDE. Technically you can use JupyterLab to create Python files as with any other IDE, so it has no special impact. I presume you are hinting at actually using .ipynb files as production-ready code containers. Afaik, Databricks isn’t Jupyter-native anyways, so that’s a separate case. Using .ipynb has a number of implications. In particular, as of today, creating importable modules and packages as notebooks isn’t well supported, which makes it hard to create more complex apps as notebooks. At least with Metaflow, we recommend that you write your ML workflows as Python files as usual (you can certainly use JupyterLab for that!) and you can use notebooks alongside for analysis (where they really shine). If you do this, all existing GitOps and CI/CD tooling work as before.
 
 
 **With big tools like Snowflake, DataBricks, and FireBolt, do you see all of these platforms trying to scoop up all of the different “data” needs? For example, Snowflake acquired Streamlit. Or will more “specialized niche” tools dominate like DBT, Airbyte, Astronomer, Prefect, etc.? Or will we see more of “single” space winning like Snowflake’s Streamlit acquisition, Snowpark, the coming of Snowflake Materialized Tables (for Kafka topics)?**
 
 
 Both Snowflake and Databricks are increasingly portraying themselves as “complete data platforms”, so they are definitely trying to bundle everything. Interestingly there’s a counterforce of unbundling the data platform through engine-agnostic open-source tools like [Iceberg](https://iceberg.apache.org/) and [Arrow](https://arrow.apache.org/), and all kinds of specific tools for data catalogs, data quality, etc. There are many benefits to bundling (potentially better user experience, less complexity, fewer vendors) and maybe equally many to unbundling (more flexibility, less vendor lock-in, maybe more cost-effective, use the best tool for the job). Hence my crystal ball says that we will stay forever floating somewhere between these two gravitational forces – some things will get bundled, some don’t.
 
 
 **How do you standardize something, say ML as a subject, that is evolving so rapidly? In ML, things come and go pretty fast. So, with the overflowing research and introduction of new topics in ML, do you think this subject will ever reach that kind of standardization or will remain research-heavy for quite some more time? Or do you think there’s a standardization that’s already taking place?**
 
 
 I think this concept of pace layering is relevant ([read more about it here](https://jods.mitpress.mit.edu/pub/issue3-brand/release/2)): 
 
 
 ![](_ml-infra-humans-ama_data/0_img)
 <Caption>Pace Layering</Caption>
 
 Some things in ML move fast, like “the fashion” and “the commerce” layers in the figure, but many things move much slower (including infrastructure like what’s handled by Metaflow). Logistic regression is still the most widely used method in businesses and it hasn’t changed in decades. Numpy is still the best library for representing numerical data in Python and it is 15 years old.
 
 
 A big goal of education is to help people distinguish between the fast and slow layers. Once you know the fundamentals, you start seeing that things that matter don’t actually move that fast. There’s just a lot of noise which you can mostly ignore.
 
 
 **What practices or tooling in the MLOps space could help personal ML projects/research that isn’t too overkill on the infrastructure side? Say for an individual with just a single GPU on a desktop.**
 
 
 I guess it depends on what you want to do. If you focus on model development/research, probably just a notebook goes a long way. Maybe combined with a tool like [Weights and Biases](https://outerbounds.com/docs/track-wandb/) or Tensorboard for basic monitoring. On the other hand, if you want to prototype end-to-end ML-powered applications, then you probably want to consider a tool like Metaflow, which gets you on the right track with minimal overhead. Just [pip install metaflow](https://docs.metaflow.org/getting-started/install) to get going!",H2,https://outerbounds.com/blog/ml-infra-humans-ama#mlops-landscape,False,5346.0,821.0
805,Building and Investing in an MLOps Organization,"**For a startup company wanting to tap into the ML domain, to build out an in-house ML team, is it necessary to have ML engineers at the beginning, or can it be covered by another engineering team? If not, at which stage, do you think it makes sense to have a dedicated ML engineer team? And what are the common mistakes you’ve seen other companies make in this journey?**
 
 
 We see many companies being successful with two roles: engineers who can help with infrastructure and data scientists who can focus on ML. With a tool like Metaflow, data scientists don’t have to worry too much about infrastructure, and engineers don’t have to worry too much about ML. Hence you don’t necessarily need to have unicorn-like ML engineers who can do everything by themselves.
 
 
 **How can you tell if investment in MLOps has made a difference in your organization? And distinguish it from ML research breakthroughs/the skill of the data scientist to make a good model?**
 
 
 There are many differences but I think it mostly boils down to the model not being an island. When doing ML research, you can consider models to be islands: you have a static input data set, and you evaluate the model in a lab setting, maybe just by yourself. In contrast, in a real-world business MLOps environment, the model is connected to:
 
 
 * constantly changing input data, which introduces many new sources of failure.
 * outside systems that interact with the model, which introduces many new requirements for the model (offline-online consistency, stability, etc.).
 * A team that probably wants to change the model/systems around it frequently.
 
 
 These points make the whole thing 100x more complicated. 
 
 
 **How do we convince our leaders to invest in MLOps?**
 
 
 Three steps:
 
 
 1. First, do you want to invest in ML/data science in the first place? You need to find and define a business scenario where ML/DS actually produces value (in the simplest case, money) for the business.
 2. Often it is possible to create a POC that proves the value before deploying a full-scale ML platform / MLOps solution. This is to justify any future investment.
 3. Once it is clear that the business really needs an ML/DS solution, MLOps is just a necessary requirement to make it work in a sustainable manner. In the same way, if the business decides to set up an e-commerce store, you need to set up web servers or sign up with a platform like Shopify. MLOps (or ML/DS infra in general) is the same deal.
 
 
 I’ve seen that companies have a hard time justifying the investment in (3) since they haven’t thought through (1) and (2) properly. A major motivating use case for why we created Metaflow at Netflix was to make it easy and quick (i.e. cheap) to do (2). Once the value was proven, Metaflow makes it possible to gradually harden the solution towards (3) as necessary.
 
 
 **As for an ML project early in development (be it personal or business-related), is it necessary/recommended to invest time and resources into MLOps and all its nuances right from the get-go? Or only when there is a stronger foundation to work on?**
 
 
 Absolutely no need to think about all production-level concerns from the get-go. In fact, doing it might be counterproductive, since everything comes with a cost. Metaflow very much advocates the idea that you can start simple and harden the project gradually as the project becomes more critical. Some projects never need all belts and suspenders. 
 
 
 ![](_ml-infra-humans-ama_data/1_img)",H2,https://outerbounds.com/blog/ml-infra-humans-ama#building-and-investing-in-an-mlops-organization,False,3511.0,610.0
806,Deploying ML Systems,"**Once an ML Pipeline is defined, what steps should be taken to constantly improve and what metrics should be tracked to see if the pipeline is itself good?**
 
 
 **As a new grad, how can I get started with MLOps?**
 
 
 For pipeline health, these basic metrics go a long way:
 
 
 * Have alerting in place that sends alerts if the pipeline fails or is delayed.
 * Have some basic insights/alerts for the final output of the pipeline (Metaflow cards can help here).
 
 
 After these two, the next step is to have a dashboard that compares the pipeline outputs/metrics over multiple runs. Catching big diffs between runs is an effective way to catch failures (in data/models).
 
 
 **How do you see MLOps fitting into cloud native environments? How different is MLOps from understanding cloud orchestration and general RESTful software development best practices?**
 
 
 You can read [this post](https://www.oreilly.com/radar/mlops-and-devops-why-data-makes-it-different/) to understand our view on what makes MLOps different. 
 
 
 **How do you make ML Infra resilient?**
 
 
 You need stable foundational infrastructure: stable data platforms (batch or real-time), compute layers and workflow orchestrators. Luckily high-quality implementations of such infra are available in open-source and/or as managed cloud services. None of these tools are ML-specific, so then it’s useful to use a layer like Metaflow on top of them, to make them more usable for ML use cases.
 
 
 **As ML becomes more prevalent, it’ll present another attack vector for malicious actors.**
 
 
 1. **What steps do you think could be taken to increase ML security?**
 2. **Do you think increasing security would impact model performance? (latency, accuracy, etc.)**
 3. **Do you think AI security also goes hand in hand with AI ethics?**
 
 
 Security, as always, is highly contextual. Some applications need to be much more careful than others – increased security comes with a cost. Consider for instance Netflix recommendations that get their data from a highly controlled environment (Netflix apps) and even if someone manages to spoof the input data for a particular user, there’s a minimal downside. Compare that to, say, facial recognition for passport control at the border where the environment is much more chaotic, there are clear incentives for malicious behavior, and the impact of security breaches is much more drastic. You would approach ML security in fundamentally different ways in the two cases.
 
 
 **Where do you see real-time model deployment going? What seems to be the bottleneck: is it the lack of tooling, model capabilities, or difficulty of filtering noise in real-time?**
 
 
 See Chip’s post on using real-time data to generate more accurate predictions and adapt models to changing environments [here](https://huyenchip.com/2022/01/02/real-time-machine-learning-challenges-and-solutions.html).
 
 
 **Between the Reliability, Scalability, Maintainability, and Adaptability features of an ML system, which one will be the very first to pay attention to?**
 
 
 The answers depend on the use case, but I’d say Maintainability. Imagine having an ML system that’s, say, scalable and reliable but not maintainable – it doesn’t sound fun! It sounds like a legacy Fortran codebase. Usually, the best way to achieve Maintainability is to keep the code simple, which is especially important with ML systems that have all kinds of obscure failure modes. As Hoare said:
 
 
 There are two ways to write code: write code so simple there are obviously no bugs in it, or write code so complex that there are no obvious bugs in it.",H2,https://outerbounds.com/blog/ml-infra-humans-ama#deploying-ml-systems,False,3624.0,581.0
807,Metaflow Questions,"**Can you version control models/experiments/data in Metaflow?**
 
 
 Indeed you can! Metaflow tracks all runs (experiments) and [artifacts](https://outerbounds.com/docs/pass-artifacts-between-steps/) (including models and data) automatically and exposes various ways to access and analyze the data afterward, like the Client API.
 
 
 **Is it possible to integrate/introduce Metaflow in a more mature project on some other cloud providers like Azure/IBM/or some other?**
 
 
 Yep! Azure support is already in Metaflow – stay tuned for an official announcement in a few weeks GCP will soon follow.",H2,https://outerbounds.com/blog/ml-infra-humans-ama#metaflow-questions,False,597.0,85.0
808,What next?[​](https://outerbounds.com/blog/book-launch-effective-data-science-infrastructure/what-next),"Read the following books:
 
 
 * [Designing Machine Learning Systems: An Iterative Process for Production-Ready Applications](https://www.amazon.com/Designing-Machine-Learning-Systems-Production-Ready/dp/1098107969)
 * [Effective Data Science Infrastructure](https://www.manning.com/books/effective-data-science-infrastructure)
 
 
 Finally, we have many more upcoming events! Next we will be speaking with Shreya Shankar about operationalizing machine learning. [Register today so you won’t miss it!](https://www.eventbrite.com/e/operationalizing-machine-learning-tickets-428382823557?aff=ss)",H2,https://outerbounds.com/blog/ml-infra-humans-ama#what-next-https-outerboundscom-blog-book-launch-effective-data-science-infrastructure-what-next,False,593.0,50.0
809,The Gradual Path to Production,"When a project graduates from the confines of a data scientist’s laptop to production, it must be prepared to play nicely with the company’s overall engineering infrastructure. Gone are the days when data science projects could stay inside walled gardens that were purpose-built for small-scale experimentation. The sooner you integrate projects into the rest of the infrastructure, the faster you can [start delivering actual business value](https://www.youtube.com/watch?v=Dr6DsWa6Dhg) which should be a key objective of every data science project in the industry.
 
 
 Following this ethos, Metaflow encourages projects to become increasingly production-ready by going through the following stages:
 
 
 ![](_human-centric-data-science-on-kubernetes-with-metaflow_data/0_img)
 In the first stage, the data scientist prototypes a project on a personal workstation. Metaflow allows them to [build and test workflows locally](https://docs.metaflow.org/metaflow/basics), making sure that no re-implementation or re-architecting is needed when the project is ready to advance to the next stage.
 
 
 Once Metaflow has been configured [to integrate with the company’s cloud infrastructure](https://outerbounds.com/docs/admin), often with help from the engineering team, the project can advance to the second stage. Without any changes in the code, the data scientist can test their workflow with much [larger scale data and compute](https://docs.metaflow.org/metaflow/scaling-out-and-up), leveraging cloud-based compute resources.
 
 
 Finally, once the project has proven to work at scale, it can be [deployed to a production-grade workflow orchestration system](https://docs.metaflow.org/going-to-production-with-metaflow/scheduling-metaflow-flows) which takes care of keeping the workflow running in a highly-available manner, tightly integrated with the surrounding production systems. At this stage, the project is solidly in production: Models are retrained regularly and fresh predictions are produced without any human intervention.",H3,https://outerbounds.com/blog/human-centric-data-science-on-kubernetes-with-metaflow#the-gradual-path-to-production,False,2037.0,260.0
810,Human-Centric Data Science on AWS,"When [Netflix open-sourced Metaflow in 2019](https://netflixtechblog.com/open-sourcing-metaflow-a-human-centric-framework-for-data-science-fa72e04a5d9), we provided a graduation path using services provided by Amazon Web Services:
 
 
 ![](_human-centric-data-science-on-kubernetes-with-metaflow_data/1_img)
 [AWS Batch](https://aws.amazon.com/batch/) provides a straightforward solution for the second stage. It takes care of launching EC2 instances on the fly whenever a data scientist wants to scale out to the cloud. AWS Batch is [easy to setup](https://outerbounds.com/docs/aws-deployment-guide), it has minimal operational overhead, and no extra cost over the standard EC2 pricing, which makes it a great choice for teams who want to benefit from cloud computing with minimal hassle. The data scientist can simply execute `run --with batch` to scale out their projects to the cloud.
 
 
 For production deployments, AWS provides another solid service, [AWS Step Functions](https://aws.amazon.com/step-functions/), which keeps workflows running without any human intervention, also with minimal engineering headache. The data scientist can simply execute `step-functions create` to deploy their project to production.
 
 
 In general, these AWS services are a great choice for companies that are already using AWS and want to provide the three-stage graduation path for projects with minimal operational overhead.",H3,https://outerbounds.com/blog/human-centric-data-science-on-kubernetes-with-metaflow#human-centric-data-science-on-aws,False,1418.0,179.0
811,New: Metaflow on Kubernetes,"The biggest benefit of the AWS-native path – its simplicity – can also be its biggest weakness. While many companies are happy with this path, over the past years we have heard from many large organizations that their infrastructure needs are more demanding and they need more flexibility on their graduation path.
 
 
 Today, the platform that provides nearly an infinite amount of flexibility for engineers to build custom infrastructure is [Kubernetes](https://kubernetes.io/). Kubernetes is to engineering and data science what flour is to bread. You are not supposed to consume it as such but you can use it as an ingredient to bake something delicious (although it takes some skill and effort):
 
 
 ![](_human-centric-data-science-on-kubernetes-with-metaflow_data/2_img)
 This characteristic makes Kubernetes a great solution for organizations that need more flexibility with their infrastructure than what the AWS-native path can afford. **Today, we are happy to announce that Metaflow supports the Kubernetes path natively, as an equal alternative to the AWS-native path**:
 
 
 ![](_human-centric-data-science-on-kubernetes-with-metaflow_data/3_img)
 It is worth emphasizing that the benefits of Kubernetes are targeted at engineers, [not data scientists who don’t have to care about the underlying infrastructure](https://huyenchip.com/2021/09/13/data-science-infrastructure.html) when using Metaflow. The data scientist can write their flows as usual and scale out to the company’s Kubernetes cluster simply by executing `run --with kubernetes`. In fact, any flow that you used to `run --with batch` can now be `run --with kubernetes` without any changes in the code!
 
 
 The flexibility of Kubernetes is also its biggest drawback: Kubernetes is a complex ecosystem of a myriad of tools and approaches. The Metaflow-Kubernetes integration doesn’t make too many assumptions about how your cluster is configured. Still, operating a production-grade Kubernetes cluster tailored for your ML workloads can require making a lot of choices, tradeoffs, and engineering effort. 
 
 
 We provide [a baseline Terraform configuration](https://github.com/outerbounds/terraform-aws-metaflow/tree/master/examples/eks) that allows you to deploy Metaflow-ready Kubernetes resources on the [AWS managed Kubernetes service, EKS](https://aws.amazon.com/eks/) with a few commands but we anticipate that most serious organizations want to customize their security policies, autoscaling policies, and observability systems. If you are curious to learn how, [join our Slack to chat about best practices](http://slack.outerbounds.co).",H3,https://outerbounds.com/blog/human-centric-data-science-on-kubernetes-with-metaflow#new-metaflow-on-kubernetes,False,2622.0,356.0
812,New: Metaflow on Argo Workflows,"To support production-grade workflow orchestration natively on Kubernetes – stage 3 – we turned into [Argo Workflows](https://argoproj.github.io/argo-workflows/). Argo Workflows is arguably the most popular Kubernetes-native workflow orchestrator which, amongst other use cases, powers [Kubeflow Pipelines](http://kubeflow.org) under the hood.
 
 
 Argo Workflows ticks all the boxes [we require from a production-grade orchestrator](https://netflixtechblog.com/unbundling-data-science-workflows-with-metaflow-and-aws-step-functions-d454780c6280): It scales well, it is highly available, and it has an excellent UI to help with observability. Similar to Kubernetes, it is not a tool that a data scientist should need to use directly, but it provides a powerful backend for a human-centric tool like Metaflow.
 
 
 To benefit from Argo Workflows, a data scientist [needs to know only a single command](http://link-to-user-docs): `argo-workflows create`. The command schedules the workflow for automated execution on production-grade Kubernetes infrastructure without any changes in the scientist’s Python code.
 
 
 To get an idea of what the graduation path looks like on Kubernetes, including Argo Workflows, take a look at the short screencast below (no sound):
 
 
 
 
 
 The Metaflow-Argo Workflows integration was originally started by [SAP](https://sap.com), a quintessential example of a large organization with demanding infrastructure needs. Here’s how **Roman Kindruk** at SAP, who has been closely involved in the integration project describes their motivation for using Argo Workflows with Metaflow:
 
 
 
 > Our data scientists appreciate the simple, straightforward Python interface provided by Metaflow. At the same time, our engineers are deeply committed to using Kubernetes and Argo Workflows, so we wanted a solution that provides the best of both worlds. We are excited to see that a humble pull request we started a while ago has evolved into a first-class support for Argo Workflows and Kubernetes in Metaflow!
 > 
 > Roman Kindruk / SAP
 
 
 After SAP’s initial contribution, we worked closely with the Argo Workflows community to make sure the integration aligns with their long-term roadmap. **Yuan Tang**, a maintainer of Argo Workflows, commented:
 
 
 
 > We have been very happy to work with the Outerbounds team to provide a first-class integration between Metaflow and Argo Workflows. We are committed to making sure that Argo Workflows is the most robust and scalable workflow orchestrator on Kubernetes, capable of satisfying the needs of even the most demanding organizations.
 > 
 > [Yuan Tang](https://terrytangyuan.github.io/about/) / Argo Workflows & Akuity
 
 
 Today, after over a year of extensive development, testing, and validation, the integration passes our high bar of production-readiness. We are happy to invite you to deploy it in your environment and give it a try!",H3,https://outerbounds.com/blog/human-centric-data-science-on-kubernetes-with-metaflow#new-metaflow-on-argo-workflows,False,2916.0,427.0
813,Next Steps,"If you want to make your existing Kubernetes infrastructure easily accessible by data scientists, Metaflow is a great choice. They can rely on Kubernetes to scale out their projects and deploy to production confidently without being exposed to the complexities of modern infrastructure.
 
 
 To get started, take a look at the [Deployment Guide for Kubernetes](https://outerbounds.com/engineering/deployment/aws-k8s/deployment/). If you are wondering whether the AWS-native or the Kubernetes path is right for you, join [the Metaflow support Slack](http://slack.outerbounds.co) for more information and guidance. In either case, your feedback and feature requests are highly appreciated! 
 
 
 Also if you want to work on topics like this full time, note that [we are hiring](https://outerbounds.com/workwithus/).",H3,https://outerbounds.com/blog/human-centric-data-science-on-kubernetes-with-metaflow#next-steps,False,813.0,113.0
814,TL;DR,"* After the adoption of the modern data stack (MDS), organizations are still early in the journey to become “data-driven” and the MDS needs to be coupled with MLOps and actual data-powered software to succeed;
 * You’ll  learn about the challenges of working with data in a fast-paced, growing organization and how to overcome them, including [scalable end-to-end code](https://github.com/jacopotagliabue/post-modern-data-stack) showing how DataOps and MLOps can join forces with state-of-the-art tooling, such as [Metaflow](https://metaflow.org/) and dbt;
 * A key point in the solution is the abstraction level operated at: from data to serving, the entire pipeline does not need any special DevOps person, infrastructure work, or yaml files. SQL and Python are the only languages in the repository.
 * In a follow up post, we’ll show how good tools provide a better way to think about the division of work and productivity, thus providing an organizational template for managers and data leaders.",H2,https://outerbounds.com/blog/modern-data-stack-mlops#tl-dr,False,999.0,151.0
815,From the Modern Data Stack to MLOps,"The modern data stack (MDS) has been consolidated as a series of best practices around data collection, storage and transformation. In particular, the MDS encompasses three pillars: 
 
 
 * A scalable ingestion mechanism, either through tools (e.g. Fivetran, Airbyte) or [infrastructure](https://github.com/jacopotagliabue/paas-data-ingestion);
 * A data warehouse (e.g. Snowflake) storing all data sources together;
 * A transformation tool (e.g. dbt), ensuring versioned, DAG-like operations over raw data using SQL.
 
 
 A lot has been said already about the MDS as such, but the situation is more “scattered” on the other side of the fence: *once data is properly transformed, how is that consumed downstream to produce business value*? At the end of the day, ingesting and transforming data is not (for most companies) an end in itself: while tech giants figured out a while ago how to “get models in production”, most companies still struggle to productionize a model in less than [3 months](https://algorithmia.com/state-of-ml).
 
 
 Our solution is to accept the fact that not every company requires elaborate and infinitely scalable infrastructure like those deployed by Googles and Metas of the world, and [that is totally ok](https://www.youtube.com/watch?v=SAal0hqqE74): [doing ML at “reasonable scale”](https://towardsdatascience.com/ml-and-mlops-at-a-reasonable-scale-31d2c0782d9c) is more rewarding and effective than ever, thanks to a great ecosystem of vendors and open source solutions. In [the companion repository](https://github.com/jacopotagliabue/post-modern-stack), we demystify deep learning pipelines by training a model for sequential recommendations: if you see a shopper interacting with *k* products, what is she going to do next? We 
 
 
 * purposefully avoid toy datasets and local-only deployments;
 * provide a cloud-ready, “reasonable scale” project;
 * show how training, testing, and serving (the *MLOps* steps) are naturally embedded in a pipeline that starts with data and features (the *DataOps* steps).
 
 
 The backbone for this work is provided by Metaflow, our open-source framework which (among other things) lowers the barrier to entry for data scientists to take machine learning from prototype to production and the general stack looks like this, although Metaflow will allow you to switch in and out any other component parts:
 
 
 ![](/blog/og/modern-data-stack-mlops.png)
 In this and a follow-up post, we tell the story of how tools and culture changed together during our tenure at a fast-growing company, Coveo, and share [an open-source repository](https://github.com/jacopotagliabue/post-modern-stack) embodying in working code our principles for data collaboration: in our experience, DataOps and MLOps are better done under the same principles, instead of “handing over” artifacts to the team on the other side of the fence. In particular, our goal is twofold: 
 
 
 * Based on our experience, we provide a *technical* template for teams starting up, and wondering how to join DataOps and MLOps efficiently;
 * We show how good tools provide a better way to think about the division of work and productivity, thus providing an *organizational* template for managers and data leaders. We’ll do this in a follow-up post.
 
 
 All in all, while we are excited by the [growth in the space](https://www.mihaileric.com/posts/mlops-is-a-mess/), we also know that the number of options out there can be intimidating *and* that the field is still chaotic and quickly evolving. Our solution is a practical stack that both provides a reliable first step for teams testing the MLOps waters and shows in detail how simple pieces go a long way towards building ML systems that scale.
 
 
 You can find a video tutorial on the repository guiding this post here:
 
 ```mdx-code-block
 import LiteYouTubeEmbed from 'react-lite-youtube-embed';
 import 'react-lite-youtube-embed/dist/LiteYouTubeEmbed.css';
 
 <div className=""video-container"">
 <LiteYouTubeEmbed
         id=""5kHDb-XGHtc""
         title=""The Post Modern Data Stack""
 />
 </div>
 ````",H2,https://outerbounds.com/blog/modern-data-stack-mlops#from-the-modern-data-stack-to-mlops,False,4089.0,603.0
816,Leveraging the Modern Data Stack for Machine Learning,"How does this stack translate good data culture into working software at scale? A useful way to isolate (and reduce) complexity is by understanding *where computation happens*. In our pipeline, we have four computing steps, and two providers:
 
 
 * Data is stored and transformed in Snowflake, which provides the underlying compute for SQL, including data transformations managed by a tool like dbt;
 * Training happens on AWS Batch, leveraging the abstractions provided by Metaflow;
 * Serving is on SageMaker, leveraging the PaaS offering by AWS;
 * Scheduling is on AWS Step Functions, leveraging once again Metaflow (not shown in the repo, but [straightforward to achieve](https://docs.metaflow.org/going-to-production-with-metaflow/scheduling-metaflow-flows)).
 
 
 A crucial point in our design is the abstraction level we chose to operate at: the entire pipeline does *not* need any special DevOps person, infrastructure work, or yaml files. SQL and Python are the only languages in the repository: infrastructure is either invisible (as in, Snowflake runs our dbt queries transparently) or *declarative* (for example, you specify the type of computation you need, such as GPUs, and Metaflow makes it happen). 
 
 
 While the DataOps part may be familiar to you, it is worth giving a high-level overview of the MLOps part. Remember: *transforming and normalizing data is rarely an end in itself – data is valuable only insofar as you get something out of it, insights from visualization or predictions from machine learning models.* 
 
 
 [Our stack leverages a SQL DAG](https://github.com/jacopotagliabue/post-modern-stack/blob/fa94da5a131f6b3548f0dd1b9454d3d05dd9166e/src/dbt/models/metaflow/nep_session_events.sql#L7) – as built by dbt –  to run the necessary transformation from raw shopping events to sequences of product interactions for our recommendation model: by using SQL we accomplish two important goals: 
 
 
 * *first*, the ML team can work directly off-the-work of analytics engineer in a unified environment (SQL is the *lingua franca* of data, so to speak);
 * *second*, we push down to Snowflake all the (distributed) computing, making the Python side of things pretty lightweight, as we are not really bound by machine memory to aggregate and filter data rows.
 
 
 ![](_modern-data-stack-mlops_data/1_img.png)
 <Caption>This code shows that since data is already transformed, a simple query is all you need to get your dataset ready for downstream modeling.</Caption>
 
 The training part itself is a vanilla deep model [for session recs](https://arxiv.org/abs/2007.14906) and nothing hinges on its details: the same principles would apply to more complex models. The exciting part is that a simple SQL query, easy to read and maintain, is all that is needed to connect feature preparation and deep learning training on a GPU in the cloud. Training a model produces an artifact (that is, a ready-to-use model!), which can now generate predictions: as it’s good practice to [test the model on unseen data](https://machinelearningmastery.com/train-test-split-for-evaluating-machine-learning-algorithms/) before deploying it on real traffic, we showcase in *test\_model* a draft of an advanced technique, [behavioral testing](https://github.com/jacopotagliabue/reclist). 
 
 
 Finally, our pipeline ends with deployment, that is, the process of shipping the artifact produced by training and validated by testing to a public endpoint that can be reached like any other API; by supplying a shopping session, the endpoint will respond with the most likely continuation, according to the model we trained. 
 
 
 This is a comfortable stack for the data scientist, seasoned ML engineer, the analytics engineer who is just getting started with Python, and even the PM monitoring the project: collaboration and *principled* division of labor is encouraged. While we present a RecSys use case, most if not all of the above considerations apply to analogous pipelines in many other industries: [you do not need a bigger boat](https://github.com/jacopotagliabue/you-dont-need-a-bigger-boat), after all, to do cutting-edge ML. 
 
 
 This stack can also be run in increasingly complex configurations, depending on how many tools / functionalities you want to include: even at “full complexity” it is a remarkably simple and “hands-off” stack for terabytes-scale processing; also, everything is fairly decoupled, so if you wish to swap SageMaker with [Seldon](https://outerbounds.com/blog/machine-learning-pipelines-from-prototype-to-production), or Comet with Weights & Biases you will be able to do it in a breeze.
 
 
 The full stack contains ready-made connections to an experiment tracking platform and a (stub of a) custom DAG card showing how the recommender system is performing according to [RecList](https://github.com/jacopotagliabue/reclist), Jacopo’s team’s open-source framework for behavioral testing. We could write an article just on this (well, we actually [did write it](https://towardsdatascience.com/ndcg-is-not-all-you-need-24eb6d2f1227), plus [two scholarly](https://arxiv.org/abs/2111.09963) [papers](https://arxiv.org/abs/2110.13601)): even as a very small testing routine, we wanted to make sure to include it as a general reminder of how important is to understand model behavior before deployment, and how readable, shareable, low-touch documentation is crucial for ML pipelines in production.
 
 
 This stack is simple, yet effective and modular: it may not be the end of the journey, but it surely makes a very good start. 
 
 
 In a follow-up post, we’ll show how good tools provide a better way to think about the division of work and productivity, thus providing an organizational template for managers and data leaders. If these topics are of interest, come chat with us on [our community slack here](http://slack.outerbounds.co/).
 
 
 **Acknowledgments**
 
 
 Special thanks: Sung Won Chung from dbt Labs, Patrick John Chia, Luca Bigon, Andrea Polonioli and Ciro Greco from Coveo. None of this would have been possible without the fantastic dataset [released by Coveo last year](https://github.com/coveooss/SIGIR-ecom-data-challenge), containing millions of real-world anonymized shopping events.
 
 
 If you liked this article, please take a second to support our open source initiatives by adding a star to our [RecList package](https://github.com/jacopotagliabue/reclist), [this repo](https://github.com/jacopotagliabue/post-modern-data-stack), and [Metaflow](https://github.com/netflix/metaflow).",H2,https://outerbounds.com/blog/modern-data-stack-mlops#leveraging-the-modern-data-stack-for-machine-learning,False,6550.0,920.0
817,Can we fix Airflow?,"While walking through a DAG seems like a schoolbook exercise, it is easy to underestimate the number of engineering-years and battle scars that it takes to [build a real-world, production-grade workflow orchestrator](https://netflixtechblog.com/orchestrating-data-ml-workflows-at-scale-with-netflix-maestro-aaa2b41b800c). These challenges are not limited to large companies. In a modern experimentation-driven culture every variant counts, so even smaller companies can accumulate surprisingly many workflows quickly. In 2023, developing and deploying a new workflow variant should be as easy as opening a pull request.
 
 
 Each workflow can spawn thousands of tasks – imagine [conducting a hyperparameter search](https://outerbounds.com/docs/grid-search-with-metaflow/) as a part of a nightly model training workflow. And every workflow and task needs to be executed in a highly available manner while reacting to a torrent of external events in real time. We talked about these topics at length when [we released our integration to another production-grade orchestrator, AWS Step Functions](https://netflixtechblog.com/unbundling-data-science-workflows-with-metaflow-and-aws-step-functions-d454780c6280).
 
 
 Herein lies the root cause of many issues in Airflow: It served its original use cases well but [its design and architecture are not suitable](https://www.uber.com/blog/managing-data-workflows-at-scale/) for the increasing demands of modern data (science) stacks. Airflow is perfectly capable of orchestrating a set of basic data pipelines, but with the increasing demands of ML and data science – and more modern data organizations – its cracks are becoming visible.
 
 
 Fixing these issues while maintaining backward compatibility with the millions of existing Airflow pipelines is nigh impossible. Airflow will surely keep improving, as it did with [the major release of Airflow 2.0](https://airflow.apache.org/blog/airflow-two-point-oh-is-here/), but migrating existing pipelines to new untried APIs is not necessarily easier than migrating to another, more modern orchestrator.
 
 
 As a result, many companies find themselves in a pickle: They have a hairball of business-critical data pipelines orchestrated by Airflow, encapsulating years of accumulated business logic. At the same time, they are becoming increasingly aware that the system is slowing down their development velocity and causing avoidable operational overhead.",H2,https://outerbounds.com/blog/better-airflow-with-metaflow#can-we-fix-airflow,False,2449.0,319.0
818,"Develop with Metaflow, deploy on Airflow","We want to provide a new path for teams that find themselves in this situation. Our new Airflow integration allows you to develop workflows in Metaflow, using its data scientist-friendly, productivity-boosting APIs, and deploy them on your existing Airflow server, as shown in [the video below](https://www.youtube.com/watch?v=TDIBX-_U7pM):
 
 <a href=""https://www.youtube.com/watch?v=TDIBX-_U7pM"">
 
 ```mdx-code-block
 import ReactPlayer from 'react-player';
 
 <ReactPlayer playing controls muted loop url='/assets/airflow_dev.mp4' width='100%' height='100%'/>
 ```
 </a>
 
 
 Deploying Metaflow to Airflow requires no changes in the code - just a single command:
 
 ```
 python flow.py airflow create airflow_dag.py
 ```
 
 Operationally nothing changes. The resulting workflows will get scheduled as any other Airflow workflows and they live happily side-by-side with your existing Airflow-native workflows. Under the hood, Metaflow translates its flows to Airflow-compatible DAGs automatically, so the operational concerns are invisible to data scientists who can benefit from the features of Metaflow.
 
 
 Consider the benefits of using Metaflow for workflow development compared to Airflow:
 
 
 * **Faster development speed** thanks to local testing and debugging experience: You can develop and execute Metaflow workflows locally, [resume executions at any point](https://docs.metaflow.org/metaflow/debugging), and use idiomatic Python for structuring your projects. This is a key reason [why](https://outerbounds.com/blog/mlops-fin-tech/) [companies](https://medium.com/cnn-digital/accelerating-ml-within-cnn-983f6b7bd2eb) [choose](https://medium.com/realtor-com-innovation-blog/improving-data-science-processes-to-speed-innovation-at-realtor-com-b6b90fa530dc) [Metaflow](https://www.rea-group.com/about-us/news-and-insights/blog/accelerating-experimentation-with-mlops/).
 * **Scalable compute** – Airflow was never designed for high-performance, large-scale compute, which is critical for data science and ML workloads. In contrast, Metaflow treats both [horizontal and vertical scalability as a first-class citizen](https://docs.metaflow.org/scaling/introduction).
 * **Seamless data flow** – It is not very convenient to maintain state across steps through the [XCOM mechanism in Airflow](https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/xcoms.html), so steps tend to be rather isolated units of execution. In contrast, [data artifacts](https://docs.metaflow.org/metaflow/basics#artifacts) are a key construct in Metaflow, making it more natural to express stateful, data-centric business logic as a workflow.
 * **Built-in versioning and tracking** – Metaflow tracks and versions all executions, artifacts, and code automatically, [enabling experiment tracking](https://docs.metaflow.org/metaflow/client), [model monitoring](https://docs.metaflow.org/metaflow/visualizing-results), model registries, and [versatile experimentation](https://docs.metaflow.org/production/coordinating-larger-metaflow-projects) out of the box.
 
 
 All these features are [critically important for data science and ML projects](https://docs.metaflow.org/metaflow/introduction) and they also [pair well with the modern data stack](https://outerbounds.com/blog/dataops-mlops-reasonable-organizations/).",H2,https://outerbounds.com/blog/better-airflow-with-metaflow#develop-with-metaflow-deploy-on-airflow,False,3316.0,343.0
819,Coexisting flows,"![](_better-airflow_data/0_img)
 The Airflow integration in Metaflow allows you to benefit from nearly all features of Metaflow, while using a central Airflow server to orchestrate all workflows. This allows data and platform engineers to schedule, monitor, and operate all DAGs on your existing Airflow deployment without changes, while allowing data scientists to benefit from a modern toolchain that matches their needs.
 
 
 Crucially, the integration doesn’t try to provide complete coverage of all Airflow features. Airflow comes with a myriad of APIs, operators, and sensors, many of which are not relevant for most data scientists and new projects. A key feature of the integration is that you can run native Airflow pipelines and newly authored Metaflow projects side-by-side, so if a feature is missing in the integration, you can implement the workflow natively for Airflow as before.
 
 
 We are excited to support basic foreaches, aka [dynamic task mapping in Airflow](https://airflow.apache.org/docs/apache-airflow/2.3.0/concepts/dynamic-task-mapping.html), as they are a key construct in Metaflow (you need Airflow 2.3 or newer to use them). However, you will get a clear error message if you try to deploy Metaflow flows containing nested foreaches, as Airflow doesn’t support them yet. Also, only [@kubernetes](https://docs.metaflow.org/scaling/remote-tasks/kubernetes) is supported as a compute layer, which maps to [KubernetesPodOperator](https://airflow.apache.org/docs/apache-airflow-providers-cncf-kubernetes/stable/operators.html) in Airflow at the moment.
 
 
 Since [Metaflow works with all the major clouds, AWS, Azure, and Google Cloud](https://outerbounds.com/blog/metaflow-on-all-major-clouds/), Metaflow's Airflow integration works in these clouds out of the box, when you use your own self-managed Airflow server.
 
 
 Besides self-managed Airflow, we have tested the integration with AWS’ managed Airflow, [Amazon MWAA](https://aws.amazon.com/managed-workflows-for-apache-airflow/), which works well with Metaflow too. If you don’t have legacy reasons to use Airflow, Metaflow’s existing integrations with [AWS Step Functions](https://docs.metaflow.org/production/scheduling-metaflow-flows/scheduling-with-aws-step-functions) or [Argo Workflows](https://docs.metaflow.org/production/scheduling-metaflow-flows/scheduling-with-argo-workflows) will likely provide a better experience, as they come with fewer limitations (in particular, nested foreaches are supported).",H2,https://outerbounds.com/blog/better-airflow-with-metaflow#coexisting-flows,False,2498.0,304.0
820,"Develop with Metaflow, choose where to deploy","![](_better-airflow_data/1_img)
 
 At some point, you may want to explore whether an alternative workflow orchestrator would provide better scalability characteristics, higher availability, and better developer experience than Airflow in your environment.
 
 Workflows written using Airflow’s APIs are naturally only compatible with Airflow, so orchestrating them elsewhere is not an option. However, workflows developed with Metaflow can be orchestrated by any system supported by Metaflow, like AWS Step Functions or Argo Workflows, without any changes in your code, as shown [in the video below](https://www.youtube.com/watch?v=dC_8O2Nd5q4):
 
 <a href=""https://www.youtube.com/watch?v=dC_8O2Nd5q4"">
 
 ```mdx-code-block
 <ReactPlayer playing controls muted loop url='/assets/airflow_argo.mp4' width='100%' height='100%'/>
 ```
 </a>
 
 
 This means that you can start authoring workflows with Metaflow today and deploy them initially on Airflow, avoiding a massive migration project upfront. Over time, as more workflows are written with Metaflow, you are able to start testing them on, say, Argo Workflows without any migration tax.
 
 
 If the results are promising, you can run Argo Workflows and Airflow side by side – soon you will be able to trigger Argo workflows from Airflow via events – again minimizing the need for abrupt changes. You can keep operating with this pattern as long as needed, for instance, if data engineers want to keep using Airflow, or you can use this as a smooth migration path to a modern workflow orchestrator.",H2,https://outerbounds.com/blog/better-airflow-with-metaflow#develop-with-metaflow-choose-where-to-deploy,False,1548.0,223.0
821,Get started today,"It is easy to get started with the new Airflow integration. Simply install Metaflow as usual
 
 
 ```
 pip install metaflow
 ```
 
 
 write a flow, save it in `flow.py`, and execute
 
 
 ```
 python flow.py airflow create airflow_dag.py
 ```
 
 
 and deploy the resulting Airflow DAG to your Airflow server!
 
 For more information, take a look at our [user-facing documentation for Airflow](https://docs.metaflow.org/production/scheduling-metaflow-flows/scheduling-with-airflow) and [Using Airflow with Metaflow](https://outerbounds.com/engineering/operations/airflow/) for engineers.
 
 If you have any questions or you want to share your thoughts or feedback about the integration, join us and over 2000 data scientists and engineers in the [Metaflow community Slack](http://slack.outerbounds.co) workspace!",H2,https://outerbounds.com/blog/better-airflow-with-metaflow#get-started-today,False,810.0,108.0
822,The Value that Machine Learning Creates for Realtor.com,"**Hugo:** Why does Realtor.com do machine learning and data science? What type of value do these functions create for Realtor.com?
 
 
 **Russell:** Yeah, ultimately, that’s the number one question to answer, right? I mean, if ML is doing nothing, and data science doing nothing, our company is just throwing money down the drain. A lot of money! I think the answer is sometimes yes, actually, for quite a few companies. But I think in aggregate, I think long story short, it’s definitely providing value for a lot of people and at Realtor.com, it’s a real estate platform that just does business in the United States. It competes with companies like Zillow, things like that, where it’s an aggregator of all the MLS data, all the real estate homes for sale rent apartments for rent across the entire country, to grab that contents, aggregate it, make it nice, presentable, clean, and, you know, help consumers find a place to live. 
 
 That’s ultimately what kind of goal of the company, and from a machine learning standpoint, you can visit, I mean, a whole suite of stuff, there’s personalization on the front end consumer-facing website, there’s search, there are recommendation engines, there’s, of course, image data that we get, NLP data that we get, we have deep learning models for that, we have models to help people find real estate professionals like real estate agents that are well suited to their interests, like their price range where they live, all sorts of different factors that they might have that are specific to them. We have probabilistic models for that, that are Bayesian, we have tree-based models, we have the old game that we have, we even have audio data that we did have some other kind of like deep learning models that can run over stuff.
 
 
 **Hugo**: Yeah, great. And something else I’m hearing in there is that I mean, a lot of things, including real estate, can boil down to matching problems in a lot of ways. So finding the right set of properties for the right user. So it’s really, I mean, machine learning seems not only interesting but almost fundamental in the computation age.
 
 
 **Russell:** Sure. I mean, there’s not only a huge variability in housing, but there’s also a huge fluctuation in housing inventory, what is available, when, where, what’s relevant, how can you notify me so I can get, you know, because I think that a people when they’re on a home buying process, or even in an apartment process, like, they have a goal, it’s a very realistic real-world thing that they need to solve, which is like, I need shelter. It’s not just like, I’ve gotta go watch some video on YouTube to entertain myself, it’s usually like, how can I tangibly solve a problem that I’m trying to solve in my life?",H2,https://outerbounds.com/blog/how-to-build-enterprise-ml-platforms#the-value-that-machine-learning-creates-for-realtorcom,False,2751.0,484.0
823,How to Demonstrate the Value of Data Science and ML in Your Organization,"**Hugo**: I’m interested in how you think about demonstrating the impact of data and machine learning functions in organizations such as Opcity, and realtor.com, essentially getting everyone on board for the project.
 
 
 **Russell**: There are definitely a lot of ways in a pure business sense, right? There’s of course monetization, right? So that’s, you know, if you have the ability to measure your impact on revenue, whether that’s directly on revenue, or via proxy metrics, you know, things like your kind of normal, say, website traffic, where it’s like, what’s the click through rate that’s, well, time, the, you know, user engagement and churn rates that, you know, kind of pretty standard website metrics nowadays, you have those. 
 
 But there’s even a lot of interesting specifics to real estate, so, you know, it’s very slow moving, it’s the real world, like buying a house takes months, even if you’re moving quickly, so being able to measure impact is not always an immediate response, where you can, you know, very quickly to say, Oh, I’m going to build a real time model feedback loop that can grab it, and it’s going to retrain every day and this and that, and Yes, I can for certain parts of it, but you also have to acknowledge that there are other parts where for ground truth data for the actuals you know, it takes months for some of this stuff. 
 
 So in terms of adding impact, you know, one way to almost add impact is even just to let people move more quickly. So we even have a whole suite of models that are basically just like, you know, time to event modeling type stuff where we can say, okay, rather than having to wait six months to observe, if something’s gonna happen, maybe we can, you know, build predictive models that with a high degree of competence can say, this will or will not happen, but instead of six months, maybe it’s one month, and in turn, we can then experiment and iterate much more quickly.
 
 
 
 ```mdx-code-block
 <div className=""video-container"">
 <LiteYouTubeEmbed
         id=""DeOyQOTk6Pg""
         title=""How to demonstrate the value of data science and ML in your organization""
 />
 </div>
 ```",H2,https://outerbounds.com/blog/how-to-build-enterprise-ml-platforms#how-to-demonstrate-the-value-of-data-science-and-ml-in-your-organization,False,2157.0,389.0
824,How to Build an Enterprise ML Platform from Scratch,"**Hugo:** you built out the entire machine learning platform at Realtor.com, taking both organizations from a bus factor of one to reproducible automated machine learning-powered software. Tell us about this journey. 
 
 
 **Russell:** Yeah, I mean, I think, an interesting component of how you build these things, there’s also the industry has been actively coalescing on its direction over time as well. I mean, if you think just like 5 or 10 years ago, machine learning was kind of it was starting to get a lot of traction, a lot of companies kind of wanted to have it didn’t really have it, I would even argue to this day, most companies especially not like tech companies are still figuring it out. They’re still trying to decide how many people do we need for this. Is it part of analytics? Is it part of engineering? Is it its own thing? 
 
 So I think there’s very much still like a lot of uncertainty even and just like the organizational sense of like, how these things work, much less how that organization, in turn, operates on infrastructure? Is it a shared AWS account? Is it a separate AWS account? Do you have engineering people that, put you in a little sandbox and say, Here’s your sandbox, have fun, don’t create production? There are so many ways you could do this, even depending on the specifics of your company, to the opposite example of being a startup. It’s kind of nice, right? Because you like a clean slate. 
 
 So it’s like, okay, one that’s, you know, there’s no tech debt. That’s cool. You don’t have to worry about like, Oh, am I going to like piss off someone when I say let’s go rewrite this thing from scratch because it sucks. Instead, it’s like, okay, I got it, I can just use industry best practices straight from the get-go. That’s cool. But, you know, the flip side of that is also, you have a clean slate, it’s kind of like, like, it’s kind of overwhelming, if you don’t even know like, what, you know, you log into AWS, there’s like, like, 1500 services, or whatever they have now. Yeah. Like, where do you start? Right? 
 
 For us, it was basically like, okay, you know, you don’t have many people. So how it started was basically just like self-solving for yourself at that point. And it’s like, Alright, let’s get some EC2 instances going, let’s just kind of do what we were doing on our laptop, let’s just do it over here on this AWS server, and kind of the laptop in the cloud kind of mentality that I know you guys have in your documentation as well, which is, I think, really empowering like cloud-first development as a paradigm, I think is here to stay, it is only going to become more prevalent, more common, just because it makes sense. 
 
 
 Imagine you’re trying to query some database, grab some data. And if you have to, download that data set, like over the public internet, to your home, Wi-Fi onto your laptop, just to like do something to them. Just keep it in AWS, it’s safer. It’s more secure. It’s higher bandwidth. It’s just better all around, really. But so basically, it’s kind of start with that and then gradually add complexity. So you start with that we’re like, okay, we need data, how do we get it do we attach to our production database, let’s just make a copy will spin up our own separate kind of replicate database, we’ll hit that. 
 
 And then gradually, it’s like, okay, we need automation. Let’s start using AWS batch and step functions. And this is actually back pre-Metaflow. At this point right now, just for kind of context so when Metaflow really kind of starts sitting in was really pretty much right after the open source release. So in 2019, I guess, I think around there, where there was an AWS reinvent presentation that I was watching. And I remember just checking it out. And as I was watching it, I was like, oh my god, this is like all the same services were already there just building a really nice Pythonic abstraction for all the same crap that I was having to do manually by hand. Like making like like AWS states language-type things for step functions. If anyone’s ever had to do that by hand, I feel sorry for you. It sucks.
 
 
 **Hugo:** Some of our audience may not know what Metaflow is. Would you give me the elevator pitch on Metaflow?
 
 
 **Russell:** I mean, I think the high level is just like, it’s a clean abstraction on top of the data science and machine learning lifecycle projects might be like, high level behind that, right? Like, what does that even mean? A machine learning lifecycle? You know, there’s infrastructure components to that? There are the organizational aspects of that, like, who owns this code? What happens when it breaks? The someone on call, how do I check what it was when it failed? So there’s like workflow components, organizational components, infrastructure, kind of like infrastructure as code kind of components to it. 
 
 But broadly, I would just say, even back then, when I saw the presentation, I was like, okay, presentation, cool. I’m gonna just kind of talk through this and see if it works. So within, a day or two, I basically just downloaded the code, install the kind of just like, made a little POC on top of some projects that I already had employed. And, to my surprise, I was like, this is actually very nice. Yeah, you know, because there were even other things that we’re trying to do the same abstractions back then for like AWS batch, and like step functions, like you could use the way it was like, the AWS CDK. And it was like this, like kind of trying to be like, infrastructure as code abstraction on top, but it still sucked. It wasn’t good.
 
 
 **Hugo:** And one of the things, I think that really resonates with me, and one way for our audience that I think about Metaflow is that it’s a framework or library that allows you to write idiomatic, Python, and write all your science stuff for modeling features, that kind of stuff, while allowing you to access all the infrastructure resources, such as batch or step functions, or Kubernetes, or whatever it is, right? So that you can write the code that you want to write without messing around in configuration files all the time, it gives you access to version control for your models, and data, and all of these, all of these types of things as well. But you raise an interesting point that one of the value props, or something I’ve heard a lot of people value in Dataflow is that it is idiomatic Python, you don’t have to learn any special SDK or anything along those lines, right? You can write the code, you know, and love as quickly as possible.
 
 
 **Russell:** I think one thing that I would like to summarize is that there are far too many tools in the ML space right now. Like, I feel like we’re at an inflection point where like, we got a lot of hype, there’s a lot of VC money coming in building a lot of tools, tools are definitely needed. But I think we also kind of need to let the tools, fight it out, print it down, and distill it down into some really nice, clean interfaces. That will be, I think, very nice for years to come in the future. But yeah, with respect to that, I think Metaflow is like, it’s one of those things where like many tools are functionally correct. But like, not very many things are tastefully correct? And the interface of Metaflow, in my mind, feels correct. That’s very, very nice.
 
 
 ```mdx-code-block
 <div className=""video-container"">
 <LiteYouTubeEmbed
         id=""ZqaP5d5VRZo""
         title=""How to build an enterprise ML platform from scratch""
 />
 </div>
 ```",H2,https://outerbounds.com/blog/how-to-build-enterprise-ml-platforms#how-to-build-an-enterprise-ml-platform-from-scratch,False,7542.0,1386.0
825,Software and Platform Engineering Skills for Data Scientists,"After our fireside chats, we have async AMAs with our guests on [our community slack](http://slack.outerbounds.co/). A lot tends to happen there so join us if you’re interested in such conversations! This week, for example, Hugo asked
 
 
 > It was really fun chatting about SWE skills for data scientists. I'm wondering what advice you would give data scientists who are interested in becoming platform engineers and vice versa?
 
 
 And Russell replied as follows:
 
 
 right back at ya Hugo!
 
 
 my advice would be that it’ll certainly pay dividends regardless of which end of that continuum you’re coming from – if it’s something that interests you, dive right in!
 
 
 there’s tons of value in having a “T-shaped” skillset, with a bit of exposure across the stack outside of your norm, even if it’s just helping you more effectively communicate across teams. I’m a fan of the [philosophy outlined here](https://multithreaded.stitchfix.com/blog/2019/03/11/FullStackDS-Generalists/).
 
 
 more concretely for each, I’d say:
 
 
 DS :arrow_right: Platform Eng
 
 
 > Get exposure to the infra that you’re already working with and using day-to-day (and is likely abstracted away a fair bit) – things like containerization, distributed processing, job schedulers/orchestration, monitoring/alerting, software-defined networking, and security.
 > It can be easy to take for granted the complexity of infra, especially when it’s working well! Pick something you use, and try to dig down into its constituent parts – e.g. if you’re using a cloud-based VSCode devcontainer, how is that provisioned onto hardware? How are you connecting to that hardware? Does it use a public IP or a service like AWS SSM for secure communication? Does it go through your company Okta SSO or another means of authentication with TTL? Peel back the abstraction layers to try to understand how your tools fit together into a platform.
 
 Platform Eng :arrow_right: DS
 > Investigate pain points you hear talking with your DS/ML teams – it’s nice to have collaboration, they’ll likely be super grateful for the help, and along the way you can gain exposure to the types of problems the business is trying to solve.
 > From those problems, find one that sounds interesting to you and use it as a creative outlet to apply what you’re learning and try to replicate some modeling techniques. Think if there are any unique new features you could add, try them out, and realize that you might have an entirely different perspective or novel insight into the business process that the DS might not have had! Share your findings and ask for help critiquing the model/features!
 
 
 [Join us on slack](http://slack.outerbounds.co/) for more such conversations and also join us for our next fireside chat:  Navigating the Full Stack of Machine Learning with Ethan Rosenthal (Square). We’ll be discussing the wild west of full stack machine learning and how to make sense of all the feature stores, metric layers, model monitoring, and more with a view to deciphering what mental models, tools, and abstraction layers are most helpful in delivering actual ROI using ML. [You can sign up here](https://www.eventbrite.com/e/navigating-the-full-stack-of-machine-learning-tickets-475440845367?aff=blogrb)!",H2,https://outerbounds.com/blog/how-to-build-enterprise-ml-platforms#software-and-platform-engineering-skills-for-data-scientists,False,3265.0,517.0
826,From notebooks to production machine learning,"[Metaflow](https://docs.metaflow.org/) is an ergonomic Python framework created at Netflix for building production ML systems. The data team at Netflix is also famous [for notebook innovation](https://netflixtechblog.com/notebook-innovation-591ee3221233) in data science workflows.  This notebook innovation was revolutionary because it provided mechanisms to integrate notebooks into production data science workflows by providing the [following features](https://netflixtechblog.com/scheduling-notebooks-348e6c14cfd6):
 
 
 ![](_notebooks-in-production-with-metaflow_data/1_img)
 There are many ways to use notebooks in production-ready ML applications, ranging from using notebooks as a complete [software development environment](https://nbdev.fast.ai/) to using notebooks as [reporting, visualization, and debugging tools](https://netflixtechblog.com/scheduling-notebooks-348e6c14cfd6), and [these are key questions in the MLOps space](https://outerbounds.com/blog/mlops-vs-devops/).  The features demonstrated in this post focus on the latter but do not preclude the former.  We think notebooks are great reporting and visualization tools because:
 
 
 * Notebooks allow data scientists to use tools they are very familiar with, including very mature visualization libraries to create visualizations that are appropriate for the problem at hand, allowing for faster iteration.
 * There is no need to push data into another system like Looker or Tableau for multi-stakeholder consumption.
 * You can author interactive visualizations with tools like Altair and Bokeh.
 * Notebooks can also offer a convenient debugging environment for your Metaflow workflows, as you can [use notebooks to interactively inspect the result of your workflows](https://docs.metaflow.org/metaflow/debugging#inspecting-data-with-a-notebook).
 
 
 **This is why we created the** [**notebook card**](https://github.com/outerbounds/metaflow-card-notebook)**, a** [**Metaflow card**](https://outerbounds.com/blog/integrating-pythonic-visual-reports-into-ml-pipelines/) **that allows you to**:
 
 
 * Access data from any step in your DAG so you can visualize it or otherwise use it to generate reports in a notebook.
 * Inject custom parameters into your notebook for execution.
 * Ensure that notebook outputs are reproducible.
 * Keep the resulting notebooks versioned and organized automatically.
 * Isolate notebook-based reporting from the other business logic, making sure that errors in the notebook cannot cause the workflow to fail.
 * Render your notebooks as reports or model cards that can be embedded in various apps.  Notebook Cards generated from a flow are shareable directly from the [Metaflow GUI](https://netflixtechblog.com/open-sourcing-a-monitoring-gui-for-metaflow-75ff465f0d60).
 * Run notebook(s) programmatically in your Metaflow DAGs. With the notebook card, you can automatically inject notebooks with the run\_id, flow\_name, and task\_id so that users can debug workflows immediately upon a failure in your workflow.
 
 
 Additionally, you can use all of the features of Metaflow to manage the execution of notebooks, for example:
 
 
 * [Managing dependencies](https://docs.metaflow.org/metaflow/dependencies) (e.g., @conda)
 * [Requesting compute](https://docs.metaflow.org/metaflow/scaling) (e.g., @resources)
 * [Parallel execution](https://docs.metaflow.org/metaflow/basics#foreach) (e.g., foreach)
 
 
 Crucially, this feature works equally on a laptop during prototyping as well as when deployed to [a cloud-based production environment](https://docs.metaflow.org/going-to-production-with-metaflow/scheduling-metaflow-flows). Traditionally notebooks have been hard to deploy into production, necessitating a separate solution for prototyping and production, causing unnecessary busywork for the data scientist.  
 
 
 Notebook cards allow you to use notebooks as reporting, visualization, and debugging tools. For example, let’s say that you wanted to visualize model performance metrics, feature importance, hyperparameter tuning, and partial dependence plots in your notebook. You can do so! Note that these are just examples of many different visualizations you could build:
 
 
 ![](_notebooks-in-production-with-metaflow_data/2_img)
 The above visualizations were created in a notebook and refreshed with data specific for the pipeline run. The [project’s README](https://github.com/outerbounds/metaflow-card-notebook) has instructions on how to do this.
 
 
 Also note that this is model framework-agnostic and so will work with all types of ML models, whether they be scikit-learn, TensorFlow, PyTorch, or XGBoost.",H3,https://outerbounds.com/blog/notebooks-in-production-with-metaflow#from-notebooks-to-production-machine-learning,False,4637.0,560.0
827,How to use notebook cards,"You can get started with Notebook cards by installing them with pip:
 
 
 
 ```
 pip install metaflow-card-notebook
 ```
 
 Afterward, you need to decorate the appropriate step with @card(type=’notebook’) and assign the nb\_options\_dict variable to the path of the notebook as illustrated in the toy example below:
 
 
 
 ```python title=""flow.py""
 from metaflow import step, current, FlowSpec, card
 from mymodel import train_model
 
 class NBFlow(FlowSpec):
     ""A toy example of using the notebook card.""
 
     @step
     def start(self):
         # Train a model, save the results in `model_results`
         self.model_results = train_model(...)
         self.next(self.end)
 
     @card(type='notebook')
     @step
     def end(self):
         # eval_model.ipynb can read `self.model_results`
         self.nb_options_dict = dict(input_path='eval_model.ipynb')
 
 if __name__ == '__main__':
     NBFlow()
 
 ```
 
 *This is a toy example of notebook cards.  You can see real examples in the* [*GitHub repo*](https://github.com/outerbounds/metaflow-card-notebook)*.*
 
 
 If you are using the [Metaflow GUI](https://netflixtechblog.com/open-sourcing-a-monitoring-gui-for-metaflow-75ff465f0d60),  you can view a card under the appropriate step(s) in a run (see the above example visualizing model performance metrics). Alternatively, you can view the rendered notebook card locally with the command:
 
 
 
 ```
 python flow.py card view <step name>
 ```
 
 Notebook cards are built on [Metaflow Cards](https://docs.metaflow.org/metaflow/visualizing-results), a general-purpose SDK that allows you to build visual artifacts of your flows.  Notebook cards use [Papermill](https://papermill.readthedocs.io/en/latest/index.html) to programmatically run notebooks as well as inject parameters into the notebook necessary to access data for a Flow.  To enable this behavior, you just need to create a notebook with the cell tag “parameters”.  At runtime, Papermill injects parameters as a new cell as illustrated below:
 
 
 ![](_notebooks-in-production-with-metaflow_data/3_img)
 With these parameters, we can retrieve data from our flow.  In our example, we saved our training results from our model to self.model\_results in the start step of our flow.  We can use Metaflow’s Task object to retrieve this data as follows:
 
 
 ![](_notebooks-in-production-with-metaflow_data/4_img)
 One useful aspect of this workflow is that rendered notebooks can be run after the Flow completes from top-to-bottom.  This is useful for both iterating on Flow dashboards or debugging, as all the information you need to inspect a flow is provided for you.  For a complete walkthrough as well as how you can customize the behavior of notebook cards, [see the project’s README](https://github.com/outerbounds/metaflow-card-notebook).",H3,https://outerbounds.com/blog/notebooks-in-production-with-metaflow#how-to-use-notebook-cards,False,2819.0,436.0
828,Get involved with notebooks in production,"We recommend visiting the [documentation](https://github.com/outerbounds/metaflow-card-notebook) for this project and studying the [examples](https://github.com/outerbounds/metaflow-card-notebook/tree/main/examples).  Notebook cards are not the only kind of card that we offer.  To understand Metaflow cards more generally, see [this overview](https://docs.metaflow.org/metaflow/visualizing-results).   Furthermore, you can even [create custom card templates](https://docs.metaflow.org/metaflow/visualizing-results/advanced-shareable-cards-with-card-templates) for the community.
 
 
 We would love to see what you create.  Please share your cards, questions, and feedback with us in the [Metaflow Community Slack Channel](http://slack.outerbounds.co), where there are over 900 data scientists and engineers. Lastly, this feature is only the tip of the iceberg of the features we plan to bring to data scientists everywhere.  Sounds interesting?  Come [work with us](https://outerbounds.com/workwithus/).",H3,https://outerbounds.com/blog/notebooks-in-production-with-metaflow#get-involved-with-notebooks-in-production,False,1004.0,106.0
829,What are Metaflow cards?,"[Metaflow cards](https://docs.metaflow.org/api/cards) offer a way to report and version results of your ML workflows in a human-readable form and make them easily accessible to anyone on your team. They are self-contained  HTML files generated through an human-friendly Python API which show your input and output dataframes, artifact values, matplotlib figures, and more.
 
 
   
 Read more about the motivation for Metaflow cards in [Integrating Pythonic Visual Reports into ML Pipelines](https://outerbounds.com/blog/integrating-pythonic-visual-reports-into-ml-pipelines/) and find [usage instructions in Metaflow documentation](https://docs.metaflow.org/api/cards).
 
 
 ![](../static/assets/integrating-pythonic-visual-reports-into-ml-pipelines.png)
 
 [Metaflow Sandboxes](/sandbox) allow you to test all features of Metaflow conveniently in the browser – cards included. Today, we are releasing a nifty little feature that allows you to share cards showcasing your sandbox experiments with anyone publicly. Take a look at [this example](https://cards.outerbounds.dev/index.html?id=pw-586955189%2F18bbcdbb458c47ed) to get an idea!
 
 
 How to use the card viewer in your sandbox
 ==========================================
 
 
 To send a card to the viewer, all you need to do is 
 
 
 1. Run a flow with @card attached to at least one of your steps (you can use either the default or custom cards).
 2. Click the card viewer in your sandbox to share.
 3. Click a card to share, copy the link.
 4. Give the link to your friends.
 5. View results of your team’s workflows.
 
 
 You can see how to do this all from your Metaflow sandbox in this video:
 
 ```mdx-code-block
 import LiteYouTubeEmbed from 'react-lite-youtube-embed';
 import 'react-lite-youtube-embed/dist/LiteYouTubeEmbed.css';
 
 <div className=""video-container"">
 <LiteYouTubeEmbed
         id=""m0qiU5uPzo4""
         title=""Metaflow Sandbox Card Viewer""
 />
 </div>
 ```
 <br></br>
 
 Get started today!
 ==================
 
 
 You can use the card viewer in your [sandbox](http://outerbounds.com/sandbox). Try it today and let us know what you create in [Slack](http://slack.outerbounds.co/)!",H2,https://outerbounds.com/blog/announcing-metaflow-card-viewer#what-are-metaflow-cards,False,2165.0,299.0
830,MLOps for Foundation Models with Metaflow,"OpenAI’s Whisper is a powerful new multitasking model that can perform multilingual speech recognition, speech translation, and language identification across multiple spoken languages. In a previous blog post titled [**MLOps for Foundation Models: Whisper and Metaflow**](https://outerbounds.com/blog/mlops-whisper-and-metaflow/), [Eddie Mattia](https://www.linkedin.com/in/eddie-mattia-68ab2984/) discussed the use of Metaflow for running OpenAI Whisper for transcribing Youtube videos. It covered the basics of Whisper, how to write a Metaflow flow and also briefly touched upon how to run such flows at scale in the cloud. Although here we’re focusing on Whisper, all of this work is generalizable to many types of foundation models (see our work on [**Parallelizing Stable Diffusion for Production Use Cases**](https://outerbounds.com/blog/parallelizing-stable-diffusion-production-use-cases/)*,* for example).
 
 
 To deploy and run their production workloads, many enterprises look to Kubernetes as it has become the de-facto way of running applications in the cloud. Its cloud-agnostic APIs (to the user), declarative syntax for resources, and a huge, open-source ecosystem make it a very attractive platform for a number of use cases. On this note, we decided to run OpenAI Whisper using Metaflow on Kubernetes.
 
 
 However, out of the box, the performance of the system can be quite bad for ML workloads. But, with a little troubleshooting, we were able to identify the bottlenecks and increase performance dramatically. The result is a production-ready, sufficiently performant ML workflow that has the capability to scale out across multiple dimensions.",H3,https://outerbounds.com/blog/whisper-kubernetes#mlops-for-foundation-models-with-metaflow,False,1666.0,230.0
831,Whisper Model Types,"OpenAI Whisper has [multiple machine learning models](https://github.com/openai/whisper/blob/main/model-card.md) of varying sizes. These were created using different sets of parameters and support different languages.
 
 
 Consider the case where a team wants to evaluate the relationship between the size of the ML model, the time for transcription and the actual quality of the output. For a few different inputs, the team would want to run Whisper using different model sizes and compare the results. Metaflow makes it easy to populate results across these dimensions, so you make informed decisions about this tradeoff without writing extra code.
 
 
 For 3 different Youtube urls and for evaluating the **tiny** and **large** models, the evaluation would look something like below:
 
 
 ![](_whisper-kuberntes_data/0_img)
 <Caption>
 Logical block diagram of the steps involved in the Metaflow run
 </Caption>
 
 Each of the circles is a **step** in Metaflow. **transcribe** is called for each URL in parallel using Metaflow’s **foreach** construct**.** Each **transcribe** does another **foreach** for **tiny** and **large** models.
 
 
 The actual transcription happens in the **tiny** and **large** steps which use Whisper’s **tiny** and **large** machine learning models respectively.
 
 
 The **join** step performs any post-processing specific to a given url. The **post-join** step can perform the actual evaluation of the quality of output, the time taken, etc.
 
 
 **The source code for this can be found here: https://github.com/outerbounds/whisper-metaflow-k8s**
 
 
 For this particular post, we used 3 videos from the Youtube channel [BBC Learning English](https://www.youtube.com/watch?v=uxtXEuK05-w) for evaluation.
 
 
 To begin with, the flow was run locally on an M1 Macbook Pro having 64GB of memory. This was mainly to confirm that the code executes correctly and the output is in the expected format.
 
 
 Thanks to Metaflow, running the flow is as simple as executing: 
 ```
 python3 sixminvideos.py run
 ```
 
 
 This run completed in approx. 10 minutes (see below) which is impressive considering that 6 different transcriptions happened in parallel during this time. Note that OpenAI Whisper downloads the machine learning model and caches it locally. In this case, these models were already cached locally. Otherwise, the time would have been longer.
 
 
 ![](_whisper-kuberntes_data/1_img)
 <Caption>
 Timeline view of the Metaflow run on M1 Mac
 </Caption>",H3,https://outerbounds.com/blog/whisper-kubernetes#whisper-model-types,False,2490.0,372.0
832,Whisper Models with Metaflow on Kubernetes,"Let’s run the same flow on Kubernetes. See [these instructions](https://docs.metaflow.org/scaling/remote-tasks/kubernetes) for more setup and details. In our setup, the Kubernetes cluster was set up to run on AWS and comprised two m5.8xlarge EC2 instances. 
 
 
 The simplest way to run the flow on Kubernetes is to run the same CLI command as above followed by  **–-with kubernetes**. Behind the scenes, Metaflow connects to the Kubernetes cluster and runs each **step** in the cluster. So, running the flow **-–with kubernetes** resulted in each step running in the cloud.
 
 
 The smallest executable unit in Kubernetes is *a pod*. In this case, Metaflow takes the code from each **step**, puts it into a Linux container, and runs every *task* corresponding to the step as a Kubernetes pod (a step may yield multiple tasks with different inputs in a foreach).
 
 
 Here’s what an execution on Kubernetes looks like: 
 ```
 python3 sixminvideos.py run --with kubernetes
 ```
 
 
 ![](_whisper-kuberntes_data/2_img_narrow.png)
 <Caption>
 Timeline view of the first Metaflow run on Kubernetes
 </Caption>
 
 
 The flow completed successfully. However, the image above shows that it took **48m15s**. That’s almost five times slower than the time it took to run this flow on a local laptop. *The promise of running flows in the cloud is to get a performance boost, not a performance penalty*!
 
 
 The source code of the flow was the same in both runs. Looking at the start and end times of the steps, it is clear that Metaflow orchestrated the steps correctly in both the runs. The step order was maintained and steps that were supposed to run in parallel, indeed ran in parallel.
 
 
 We know that the comparison between the local run on M1 Mac and the run on Kubernetes is not entirely fair, as each pod in Kubernetes lacks shared storage. As a result, models were re-downloaded every time the pods started, adding some overhead. Also, if our Docker image that contains dependencies such as *ffmpeg* is not cached on the node, it needs to be downloaded adding further overhead. 
 
 
 Still, the fivefold slowdown doesn’t feel right. Could there be some misconfiguration in Kubernetes that caused the flow to take this long? Let’s find out!",H3,https://outerbounds.com/blog/whisper-kubernetes#whisper-models-with-metaflow-on-kubernetes,False,2241.0,374.0
833,Analyzing workload performance in Kubernetes,Let’s dig deeper into what might be the bottleneck for this performance degradation.,H3,https://outerbounds.com/blog/whisper-kubernetes#analyzing-workload-performance-in-kubernetes,False,84.0,13.0
834,CPU and Memory consumption,"When creating a pod, Kubernetes allows the user to **request** for a certain amount of memory and CPU resources. These resources are reserved for the pod at the time of creation. The actual amount of memory and CPU used could vary over the lifetime of the pod. Also, depending on your Kubernetes configuration, a pod could actually use more than the requested amount of resources. However, there is no guarantee that the pod would actually get the resources when they exceed the requested amount.
 
 
 The following charts show the relationship between the resources requested and used by each pod in this flow (these charts were obtained using [this](https://grafana.com/grafana/dashboards/6417-kubernetes-cluster-prometheus/) Grafana dashboard running in the cluster).
 
 
 In the image below, the green line indicates the pod's resource request and the yellow line indicates the actual resource usage. The Y axis shows the number of CPU cores fully utilized by the pod.
 
 
 ![](_whisper-kuberntes_data/grafana-1.png)
 <Caption>
 Graph of CPU utilization vs CPU requested by steps in Metaflow
 </Caption>
 
 
 Turns out that all the pods in this flow requested for 1 CPU but actually landed up using much more, almost 13 cores.
 
 
 In this chart, the Y axis shows the memory request (green line) and vs. the actual memory usage (the yellow line).
 
 
 ![](_whisper-kuberntes_data/grafana-2.png)
 <Caption>
 Graph of Memory utilization vs Memory requested by steps in Metaflow
 </Caption>
 
 
 In the case of memory, the pods were requesting 4GB of memory but landed up using almost 12GB of memory.
 
 
 A Kubernetes cluster comprises multiple servers (physical server machines, VMs, EC2 instances, etc.). These are called *nodes*. When a pod is submitted to Kubernetes, it chooses one of the nodes in the cluster to run the pod.
 
 
 As you can see in the following screenshot of stdout, all pods for this flow ran on the same node: **ip-10-10-27-207.us-west-2.compute.internal**
 
 
 ![](_whisper-kuberntes_data/5_img)
 <Caption>
 List of hosts used for running steps in Metaflow
 </Caption>",H4,https://outerbounds.com/blog/whisper-kubernetes#cpu-and-memory-consumption,False,2096.0,331.0
835,Observations,"The key problem is that while the run is able to complete successfully, Kubernetes decided to schedule all tasks on the same node, resulting in suboptimal performance.
 
 
 In the chart below, we can see the two nodes of our cluster. The node depicted by the yellow line has its CPUs fully utilized at ~100% while the green node is idling. Correspondingly, no memory is used on the green node.
 
 
 ![](_whisper-kuberntes_data/grafana-3.png)
 <Caption>
 CPU utilization percentage of the node
 </Caption>
 
 
 ![](_whisper-kuberntes_data/grafana-4.png)
 <Caption>
 Memory utilization percentage of the node
 </Caption>
 
 In this case, tasks were able to burst above their requested amount of CPU but due to co-location on the same node, they were competing on the same scarce resources. This is a clear demonstration of the *noisy neighbor problem* that often plagues multi-tenant systems.
 
 
 Confusingly, the scheduling behavior depends on the overall load on the cluster, so you may experience a high variance in the total execution time, depending on what other workloads happen to execute on the system at same time. Variability like this is hard to debug and clearly undesirable, especially for production workloads that should complete within a predictable timeframe.
 
 
 Issues like this are not immediately obvious on the surface, as the run completes eventually without errors. These types of issues can easily go unnoticed, resulting in wasted human and computer time and increased cloud costs.",H4,https://outerbounds.com/blog/whisper-kubernetes#observations,False,1508.0,238.0
836,Fixing the Performance Issues,"Metaflow supports providing specific resource requirements to individual steps. These requirements are written in the code as decorators. They are translated into CPU and memory requests in the Kubernetes pod specification. Instead of relying on opportunistic bursting, we fix the resource requirements to reflect the actual usage.
 
 
 * The **tiny** steps can use 8 vCPUs and 2GB of memory
 * The **large** steps can use 22 vCPUs and 12GB of memory.
 
 
 So, the Metaflow flow was set up accordingly and run again.
 
 
 
 
 ```
     ...
 
     @kubernetes(cpu=8, memory=2048, image=”https://public.ecr.aws/outerbounds/whisper-metaflow:latest”)
     @step
     def tiny(self):
         print(f""*** transcribing {self.input} with Whisper tiny model ***"")
         cmd = 'python3 /youtube_video_transcriber.py ' + self.input + "" tiny""
         p = subprocess.run(shlex.split(cmd), capture_output=True)
         json_result = p.stdout.decode()
         print(json_result)        
         self.result = json.loads(json_result)
         self.next(self.join)
 
     @kubernetes(cpu=22, memory=12288, image=”https://public.ecr.aws/outerbounds/whisper-metaflow:latest”)
     @step
     def large(self):
         print(f""*** transcribing {self.input} with Whisper large model ***"")        
         cmd = 'python3 /youtube_video_transcriber.py ' + self.input + "" large""
         p = subprocess.run(shlex.split(cmd), capture_output=True)
         json_result = p.stdout.decode()
         print(json_result)
         self.result = json.loads(json_result)
         self.next(self.join)
 
     ...
 ```
 
 
 With this setup, the flow completed in **4m 58s**
 
 
 ![](_whisper-kuberntes_data/8_img)
 <Caption>
 Timeline view of the efficient Metaflow run on Kubernetes
 </Caption>
 
 
 Looking at the node CPU utilization, it can be seen that this time, tasks were run on multiple nodes. Specifically, the CPU was not pegged at 100%.
 
 
 ![](_whisper-kuberntes_data/grafana-5.png)
 <Caption>
 Graph of CPU utilization of the nodes in Kubernetes
 </Caption>",H3,https://outerbounds.com/blog/whisper-kubernetes#fixing-the-performance-issues,False,2045.0,397.0
837,Conclusions,"After peeking under the hood, we were able to reduce the previous completion time of 48 minutes to approximately 5 minutes, resulting in an impressive **9x** improvement in performance, simply by adjusting two resource specifications.
 
 
 The optimization of the run has numerous benefits, including the ability to conduct more experiments. Instead of being limited to testing only **tiny** and **large** models, it will now be possible to test various model sizes by adding more parallel steps, each of which will run as separate pods with their own dedicated resources. Additionally, the flow can now transcribe a larger number of videos, instead of being limited to just 3.",H3,https://outerbounds.com/blog/whisper-kubernetes#conclusions,False,677.0,109.0
838,Next Steps,"If you want to deploy Metaflow and Kubernetes in your own environment, you can do so easily [using our open-source deployment templates](https://outerbounds.com/engineering/welcome/). You can customize the templates as needed and hook them up in your existing monitoring solutions, so you are able to debug issues as illustrated above. We are happy to help you on [our Slack channel](http://slack.outerbounds.co), should you have any questions.
 
 
 If you want to play with the Whisper model with Metaflow by yourself, you can do it easily for free in our hosted Metaflow Sandbox, which comes with a managed Kubernetes cluster – no infrastructure expertise required! We provide a pre-made workspace specifically for Whisper, containing all dependencies needed. Simply [click the Whisper workspace at the Metaflow sandbox homepage](https://outerbounds.com/sandbox).
 
 
 Finally, if you need a full-stack environment for serious ML workloads but you would rather avoid debugging Kubernetes and optimizing workloads manually altogether, take a look at [Outerbounds Platform](https://outerbounds.com/platform/). It provides a customized Kubernetes cluster, optimized to avoid issues like the one described here, deployed on your cloud account, fully managed by us.",H3,https://outerbounds.com/blog/whisper-kubernetes#next-steps,False,1262.0,179.0
839,Motivation,"Unsurprisingly, as of June 2023, building production systems that leverage custom large language models takes a lot of work and money. As usual in any novel technical field, one faces a diverse, quickly moving landscape of immature tooling, missing documentation, and vibrant but chaotic communities. Developers face complex and varying dependencies of LLMs, spanning hardware drivers and bleeding-edge software frameworks, which can take hours and days to figure out for any given use case.
 
 Some astute companies may decide to wait a few years for the field to mature. However, many companies need or want to start innovating faster, and mature tools like [Metaflow](https://metaflow.org/) can help. Metaflow helps you design robust workflows to support any data-intensive application, allowing you to amortize the fixed cost of setting up systems - such as LLM experimentation pipelines - over the project's life. Using Metaflow, teams can set up appropriate human and technical workflows for LLM development and deployment, allowing them to focus on the business's specific needs.
 
 Metaflow has been supporting serious ML use cases like this for years. The foundational needs of LLM development don’t differ much from other earlier models (besides scale!), so Metaflow is a natural fit for building systems around custom LLMs.",H2,https://outerbounds.com/blog/llm-tuning-metaflow#motivation,False,1334.0,205.0
840,Foundational infrastructure for LLMs,"If you just want to run the code and do not care about owning the supply chain or deploying the results to a production system, [Colab](https://colab.google/) and similar GPU-powered-notebook-as-a-service providers are a convenient way to access prototyping the needed infrastructure fast. Alternatively, projects like [GPT4All](https://gpt4all.io/index.html) make running LLM inference on a laptop without a GPU straightforward. 
 
 However, as we discussed in [part one](/blog/custom-llm-tuning) of this series, there are cases where organizations that want to move beyond prototyping in notebooks and consider using LLMs in production need more substantial ML tooling and infrastructure: You need to consider how to connect models to surrounding systems, run training and inference and all supporting steps on a scalable compute platform, iterate and track models, code and data across versions, and have both technical and human workflows in place for continuous improvement and experiments. All this in business-critical systems with tight SLA, security, and compliance requirements.",H2,https://outerbounds.com/blog/llm-tuning-metaflow#foundational-infrastructure-for-llms,False,1088.0,153.0
841,How Metaflow helps,"This article shows how to use Metaflow to instruction-tune a custom LLM, accompanied by [the full stack of production-grade ML tooling](https://outerbounds.com/docs/infra-stack/) that Metaflow provides. We use a few lines of Metaflow code to access and monitor GPUs in CoreWeave’s cloud, making it easy to produce, track, and report results in shareable [Metaflow cards](https://docs.metaflow.org/api/cards) when running off-the-shelf open-source LLM code.
 
 If you are unfamiliar with the features of Metaflow, we encourage you to look at the [Metaflow documentation](https://docs.metaflow.org). Crucially, all the functionality Metaflow provides can be helpful when developing and deploying real-world systems powered by LLMs.
 
 For instance, Metaflow enables us to explore different model sizes and variants while quickly tuning infrastructure requirements and scheduling runs by changing single values in Python code. Switching between models helps us explore rapidly without unnecessarily wasting money on under-utilized GPUs. 
 
 ![](../static/assets/huggingface-instruction-tuning.png)
 <Caption> Top left: The lineage of the Alpaca-LoRA model used as a default in this post; Bottom: Steps in the Metaflow workflow; Top right: The output of the fine-tuning step is model checkpoints in HuggingFace  </Caption>
 
 As Metaflow runs any Python code, you can easily leverage state-of-the-art models and APIs from your favorite modeling framework. For instance, In the example workflow, we push checkpoints to the HuggingFace hub with a few lines of code. In our example, we followed the [HuggingFace documentation](https://huggingface.co/docs/transformers/main_classes/trainer#checkpoints) to save all the checkpoints to a local folder, using a specific pattern that is checkpoint-xxx/ where xxx is the number of steps. 
 
 Using HuggingFace APIs with Metaflow makes it easy to create a model repo for each checkpoint, train on whatever hardware you need, and push the results back to the hub to use in an application and for downstream testing. Doing this and sharing incremental model checkpoints on teams can also hugely reduce costs. People need not recompute the same model training results and can instead readily pick up training from where a colleague left off.",H2,https://outerbounds.com/blog/llm-tuning-metaflow#how-metaflow-helps,False,2274.0,325.0
842,Instruction tuning with Metaflow,"For example, let’s fine-tune a recent LLaMA variant, taking an already trained LLaMA language model and training it further on an instruction tuning dataset. The learning task is still language modeling, but the training dataset contains specific instruction-following examples.
 
 In this case, the model learns to do language modeling relevant to the instructions in the Alpaca dataset. You can imagine using any dataset, such as how the Databricks team created a custom instruction-tuning [dataset](https://huggingface.co/datasets/databricks/databricks-dolly-15k) in [Dolly v2](https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm). You can find additional information about Metaflow and large language models in our [blog post on tuning Dolly](https://outerbounds.com/blog/train-dolly-metaflow/).",H2,https://outerbounds.com/blog/llm-tuning-metaflow#instruction-tuning-with-metaflow,False,849.0,96.0
843,Show me the code,"The core concepts of this workflow template are the following:
 1. HuggingFace to access the upstream models and to checkpoint our fine-tuned models,
 2. CoreWeave to access GPU resources, and
 3. Metaflow to structure the Python code, run it on the GPUs (carefully monitoring resource utilization with a Metaflow card), and track versions of code and models.
 
 ![](../static/assets/coreweave-instruction-tuning.png)
 
 The following code shows how to call the main fine-tuning logic from the [Alpaca LoRA codebase](https://github.com/tloen/alpaca-lora). 
 
 <CustomAdmonition title=""Instruction tuning with Metaflow"" link=""https://github.com/outerbounds/metaflow-instruction-tuning"" color=""#F9F7F5"" image=""/icons/common/github.png"">
     Use PyTorch, HuggingFace and Metaflow to build a workflow for fine-tuning LLMs.
 </CustomAdmonition>
 
 ```python",H3,https://outerbounds.com/blog/llm-tuning-metaflow#show-me-the-code,False,853.0,105.0
844,Models,"We fine-tuned four of the recent LLaMA models on the same dataset with a fixed computing budget for each model; we used [Low-Rank Adaptation](https://arxiv.org/abs/2106.09685), making use of the recent [Alpaca LoRA repository](https://github.com/tloen/alpaca-lora). The models we fine-tuned are the 7B, 13B, 33B, and 65B parameters models, with the idea that larger models should provide better performance and answers. We saved multiple checkpoints for each model and chose the best one by validation loss after training. 
 
 We chose the Alpaca dataset - built by generating instructions from GPT-3. We use a [ version](https://huggingface.co/datasets/yahma/alpaca-cleaned) of the dataset in which some errors are corrected. The dataset contains 50K instructions. You can find more details about how we used Metaflow in the practical considerations section.",H2,https://outerbounds.com/blog/llm-tuning-metaflow#models,False,859.0,122.0
845,Quantitative evaluation,"We wanted to assess if our fine-tuned models are still performing as expected after fine-tuning. While there is still active discussion about how to evaluate instruction and chat models (see the [ChatBot Arena](https://lmsys.org/blog/2023-05-03-arena/), for example), we decided to evaluate our models against standard LLM benchmarks as a more customized evaluation approach is beyond the scope of this post. Specifically, we make use of the [LM-Harness](https://github.com/EleutherAI/lm-evaluation-harness) package developed by EleutherAI to evaluate our fine-tuned models on these different tasks in a zero-shot manner. We chose three question-answering benchmarks: [PIQA](https://huggingface.co/datasets/piqa) and [BoolQ](https://huggingface.co/datasets/boolq), and [OpenBookQA](https://huggingface.co/datasets/openbookqa). 
 
 Here is an example from PIQA (the other datasets are similar):
 
 > “To separate egg whites from the yolk using a water bottle, you should…”
 
 The dataset has two possible answers: 
 
 > “Squeeze the water bottle and press it against the yolk. Release, which creates suction and lifts the yolk.”
 
 > “Place the water bottle and press it against the yolk. Keep pushing, which creates suction and lifts the yolk.” 
 
 Given that this is a multiple choice question task, [LM-Harness](https://github.com/EleutherAI/lm-evaluation-harness/blob/big-refactor/docs/new_task_guide.md#multiple-choice-format) runs the evaluation by comparing the likelihood of the label words.
 
 So, how well do these models work? Let’s look at some results (the evaluation metric here is accuracy).
 
 <div style={{alignItems: 'center', justifyContent: 'center', display: 'flex'}}>
 <div style={{textAlign: 'center'}}>
 
 | Model | PIQA | BOOLQ | OPENBOOKQA |
 | ----- | ---- | ----- | ---------- |
 | LLAMA 7B | 0.7916 | 0.7511 | 0.4440 |
 | ALPACA 7B | 0.8030  | 0.7835  | 0.4420 |
 | LLAMA 13B | 0.8003 | 0.7798 | 0.4480 |
 | ALPACA 13B | 0.8030 | 0.8150 | 0.4620 |
 
 </div>
 </div>
 
 These results seem coherent with what was expected based on various leaderboards (see [LLaMA results](https://paperswithcode.com/paper/llama-open-and-efficient-foundation-language-1), for example) and papers. There is not a notably high increase in performance, as we expected. 13B parameters models do slightly better than their 7B counterparts.  We do not expect a substantial increase in performance with respect to the original LLaMA models; instead, we would like to maintain similar performances on these benchmarks. Instruction tuning should give us models that follow instructions better and do not perform worse on general benchmarks. Our evaluation shows we have nicely performing models on the latter, but do they follow instructions?",H3,https://outerbounds.com/blog/llm-tuning-metaflow#quantitative-evaluation,False,2742.0,390.0
846,Qualitative evaluation,"When working with LLMs, being mindful that the research field is new and changing rapidly is essential. Therefore, when considering using this technology in a production environment, we need to evaluate qualitatively and measure performance against quantitative benchmarks, which may need significant iteration and revision for assessing performance in a way that makes sense in any given production context.
 
 Even with the small training dataset we used and only three epochs of training, our models already see exciting results. The bigger the model is, the better (and more valuable) its generations become.
 
 <div style={{alignItems: 'center', justifyContent: 'center', display: 'flex'}}>
 
 | Instruction | 7B | 30B |
 | ----------- | -- | --- |
 | **Write a poem about capybaras** | Capybaras, the largest rodent in the world, <br></br><br></br> With their shaggy coats and <span style={{color: 'red'}}> bushy tails</span>, <br></br><br></br> They roam the grasslands of South America, <br></br><br></br> In search of food and water to quench their thirst. <br></br><br></br> Capybaras are herbivores, and they love to graze, <br></br><br></br> Eating grasses, fruits, and vegetables, too, <br></br><br></br> They are social creatures, living in herds, <br></br><br></br> And they're known for their playful and friendly deeds. | A Capybara's Life <br></br><br></br> A capybara's life is filled with joy, <br></br> As they frolic in the grass and play in the sun. <br></br><br></br> They're the largest rodents in the world, <br></br><br></br> <span style={{color: 'green'}}>With bodies built for swimming and for fun.</span> <br></br><br></br> They're social creatures, always in a group,<br></br><br></br> Enjoying each other's company, without a flop. <br></br><br></br> They love to eat grass and aquatic plants,<br></br><br></br> A diet that keeps them healthy and well-fed. |
 | **Can Geoffrey Hinton have a conversation with George Washington? Give the rationale before answering.** (From Flan) | No, it is not possible for Geoffrey Hinton to have a conversation with George Washington. This is because George Washington is a historical figure who lived from 1732 to 1799, while Geoffrey Hinton is a living person who was born in <span style={{color: 'red'}}>1957</span>. Time travel is not possible, so it is not possible for two people from different eras to have a conversation with each other. | No, it is not possible for Geoffrey Hinton to have a conversation with George Washington. Geoffrey Hinton is a living person who was born in 1947, while George Washington died in 1799. It is not possible for a living person to have a conversation with someone who has been dead for more than 200 years. |
 | **Classify this document in one of these categories, ""sport"", ""politics"", or ""technology"".** <br></br><br></br> **“The LLaMA models are a family of Natural Language Processing LLMs that can be applied to many different tasks.”** | technology | technology |
 | **Do aliens invest in stocks?** | No, aliens do not invest in stocks. Stock investing is a human activity that involves buying and selling shares of publicly traded companies in order to profit from changes in their share prices. Aliens do not have access to the same financial markets and systems that humans do, so they would not be able to participate in stock investing. <br></br><br></br> <span style={{color: 'orange'}}>However, it is possible that aliens could invest in other types of financial instruments</span>, such as bonds, mutual funds, or real estate, if they had access to the appropriate financial markets and systems. | I'm sorry, but as an AI language model, I don't have access to any information about aliens or their investment habits. It's possible that some alien civilizations may invest in stocks, but it's also possible that they may have different investment strategies or may not invest in stocks at all. It's difficult for me to give a definitive answer to this question without more information or data. It's possible that some alien civilizations may invest in stocks, but it's also possible that they may have different investment strategies. |
 | **To separate egg whites from the yolk using a water bottle, you should…** (from PIQA) | To separate egg whites from the yolk using a water bottle, you should follow these steps: <br></br><br></br> 1. Crack the egg into a small bowl and separate the yolk from the egg white. <br></br> <span style={{color: 'red'}}>2. Fill the water bottle with cold water. </span><br></br> 3. Place the egg white into the water bottle. <br></br> <span style={{color: 'red'}}>4. Shake the water bottle vigorously to separate the egg white from the yolk. </span><br></br> 5. Remove the egg white from the water bottle and discard the yolk. <br></br> 6. Repeat the process for the remaining eggs. | To separate egg whites from the yolk using a water bottle, you should follow these steps: <br></br><br></br> 1. Start by cracking an egg into a small bowl or cup. <br></br> 2. Hold the water bottle upside down over the bowl or cup. <br></br> 3. Slowly squeeze the water bottle, allowing the <span style={{color: 'red'}}>water to flow</span> into the bowl or cup. <br></br> 4. As the water flows into the bowl or cup, it will push the egg yolk towards the bottom of the bowl or cup. |
 | **Best way to pierce ears** (from PIQA)| The best way to pierce ears is to use a sterilized, hygienic piercing gun or needle. It is important to follow the instructions provided by the piercing professional and to clean the area thoroughly before and after the piercing. It is also important to use a high-quality, non-allergenic jewelry and to take proper care of the piercing by cleaning it regularly and avoiding contact with water while the piercing is healing. It is recommended to wait at least 6-8 weeks for the piercing to heal completely before changing the jewelry. | <span style={{color: 'green'}}>The safest and most effective way to pierce your ears is to have it done by a professional piercer at a reputable piercing studio.</span> A professional piercer will use sterilized equipment and follow strict hygienic procedures to minimize the risk of infection or other complications. They will also be able to advise you on the best placement and type of jewelry for your piercing, as well as provide aftercare instructions to ensure proper healing. |
 | **What's the best way to drink water if you are juggling three balls?** | If you are juggling three balls, the best way to drink water is to use a straw. This will allow you to keep your hands free to continue juggling, while also ensuring that you don't spill any water on yourself or your juggling equipment. Additionally, it's important to drink small sips of water, rather than gulping down large amounts at once. This will help you to stay hydrated without interfering with your juggling routine. | <span style={{color: 'green'}}>If you are juggling three balls, the best way to drink water would be to take a break from juggling, set the balls down on a safe surface, and take a sip of water from a cup or bottle.</span> Juggling and drinking at the same time can be difficult and may result in spilling water or dropping the balls. It's important to stay hydrated while juggling, but it's best to do so in a way that doesn't interfere with your juggling performance. |
 
 </div>
 
 These results show multiple things: first off, the models are good at replying to questions and instructions. They all come up with coherent text that appears correct. The 7B model is more prone to generating incorrect stuff, such as getting the wrong year of birth of someone or suggesting something that might be unsafe. We have color coded some replies to highlight possible good (green), wrong (red), or unclear (orange) answers.",H3,https://outerbounds.com/blog/llm-tuning-metaflow#qualitative-evaluation,False,7821.0,1266.0
847,Summary,"You can use the above Metaflow workflow as a generic template for fine-tuning models available on HuggingFace. Besides the Alpaca instruction dataset we used to fine-tune various sizes of LLMs, you can apply the same approach to fine-tune models with your instructions to optimize models for your use cases. We are happy to help you get started with this template and Metaflow in general. Join us and thousands of other ML engineers, AI enthusiasts, and data scientists on  [Metaflow Community Slack](http://slack.outerbounds.co/)!",H2,https://outerbounds.com/blog/llm-tuning-metaflow#summary,False,531.0,82.0
848,Acknowledgments,"We would like to thank Mert Yuksekgonul, Mirac Suzgun, Patrick John Chia, Silvia Terragni, Giuseppe Attanasio, and Jacopo Tagliabue for feedback and suggestions on a previous version of this blog post.",H2,https://outerbounds.com/blog/llm-tuning-metaflow#acknowledgments,False,201.0,31.0
849,What we do at Outerbounds,"At Outerbounds, we help data scientists in their day-to-day tasks, both big and small. We believe that the best way to do this is by investing in both technology as well as human empowerment.
 
 
 First, we reduce unnecessary complexity in machine learning and data science applications by **providing delightfully usable and reliable infrastructure for data scientists and engineers** in the form of [Metaflow](https://metaflow.org), an open-source library we started developing at Netflix years ago. Metaflow doesn’t remove all the tasks at the edges but it allows you to focus on tasks that matter: Consider it a foundation for your innovative, custom solutions.
 
 
 Today, Metaflow is used by hundreds of enterprises across industries. Outerbounds helps them set up modern, full-stack data science environments so they can build end-to-end data science solutions quickly using Metaflow and the infrastructure underneath it.
 
 
 Second, we **share real-world knowledge and experience with data scientists and engineers** so they are able to build data science applications successfully. Starting today, we offer a free, growing knowledge base of how-to articles here at [Outerbounds.com](https://outerbounds.com/docs/) covering a diverse set of topics, techniques, and libraries. To put these articles in context, we provide topic guides, such as this one about [reproducibility](https://outerbounds.com/docs/reproducible-ml), which help you connect the dots. If you prefer long-form content, you can find many of these topics also covered in a new book, [Effective Data Science Infrastructure](https://www.manning.com/books/effective-data-science-infrastructure). 
 
 
 In addition, we will continue investing in the open-source documentation at [docs.metaflow.org](https://docs.metaflow.org). In [the space of different types of documentation](https://documentation.divio.com/), the resources at outerbounds.com fall on the practical axis, since helping companies and data scientists is our day job, whereas the open-source documentation gives a broad overview of the project. We hope that these complementary materials will help you as you solve problems, big and small, at the outer bounds of data science projects.",H2,https://outerbounds.com/blog/metaflow-resources-data-science-engineering#what-we-do-at-outerbounds,False,2224.0,309.0
850,Help us help you,"We have bootstrapped the initial batch of how-to articles based on hundreds of questions we have heard in the Metaflow community. A big thanks to all of you who contributed to the effort by patiently explaining your challenges! You can [join our community Slack today](http://slack.outerbounds.co) to share your challenges and solutions, and get support from us and over a thousand other like-minded data scientists and engineers.
 
 
 If there’s a topic or a question – no matter how simple, advanced, mundane, or sophisticated that you would like to see covered by an article, or you need help setting up modern data science infrastructure in your organization, don’t hesitate to [contact us on Slack](http://slack.outerbounds.co)!",H2,https://outerbounds.com/blog/metaflow-resources-data-science-engineering#help-us-help-you,False,733.0,114.0
851,The Cloud-Native Stack for ML,"Data science and ML can’t live on an island of their own. To produce sustainable business value, ML infrastructure must integrate in the same cloud environment that is already used by the rest of the organization.
 
 
 Fortunately all the major clouds – Azure, AWS, and GCP – provide mature, cost-effective, functionally equivalent solutions for the foundations of this stack, so companies can choose their cloud vendor based on other business considerations, including pricing. In particular, scalable data storage and compute have become commodities, which is a boon to resource-hungry ML workloads.
 
 
 The low-level APIs provided by the clouds are not suitable for human consumption directly. However, when wrapped in a human-friendly framework like Metaflow, the cloud frees the data scientist from the confines of their laptop or running notebooks on a single instance. From the productivity perspective, it is highly beneficial to convert cheap CPU cycles to human happiness and effectiveness.
 
 
 A main goal of Metaflow is to make projects more cloud-native and hence more production-ready by design, so they can be deployed to production quickly and frequently. Data scientists can execute more experiments concurrently and develop flows that are scalable and robust by default, avoiding expensive rewrites and tedious handovers from data scientists to engineers.",H2,https://outerbounds.com/blog/metaflow-on-all-major-clouds#the-cloud-native-stack-for-ml,False,1375.0,212.0
852,Metaflow on Google Cloud,"For an idea how Metaflow works on Google Cloud, take a look at this short video:
 
 ```mdx-code-block
 import LiteYouTubeEmbed from 'react-lite-youtube-embed';
 import 'react-lite-youtube-embed/dist/LiteYouTubeEmbed.css';
 
 <div className=""video-container"">
 <LiteYouTubeEmbed
         id=""-Io_xpKYDmY""
         title=""Metaflow on GCP""
 />
 </div>
 ```
 <p></p>
 
 Behind the scenes, Metaflow uses an auto-scaling GKE cluster to execute your models and business logic. You can use any Google Compute instances in your GKE cluster, including GPU instances. The workflow state and its results are persisted in Google Cloud Storage.
 
 
 By design, these technical details are hidden from data scientists focusing on models and data, as illustrated below. Also by design, the technical details are not heavy-handedly abstracted away from engineers who want to stay in control, and who need to make sure that ML applications adhere to the company’s security and other policies, and integrate to surrounding systems seamlessly.
 
 
 ![](_metaflow-on-all-major-clouds_data/3_img)
 Workflow orchestration is handled by [Metaflow’s integration with Argo Workflows](https://outerbounds.com/blog/human-centric-data-science-on-kubernetes-with-metaflow/) and soon, Airflow. Versioning and metadata tracking are handled by Metaflow’s built-in metadata service. All of these services are deployed on GKE as a part of the stack automatically.
 
 
 Thanks to Metaflow’s openness and extensibility, you can easily use it with other Google services, such as Cloud TPUs for deep learning, Vision API for image recognition, or BigQuery for data processing. Importantly, you can develop on Google managed notebooks or locally on your favorite IDE.
 
 
 To get started with Metaflow on GCP, simply apply our open-source Terraform templates following [the instructions at Metaflow Resources for Engineers](https://outerbounds.com/engineering/welcome/). Deploying the full stack on your GCP account takes about 15-30 minutes.",H2,https://outerbounds.com/blog/metaflow-on-all-major-clouds#metaflow-on-google-cloud,False,2002.0,282.0
853,Metaflow vs. Bundled ML Platforms,"We get this question often: Why should I use Metaflow instead of Sagemaker on AWS, Azure ML Studio on Azure, or Vertex AI on Google? All these platforms provide a full stack of infrastructure for data science and ML, conveniently bundled in your existing relationship with a cloud vendor.
 
 
 Consider the following reasons:
 
 
 * **Human-friendly, Pythonic user experience** – Metaflow provides a coherently designed, concise Python API that is easy to understand and learn by anyone who has used Python in notebooks in the past. The bundled platforms tend to be complex portfolios of loosely coupled, and sometimes quickly evolving components, each with their bespoke paradigms, style of configuration, and SDKs.
 
 * **Speed of iteration** – Metaflow treats prototyping and the local development experience as the first class concern. You can edit, run, and debug code locally on your laptop or favorite development environment and scale to the cloud only when needed.
 
 * **Extensibility** – Proprietary ML platforms are opaque black boxes. They can be great when they work but if they don’t, your options are limited. An open-source framework like Metaflow is fully transparent and infinitely extensible based on your bespoke needs and business requirements. Teams can build their own company-specific abstractions
 on top of Metaflow, as [Netflix](https://netflixtechblog.com/supporting-content-decision-makers-with-machine-learning-995b7b76006f), [Zillow](https://www.zillow.com/tech/serving-machine-learning-models-efficiently-at-scale-at-zillow/), and [SAP](https://blogs.sap.com/2022/04/20/train-your-model-in-sap-ai-core-using-the-metaflow-argo-plugin/) have done, for instance.
 
 * **Stability –** Metaflow provides a strong backwards compatibility guarantee: Metaflow flows written in 2018 run without changes four years later, and they will continue doing so in the future. This is a critical feature of Metaflow as it powers business-critical workflows at hundreds of companies. In general, companies prefer open-source as their application backbone, as it allows them to avoid vendor-controlled systems that change out of their control,
 causing unnecessary migration tax.
 
 * **Understandable costs**  – Vertex AI and Sagemaker impose an additional surcharge of 10-20% over standard instance pricing for ML workloads. In addition, every component of the stack incurs additional costs, so estimating the total cost of ownership of bundled platforms can be tricky. With Metaflow, you only pay for the basic compute and data which are likely to be line items in your cloud bill already. Furthermore, you can optimize your costs freely, e.g. by using spot/ephemeral instances or by leveraging volume discounts.
 
 * **Avoid cloud lock-in** – the abstractions provided by each cloud are highly bespoke. For instance, migrating an application built on, say, Sagemaker, would require a nearly complete rewrite to run on Vertex AI. Migrating an application to run on your own infrastructure, say, a Kubernetes cluster, would be even harder.
 
 
 Thankfully, the modern clouds provide a level playing field when it comes to the foundational layers, data and compute. The bundled platforms rely on the same underlying services such as EC2 and S3 as everyone else, so open-source frameworks like Metaflow can provide an equal level of high availability and scalability as them, addressing the most critical technical consideration.
 
 
 Also, the choice doesn’t have to be either-or: you can use specific bundled services, say, Vertex AI Data Labeling,
 or [Sagemaker for model hosting](https://outerbounds.com/docs/deploy-with-sagemaker/) in conjunction with Metaflow and evolve the service mix over time as your needs grow.",H2,https://outerbounds.com/blog/metaflow-on-all-major-clouds#metaflow-vs-bundled-ml-platforms,False,3736.0,535.0
854,Get Started Easily,"To get started with Metaflow on Google Cloud – or any other major cloud – [follow the instructions here](https://outerbounds.com/engineering/welcome/). To learn more about Metaflow, see our [resources for data scientists](https://outerbounds.com/docs/data-science-welcome/) or [try Metaflow in the browser](https://outerbounds.com/sandbox/)!
 
 Most importantly, [join our welcoming community of 1800+ data scientists, ML and platform engineers on Slack](http://slack.outerbounds.co/). We look forward to hearing your questions and feedback there!",H2,https://outerbounds.com/blog/metaflow-on-all-major-clouds#get-started-easily,False,547.0,63.0
855,What type of ML questions do you work on at MoneyLion and what type of business questions do they solve?,"At MoneyLion our mission is to rewire the financial system to positively change the path of every hard-working American. We offer an all-in-one mobile banking experience, covering banking, zero-interest cash advances, and much more. We are a data-driven organization that uses data science to empower our customers, and we use machine learning heavily in our day-to-day operations, including but not limited to: credit scoring, fraud detection, bank data enrichment, and personalized recommendations.",H2,https://outerbounds.com/blog/mlops-fin-tech#what-type-of-ml-questions-do-you-work-on-at-moneylion-and-what-type-of-business-questions-do-they-solve,False,500.0,72.0
856,What did your ML stack look like before you adopted Metaflow and what pain points did you encounter?,"Prior to adopting Metaflow our data science team built machine learning pipelines mostly within a single-node HPC environment (an AWS EC2 instance with 500GB RAM and 64 CPU Cores) and using Apache Airflow running on Kubernetes which is used heavily by our data engineering team. 
 
 
 We were hitting the limits of vertically scaling our HPC environment as we scaled up to 40+ data scientists this year. All of them had access to it and we were running their workflows of differing compute needs on it, which resulted in constant memory alerts and crashes of the entire environment which impacted all workflows on it. While it was easy to run workflows this way, it was also difficult to scale them up anymore and ensure the independence of workflow execution. 
 
 
 In the MLOps team, I was tasked with building a continuous training pipeline for one of our credit scoring models and decided to use our Apache Airflow setup instead due to the aforementioned reasons. It solved my issue of being able to run a complex pipeline with high compute resources but came at the cost of being much more complex to set up. The team had a goal of ensuring that any data scientist could set up these continuous training pipelines on their own but Apache Airflow was too complex due to the fact that data scientists need to know:
 
 
 1. How to use Docker,
 2. How to build a CI/CD pipeline to push the Docker containers to AWS ECR, and
 3. How to build a CLI application.
 
 
 These things can be learned over time but I felt like there must have been an easier way.",H2,https://outerbounds.com/blog/mlops-fin-tech#what-did-your-ml-stack-look-like-before-you-adopted-metaflow-and-what-pain-points-did-you-encounter,False,1555.0,282.0
857,What questions do you think about answering when adopting new ML technologies and how do you make decisions about what to adopt?,"The MLOps space is getting overcrowded with tools that promise heaven and earth so I really do suffer from decision paralysis when trying to pick a tool to adopt. I see tools being split into two different categories (note that *mutability* here refers to whether or not we need to replace preexisting tools in an organization):
 
 
 1. Mutable tools:
 	1. Tools that are meant to replace preexisting tools in your organization;
 	2. Should be a very strict set of requirements that the new tool should be better at than the current tool in use;
 	3. Takes more time to roll out;
 	4. Example: Migrating from AWS Redshift to Snowflake;
 2. Immutable tools:
 	1. Tools that solve a problem that the organization does not currently handle;
 	2. Requirements are not as strict as the area the tool is solving is still filled with uncertainty;
 	3. Takes less time to roll out;
 	4. Example: Using [Great Expectations](https://greatexpectations.io/) for data quality checks when data quality checks were not incorporated into ETL pipelines prior to it.
 
 
 Regardless of the type of tool, the evaluation framework I employ is the same. 
 
 
 One of the very first things to do is to actually ask yourself if the tool you are considering is solving a big enough problem for the organization. For example, if you are trying to consider tool X which supposedly makes model serving Y% faster: Is model serving such a bottleneck to the organization that it requires a complete re-architecture in order to be improved? If it is not then your time is better spent doing another task and/or evaluating a more pressing business problem. 
 
 
 Every day I get blasted with all these new and great tools to use to improve our machine learning development process and it’s so easy to suffer from FOMO but it’s very important to not get distracted from the bigger picture in terms of organizational needs.
 
 
 Once you have determined that tool X is actually worth spending time trying out, I have a very simple litmus test to determine if the tool is worth spending time on, which is to find out the *Time To Complete A Basic Example*. If it takes more than a couple of hours to complete a basic example then it usually means that the tool is not worth my time.
 
 
 At this point, if it passes on, I will spend more time with more complex examples related to our work at MoneyLion and if I am happy with the tool at this point I start drafting a POC proposal document. 
 
 
 This document will highlight to important stakeholders:
 
 
 1. What is the problem and how does this tool solve it?
 2. What are some metrics that can be improved using this tool?
 3. A basic example of the tool with some code;
 4. A comprehensive overview of the POC:
 	1. Concrete use cases to test the tool on;
 	2. The time needed for the POC;
 	3. Manpower needed for the POC.
 
 
 Once the stakeholders give their feedback we can then make a decision to fit it into our sprint planning goals if the POC is approved. I would like to highlight how blessed I am to have a culture that prioritizes continuous innovation at MoneyLion, without this, working in MLOps would be much more difficult.",H2,https://outerbounds.com/blog/mlops-fin-tech#what-questions-do-you-think-about-answering-when-adopting-new-ml-technologies-and-how-do-you-make-decisions-about-what-to-adopt,False,3159.0,568.0
858,Why did you choose Metaflow?,"Metaflow helps us achieve the following four V’s:
 
 | Feature     | Motivation | 
 | :---:        |    :----:   |   
 | Volume | We want to support a large number of data science applications |
 | Velocity | We want to make it easy and quick to prototype and productionize data science applications |
 | Validity | We want to make sure that the results are valid and consistent | 
 | Variety | We want to support many different kinds of data science models and applications |
 
 
 
 
 ![](_mlops-fin-tech_data/0_img)",H2,https://outerbounds.com/blog/mlops-fin-tech#why-did-you-choose-metaflow,False,517.0,112.0
859,"What does your stack look like now, including Metaflow?","There are many ways to slice the stack but a useful approximation is into data, model development, model and data validation, deployment, and monitoring:
 
 
 * For the data layer, we use [Snowflake](https://www.snowflake.com/en/);
 * For model development, we use a number of tools:
 	+ [PyTorch](https://pytorch.org/),
 	+ [Tensorflow](https://www.tensorflow.org/),
 	+ [LightGBM](https://lightgbm.readthedocs.io/en/v3.3.2/),
 	+ [MLFLow](https://mlflow.org/), and
 	+ [Optuna](https://optuna.org/);
 * For model and data validation, we use [deepchecks](https://deepchecks.com/);
 * For model deployment, we use [Flask](https://flask.palletsprojects.com/en/2.2.x/) running on [Kubernetes](https://kubernetes.io/);
 * For model monitoring, we use [Arize.ai](http://arize.ai/);
 
 
 ![](_mlops-fin-tech_data/1_img)",H2,https://outerbounds.com/blog/mlops-fin-tech#what-does-your-stack-look-like-now-including-metaflow,False,814.0,84.0
860,What did you discover about Metaflow after using it? What Metaflow features do you use and what do you find useful about them?,"Easily one of my favorite features of Metaflow is that it has the easiest way to store/share/version data artifacts between different steps in a distributed workflow from any tool that I have tried. Coming from Apache Airflow I would have to use [XComs](https://airflow.apache.org/docs/apache-airflow/stable/concepts/xcoms.html) in order to share data between different steps in a workflow but you could only share small data artifacts, such as a very simple JSON object. Anything bigger would require you to explicitly store/manage the data in an object store such as AWS S3. 
 
 
 Piggybacking off this point, I love the Metaflow Client API and how easy it is to share data between different users. It is such a common use case where data scientist A needs to leverage some data computed some time back from data scientist B for their work. Previously, we were always praying that data scientist A did save the data and that they remember where they saved it. If not we would have to recompute the data (if we even have the code to do so!) which slows down data scientist B significantly and increases our cost, but now as long as the workflow is done via Metaflow we never have to worry about it again as the Client API and the artifact store make it super easy to fetch the data.",H2,https://outerbounds.com/blog/mlops-fin-tech#what-did-you-discover-about-metaflow-after-using-it-what-metaflow-features-do-you-use-and-what-do-you-find-useful-about-them,False,1283.0,222.0
861,How has using Metaflow changed ML at MoneyLion?,"We are still in the early stages of using Metaflow at MoneyLion but based on feedback from the Data Scientists who do use Metaflow the main thing that it has enabled us to do is to increase the velocity at which we as an organization operate. Less time is spent building and scaling pipelines, less time is spent figuring out how to share data between users and workflows, less time is spent trying to provision resources for workflows, and importantly more time is spent doing data science.",H2,https://outerbounds.com/blog/mlops-fin-tech#how-has-using-metaflow-changed-ml-at-moneylion,False,491.0,87.0
862,Join our community,"If these topics are of interest, you can find more interviews in [our Fireside Chats](https://www.youtube.com/watch?v=Dr6DsWa6Dhg&list=PLUsOvkBBnJBeL8tsNGMdDKQjjOdU29sME&index=1) and come chat with us on [our community slack here](http://slack.outerbounds.co/).",H2,https://outerbounds.com/blog/mlops-fin-tech#join-our-community,False,261.0,25.0
863,Experiment freely,"Consider the new workspaces as starting points for your prototypes and demos. Beyond pre-built content in the sandbox, you can use the workspaces to experiment as you see fit. After all, data science and ML are all about experimentation and iteration!
 
 
 If you’re looking to deploy a recommender system, building upon a foundation model, or you are working on a more traditional business data science project, the sandboxes provide an ideal platform for testing out ideas in a realistic environment.
 
 
 Sandboxes are not a toy environment. Your experiments run on a [production-grade infrastructure stack](https://docs.metaflow.org/getting-started/infrastructure), leveraging common ML libraries, cloud-based data, Kubernetes-based compute and workflow orchestration. Once you are ready to move your experiments to production, you can deploy the infrastructure stack in your environment, either through [open-source templates](https://outerbounds.com/engineering/welcome/) or through our managed service, [Outerbounds Platform](https://outerbounds.com/platform/), both of which run on your cloud account.",H2,https://outerbounds.com/blog/learn-ml-in-the-browser#experiment-freely,False,1109.0,147.0
864,Get started today,"[It is easy to get started: All you need is a Google or GitHub account](/sandbox). The sandboxes are fully browser-based, including a cloud workstation with a built-in VSCode, pre-installed with your personal, cloud-based Metaflow stack. [Pick one of our existing tutorials or other workspaces](https://outerbounds.com/sandbox) and start learning and exploring! Also, [join us and over 2000 other data scientists and engineers on Slack](http://slack.outerbounds.co/) for support and to share ideas for future content.",H2,https://outerbounds.com/blog/learn-ml-in-the-browser#get-started-today,False,517.0,71.0
865,What type of ML questions do you work on at Hypefactors and what type of business questions do they solve?,"At Hypefactors we apply AI for media intelligence. We leverage AI-based automation to derive and massively scale the real-time insights from the ongoing media landscape and handle the increasing volume of data.
 
 
 We have a plethora of models, ranging from transformer-based NLP, computer vision, statistical models, and automated reasoning models. In terms of NLP, we do tasks ranging from sequence classification to text generation and span classification tasks.
 
 
 All those models enable us to digest the media landscape in real-time and extract information for our clients. Some of our models, for example, our [named-entity recognition model,](https://hypefactors.com/blog/noi-names-of-interest-teknologien-tager-hypefactors-media-intelligence-losning-til-et-for-uset-niveau-for-praecision/) are fundamental to powering our use cases. Our approach is to scale the AIs to comprehend hundreds of languages that are in use in the media. This allows us to cover and monitor a vast range of media sources from all corners of the world, which would be intractable without AI.",H2,https://outerbounds.com/blog/mlops-media-intelligence#what-type-of-ml-questions-do-you-work-on-at-hypefactors-and-what-type-of-business-questions-do-they-solve,False,1079.0,150.0
866,What did your ML stack look like before you adopted Metaflow and what pain points did you encounter?,"I would first split our tool stack into 4 components: data, modeling, testing, optimization, and inference.
 
 
 On the data part, we typically use [Label Studio](https://labelstud.io/) for data annotation and an in-house tool called “bundler” for label QA and exploration. We then version our experiments and associated data using [Neptune.ai](https://neptune.ai/) and another in-house tool.
 
 
 Regarding model development, we use a combination of [PyTorch Lightning](https://www.pytorchlightning.ai/) and the [Hugging Face ecosystem](https://huggingface.co/docs). [Hydra](https://hydra.cc/docs/intro/) enables us to experiment with different configurations faster and is also used along with Neptune for experiment tracking.
 
 
 Once we are satisfied with the trained models we go on and validate them. We have several tools at our disposal, among which are [Checklist](https://github.com/marcotcr/checklist) for behavioral testing and [Gradio](https://gradio.app/docs/) for ad-hoc testing. 
 
 
 Finally, we optimize our models for handling billions of GPU-accelerated LLM inferences a day. This is a huge workload. Also here we have a plethora of tools we apply. [ONNX](https://onnx.ai/)‘s facilities and [BERT squeeze](https://github.com/JulesBelveze/bert-squeeze) are what we use most frequently.
 
 
 Regarding pain points, you can probably see that our machine learning workflows result in many different tools which require a non-neglectable time to get a grasp on all of them. It’s a very polished workflow with many interconnected cogs resulting from years of continuous improvement on our in-house MLOps processes.
 
 
 But to me, the main pain point of this stack is the amount of manual work needed to stitch the 4 components together. I had to 
 
 
 * manually pull data, transform it and version it;
 * use that data to train different model configurations;
 * test the most promising one before finally optimizing and deploying it.
 
 
 The main reason for such a stack choice is that we started adopting tools incrementally based on our immediate needs. This required a lot of redundant work and quite some productivity downtime, mainly to wait for one task to finish before running the next one.
 
 
 ![](_mlops-media-intelligence_data/og.png)",H2,https://outerbounds.com/blog/mlops-media-intelligence#what-did-your-ml-stack-look-like-before-you-adopted-metaflow-and-what-pain-points-did-you-encounter,False,2265.0,328.0
867,What questions do you think about answering when adopting new ML technologies and how do you make decisions about what to adopt?,"Before even considering adopting a new ML tool or techno it needs to answer at least one of the following questions: 
 
 
 * Is it automating something we are doing manually?
 * Is it improving something we are already doing?
 * Or is it doing something we want to do, such that we do not have to reinvent the wheel?
 
 
 If it does then we assess it on a bunch of criteria:
 
 
 * We start by evaluating its ease of adoption. Indeed, being a relatively small team we cannot afford to spend two months setting up an experiment tracker;
 * I personally like to check the quality of the tool’s docs and ask the community for feedback before adopting it;
 * Finally, a non-neglectable aspect is whether it is open-source and if not how much does it cost?",H2,https://outerbounds.com/blog/mlops-media-intelligence#what-questions-do-you-think-about-answering-when-adopting-new-ml-technologies-and-how-do-you-make-decisions-about-what-to-adopt,False,751.0,143.0
868,Why did you choose Metaflow?,"When we identified the pain points of our ML stack an obvious solution was to find a way to stitch together the different components of our workflow. Automating it would save us a consequent amount of time and enable faster iteration. For this reason, we started looking for a workflow orchestrator and a bunch of options were in front of us. One of the main reasons we chose Metaflow was its ease of adoption and maintenance. After a few hours, we had a POC running. Also, as you might have noticed from our tool choices we tend to go for tools that do one thing and do it well. Instead of having one tool that tries to encompass many components of the ML workflow or with superfluous features. In the end, Metaflow
 
 
 * Perfectly answered our needs without extra components,
 * It’s easy to use and maintain, and
 * We then discovered other cool features, such as flow inspection and debugging.
 
 
 Before adopting Metaflow we also looked into Kubeflow and Airflow. The reasons we chose Metaflow were that it’s easy to set up and maintain even by a small data team. In addition, Metaflow’s ability to share data and other components between steps was a huge plus for us.",H2,https://outerbounds.com/blog/mlops-media-intelligence#why-did-you-choose-metaflow,False,1175.0,213.0
869,What did you discover about Metaflow after using it? What Metaflow features do you use and what do you find useful about them?,"As described earlier when we adopted Metaflow we were primarily using it as a workflow orchestrator but with time we started using more of its components.
 
 
 Currently, a feature that is a key element of our workflows is Metaflow’s Card. We have extended [Jacopo’s model cards](https://github.com/jacopotagliabue/dag-card-is-the-new-model-card) to generate a training report for each experiment we run. Those reports feature information about the model used, the datasets, various metrics of interest, and the results of behavioral tests written for this specific task. We then share these reports with different stakeholders.
 
 
 As our data pre-processing is part of our workflow we also leverage Metaflow’s parallelization feature. Another feature that became quite handy is the client API. By using it we keep track of and inspect the data state during the different processing steps.
 
 
 We are extremely excited about the upcoming GKE integration and cannot wait to deploy it to our Kubernetes cluster with Argo!",H2,https://outerbounds.com/blog/mlops-media-intelligence#what-did-you-discover-about-metaflow-after-using-it-what-metaflow-features-do-you-use-and-what-do-you-find-useful-about-them,False,1022.0,158.0
870,How has using Metaflow changed ML at Hypefactors?,"The adoption of Metaflow has surely smoothened the whole data science workflow and iteration process but I see two obvious benefits.
 
 
 First of all, it has enabled us to save a significant amount of time. We now only need to trigger one workflow to run the whole pipeline. No need to wait for one step to finish before running the next one. That way we can focus on something else while the workflow runs.
 
 
 Also, it saves us time as most components of our workflows are reusable across projects. This is also quite valuable when onboarding new people. In the past, it was quite difficult for newcomers to assimilate all the different tools and how they articulate with each other. Now they can extend the templates we have according to their needs without having to deeply understand all the machinery behind it.",H2,https://outerbounds.com/blog/mlops-media-intelligence#how-has-using-metaflow-changed-ml-at-hypefactors,False,819.0,146.0
871,Next Steps,"If these topics are of interest, you can find more interviews in our [Fireside Chats](https://www.youtube.com/watch?v=Dr6DsWa6Dhg&list=PLUsOvkBBnJBeL8tsNGMdDKQjjOdU29sME&index=1) and come chat with us on our community slack [here](http://slack.outerbounds.co/).",H2,https://outerbounds.com/blog/mlops-media-intelligence#next-steps,False,261.0,25.0
872,**A Healthy Obsession with Productivity in Data Science**,"At Outerbounds, we are obsessed with data scientist productivity, and making sure that first-class machine learning infrastructure is accessible to everyone and delightful to work with. Our mission is to remove all possible infrastructure-related friction in data scientists’ day-to-day work and make sure they can experiment with ideas quickly and deploy to production confidently. For the past four years, we have been developing an open-source framework, Metaflow, to make this a reality for most organizations.
 
 
 
 
 Oleg Avdeev, co-founder of Outerbounds, talking about Metaflow’s journey to K8s.",H3,https://outerbounds.com/blog/kubernetes-to-metaflow#a-healthy-obsession-with-productivity-in-data-science,False,604.0,90.0
873,**The Infrastructure Paradox: Why Kubernetes? And why not?**,"*Data scientists should not need to know lower level infrastructural tools such as Kubernetes but Kubernetes is a key part of the “modern production infrastructure” 🤔*
 
 
 Today, in 2022, there is no question that Kubernetes (often abbreviated K8s) is eating the world of container orchestration. You can even argue that it is approaching the “plateau of productivity” of the [hype cycle](https://en.wikipedia.org/wiki/Hype_cycle), where the tooling is getting mature enough and the K8s skills are (almost) widespread enough among DevOps engineers. Betting on K8s in your org is no longer the choice of only the brave and technically forward-thinking organizations that are ready to deal with the rough edges. These days, thanks to packaged offerings like [EKS on AWS](https://aws.amazon.com/eks/), any skilled platform team can make it work without paying too much of an early adopter penalty.
 
 
 **What’s so good about it,** and what’s its unique promise for infrastructure teams? There are many answers to this question, but here’s one take. It is not so much about the software implementation itself but having :
 
 
 * A set of standard APIs to manage containerized infrastructure
 * A (mostly) standardized set of tooling and processes to deploy and evolve this infrastructure using best practices of infrastructure-as-code
 
 
 In other words, using Kubernetes, your friendly ops team can assemble a tailored, cohesive “platform” from the building blocks and operate it using industry-standard practices. The combination of the blocks is unique to your business needs, but the blocks themselves and tooling around them aren’t.
 
 
 But here’s a paradox: data scientists should not need to know lower-level infrastructural tools such as Kubernetes, as [Chip Huyen has written](https://huyenchip.com/2021/09/13/data-science-infrastructure.html). However, to make it easy to move machine learning projects from prototype, iteration, and experiment to production, machine learning applications need to play nicely with production infrastructure, which very much includes Kubernetes in a growing number of organizations. In a word, machine learning and MLOps are not islands. To state the paradox another way, if you’re the leader of a data science team, you can ignore K8s…. but you also can’t.
 
 
 ![](_kubernetes-to-metaflow_data/0_img)How do we fold existing DevOps practices into modern ML data-centric and MLOps workflows?
 **This is why we’re so excited to integrate Kubernetes to Metaflow**: With Metaflow, data scientists can leverage K8s clusters for their work without having to know K8s. Metaflow presents a user-friendly UX to data scientists while working nicely with production infrastructure, which engineers can appreciate. You need both Luigi and Mario to complete this level but you don’t need to play as both by yourself!
 
 
 ![](_kubernetes-to-metaflow_data/1_img)",H3,https://outerbounds.com/blog/kubernetes-to-metaflow#the-infrastructure-paradox-why-kubernetes-and-why-not,False,2892.0,433.0
874,**Metaflow as the data scientist’s interface to Kubernetes**,"The key to successful organizational adoption is making sure that the K8s-based platform is easy to use, and that everyone on your data science team can fully benefit from it [without learning the nitty-gritty details about Kubernetes](https://huyenchip.com/2021/09/13/data-science-infrastructure.html).
 
 
 The first step is to enable data science and machine learning practitioners to use K8s clusters as pools of compute resources. Data science is an inherently compute-heavy activity and we can use K8s as an engine to power it. Metaflow makes it easy to start by running everything locally but when you need to scale, it allows you to smoothly move parts of your workflows to K8s by adding a single line of code. 
 
 
 ![](_kubernetes-to-metaflow_data/2_img)
 
 
 By running a compute layer in K8s, the platform engineer gets K8s-native infrastructure with all the industry-standard management and observability tools. And as a data scientist, you can keep using all the cool features that Metaflow provides out of the box, such as 
 
 
 * [Human-friendly, idiomatic Python API](https://docs.metaflow.org/metaflow/basics)
 * [Vertical and horizontal scalability](https://docs.metaflow.org/metaflow/scaling)
 * [Automated tracking of experiments, models, and other artifacts](https://docs.metaflow.org/metaflow/client)
 * [Built-in dependency management for machine learning libraries](https://docs.metaflow.org/metaflow/dependencies).",H3,https://outerbounds.com/blog/kubernetes-to-metaflow#metaflow-as-the-data-scientists-interface-to-kubernetes,False,1440.0,191.0
875,**Try it out and give us feedback**,"The beta version of [the Kubernetes integration](https://github.com/Netflix/metaflow/tree/master/metaflow/plugins/aws/eks) is available in Metaflow today. The feature is not yet documented, as we want to ensure that the user experience it provides meets our high bar. If you would like to give it a try, [join our Slack](http://slack.outerbounds.co) and shoot a note on the #dev-metaflow channel. We want to make sure the integration plays nicely with different K8s-native observability tools, access control tooling, autoscalers, job queues, and GPUs. We don’t want to underestimate the effort it takes to sort out all these paper cuts. If you have war stories, feedback, or requirements you want to share related to the K8s ecosystem, [please reach out to us](http://slack.outerbounds.co)! Also, if the idea of working in the intersection of K8s, modern cloud tooling, and machine learning excites you, [we are hiring](https://outerbounds.com/workwithus/)!",H3,https://outerbounds.com/blog/kubernetes-to-metaflow#try-it-out-and-give-us-feedback,False,958.0,134.0
876,What type of ML questions do you work on at Realtor.com and what type of business questions do they solve?,"Realtor.com is one of the nation’s most popular home search destinations, with more than 100 million users visiting the site each month and hundreds of thousands of [property listings](https://www.realtor.com/realestateforsale). There are tons of ML use cases and they cover the gamut of modeling techniques, including:
 
  - A consumer-facing website that people might be familiar with, which has many common website optimizations such as recommendation systems, search ranking, consumer segmentation, and models for personalization to help people find relevant homes for their specific needs.
 
  - A variety of image and NLP models to enrich property content.
 
  - Forecasting for housing trends and strategic planning, sales/pricing optimization models, and surrogate experimentation metrics for AB testing and dashboards.
 
  - Models to help consumers find the best matching real estate professionals based on their needs.
 
 Everything from deep learning models, Kalman Filters, Bayesian probabilistic models, and every flavor of boosted trees you could hope to encounter.
 
 Each day these ML models output billions of predictions that get used across the business – all in service of empowering consumers to feel confident in their home buying or rental journey, alongside providing real estate professionals with tools to operate more effectively.",H2,https://outerbounds.com/blog/mlops-real-estate#what-type-of-ml-questions-do-you-work-on-at-realtorcom-and-what-type-of-business-questions-do-they-solve,False,1358.0,205.0
877,What did your ML stack look like before you adopted Metaflow and what pain points did you encounter?,"There was a mixed bag of AWS services, largely stitched together manually. Back when Metaflow was first open-sourced in late 2019, we were using many of the same AWS services that Metaflow supports like Batch, Step Functions, and S3 as core components of our ML pipelines.
 
 It was quite tedious to maintain as there weren’t many good infra-as-code options out there, especially that wasn’t a pain from a typical ML workflow perspective. If I have to write more YAML/JSON than Python, something has gone wrong!
 
 Creating the state machines was a pain, and back then there wasn’t even baked-in support for Batch jobs as standalone steps, so you had to create Lambda functions to submit the Batch jobs, poll for job status changes, and orchestrate those cyclical job dependencies in the state machine yourself.
 
 At that time, we were also creating the Batch job definitions separately for each component of a workflow, each using its own Docker image with the source files and Python environment built into it. Having to rebuild/sync docker images to ECR during dev to propagate changes onto Batch jobs adds friction.
 
 However, that was seen as easier from a dev workflow perspective than relying on git to ship code / dev branches onto the remote jobs (Metaflow’s code artifacts are an even better dev experience!). Another common pain point was state transfer and sharing of data between steps. We had some tools for dataset abstractions that would also handle reading/writing to a versioned S3 bucket to allow for run-over-run comparisons, but even after a couple of iterations it still felt a bit lacking from a developer experience.",H2,https://outerbounds.com/blog/mlops-real-estate#what-did-your-ml-stack-look-like-before-you-adopted-metaflow-and-what-pain-points-did-you-encounter,False,1642.0,275.0
878,Choices that have aged well,"All that said, some early decisions of the tech stack have aged well over time, like focusing on cloud-first development, creating a separate AWS account for our ML/DS org, and creating clear handoff rules of engagement with our engineering teams for batch/real-time model deployments. We have also always emphasized simplicity, e.g. vertically scaling before refactoring for distributed approaches. 
 
 This was back in 2017-2018 as much of the Hadoop/Spark hype was starting to taper off in favor of managed offerings like Redshift/Snowflake for a data platform. Between offloading large dataset preprocessing onto those tools and utilizing very large EC2 instances, we were often able to accommodate our most demanding workloads on a single node.
 
 For cloud-first development at that time, we provisioned each ML/DS individual with their own EC2 instance as their “laptop in the cloud” and we had a shared docker image for a consistent toolchain with a preconfigured python environment and tools to query various databases. VSCode/PyCharm support for remote containers was still in its infancy back then, so we had tools to SSH and do port forwarding to those development boxes, and most people used JupyterLab as their remote IDE.",H3,https://outerbounds.com/blog/mlops-real-estate#choices-that-have-aged-well,False,1236.0,195.0
879,Model deployments,"For model deployments at that time (and largely still the case now), we try to utilize batch deployments as often as possible, which usually involves the ML workflow outputting data which then gets ingested into a DB to be accessed later as needed by applications, dashboards, etc. These batch deployments cover ~95% of our ML use cases.
 
 We’ve always worked to empower our ML/DS teams to be able to own batch deployments entirely end-to-end. In addition to often being a simpler deployment, with fewer teams involved, and fewer potential points of failure, they’re also often the best performance at runtime since the model outputs are effectively cached in a DB and ready to be queried by the apps/services.
 
 For real-time model deployments, it can vary a bit case by case, but generally, that’s a handoff to our engineering teams who own that particular service, e.g. pickle to serialize a trained model artifact which can be loaded onto a flask web server for inference as needed.
 
 ```mdx-code-block
 import LiteYouTubeEmbed from 'react-lite-youtube-embed';
 import 'react-lite-youtube-embed/dist/LiteYouTubeEmbed.css';
 
 <div className=""video-container"">
 <LiteYouTubeEmbed
         id=""N8UHbuEuRDc""
         title=""How to stay on top of all the ML tooling""
 />
 </div>
 ```",H3,https://outerbounds.com/blog/mlops-real-estate#model-deployments,False,1286.0,209.0
880,What questions do you think about answering when adopting new ML technologies and how do you make decisions about what to adopt?,"There’s a myriad of tools and services that exist, and for any given task, there are often many totally viable options to choose from, so it’s almost less about being functionally correct and more about having good taste. Good taste can also depend on many circumstances of your team, business maturity, and existing tech debt business investments.
 
 Often though, I think it’s about making decisions that let you focus your energy on solving your actual business problems, rather than debugging infrastructure (e.g. leaning more heavily on managed services rather than self-hosting) or recreating some functionality that could be have been solved using open source solutions.
 
 Beyond the core questions around what capabilities are being provided and what breadth the scope of any potential benefits might be – some additional considerations are:
 
 
 * Developer experience
 	+ What’s the learning curve like? How good is the documentation and is it accessible in less than 3 clicks? Does the tool have a janky DSL or a custom flavor of SQL?
 	+ Is there a community (e.g. slack, Stack Overflow) around the tool? A big chunk of software dev is googling and finding posts from other people who have solved similar challenges. There can be feedback effects that spur more growth from community momentum that continuously makes the dev experience even better.
 	+ Is the tool self-service? Can I grab an API key and spin up a prototype in an afternoon, without having to schedule a meeting with a salesperson or having to sit through a demo with someone who may not be able to answer technical questions?
 * Open source / hosted offering / cost
 	+ Open source can be considered a type of freemium model for many tools, e.g. feel free to run it yourself and once you’re hooked you can pay for our super powerful cloud-hosted solution.
 	+ If it’s a closed-source or a paid offering, is there a free tier to play around with, without having to rope in procurement teams and deal with MNDAs/supporting contracts?
 	+ If it’s self-hosted, how much of a pain is it to set up and maintain?",H2,https://outerbounds.com/blog/mlops-real-estate#what-questions-do-you-think-about-answering-when-adopting-new-ml-technologies-and-how-do-you-make-decisions-about-what-to-adopt,False,2086.0,356.0
881,Why did you choose Metaflow?,"Given all the pain points previously mentioned, when I saw the [AWS ReInvent demo](https://www.youtube.com/watch?v=DJVJx2fSf90) for Metaflow in 2019 it piqued my interest and as a potential new tool, I took it for a spin to get hands-on experience with it and provisioned the stack alongside our AWS infra pretty much right after.
 
 Initially, I was excited just for the Pythonic abstractions to wire up those AWS services (the AWS CDK was pretty terrible at that time), much less all the other benefits from Metaflow like versioning and tracking metadata, providing clean S3 interfaces along with state transfer within workflows, and reducing the gap between dev and production.
 
 There were some other tools coming out around the same time like Prefect that looked promising, and some other teams across the company were using Airflow, Redis Queues, and other flavors of job frameworks – all in all, Metaflow was a great fit with our existing ML infrastructure, is easy to use and maintain, and has great ergonomics with clean interfaces to track/share workflows and their artifacts.",H2,https://outerbounds.com/blog/mlops-real-estate#why-did-you-choose-metaflow,False,1087.0,176.0
882,"What does your stack look like now, including Metaflow?","Right out of the gate, Metaflow provided much cleaner abstractions for many of the core AWS services that our ML pipelines relied on. Within a few weeks of that initial proof of concept, we had already refactored significant portions of our ML codebase to utilize Metaflow instead of our legacy utilities. As [the saying](https://programmingisterrible.com/post/139222674273/write-code-that-is-easy-to-delete-not-easy-to) goes, “Write code that is easy to delete, not easy to extend”, and it’s always satisfying to clean up and remove your old code in favor of better packages.
 
 Beyond the myriad of AWS services, we’ve consolidated onto Snowflake as a data platform, dbt is used for many business-layer abstractions within our data models, Fivetran is used to replicate and pipe data between Snowflake and other application databases (e.g. Postgres, DynamoDB) and various 3rd party integrations.
 
 Cloud-native development has really taken off in the past few years, with tools like VSCode remote containers making significant advancements. We still provision each DS/ML individual with their own EC2 instance as their “laptop in the cloud” and that’s continued to improve over time.
 
 We have a small local Python environment for basic connectivity tools and helper utilities that everyone uses on their laptop, for things like Okta SSO authentication that can be paired with AWS SSM to establish secure connections to those instances while keeping them in private subnets. On those instances, we use a Docker container as a full-featured development environment so that we have a consistent, easily reproducible toolchain across our team.
 
 Utilizing VSCode remote containers we can have what feels like a local IDE, but it’s actually running in AWS with all the compute flexibility that entails – plus having Metaflow preconfigured, environmental/DB permissions ready to go, and some hygiene tools for coding standards like `black`, `ruff`, `precommit`, etc. The ambition for developer experience has always been that a new hire should be empowered to go from nothing to a fully provisioned setup, capable of submitting a github PR, within an afternoon.
 
 Relative to our Python stack, I’d say it spans a significant portion of the PyData stack and modeling frameworks. Metaflow provides significant flexibility on that front, so there aren’t many limitations, and people are empowered to use the best tool/package for the job.
 
 * PyTorch is our deep learning framework of choice, including PyTorch Lightning and PyTorch Forecasting.
 * CatBoost, XGBoost, LightGBM, sklearn, Huggingface, PyMC, Pyro, PyStan, Prophet, UMAP, HDBSCAN are all quite common across our ML projects.
 * In addition to Snowflake/DB connectors for querying data, AWSWrangler (now the AWS SDK for pandas) is frequently used for ETL and working with Parquet datasets.
 * For reporting and monitoring there’s a mix of notebooks, Metaflow cards, Slack integrations, Snowsight Dashboards, and Streamlit apps depending on the needs and flexibility required (e.g. interactive Altair plots, SHAP analysis, Kepler.gl viz).",H2,https://outerbounds.com/blog/mlops-real-estate#what-does-your-stack-look-like-now-including-metaflow,False,3098.0,470.0
883,What did you discover about Metaflow after using it? What Metaflow features do you use and what do you find useful about them?,"The AWS abstractions and workflow orchestration components of Metaflow were the most significant quality-of-life improvements during the initial adoption, and over time the metadata tracking became just as important if not even more so.
 
 Being able to quickly debug workflows by retroactively grabbing the exact data and model artifacts, reproducing the issue in a notebook, and interactively solving the problem is a huge productivity boost. Being able to have namespace isolation to see who ran what and when along with the separation of production from dev is huge. Utilizing project branching has been a great way to quickly experiment on existing processes. 
 
 The Metaflow UI has been amazing. It wasn’t around when we first started using Metaflow, nor were cards, and they’ve both been super helpful for monitoring, quickly navigating through metadata, and even replacing a fair bit of team-specific basic reporting needs (e.g. cards with AB testing results that get refreshed on a schedule, rather than a full-fledged dashboard).
 
 ```mdx-code-block
 <div className=""video-container"">
 <LiteYouTubeEmbed
 	id=""9gQbL6OjfTk""
 	title=""The moving parts of the full ML stack and how to choose""
 />
 </div>
 ```",H2,https://outerbounds.com/blog/mlops-real-estate#what-did-you-discover-about-metaflow-after-using-it-what-metaflow-features-do-you-use-and-what-do-you-find-useful-about-them,False,1217.0,184.0
884,How has using Metaflow changed ML at Realtor.com?,"Metaflow has helped our teams have a much smoother development experience, streamlined our toolchain, and shaved months off the time it took to go from ideation to production deployed pipelines. 
 
 It’s significantly easier to build and scale workflows, experimentation friction has been greatly reduced, and we’ve also had improved collaboration with other organizations across the company that have benefited from more consistent coordination of models and rapid prototyping.
 
 Metaflow has become the backbone of ML projects at Realtor.com, and enables our teams to quickly iterate and deliver value to the business and our consumers.",H2,https://outerbounds.com/blog/mlops-real-estate#how-has-using-metaflow-changed-ml-at-realtorcom,False,639.0,97.0
885,Join our community,"If these topics are of interest, you can find more interviews in [our Fireside Chats](https://www.youtube.com/watch?v=Dr6DsWa6Dhg&list=PLUsOvkBBnJBeL8tsNGMdDKQjjOdU29sME&index=1) and come chat with us on our [community slack here](http://slack.outerbounds.co/).",H2,https://outerbounds.com/blog/mlops-real-estate#join-our-community,False,261.0,25.0
886,What kind of foundation would the modern ML application require?,"It should combine the best parts of modern production infrastructure to ensure robust deployments, as well as draw inspiration from data-centric programming to maximize productivity.
 
 
 While implementation details vary, the major infrastructural layers we’ve seen emerge are relatively uniform across a large number of projects. Let’s now take a tour of the various layers, to begin to map the territory. Along the way, we’ll provide illustrative examples. The intention behind the examples is not to be comprehensive (perhaps a fool’s errand, anyway!), but to reference concrete tooling used today in order to ground what could otherwise be a somewhat abstract exercise.
 
 
 
 ![](_the-modern-stack-of-ml-infrastructure_data/0_img)Adapted from the book [*Effective Data Science Infrastructure*](https://www.manning.com/books/effective-data-science-infrastructure)",H2,https://outerbounds.com/blog/the-modern-stack-of-ml-infrastructure#what-kind-of-foundation-would-the-modern-ml-application-require,False,868.0,114.0
887,Foundational Infrastructure Layers,,H2,https://outerbounds.com/blog/the-modern-stack-of-ml-infrastructure#foundational-infrastructure-layers,False,0.0,1.0
888,Data,"**Data** is at the core of any ML project, so data infrastructure is a foundational concern. ML use cases rarely dictate the master data management solution, so the ML stack needs to integrate with existing data warehouses. Cloud-based data warehouses, such as **Snowflake**, AWS’ portfolio of databases like **RDS, Redshift** or **Aurora**, or an **S3-based data lake**, are a great match to ML use cases since they tend to be much more scalable than traditional databases, both in terms of the data set sizes as well as query patterns.",H3,https://outerbounds.com/blog/the-modern-stack-of-ml-infrastructure#data,False,537.0,89.0
889,Compute,"To make data useful, we must be able to conduct large-scale **compute** easily. Since the needs of data-intensive applications are diverse, it is useful to have a general-purpose compute layer that can handle different types of tasks from IO-heavy data processing to training large models on GPUs. Besides variety, the number of tasks can be high too: imagine a single workflow that trains a separate model for 200 countries in the world, running a hyperparameter search over 100 parameters for each model – the workflow yields 20,000 parallel tasks.
 
 
 Prior to the cloud, setting up and operating a cluster that can handle workloads like this would have been a major technical challenge. Today, a number of cloud-based, auto-scaling systems are easily available, such as **AWS Batch. Kubernetes,** a popular choice for general-purpose container orchestration, can be configured to work as a scalable batch compute layer, although the downside of its flexibility is increased complexity. Note that container orchestration for the compute layer is not to be confused with the workflow orchestration layer, which we will cover next.",H3,https://outerbounds.com/blog/the-modern-stack-of-ml-infrastructure#compute,False,1133.0,179.0
890,Orchestration,"The nature of computation is structured: we must be able to manage the complexity of applications by structuring them, for example, as a graph or a workflow that is **orchestrated**.
 
 
 ![](_the-modern-stack-of-ml-infrastructure_data/1_img)
 The workflow orchestrator needs to perform a seemingly simple task: given a workflow or DAG definition, execute the tasks defined by the graph in order using the compute layer. There are countless systems that can perform this task for small DAGs on a single server. However, as the workflow orchestrator plays a key role in ensuring that production workflows execute reliably, it makes sense to use a system that is both scalable and highly available, which leaves us with a few battle-hardened options, for instance: [**Airflow**](https://airflow.apache.org/), a popular open-source workflow orchestrator; [**Argo**](https://argoproj.github.io/)**,** a newer orchestrator that runs natively on Kubernetes, and managed solutions such as [**Google Cloud Composer**](https://cloud.google.com/composer) and [**AWS Step Functions**](https://aws.amazon.com/step-functions/).",H3,https://outerbounds.com/blog/the-modern-stack-of-ml-infrastructure#orchestration,False,1114.0,145.0
891,Software Development Layers,"While these three foundational layers, data, compute, and orchestration, are technically all we need to execute ML applications at arbitrary scale, building and operating ML applications directly on top of these components would be like hacking software in assembly language: technically possible but inconvenient and unproductive. To make people productive, we need higher levels of abstraction. Enter the software development layers.",H2,https://outerbounds.com/blog/the-modern-stack-of-ml-infrastructure#software-development-layers,False,435.0,61.0
892,Versioning,"ML app and software artifacts exist and evolve in a dynamic environment. To manage the dynamism, we can resort to taking snapshots that represent immutable points in time: of models, of data, of code, and of internal state. For this reason, we require **a strong versioning layer**.
 
 
 While **Git**, **GitHub,** and other similar tools for software version control work well for code and the usual workflows of software development, they are a bit clunky for tracking all experiments, models, and data. To plug this gap, frameworks like [**Metaflow**](https://docs.metaflow.org/) or [**MLFlow**](https://mlflow.org/) provide a custom solution for versioning.",H3,https://outerbounds.com/blog/the-modern-stack-of-ml-infrastructure#versioning,False,661.0,98.0
893,Software Architecture,"Next, we need to consider who builds these applications and how. They are often built by data scientists who are not software engineers or computer science majors by training. Arguably, high-level programming languages like Python are the most expressive and efficient ways that humankind has conceived to formally define complex processes. It is hard to imagine a better way to express non-trivial business logic and convert mathematical concepts into an executable form.
 
 
 However, not all Python code is equal. Python written in Jupyter notebooks following the tradition of data-centric programming is very different from Python used to implement a scalable web server. To make the data scientists maximally productive, we want to provide supporting **software architecture** in terms of APIs and libraries that allow them to focus on data, not on the machines.",H3,https://outerbounds.com/blog/the-modern-stack-of-ml-infrastructure#software-architecture,False,867.0,135.0
894,Data Science Layers,"With these five layers, we can present a highly productive, data-centric software interface that enables iterative development of large-scale data-intensive applications. However, none of these layers help with modeling and optimization. We cannot expect data scientists to write modeling frameworks like PyTorch or optimizers like Adam from scratch! Furthermore, there are steps that are needed to go from raw data to features required by models.",H2,https://outerbounds.com/blog/the-modern-stack-of-ml-infrastructure#data-science-layers,False,447.0,65.0
895,Model Operations,"When it comes to data science and modeling, we separate three concerns, starting from the most practical progressing towards the most theoretical. Assuming you have a model, how can you use it effectively? Perhaps you want to produce predictions in real-time or as a batch process. No matter what you do, you should monitor the quality of the results. Altogether, we can group these practical concerns in the **model operations** layer. There are many new tools in this space helping with various aspects of operations, including [**Seldon**](https://www.seldon.io/tech/products/deploy/) for model deployments, [**Weights and Biases**](https://wandb.ai/site) for model monitoring, and [**TruEra**](https://truera.com/) for model explainability.",H3,https://outerbounds.com/blog/the-modern-stack-of-ml-infrastructure#model-operations,False,744.0,101.0
896,Feature Engineering,"Before you have a model, you have to decide how to feed it with labelled data. Managing the process of converting raw facts to features is a deep topic of its own, potentially involving feature encoders, feature stores, and so on. Producing labels is another, equally deep topic. You want to carefully manage consistency of data between training and predictions, as well as make sure that there’s no leakage of information when models are being trained and tested with historical data. We bucket these questions in the **feature engineering** layer. There’s an emerging space of ML-focused feature stores such as [**Tecton**](https://www.tecton.ai/) or labeling solutions like [**Scale**](https://scale.com/) and [**Snorkel**](https://snorkel.ai/). Feature stores aim to solve the challenge that many data scientists in an organization require similar data transformations and features for their work and labeling solutions deal with [the very real challenges associated with hand labeling datasets](https://www.oreilly.com/radar/arguments-against-hand-labeling/).",H3,https://outerbounds.com/blog/the-modern-stack-of-ml-infrastructure#feature-engineering,False,1064.0,145.0
897,Model Development,"Finally, at the very top of the stack we get to the question of mathematical modeling: What kind of modeling technique to use? What model architecture is most suitable for the task? How to parameterize the model? Fortunately, excellent off-the-shelf libraries like [**scikit-learn**](https://scikit-learn.org/stable/) and [**PyTorch**](https://pytorch.org/) are available to help with **model development**.",H3,https://outerbounds.com/blog/the-modern-stack-of-ml-infrastructure#model-development,False,407.0,52.0
898,An overarching concern: Correctness and Testing,"Regardless of the systems we use at each layer of the stack, we want to guarantee the correctness of results. In traditional software engineering we can do this by writing tests: for instance, a unit test can be used to check the behavior of a function with predetermined inputs. Since we know exactly how the function is implemented, we can convince ourselves through inductive reasoning that the function should work correctly, based on the correctness of a unit test.
 
 
 This process doesn’t work when the function, such as a model, is opaque to us. We must resort to *black box testing* – testing the behavior of the function with a wide range of inputs. Even worse, sophisticated ML applications can take a huge number of contextual data points as inputs, like the time of day, user’s past behavior, or device type into account, so an accurate test set up may need to become a full-fledged simulator.
 
 
 Since building an accurate simulator is a highly non-trivial challenge in itself, often it is easier to use a slice of the real-world as a simulator and A/B test the application in production against a known baseline. To make A/B testing possible, all layers of the stack should be be able to run many versions of the application concurrently, so an arbitrary number of production-like deployments can be run simultaneously. This poses a challenge to many infrastructure tools of today, which have been designed for more rigid traditional software in mind. Besides infrastructure, effective A/B testing requires a control plane, a modern experimentation platform, such as [StatSig](https://statsig.com/).
 
 
 Thanks for reading and keep your eyes open for our post on **how** you can start applying the stack in practice today! If you’d like to get in touch or discuss such issues, [come say hi on our community Slack](http://slack.outerbounds.co/)! 👋",H3,https://outerbounds.com/blog/the-modern-stack-of-ml-infrastructure#an-overarching-concern-correctness-and-testing,False,1865.0,310.0
899,"Human-readable, visual reports for every ML pipeline","*Model documentation needs to be about the entire ML pipeline.*
 
 
 Data science and machine learning practitioners require better, more ergonomic tools to visualize and interrogate their workflows. Just focusing on model monitoring is not enough, as modern ML workflows involve a heterogeneous set of functionalities: To put models into production, you need to pay a great deal of attention to what comes before training (such as data collection, preparation, and labeling) and to what comes after (for example, testing, serving, monitoring, and data drift). Put simply, machine learning consists of building, testing, deploying, and maintaining models, each step of which requires multiple substeps.
 
 
 This has become clear in a major shift we believe is particularly important: The switch of focus from modeling to data, often referred to as Data-Centric AI. Even though some prominent scholars have been [talking about it](http://ai.stanford.edu/blog/data-centric-ai-retrospective/) for some time now, Data-Centric AI rose to the attention of the whole community only quite recently. In late 2021, [NeurIPS](https://nips.cc/) hosted the first [workshop on Data-Centric AI](https://datacentricai.org/), where we were fortunate enough to be presenting [our work on DAG cards for machine learning workflows](https://arxiv.org/pdf/2110.13601.pdf).
 
 
 As a concrete example, consider this recommendation pipeline from [Chia et al](https://arxiv.org/abs/2107.03256), in which the end-to-end pipeline is a series of tasks with explicit dependencies. Some tasks must be executed before others, some can run in parallel (red box); some require GPUs (blue), some are light computations; some may require restarting some others when they fail, some may not – and so on. 
 
 
 ![ML pipeline with many steps from Chia et al.](_integrating-pythonic-visual-reports-into-ml-pipelines_data/0_img)
 We take this last point very seriously and we believe that documentation should incorporate those aspects as well: **model documentation needs to be integrated into the entire ML pipeline**. And since the ML pipeline is, in practice, a Directed Acyclic Graph (DAG) we are excited to introduce DAG Cards for [Metaflow](https://metaflow.org), an open-source framework we started at Netflix and continue developing at Outerbounds. For the past four years, we have been systematically developing Metaflow together with data scientists at a diverse set of companies, adding features like cards that improve their productivity and the quality of projects delivered.  
 
 
 This idea of DAG Cards was originally developed by [Jacopo Tagliabue](https://towardsdatascience.com/dag-card-is-the-new-model-card-70754847a111) and the team at [Coveo](https://www.coveo.com/en), a B2B company providing ML-powered search and recommendations for a wide range of businesses, from e-commerce to customer service going through the workplace. Here’s how Jacopo describes the origin story of the idea:
 
 
 
 > We have tons of models in production, but only some of them are completely self-explanatory. The models we have do depend quite a lot on the use case, because that changes the data flow and the user behavior. In the end, to understand what many of our models do a certain degree of domain knowledge is required. Since our team finds Metaflow to be a flexible, ergonomic tool for ML pipelines, the natural thing to do was try building a DAG Card generator that lives on top of Metaflow existing classes. 
 > 
 > 
 
 
 
 > We were initially inspired by the concept of [Model Cards](https://arxiv.org/pdf/1810.03993.pdf), introduced in 2019 by Margaret Mitchell and colleagues and developed further by [Google Cloud](https://modelcards.withgoogle.com/about). Model cards are short documents accompanying trained ML models that provide benchmarked evaluation and crucial information about the model behavior in different conditions. We loved the idea right away. The beauty of the proposal lies in its inclusiveness: Technical folks can use Model Cards to build better ML systems while non-technical people can use them to ponder the strengths and weaknesses of models and make responsible use of them. We realized how useful this could be for working data scientists and loved the idea of getting a human-readable report for every workflow. But how should the reports be produced in practice?
 > 
 >",H3,https://outerbounds.com/blog/integrating-pythonic-visual-reports-into-ml-pipelines#human-readable-visual-reports-for-every-ml-pipeline,False,4377.0,645.0
900,How to produce cards?,"*Finding an ergonomic and effortless way to produce static, visual reports, like DAG cards, that complements existing tools.*
 
 
 A reason why many real-world ML pipelines remain under-documented and under-monitored is that these features don’t come for free. In particular, when a data scientist wants to monitor business metrics and attributes that are specific to a use case, they need to spend a non-trivial amount of time finding a suitable reporting tool, designing and implementing a report, and connecting it to production workflows.
 
 
 The last point in particular can be painful. While modern tools make it possible to build stunning dashboards, making the dashboard present the latest (not to mention historical) results from modeling workflows is not easy, as you have to bridge two separate tools. Still, we would like to be able to see a report for every model ever produced, across both prototyping and production.
 
 
 After working on many data science projects that faced this problem in various forms, we developed the following high-level understanding of the tooling landscape of today:
 
 
 
 ![The landscape of tooling.](_integrating-pythonic-visual-reports-into-ml-pipelines_data/1_img)
 There’s an inherent tradeoff between the versatility of a tool – how complex and advanced reports you can build with it – and the effort required to get even the simplest report produced. To be clear, this assessment is made specifically from the point of view of producing reports in ML pipelines. For a different set of requirements, the ranking would look different (and we would encourage you to perform such an assessment much of the time).
 
 
 In the lower-left corner we have the most effortless solution: Just produce textual logs from each modeling task, which is straightforward. Storing and accessing logs separately for each model isn’t hard either. A challenge is that textual logs are not versatile – in particular, one can’t use them to produce visualizations of any kind, which are key for data science.
 
 
 Notebooks are a popular choice for crafting visualizations. It doesn’t take much effort to open a notebook and produce a chart. However, producing a notebook automatically based on an existing parameterized template, while possible using a tool like [Papermill](https://papermill.readthedocs.io/), requires much more tooling and infrastructure, as well as a new mental model for using notebooks in a novel way. Notebooks certainly remain [a key part of the Metaflow stack](https://docs.metaflow.org/metaflow/client), as they are indispensable for ad-hoc exploration and quick prototyping, while cards come in handy when you know what aspects of the pipeline benefit from automated reports.
 
 
 Tools like [Streamlit](https://streamlit.io/) and [Plotly Dash](https://plotly.com/dash/) make it delightfully easy to create interactive dashboards, but connecting them to workflows is non-trivial, and hosting them reliably in production environments requires effort. These tools may even be too powerful for this use case, as our cards don’t need to be interactive. Along the same lines, towards the upper-right corner, there are other sophisticated tools like Tableau and custom web applications, which require so much upfront investment that we have seen them being used mainly for a select few, advanced use cases. **Our goal is to make the cost of reporting so low that we can expect every pipeline to include at least one basic card**.
 
 
 To make this vision a reality, we need an exceptionally simple solution that 
 
 
 * Allows rich, visual output,
 * Can be used as effortlessly as a simple print() statement,
 * Connects to ML workflows trivially easily, and
 * Can be easily shared with non-technical stakeholders.
 
 
 These requirements, fulfilled by Metaflow Cards, occupy a unique spot on the chart: They make it straightforward to produce static, visual reports, like DAG cards, in every pipeline. Since they focus on a specific set of use cases, they complement rather than compete with existing tools.",H3,https://outerbounds.com/blog/integrating-pythonic-visual-reports-into-ml-pipelines#how-to-produce-cards,False,4059.0,634.0
901,Getting started with Cards,"If you are new to Metaflow, start by [installing Metaflow](https://docs.metaflow.org/getting-started/install) and take a look at [Metaflow tutorials](https://docs.metaflow.org/getting-started/tutorials). After this, you are ready to start using cards!
 
 
 The latest version of Metaflow comes with a built-in *[Default Card](https://docs.metaflow.org/metaflow/visualizing-results/effortless-task-inspection-with-default-cards)* which you can attach to any existing flow – no changes in the code required! Just run your flow “––with card” as such:
 
 
 
 ```
 python myflow.py run –-with card
 ```
 
 After the run has finished, you can view cards produced by the run by specifying a step name. For instance, to view the card produced by the start step, type:
 
 
 
 ```
 python myflow.py card view start
 ```
 
 This will open the card in your browser. The default card shows all artifacts produced by a task, metadata about it, and a visualization of the flow DAG. This information can come in handy during development, as you can quickly eyeball whether the results of a task look valid.
 
 
 Following our original motivation, we want to make it trivially easy to produce rich visual output from any Metaflow task. For instance, this code snippets outputs a table:
 
 
 
 ```python
 from metaflow import card, current
 from metaflow.cards import Table, Markdown
 
 @card(type='blank')
 @step
 def start(self):
     current.card.append(Markdown(""# Here’s a table""))
     current.card.append(Table([['first', 'second'],
                                [1, 2]]))
 ```
 
 The card, which you can see with “card view start” as before, looks like this:
 
 
 ![](_integrating-pythonic-visual-reports-into-ml-pipelines_data/2_img)
 See the documentation for *[Card Components](https://docs.metaflow.org/metaflow/visualizing-results/easy-custom-reports-with-card-components)* for more details and examples. If you have [the Metaflow GUI](https://netflixtechblog.com/open-sourcing-a-monitoring-gui-for-metaflow-75ff465f0d60) installed, cards are automatically visible in the task view, like this card that uses [Altair](https://altair-viz.github.io/) for visualization:
 
 
 
 ![Card that uses Altair for visualization.](_integrating-pythonic-visual-reports-into-ml-pipelines_data/3_img)
 
 As shown above, you can use cards to embed custom visualizations and charts in the GUI. For instance, you can have a custom benchmarking card attached to the model training step which automatically ranks the performance of a new model against existing ones. Anyone participating in the project can see the results in real-time in the GUI.
 
 
 To support more advanced use cases, you can create a custom *[Card Template](https://docs.metaflow.org/metaflow/visualizing-results/advanced-shareable-cards-with-card-templates)* that can use any Javascript libraries and output arbitrary HTML. Card Templates can be published and shared as a public Python package, so you can share custom visualizations with your team and the wider Metaflow community.
 
 
 Thanks for reading! We’d love to hear your feedback for this new feature, as well as learn about ideas for new card templates. The easiest way to do this is to [join us and over 900 data scientists and engineers at Metaflow Community Slack](http://slack.outerbounds.co) 👋",H3,https://outerbounds.com/blog/integrating-pythonic-visual-reports-into-ml-pipelines#getting-started-with-cards,False,3308.0,478.0
902,Developing safe and reliable ML products at 23andMe,"![](_metaflow-highlights-2021_data/0_img)
 [23andMe has been using Metaflow for much of their machine learning](https://medium.com/23andme-engineering/machine-learning-eeee69d40736), which includes polygenic modeling for reports on conditions such as Typ2 Diabetes, HDL Cholesterol, Severe Acne, and Gallstones. 
 
 
 They identified many challenges in ML projects which are familiar to many businesses, such as the difficulties in testing ML systems, organizational and technical fragmentation, siloing, and the need for specialization in organizations, along with team independence (e.g. ML research versus ML engineering). They’re also working in a domain that is regulated and which also needs to take the privacy concerns of its stakeholders seriously.
 
 
 For many of their projects, model, data, and pipeline lineage are key, which makes Metaflow a good fit for them. On top of this, they also want to be able to 
 
 
 * develop models locally and
 * be able to move seamlessly between local and remote environments, and
 * perform continuous improvements at a low marginal cost.
 
 
 For them, Metaflow facilitates all of this, and helps them with debuggability as “all run parameters and model metrics are conveniently located alongside the run to aid in such debugging.”
 
 
 They also identified the need for specialized domain knowledge and made clear how difficult it is to find a set of individuals with the right mix of expertise across engineering, genomics research to build these systems.
 
 
 In our experience shipping ML products at 23andMe, it’s increasingly clear that both engineers and scientists developing models should be aware of the infrastructure and product context (failure modes, performance SLAs, compliance concerns, etc) in which the models are trained and deployed.",H3,https://outerbounds.com/blog/metaflow-highlights-2021#developing-safe-and-reliable-ml-products-at-23andme,False,1804.0,272.0
903,Improving Data Science Processes to Speed Innovation at Realtor.com,"Realtor.com, one of the nation’s most popular home search destinations with more than 100 million users visiting the site each month, use machine learning for
 
 
 * personalized searches,
 * recommendations,
 * forecasting housing trends,
 * helping consumers find the best agent to help them along their home shopping journey.
 
 
 [They began experimenting *with Metaflow with the goal of accelerating their machine learning function*](https://medium.com/realtor-com-innovation-blog/improving-data-science-processes-to-speed-innovation-at-realtor-com-b6b90fa530dc). The idea was to accelerate the ML lifecycle:
 
 
 Rapid prototype → deploy → feedback → iterate cycle
 
 
 We enjoyed reading their description of the Metaflow project:
 
 
 * “The team created a standardized access framework for data from multiple cloud platforms that manifests itself as a Python Library that any data scientist can easily access.”
 * “The initiative created a standardized process, infrastructure, and tooling for deploying machine learning algorithms in a directed acyclic graph (DAG) such that they become automated, scalable, and simple.”
 
 
 Most importantly, the team took months off the time to build a productionized machine learning model, which also had a clear impact on the speed of business.
 
 
 They were also able to develop a ***repeatable, automated deployment methodology and tooling***that also coordinates well with the rest of the organization.",H3,https://outerbounds.com/blog/metaflow-highlights-2021#improving-data-science-processes-to-speed-innovation-at-realtorcom,False,1455.0,209.0
904,Accelerating ML within CNN,"CNN, which averaged more than 200 million unique global visitors every month of 2020, was [“able to test twice as many models in Q1 2021 as they did in all of 2020” due to their adoption of Metaflow](https://medium.com/cnn-digital/accelerating-ml-within-cnn-983f6b7bd2eb)!
 
 
 Their huge audience requires both trusted and personalized news, and they want their data scientists to be focused on improving this experience, instead of constantly having to coordinate compute, clusters, and YAML. As there are always too many interesting ideas to explore, it’s key that the research to test and iterate time is as short as possible.
 
 
 Their previous process was “not designed for lightweight experimentation, and because of that, the research iteration process was frustratingly slow.” What they like about Metaflow is 
 
 
 
 > The straightforward Python interface meant that it instantly felt familiar to our ML engineers. Additionally, the seamless integration with AWS Batch gave us straightforward and simple scalability options.
 > 
 > 
 
 
 And
 
 
 
 > [Their] ML Ops engineers were able to implement a rich compute substrate that met the security/scalability requirements of CNN, and Metaflow was able to leverage that substrate using AWS Batch to scale up (and spin down) resources dynamically. This allowed [their] researchers to rapidly iterate with a small set of data locally.
 > 
 > 
 
 
 On top of this, they were able to scale without rewriting their code!
 
 
 ![](_metaflow-highlights-2021_data/1_img)
 They also loved that all runs are reproducible and that all code, data, and model artifacts are snapshotted to s3. An interesting side effect was that they could transition production training to this workflow also. It is wonderful to have them as part of the community and they even submitted PRs back to Metaflow – including Terraform support. In fact, CNN was instrumental in driving the Terraform support for Metaflow – their engineers delivered the entire support end-to-end.
 
 
 
 > “At this point, we’ve been able to entirely switch over our research experimentation process to rely on this Metaflow-powered workflow and with great success. Our researchers are able to spend more of their time improving their models for our audience, and our engineers are able to provide a rich compute substrate without having to manually manage clusters. As an informal estimate, our data science team believes they were able to test twice as many models in Q1 2021 as they did in all of 2020, with simple experiments that would have taken a week now taking half a day.”
 > 
 >",H3,https://outerbounds.com/blog/metaflow-highlights-2021#accelerating-ml-within-cnn,False,2595.0,426.0
905,Brand Tracking with Bayesian Statistics and AWS Batch,"Bayesian inference is a wonderful part of statistics and one limitation is that its computational methods, such as [MCMC](https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo) and [NUTS](http://www.stat.columbia.edu/~gelman/research/published/nuts.pdf), can require a lot of compute, which is why [it was exciting to see Metaflow and Batch used for parallelizing hundreds of models](https://aws.amazon.com/blogs/startups/brand-tracking-with-bayesian-statistics-and-aws-batch/).
 
 
 The use case of generative Bayesian models here is for consumer perception of brands, for example, answering questions such as “how many people from a specific demographic know a particular brand?”
 
 
 ![](_metaflow-highlights-2021_data/2_img)
 The Bayesian framework here helps with 
 
 
 * small target groups (through the hierarchical groups),
 * distinguishing between signal and noise,
 * Poststratification to deal with the problem of biased samples.
 
 
 ![](_metaflow-highlights-2021_data/3_img)
 To quote the authors of the blog post,
 
 
 
 > In each model, the same predictor variables are used (the demographic variables), and only the dependent variable changes (the question we predict). It thus makes sense to do all the data transformation once and only parallelize the models itself. Using AWS Batch, this means we would have one batch job to load and transform the data, then one job per model in parallelism, and a single job afterward to collect and combine the results. However, this also means we need to take care of data sharing between the different batch jobs. We quickly noticed that orchestrating the different batch jobs ourselves was difficult to maintain and was also taking away our time and resources that we’d rather spend on improving the statistical models.
 > 
 > 
 
 
 They also found the following features useful:
 
 
 * Metaflow took care of saving and sharing data across the steps as well as handling the parallelization.
 * One of the advantages when working with Metaflow is that there was barely any difference between running the code locally and running the code on AWS Batch.
 * Using Metaflow ensured that the same piece of code was run, locally and on AWS Batch, and it also created the same conda environment for both environments. This helped tremendously with reproducibility and generally made development easier since they could be sure that what “works on their machine” will also work in the cloud and give the same results.
 * Switching to Metaflow with AWS Batch really allowed them to speed up their development process and focus on improving the statistical models.",H3,https://outerbounds.com/blog/metaflow-highlights-2021#brand-tracking-with-bayesian-statistics-and-aws-batch,False,2613.0,394.0
906,"Open-Sourcing a Monitoring GUI for Metaflow, Netflix’s ML Platform","2021 was a big year of growth for Metaflow inside Netflix as well. Internally, Netflix’s Metaflow deployment covers millions of runs and billions of artifacts produced. In a large-scale environment like this, it can be hard for an individual data scientist to know how their runs, and other runs they are interested in, are behaving. To address this pain point, already in 2020 Netflix had started developing a monitoring GUI for Metaflow, which was [finally released in open-source in October 2021](https://netflixtechblog.com/open-sourcing-a-monitoring-gui-for-metaflow-75ff465f0d60).
 
 
 The monitoring GUI for Metaflow allows data scientists to 
 
 
 * monitor their workflows in real-time,
 * track experiments, and
 * see detailed logs and results for every executed task.
 
 
 On top of this, the GUI can be extended with plugins, allowing the community to build integrations to other systems and [embed custom visualizations via Metaflow Cards](https://outerbounds.com/blog/integrating-pythonic-visual-reports-into-ml-pipelines/) directly into its views.
 
 
 When you open the GUI, you see an overview of all flows and runs, both current and historical, which you can group and filter in various ways:
 
 
 ![](_metaflow-highlights-2021_data/4_img)
 Note that you can also use this view for experiment tracking!
 
 
 There is also a timeline view, which is extremely useful in understanding performance bottlenecks, distribution of task runtimes, and finding failed tasks:
 
 
 ![](_metaflow-highlights-2021_data/5_img)
 You can jump into tasks and the task view also, which includes “logs produced by a task, its results, and optionally links to other systems that are relevant to the task.”
 
 
 If, for example, the task had deployed a model to a model serving platform, the view could include a link to a UI used for monitoring microservices.
 
 
 ![](_metaflow-highlights-2021_data/6_img)
 Check out [the blog post for more](https://netflixtechblog.com/open-sourcing-a-monitoring-gui-for-metaflow-75ff465f0d60), or, if you can go ahead and [deploy the GUI in your own environment](https://github.com/netflix/metaflow-ui).
 
 
 Thanks for reading! We’d love to hear your thoughts on the ML stack and Metaflow, as well as learn about what you’re working on. The easiest way to chat is to join us and over 900 data scientists and engineers at [Metaflow Community Slack](http://slack.outerbounds.co/) 👋",H3,https://outerbounds.com/blog/metaflow-highlights-2021#open-sourcing-a-monitoring-gui-for-metaflow-netflixs-ml-platform,False,2413.0,348.0
907,Successful collaboration in data science projects,"Consider managing an important tabular dataset that several people are regularly reading and writing new features to. A common workflow in such a setting is to iterate on many feature transformation and downstream model building steps before selecting a satisfactory model. 
 
 
 As the project evolves and more team members join
 
 
 * How do you manage the feature transformation history?
 * How do you ensure modifications don’t change feature behavior inadvertently?
 * How do you know what data/feature is related to what?
 * How do you version your machine learning workflow metadata and dependencies?
 * How does the team scale workflow steps by accessing necessary compute resources?
 
 
 To solve the feature transformation issues listed above, encapsulating dataframe transformations in functions is an obvious way to modularize a code base. However, this doesn’t address important questions that don’t look important when first getting started, but matter when the code grows and changes hands like:
 
 
 * What should you pass as input to and return from functions?
 	+ Do you pass the whole dataframe? Do you pass specific columns of the dataframe?
 * How are feature transformations related?
 
 
 Moreover, data scientists know how important iteration is across steps in their workflows and that there is often much debugging to be done. As you iterate:
 
 
 * How do you avoid recomputing work you have already done?
 	+ If you only want a subset of features from the raw data, how do you avoid unnecessary feature transformations without re-writing your transformation code?
 	+ If a workflow step fails, do you recompute everything prior to that step?
 * How do you debug what steps in the workflow failed?
 	+ How do you capture the state of the environment where failures occurred?
 
 
 Another consideration is that as the number of transformations in a codebase grows overtime, determining feature dependencies by inspecting the codebase becomes more onerous. This makes discovering relationships and lineages through visualizations more valuable. 
 
 
 In data science projects, outcomes like unnecessary computation, code base complexity, and lack of discoverability can become significant sources of friction for developers. This is a problem if it blocks data scientists too much from focusing on levels of the stack where they contribute the most or have the most leverage.   
 
 
 This post highlights DAGs as a useful way to structure data science workflows through an example using Hamilton and Metaflow. Both tools ask you to structure data science code as DAGs. 
 
 
 To read more about how these benefits emerge, check out: [Why Should I Care About DAGs and Workflows in Data Science?](https://outerbounds.com/docs/dags-in-data-science)",H3,https://outerbounds.com/blog/developing-scalable-feature-engineering-dags#successful-collaboration-in-data-science-projects,False,2768.0,448.0
908,Intro to Hamilton,"Hamilton is a micro-framework that helps you easily express and maintain feature transformation code as a DAG. It came from [Stitch Fix](https://towardsdatascience.com/functions-dags-introducing-hamilton-a-microframework-for-dataframe-generation-more-8e34b84efc1d) and helps the Forecasting and Estimation of Demand (FED) team and their codebase stay productive. Managing a pandas code base of feature transforms over a few years usually isn’t a pleasant experience, but with Hamilton, you’ll avoid most of the pitfalls, and it’ll become an enjoyable experience, no matter the number of features you add, or who writes them! 
 
 
 Specifically, Hamilton forces data scientists to write Python functions that declare what they depend on via function parameters, and the function name specifies what the output of the function is. 
 
 
 ![](_developing-scalable-feature-engineering-dags_data/0_img)
 <Caption>Hamilton structure for defining feature transformation functions</Caption>
 
 From this, the Hamilton library can build a DAG. With a DAG, which you get by instantiating a Hamilton Driver, then you just need to declare:
 
 
 * the outputs you want
 * the right feature names as input
 
 
 and Hamilton will execute only the functions that are required to produce the outputs.",H3,https://outerbounds.com/blog/developing-scalable-feature-engineering-dags#intro-to-hamilton,False,1282.0,182.0
909,A guiding example,"You can find the code example mentioned in the rest of this post in this [repository](https://github.com/outerbounds/hamilton-metaflow). The example workflow includes: using Hamilton for feature transformation, several feature importance computations, and Metaflow for modeling steps, metadata tracking and workflow orchestration. You can find an introduction to Metaflow [here](https://outerbounds.com/docs/data-science-welcome).
 
 
 The learning task in the example is multiclass classification and is defined in the paper: [Prediction of absenteeism at work with a multinomial logistic regression model](https://www.researchgate.net/publication/358900589_PREDICTION_OF_ABSENTEEISM_AT_WORK_WITH_MULTINOMIAL_LOGISTIC_REGRESSION_MODEL). The dataset was originally curated in the paper [Application of a neuro fuzzy network in prediction of absenteeism at work](https://ieeexplore.ieee.org/document/6263151). The original dataset has 20 features and labels encoded as integers. 
 
 
 Following the first paper in the previous paragraph our example encodes three label buckets: 
 
 
 ![](_developing-scalable-feature-engineering-dags_data/1_img.png)
 <Caption>Label distribution for our classification task (left) and raw data (right)</Caption>
 
 Our code example applies several feature importance methods and scikit-learn classifiers. For modeling steps, we track classifier performance on the following metrics: accuracy, macro-weighted precision, macro-weighted recall, macro-weighted f1, training time, and prediction time.
 
 
 The remainder of this post describes how DAGs implemented in Hamilton and Metaflow can structure a workflow.",H3,https://outerbounds.com/blog/developing-scalable-feature-engineering-dags#a-guiding-example,False,1642.0,192.0
910,DAGs inside DAGs with Hamilton and Metaflow,"Like Hamilton, Metaflow asks data scientists to structure code in DAGs. In Metaflow this is referred to as steps in a flow. In the code example guiding this post the flow leverages the following Metaflow features:
 
 
 * [branching](https://docs.metaflow.org/metaflow/basics#branch) and [looping](https://docs.metaflow.org/metaflow/basics#foreach) in the Metaflow DAG
 * running arbitrary Python code – such as Hamilton feature transformation DAGs – in tasks
 * dependency management with [Metaflow’s conda integration](https://docs.metaflow.org/metaflow/dependencies#managing-dependencies-with-conda-decorator)
 * [cards](https://docs.metaflow.org/metaflow/visualizing-results#what-are-cards) to visualize artifacts produced by steps
 
 
 ![](_developing-scalable-feature-engineering-dags_data/2_img)
 <Caption>Metaflow DAG Visualized</Caption>",H3,https://outerbounds.com/blog/developing-scalable-feature-engineering-dags#dags-inside-dags-with-hamilton-and-metaflow,False,845.0,82.0
911,Applying Hamilton DAGs in our example,"In our example we have created several modules that house our Hamilton functions:
 
 
 * [`data_loader.py`](https://github.com/outerbounds/hamilton-metaflow/blob/main/absenteeism/data_loader.py) – contains functions to load data
 * [`normalized_features.py`](https://github.com/outerbounds/hamilton-metaflow/blob/main/absenteeism/normalized_features.py) – contains all numeric features
 * [`normalized_features_condensed.py`](https://github.com/outerbounds/hamilton-metaflow/blob/main/absenteeism/normalized_features_condensed.py) – identical to normalized\_features, but shows you some superuser ways to express Hamilton functions. See comments in the file for why or why not to use this approach.
 * [`feature_logic.py`](https://github.com/outerbounds/hamilton-metaflow/blob/main/absenteeism/feature_logic.py) – contains the rest of our features.
 
 
 The numeric feature transformation functions are defined in `normalized_features.py`. For example, one function in this file extracts the mean age of people represented in data records: 
 
 
 
 ```py
 def age_mean(age: pd.Series) -> float:
    """"""Average of age""""""
    return age.mean()
 ```
 
 The rest of the feature transformation functions are defined in `feature_logic.py`. For example, one function in this file extracts week days:
 
 
 
 ```py
 @extract_columns('day_of_the_week_2',
                  'day_of_the_week_3',
                  'day_of_the_week_4',
                  'day_of_the_week_5',
                  'day_of_the_week_6')
 def day_of_week_encoder(
    day_of_the_week: pd.Series) -> pd.DataFrame:
    """"""One hot encodes day of week into 
       five dimensions - 
       Saturday & Sunday weren't present.
       1 - Sunday, 2 - Monday, 3 - Tuesday, 
       4 - Wednesday, 5 - Thursday, 
       6 - Friday, 7 - Saturday.
    """"""
    return pd.get_dummies(day_of_the_week,          
                          prefix='day_of_the_week')
 ```
 
 Hamilton’s [`@extract_columns`](https://hamilton-docs.gitbook.io/docs/reference/api-reference/available-decorators#extract_columns) decorator works on a function that outputs a dataframe that we want to extract the columns from and make them individually available for consumption. It expands a single function into many functions, each of which take in the output dataframe and output a specific column as named in the decorator.
 
 
 To run it we just then instantiate a Hamilton Driver, passing it what it needs, and then executing it, requesting the features & outputs that we want back out.
 
 
 In the example, we run a Hamilton Driver in two steps of a Metaflow flow. The flow is defined in the `flow.py` file. The two steps are `featurize_and_split` and `feature_importance_merge`. Hamilton DAGs are embedded in these steps of the Metaflow DAG. The Hamilton DAG nodes are features and edges are transformations. The Metaflow DAG nodes are more general workflow steps and edges that tell an orchestrator the order steps should be executed. With Hamilton we are modeling a specific aspect of the data science workflow, namely feature transformation. With Metaflow we model a macro view of the workflow(s) describing our project. Metaflow steps can contain anything you express in Python code. 
 
 
 The main thing to pay attention to is that in addition to the self-documenting, easy-to-read transformation code Hamilton enforces we are re-using the same code in both steps to request different feature subsets. In `featurize_and_split` we execute the following [Driver](https://github.com/dagworks-inc/hamilton/blob/main/hamilton/driver.py) code to select all features. 
 
 
 ```py
 from hamilton import driver
 import data_loader
 import feature_logic
 import normalized_features
 
 dr = driver.Driver({
        ""location"": Config.RAW_FEATURES_LOCATION},
        data_loader, 
        feature_logic, 
        normalized_features)
 
 features_wanted = [
    n.name for n in dr.list_available_variables()
    if n.name not in EXCLUDED_COLS and \
    n.type == pd.Series]
 
 full_featurized_data = dr.execute(features_wanted)
 ```
 
 ![](_developing-scalable-feature-engineering-dags_data/3_img)
 
 <Caption>The full feature set produced by Hamilton using visualize_execution</Caption>
 
 In the `feature_importance_merge` step we select only the top k features based on a feature selection policy applied to the outputs of earlier steps in the DAG that represents the flow. 
 
 
 ![](_developing-scalable-feature-engineering-dags_data/4_img)
 <Caption>10 important features produced by Hamilton using visualize_execution</Caption>",H3,https://outerbounds.com/blog/developing-scalable-feature-engineering-dags#applying-hamilton-dags-in-our-example,False,4556.0,733.0
912,Better together,"We’ve explained the code, now let’s dive into the benefits.
 
 
 Metaflow allows a macro overview of the model building process, Hamilton allows a micro view into a complex part of the model building process – creating features. Both are DAGs and guide users to structuring code bases appropriately.",H4,https://outerbounds.com/blog/developing-scalable-feature-engineering-dags#better-together,False,299.0,50.0
913,**Maintainable and naturally documented feature transformation history**,"Hamilton functions are documentation friendly and always unit testable. Metaflow has cards to visualize contents of a step and share them easily. In our example code, you should look at how readable the metaflow steps are and the Hamilton feature transform functions. Take a look at the [readme](https://github.com/outerbounds/hamilton-metaflow/blob/main/README.md#inspecting-results) for the guiding example to see how you can visualize steps with Metaflow cards.",H5,https://outerbounds.com/blog/developing-scalable-feature-engineering-dags#maintainable-and-naturally-documented-feature-transformation-history,False,464.0,62.0
914,**Modular execution of feature transformation pipelines – only compute what you need to**,Hamilton enables one to easily describe the superset of features and to only compute what is needed at your request. You can see this subset happening in the `feature_importance_merge` of the flow. Metaflow allows you to model different feature needs as different steps.,H5,https://outerbounds.com/blog/developing-scalable-feature-engineering-dags#modular-execution-of-feature-transformation-pipelines-only-compute-what-you-need-to,False,270.0,43.0
915,**Insight into the relationship DAG between feature transformations**,"You can visualize the macro workflow with Metaflow. You can visualize the micro feature transformation workflow with Hamilton. Enabling both allows you to see end-to-end relationships, as well as interactions within a complex step. You can use Metaflow features to debug a failure in a task (steps become tasks at runtime) in the flow. Should your debugging lead you to a featurization step, you can then access the state and peer into the Hamilton DAG, where you can step through individual feature steps.",H5,https://outerbounds.com/blog/developing-scalable-feature-engineering-dags#insight-into-the-relationship-dag-between-feature-transformations,False,506.0,83.0
916,**Built-in versioning of your machine learning workflows including dependency management**,"Metaflow captures the state of your flow runs including dependencies and any metadata you want to track. This enables you to resume flows from failed tasks and helps you share workflows in a more portable, production-ready form. On a more granular level, Hamilton enables feature versioning via Git and Python packaging.",H5,https://outerbounds.com/blog/developing-scalable-feature-engineering-dags#built-in-versioning-of-your-machine-learning-workflows-including-dependency-management,False,320.0,51.0
917,**DAGs to scale steps in your workflow to any compute resources**,"No one likes migrations. When dealing with large datasets and models, an extension to the flow template provided in this example is to have Metaflow steps run on remote machines. To do this, you can add Metaflow’s `@batch` or `@kubernetes` decorator above a step and specify how much processing power and memory the step requires. Alternatively you can send all steps to one of these compute environments by adding `--with=batch` or `--with=kubernetes` in the command line. 
 
 
 Since by default Hamilton expects dataframes it operates on to fit in memory, a convenient pattern for scaling Hamilton is to wrap the Metaflow step containing Hamilton driver execution with an aforementioned decorator for defining compute requirements. Another route is to leverage Hamilton’s ability to scale your dataframe transformations with [Dask](https://github.com/dagworks-inc/hamilton/tree/main/examples/dask), [Ray](https://github.com/dagworks-inc/hamilton/tree/main/examples/ray), or [Pandas on Spark](https://github.com/dagworks-inc/hamilton/tree/main/examples/spark). Then you can scale the RAM required to support a Hamilton feature transformation run by changing the amount of memory requested for the step. [Message us on slack](http://slack.outerbounds.co/) if you have ideas for how you would design a system to scale Hamilton with Metaflow!",H5,https://outerbounds.com/blog/developing-scalable-feature-engineering-dags#dags-to-scale-steps-in-your-workflow-to-any-compute-resources,False,1340.0,179.0
918,Summary,"Given the experimental nature of their work, data scientists often need to prioritize iterating quickly on current tasks. However, in the long run, it can increase the speed of future iterations to focus on workflow structure at the beginning. 
 
 
 “Give me six hours to chop down a tree and I will spend
 the first four sharpening the axe.” – Abraham Lincoln
 
 
 The benefits of structuring code projects well compound over time and across projects. Make sure to pick tools that help you structure projects, iterate on experiments, and move to production without adding headaches.
 
 
 Check out these links to get started developing with [Hamilton](https://github.com/dagworks-inc/hamilton) and [Metaflow](https://github.com/Netflix/metaflow) today!",H3,https://outerbounds.com/blog/developing-scalable-feature-engineering-dags#summary,False,753.0,114.0
919,Large language models and the ML infrastructure stack,"As we have seen happen in the past with computers, the internet, or mobile apps, the real revolution happens when the new technology matures from being [an expensive](https://en.wikipedia.org/wiki/Edison%27s_Phonograph_Doll) [party trick](https://en.wikipedia.org/wiki/Aeolipile), [controlled by a few](https://en.wikipedia.org/wiki/The_Mother_of_All_Demos), to something that is easily accessible by countless companies and individuals. Learning from these past adoption curves, we can anticipate what is ahead of us.
 
 
 Firstly, there’s no reason to believe that LLMs and other foundation models would change the way companies adopt technology. As with any other technical component, the adoption of LLMs requires rounds of evaluation and experimentation. Workflows using LLMs need to be developed and integrated into the surrounding systems and they need to be constantly improved, adjusted, monitored, and controlled by human beings. As before, some companies are ready to embark on this journey earlier than others.
 
 
 Secondly, beyond proofs of concept, new technology doesn’t give a free pass to provide unpredictable or undesirable user experiences. LLMs will become a new tool in the toolbox, enabling companies to deliver delightful and differentiated experiences, which some companies will learn to leverage better than others. In particular, companies in regulated industries need to treat LLMs as carefully as any other part of their tech stack, as regulators will surely have their spotlight on AI.
 
 
 From this point of view, LLMs are models amongst other models, a layer amongst other layers, in the ML infrastructure stack:
 
 
 ![](_llm-infrastructure-stack_data/dolly-stack1.png)
 In [our original illustration](https://netflixtechblog.com/open-sourcing-metaflow-a-human-centric-framework-for-data-science-fa72e04a5d9) of the stack above, the data scientist on the left mostly focused on the concerns closely related to the modeling, and gradually less on the lower-level infrastructure, while infrastructure providers, such as we at [Outerbounds](https://outerbounds.com/platform/) or internal platform teams, have an inverse focus. Lately, LLMs have made everyone’s focus more extreme, as depicted below:
 
 
 ![](_llm-infrastructure-stack_data/dolly-stack2.png)
 Most of the discussion has revolved around the models themselves and [the](https://www.midjourney.com/) [exciting](https://chat.openai.com/) [demos](https://bard.google.com/) they have enabled. Or, on the infrastructure side, the focus has been on the [massive-scale](https://pile.eleuther.ai/) [datasets](https://www.together.xyz/blog/redpajama) or the [immense amount of compute power](https://www.cerebras.net/blog/cerebras-gpt-a-family-of-open-compute-efficient-large-language-models/) that it takes to train the models. The operational concerns in the middle have received less attention. We believe that is about to change, as companies start seriously considering how to use these models in practice.",H2,https://outerbounds.com/blog/llm-infrastructure-stack#large-language-models-and-the-ml-infrastructure-stack,False,2998.0,374.0
920,ML projects in 2025 and beyond,"We believe that LLMs and other foundation models will get adopted as a powerful new model family in the ML stack – augmenting, not replacing it. In some cases, especially when text and images are involved, the new techniques will enable whole new use cases. In other cases, they will be used to improve existing ML models and data science projects, as depicted below.
 
 
 ![](_llm-infrastructure-stack_data/dollydiag.png)
 Depending on the use case, companies choose to own some models, like a fine-tuned LLM or an embedding-producing LLM based on a common foundation model in the illustration. In other cases, it makes sense to use a vendor-owned model through an API. In all cases, companies want to carefully consider what parts of the system they want to own and control within their governance boundary, and what functionality can be safely outsourced to a 3rd party vendor.
 
 
 All these systems will be engineered by humans by combining data, code, and models. Correspondingly, all the fundamental concerns addressed by the ML infrastructure stack remain intact: The systems need easy access to *data* and *compute*. They are structured as *workflows*, built iteratively through multiple *versions*. The results are *deployed* in various ways, leveraging *models* that are best suited for each job.
 
 
 Today, some believe that LLMs and other foundation models will require a fundamentally different approach compared to other ML models. Maybe the new models are too large, complex, and expensive to be handled by most companies. Or maybe a universal model can be provided as a service, alleviating the need for companies to deal with their own. 
 
 
 We believe that there are two strong tailwinds acting as counterforces:
 
 
 1. On the engineering side, the collective force of academia, vendors, and non-profits will produce [permissively-licensed, optimized foundation models](https://stability.ai/blog/stability-ai-launches-the-first-of-its-stablelm-suite-of-language-models), datasets, and libraries, which will make training and fine-tuning of domain-specific models [radically cheaper and hence attainable](https://til.simonwillison.net/llms/llama-7b-m2) to reasonable-scale companies. In addition, techniques like [model distillation](https://towardsdatascience.com/knowledge-distillation-simplified-dd4973dbc764) make it possible to reduce the size and cost of production models radically.
 2. On the modeling side, many companies will realize that they can – and they must – treat AI as a part of their usual ML initiatives. They want to retain control over their data, product experience, and proprietary IP as before, keeping [their custom models inside their governance boundary](https://maithraraghu.com/blog/2023/does-one-model-rule-them-all/).
 
 
 As a result, more companies are able and willing to innovate independently. It is likely that we will see smaller models [built by companies with deep domain expertise](https://www.bloomberg.com/company/press/bloomberggpt-50-billion-parameter-llm-tuned-finance/) outperform generic models built without domain expertise. The new AI techniques will become a competitive advantage to companies that learn to integrate them seamlessly into their existing products and workflows.",H2,https://outerbounds.com/blog/llm-infrastructure-stack#ml-projects-in-2025-and-beyond,False,3252.0,450.0
921,Next steps,"An important implication of the new AI techniques becoming a part, not a replacement, of the ML infrastructure stack is that you can continue working on modernizing your data and ML stacks with future-proof tools like open-source [Metaflow](https://outerbounds.com/metaflow). Realistically, a vast majority of companies are still early on their ML journey, so it makes sense to start by laying a solid foundation.
 
 
 All work towards this goal remains relevant and will pay dividends once you are ready to start adopting the latest AI techniques. If you want a solid foundation with minimal engineering effort, [we can get you started quickly with the managed Outerbounds Platform](https://outerbounds.com/platform) which provides an easy-to-use, optimized, and secure environment for all ML/AI experiments and production applications.
 
 
 To learn more about Metaflow and foundation models, take a look at our newest article on the topic: [Training a large language model with Metaflow, featuring Dolly](/blog/train-dolly-metaflow/) which demonstrates how to fine-tune a state-of-the-art LLM, Databricks’ Dolly, using Metaflow. You can also read our previous articles about [image generation using Stable Diffusion](/blog/parallelizing-stable-diffusion-production-use-cases/) and [text-to-speech translation using Whisper](/blog/whisper-kubernetes/).
 
 
 
 PS. To learn more about LLMs in the context of business use cases, [join our next fireside chat](https://www.eventbrite.com/e/large-language-models-beyond-proofs-of-concept-tickets-617461492947) on May 4th!
 
 <a href=""https://www.eventbrite.com/e/large-language-models-beyond-proofs-of-concept-tickets-617461492947"">
 
 ![](_train-dolly-metaflow_data/5_img)
 
 </a>",H2,https://outerbounds.com/blog/llm-infrastructure-stack#next-steps,False,1728.0,212.0
922,Robustness is not the enemy of velocity,"We’re driven by the belief that infrastructure must make developers and data scientists happier and more productive, and it can do so while improving reliability, debuggability, reproducibility, and security.
 
 How can this be possible? The answer: thoughtful design executed well. For instance, software must do the right thing by default. At Chainguard, that means secure by default: security measures must be integrated into every phase of the software development lifecycle, from dependencies to development to production. At Outerbounds, this means that the simplest way to write machine learning workflows that seamlessly scale from your laptop to large fleets of GPUs and back, allowing rapid iteration between prototype and production. In both cases, this allows data scientists, software engineers, and MLEs to focus on the top layers of the stack, while having easy and robust access to the infrastructural layers:
 
 ![](/assets/stack-triangles.png)
 
 Because we share this philosophy, together we have started to explore what it would mean to secure the machine learning supply chain without sacrificing velocity: operations teams and scientists can get along if they have the right tools.",H2,https://outerbounds.com/blog/secure-ml-secure-software-dependencies#robustness-is-not-the-enemy-of-velocity,False,1203.0,181.0
923,What are we seeing today?,"The correctness and security of a machine learning model depends on the training data itself, secure delivery of the training data, software dependencies (e.g. TensorFlow), the model code, the training environment, build steps to package up the trained model, and the deployment of that model to production. Together, these form the _machine learning supply chain_.
 
 Chainguard has [previously argued](https://www.chainguard.dev/unchained/securing-the-machine-learning-supply-chain) that there’s no difference in principle between a software supply chain and a machine learning supply chain when it comes to security—and the difference in practice is that the ML world is much less mature. Remember how software development looked 20 years ago, with no automated testing, code review, or even version control at many shops? Data science workflows are at a similar evolutionary phase.
 
 ‍This isn’t the fault of data scientists, who need to use cutting-edge techniques (and consequently, bleeding-edge, barely-tested libraries, which [may have malware](https://pytorch.org/blog/compromised-nightly-dependency/)) and for whom rapid experimentation is table stakes. Rather, it reflects the early stages of tooling and infrastructure supporting these workflows.
 
 Below, we describe a few desirable properties for ML workflows, and contrast standard practice with a gold standard workflow, with stories of the consequences.",H2,https://outerbounds.com/blog/secure-ml-secure-software-dependencies#what-are-we-seeing-today,False,1423.0,194.0
924,Reproducibility,,H2,https://outerbounds.com/blog/secure-ml-secure-software-dependencies#reproducibility,False,0.0,1.0
925,Data sources and formats,"The data used as input to an ML workflow can introduce vulnerabilities and run attacker code just like software dependencies (especially in the [insecure](https://huggingface.co/docs/hub/security-pickle) `pickle format`). Unlike attacker code in a dependency, it tends not to have obvious giveaways, like `eval()` functions run on long base64-encoded blobs.
 
 **Typical**. A data scientist uses `curl` to fetch a dataset from some strange corner of the web; they record where in a comment. The training code loads data from `/Users/alice/data-v2-final-actually-final.pickle`. They save their trained model (again in `.pickle` format) and attach it to their repository using Git LFS.
 
 When the dataset gets taken down, nobody is able to change the model ever again. And when the domain hosting the dataset expires, an attacker registers it and puts a `.pickle` file that looks for credit card numbers from the machine and sends them to a hostile server.
 
 **Better**. Our scientist retrieves data from known sources (who have granted access), and copies it to their organization’s servers before using it. The training code fetches the data in an organization-wide way by some canonical name. The input data is a CSV, but the resulting model is pickled.
 
 Others are able to work on the model, but weird inconsistencies arise when it turns out that there are two different versions of `housing-data-2017.csv`. When an advanced attacker compromises the training infrastructure, they add a backdoor into the resulting `.pickle` model and start exfiltrating data from production.
 
 **Best**. Data comes from known sources with accompanying [data bills of materials](https://github.com/spdx/meetings#ai-and-data-profiles-group-meetings), indicating the source (and a description) of the data, along with a cryptographic hash of the data itself and a signature from the party that produced the data set. Nothing is ever pickled—the data lives in formats like AVRO and Parquet.",H2,https://outerbounds.com/blog/secure-ml-secure-software-dependencies#data-sources-and-formats,False,1976.0,296.0
926,Data and machine learning pipeline security,"Training a model is a direct analog to building a software artifact. Efforts like [SLSA](https://slsa.dev/) stress the importance of moving builds off of developer laptops to locked-down build machines which are secured just as well as production systems. This is doubly important for ML systems, where the “source” (data) might contain sensitive customer information that could cause catastrophe if leaked.
 
 **Typical**. Data scientists copy a production database containing personally identifiable information (PII) to their laptop in order to train their models.
 
 One employee uses this as an opportunity to stalk their ex. After clicking a link in an email, one data scientist accidentally installs some malware which exfiltrates all of these users’ credit card numbers to a cybercrime gang’s servers. When another data scientist travels through a foreign airport, the security agent confiscates it and clones the hard drive; the foreign government now knows the health status of millions of people.
 
 **Better**. Models are trained on a secure training platform. Data scientists don’t have direct access to any of this data: the credentials live on this platform.
 
 User data is much safer: nobody can just flip through it. A new deployed model never has a regression because somebody had an old version of PyTorch.
 
 The scientists grumble because their job just got much more tedious. Without any access to the training process, they accidentally deploy a model with terrible performance because it was trained against the wrong dataset. When a state-sponsored attacker compromises the Jenkins box, which hasn’t received a security update in 5 years, they’re able to backdoor the output model in a [Solarwinds-style](https://www.techtarget.com/whatis/feature/SolarWinds-hack-explained-Everything-you-need-to-know) attack; this attack persists for months.
 
 **Best**. Training happens in a hosted, production-caliber environment like [Outerbounds Platform](https://outerbounds.com/platform/). Only reviewed workloads can see the data.",H2,https://outerbounds.com/blog/secure-ml-secure-software-dependencies#data-and-machine-learning-pipeline-security,False,2048.0,295.0
927,What should we do about it?,"While the picture painted above looks a little bleak, our prognosis is positive. Just as the maturity of software development skyrocketed over the past 20 years, the maturity of developing and deploying ML systems will soon take off. Efforts from governments, open source communities, and for-profit companies are currently articulating risks and developing best practices and tooling to support these practices. For instance, [NIST’s AI Risk Management Framework](https://www.nist.gov/itl/ai-risk-management-framework) (RMF) provides organization-level guidance for how to invest to mitigate these risks. The OpenSSF’s [proposed AI/ML working group](https://github.com/ossf/tac/issues/175) aims to develop standards and best practices with a focus on ML supply chains that include open source (pretty much all of them). The [Sigstore](https://docs.sigstore.dev/) project enables signing both traditional software and ML data, source, and models, and [Chainguard Enforce](https://www.chainguard.dev/chainguard-enforce) can guarantee that production workloads have all required signatures. Outerbounds developed an open-source tool, [Metaflow](https://outerbounds.com/metaflow/), that’s easier than doing things the “typical” way but supports the best practices highlighted above. And using Chainguard’s [Wolfi](https://github.com/wolfi-dev) as a base operating system for running notebooks, training workflows, or deployed models minimizes the attack surface.",H2,https://outerbounds.com/blog/secure-ml-secure-software-dependencies#what-should-we-do-about-it,False,1459.0,177.0
928,Learn more,"If you’d like to learn more, attend the upcoming [Supply Chain Security in Machine Learning Fireside Chat](https://www.eventbrite.com/e/supply-chain-security-in-machine-learning-tickets-680422601317?aff=ebdsoporgprofile) on Thursday, August 24 at 12pm EST, featuring Chainguard’s Research Scientist Zack Newman and Outerbounds’ Head of Developer Relations Hugo Bowne-Anderson.
 
 <a href=""https://www.eventbrite.com/e/supply-chain-security-in-machine-learning-tickets-680422601317""> 
 
 ![](/assets/chainguard-fs-chat.png)
 
 </a>",H2,https://outerbounds.com/blog/secure-ml-secure-software-dependencies#learn-more,False,530.0,46.0
929,ML pipelines and the MLOps workflow,"A common pattern in ML is
 
 
 ![A classic pipeline pattern in ML.](_machine-learning-pipelines-from-prototype-to-production_data/0_img)
 In this pattern, you need to ingest the data to train your model on, do the appropriate feature engineering, followed by a training stage where you apply specific machine learning algorithms to create a model artifact that can be used for the desired prediction task. After this, you’ll need to deploy it so predictions can be made. You’ll also need to monitor it for performance, data/model drift, and outliers, as well as explain predictions to interested stakeholders.
 
 
 In this post, we’ll show you how to build out the full ML pipeline using Metaflow, a project that focuses on the first three steps, and Seldon, which concerns itself with the deployment stages of the last three.: 
 
 
 ![A classic pipeline pattern in ML, with Metaflow and Seldon.](_machine-learning-pipelines-from-prototype-to-production_data/1_img)",H3,https://outerbounds.com/blog/machine-learning-pipelines-from-prototype-to-production#ml-pipelines-and-the-mlops-workflow,False,965.0,143.0
930,Local deployment vs cloud deployment,"Data scientists, for the most part and for good reason, prefer to explore, iterate, and prototype their data, models, and pipelines locally. But at some point in time, **they’ll need to move to a cloud-based development and testing environment**. This could be for any number of reasons, whether they’re required to use large data lakes for training data or the need to train models at scale (with many GPUs and/or massively parallelizing a hyperparameter sweep, for example.
 
 
 **They will also need to deploy their model remotely.** This also could be for any number of reasons, including the need for a remote, scalable endpoint with no downtime for end-users of the need to test their models at scale.
 
 
 So how do we meet this challenge of allowing both modes of work, local prototyping and remote deployment, with seamless transitions between the two, while having the most ergonomic abstraction layers so that data scientists do not need to get involved in the lower levels of infrastructure?
 
 
 ![The MLOps stack and what data scientists care about.](_machine-learning-pipelines-from-prototype-to-production_data/2_img)",H3,https://outerbounds.com/blog/machine-learning-pipelines-from-prototype-to-production#local-deployment-vs-cloud-deployment,False,1133.0,179.0
931,Reproducible ML pipelines with Metaflow,"To help data scientists focus on the parts of the stack they really care about, such as feature engineering and model development, and abstracting over the aspects that they care about less, such as organizing, orchestrating and scheduling jobs, interacting with the data warehouses, and so on, we’re excited to show you Metaflow, an open source framework allowing data scientists to build production-ready machine learning workflows using a simple Python API, originally developed at Netflix for this specific challenge. As you’ll see, Metaflow not only provides an out-of-the-box way to build ML pipelines, and track and version the results, ensuring every run is reproducible, but it also allows you to move seamlessly between your local prototyping environment and cloud-based deployments.",H3,https://outerbounds.com/blog/machine-learning-pipelines-from-prototype-to-production#reproducible-ml-pipelines-with-metaflow,False,793.0,120.0
932,Open source and ergonomic model deployment with Seldon,"The second moving part that we need is a framework that allows any set of machine learning models to be deployed, scaled, updated, and monitored. The OSS Seldon ecosystem of tools provides flexible functionality for these sets of tasks (moreover, it can be included in a Metaflow flow!):
 
 
 ![The Seldon project ecosystem.](_machine-learning-pipelines-from-prototype-to-production_data/3_img)
 In particular, [Seldon Tempo](https://tempo.readthedocs.io/en/latest/) is an open source Python SDK that allows data scientists to describe inference components in Python, test locally on Docker and deploy to Kubernetes without needing to have any Kubernetes knowledge or interact with YAML and Kubernetes command line tools. Check out the types of constructs, such an example Tempo Python model artifact, that users can create: 
 
 
 
 ```
 sklearn_model = Model(
   name=""test-iris-sklearn"",
   platform=ModelFramework.SKLearn,
   local_folder=sklearn_local_path,
   uri=sklearn_url,
   description=""An SKLearn Iris classification model"",
 )
 ```
 
 
 Data scientists can define simple Model classes like this or via decorators and can also combine components with any custom Python code and library, allowing custom business logic for inference. Given such models or pipelines, Tempo can deploy your models locally or on the cloud and you can then run predictions against them.",H3,https://outerbounds.com/blog/machine-learning-pipelines-from-prototype-to-production#open-source-and-ergonomic-model-deployment-with-seldon,False,1376.0,198.0
933,Production ML made easier,"Let’s now consider a typical example of an ML pipeline and show how we can use Metaflow and Seldon together to make production ML easier for working data scientists. We’ll be using the Iris dataset and building two ML models, one from scikit-learn, the other from xgboost. Our steps will be to:
 
 
 * Define Tempo inference artifacts that
 	+ Call these two models with custom business logic and
 	+ Deploy the models.
 * Use Metaflow to run these steps as shown below.
 
 
 ![What Metaflow does here and what Seldon does.](_machine-learning-pipelines-from-prototype-to-production_data/4_img)
 Note that Metaflow will save all model and data artifacts locally for you. You can run the flow with:
 
 
 
 ```
 python src/irisflow.py  --environment=conda  run
 ```
 
 
 
 Once this has completed, you’ll find three Docker containers. This is where Metaflow comes into its own, allowing us to retrieve model artifacts and use them to make predictions:
 
 
 ![Making predictions with Metaflow Temp artifact.](_machine-learning-pipelines-from-prototype-to-production_data/6_img)",H3,https://outerbounds.com/blog/machine-learning-pipelines-from-prototype-to-production#production-ml-made-easier,False,1073.0,164.0
934,Moving to cloud deployment,"What we want to now do is be able to move between our local workstation and the cloud with as little friction as possible. Once you have the correct infrastructure set up (see details below), the same ML pipeline can be deployed on AWS with the following:
 
 
 
 ```
 python src/irisflow.py --environment=conda --with batch:image=seldonio/seldon-core-s2i-python37-ubi8:1.10.0-dev run
 ```
 
 Note that this is pretty close to the code you wrote to run your pipeline locally:
 
 
 
 ```
 python src/irisflow.py  --environment=conda  run
 ```
 
 
 Once this has completed, you’ll find Seldon Deployments running in your cluster:
 
 
 
 ```
 kubectl get sdep -n production
 
 NAME            AGE
 classifier      2m58s
 test-iris-sklearn   2m58s
 test-iris-xgboost   2m58s
 ```
 
 You can also use your artifact to make predictions, exactly as before!
 
 
 ![Making predictions with Metaflow Temp artifact on the cloud.](_machine-learning-pipelines-from-prototype-to-production_data/6_img)
 On the infrastructural side, to do this, you need to install [Metaflow on AWS](https://docs.metaflow.org/metaflow-on-aws/metaflow-on-aws) and have either a GKE or EKS Kubernetes cluster available with Seldon Core installed. Running remotely will allow us to execute on AWS Batch with Metaflow and scale out our training as needed. Similarly, in the above, we have deployed our Tempo artifacts with Seldon Core onto a Kubernetes cluster which allowed production scale out, monitoring and management. This is what the cloud/remote setup looks like:
 
 
 ![Metaflow and Seldon working together with AWS and K8s.](_machine-learning-pipelines-from-prototype-to-production_data/7_img)",H3,https://outerbounds.com/blog/machine-learning-pipelines-from-prototype-to-production#moving-to-cloud-deployment,False,1666.0,254.0
935,Wrapping up,"A big challenge in the MLOps and machine learning deployment space is figuring out tools, abstraction layers, and workflows that allow data scientists to move from prototype to production. By combining Metaflow and Tempo, ML practitioners have a powerful combination to develop end-to-end machine learning pipelines with the ability to switch between local and remote development and deployment, as needed. On top of this, both projects provide auditing and reproducibility. If you’d like to learn more, check out [this fully-worked example here.](https://tempo.readthedocs.io/en/latest/examples/metaflow/README.html)
 
 
 You can also learn more by joining [Metaflow on slack here](http://slack.outerbounds.co/) and [Seldon on slack here](https://join.slack.com/t/seldondev/shared_invite/zt-vejg6ttd-ksZiQs3O_HOtPQsen_labg).",H3,https://outerbounds.com/blog/machine-learning-pipelines-from-prototype-to-production#wrapping-up,False,825.0,100.0
936,On the need for MLOps,"*The key feature of ML-powered software is that it is directly exposed to real-world data and the complexity and messiness inherent in it.*
 
 
 All machine-learning projects and data-intensive applications are software projects, often consisting of repositories of Python code under the hood and involving the operation of containers and operational dashboards in production, not dissimilar to most other software services. So why don’t we treat machine learning-powered software as business-as-usual software engineering projects, educating ML practitioners about standard best practices?
 
 
 To answer this question, let’s tease apart the key differences by first considering the job of a non-ML engineer and then jumping into ML-powered applications:
 
 
 * First, note that traditional software engineering deals with well-defined and narrowly-scoped inputs, and the engineer can cleanly and comprehensively model these in the code. In practice, the software engineer defines the world and frame in which the software operates.
 * In stark contrast to this, perhaps the most important feature of ML-powered and data-intensive software applications is that they are, by definition, directly exposed to real-world data and the complexity and messiness inherent in it. Here, you are just not able to cleverly architect your code by hand and are forced to work with less transparent and data-driven optimization processes. The trade-off here is that you are no longer able to completely control and understand the system *but* you can build software that would have been impossible, even five years ago.
 
 
 ![Inputs and outputs for ML applications and traditional applications.](_mlops-vs-devops_data/0_img)",H3,https://outerbounds.com/blog/mlops-vs-devops#on-the-need-for-mlops,False,1711.0,254.0
937,From data-centric programming to production ML pipelines,"*This quality makes ML-powered software inherently different from traditional applications.*
 
 
 Our hypothesis is that it is this quality that makes ML-powered software inherently different from traditional applications. Although this is relatively straightforward to state, it has wide-ranging implications with respect to who is best positioned to develop such software and how they should be doing so:
 
 
 1. **ML applications are directly exposed to the constantly changing real world through data**, in contrast to traditional software, which operates in a simplified, static, abstract world directly constructed by the engineer.
 2. **ML apps have to be developed through cycles of experimentation:** as a result of the constant exposure to data, we cannot learn the behavior of ML apps through logical reasoning but through empirical observation. In contrast to traditional software, we can’t predict the behavior of even the simplest prototypes without testing the ideas in practice.
 3. **The background and the skillset of people building the applications are realigned:** although it is still effective to express software in code, the emphasis shifts to data and experimentation, far closer to empirical science than traditional software engineering.
 
 
 Although this is a paradigm shift from mainstream software engineering, note that it is by no means a new approach. This is science, my friend! More than this, we have a long tradition of what is becoming known as *data-centric programming*: researchers, scientists, and developers who have been using data-centric IDEs, such as RStudio, Matlab, Jupyter Notebooks, or even Excel to model complex real-world phenomena, should find this paradigm familiar. In fact, this rich history and practice go back at least as far as [Mathematica notebooks](https://www.wolfram.com/notebooks/)! As has become apparent, however, such tools are wonderful for exploration, experimentation, and prototyping but can be found to lack across several dimensions when it comes to working with models in production.",H3,https://outerbounds.com/blog/mlops-vs-devops#from-data-centric-programming-to-production-ml-pipelines,False,2063.0,307.0
938,Machine learning software needs to be production-ready,"*It must adhere to the same set of standards as traditional software.*
 
 
 As ML-powered applications become more commonplace and progress from R&D to front-line production, however, you also need to deploy them in conjunction with more traditional forms of software. Ideally, your data-centric software can be written bottom up so that it can be incorporated into production workflows both ergonomically and seamlessly. To be clear, the entire industry is currently rethinking the now-antiquated approach which sees data scientists using isolated environments to prototype models that need to be reimplemented for production because (1) it is too slow and (2) it blocks iterative development as production deployments are a one-way street.
 
 
 This raises a key point: *we need to make ML software production-ready from the get-go*and, to achieve this, *it must adhere to the same set of standards as traditional software*, leading to another set of requirements:
 
 
 1. **The scale of operations** is often significantly larger than in the data-centric, exploration-driven notebook-style environments. Data is often larger, but so are models, particularly deep learning models.
 2. **ML applications must be carefully orchestrated**: with the significant rise in the complexity of apps, which often require dozens of interconnected steps, developers need better paradigms for software development and deployment, such as first-class DAGs.
 3. **We need robust versioning for data, models, code, along with the internal state of applications** — think Git on steroids to answer inevitable questions: Why did something break? What changed? Who did what and when? How do two iterations compare?
 4. **The applications must be integrated into the surrounding business systems** so that you can test and validate ideas in a controlled manner in the real world.
 
 
 Looking at these two lists, notice that they describe the convergence of two macro trends, (1) the practice of data-centric programming and the value it delivers, along with (2) the requirements of large-scale, production software for businesses. Each of these alone is inadequate for the challenge at hand: you wouldn’t want to build modern ML-powered software in Excel, nor would it be much fun acting as though ML applications can be built with standard software toolchains.
 
 
  
 
 
 ![](_mlops-vs-devops_data/1_img)",H3,https://outerbounds.com/blog/mlops-vs-devops#machine-learning-software-needs-to-be-production-ready,False,2388.0,369.0
939,Wrapping up,"These are the aspects of software engineering and the stack that need to shift when moving to a world populated with ML-powered software. In future posts, we’ll begin to answer the remaining foundational questions:
 
 
 1. **What** does a modern technology stack for streamlined ML processes look like?
 2. **How** can you start applying the stack in practice today?
 
 
 Thanks for reading! If you’d like to get in touch or discuss such issues, [come say hi on our community Slack](http://slack.outerbounds.co/)! 👋",H3,https://outerbounds.com/blog/mlops-vs-devops#wrapping-up,False,515.0,84.0
940,The Solution,"We have always treated the [documentation](https://docs.metaflow.org/) for Metaflow as a top priority. This is reflected in recent efforts, which include a new [book](https://www.manning.com/books/effective-data-science-infrastructure), and the site [outerbounds.com](https://outerbounds.com/), containing additional examples, how-to guides and tutorials. When we started to [scale up our efforts](https://outerbounds.com/blog/metaflow-resources-data-science-engineering), we knew we needed better tools. We wanted:
 
 1. A modern, customizable Content Management System (CMS) that allows us to publish a wide range of content: code examples, prose and blog articles.
 2. The ability to test and refresh outputs of code snippets automatically.
 3. An authoring experience that doesn’t involve copy and pasting code, with as close to a [WYSIWYG](https://en.wikipedia.org/wiki/WYSIWYG) experience as possible. Good authoring tools are important in both encouraging documentation but also in allowing the process to scale by removing friction.",H2,https://outerbounds.com/blog/technical-docs-with-docusaurus-and-notebooks#the-solution,False,1040.0,127.0
941,Static Site Generator + Design,"For our front-end-framework, we chose to customize [Docusaurus](https://docusaurus.io/).  Docusaurus is a modern, beautiful static site generator developed at Facebook specifically for technical documentation and blog posts.  We liked Docusaurus for its extensibility and that we could use use custom React components.  When customizing our site, we drew inspiration from both [Stripe](https://stripe.com/docs) and [spaCy](https://spacy.io/): We loved the ability to view both text / code side-by-side, which maximizes vertical real estate and minimizes context switching.  Below is a screenshot of of how we implemented this _(you can view the live page [here](https://outerbounds.com/docs/specify-conda-channels))_:
 
 ![](_technical_documentation_data/paradigm-shift-documentation-01.png)
 
 As you can see, we prioritized having a minimalistic design where code is present on the right-hand side without distraction, with the options of adding additional callouts and line highlighting in code. 
 
 Furthermore, [Docusaurus uses MDX](https://docusaurus.io/docs/markdown-features/react), which is more flexible than plain Markdown. Thanks to MDX, we were able to implement the two-pane design shown above and give the author fine grained control on the placement.  Docusaurus has provided an amazing amount of flexibility while at the same time including features specific to technical documentation such as code highlighting and magic comments, as well as being very fast compared to other static site generators.",H3,https://outerbounds.com/blog/technical-docs-with-docusaurus-and-notebooks#static-site-generator-design,False,1517.0,204.0
942,Authoring,"The prevailing medium through which code and prose are co-located in traditional document systems are Markdown files. This presents the challenge of testing, refreshing and authoring code examples since Markdown is a fixed, static environment. Thankfully, there already exists a medium that naturally supports writing prose and code together in a live environment where code can also be run and tested for many scenarios: Jupyter Notebooks. However, notebooks do not offer facilities for documentation out of the box. While there are several projects such as [JupyterBook](https://jupyterbook.org/en/stable/intro.html), [Sphinx](https://nbsphinx.readthedocs.io/en/0.8.9/) and [Quarto](https://quarto.org/) that offer publishing from notebooks, we needed the following additional capabilities:
 
 
 * Ability to add custom pre and post-processing directives to code cells to control how code and outputs are displayed.
 * Ability to write tests in-situ and configure their visibility and characteristics.
 * Facilities for rendering api docs interspersed with code examples or prose that can be automatically inferred from docstrings.
 * Freedom to use any static site generator
 
 To accomplish these goals, we extended [nbdev](https://nbdev.fast.ai/), a documentation-first python development framework by [fast.ai](https://github.com/fastai/). Concretely, we extended this framework to allow for documenting existing code bases that were written outside nbdev. We also are intimately familiar with this framework as we employ [one of nbdev’s core contributors](https://github.com/hamelsmu)!  To get an idea of how authoring in notebooks works, below is a whirlwind tour.
 
 In this example, we can see that we write prose + code in a notebook, which gets immediately rendered in the docs:
 
 ```mdx-code-block
 import LiteYouTubeEmbed from 'react-lite-youtube-embed';
 import 'react-lite-youtube-embed/dist/LiteYouTubeEmbed.css';
 
 <div className=""video-container"">
 <LiteYouTubeEmbed
         id=""VuEYENFfiOQ""
         title=""Basic Authoring""
 />
 </div>
 <br />
 ````
 
 Since Metaflow is designed primarily as a command line tool, we make sure you can render python scripts in the docs like this:
 
 ```mdx-code-block
 <div className=""video-container"">
 <LiteYouTubeEmbed
         id=""3brlbNrksWA""
         title=""Python Scripts""
 />
 </div>
 <br />
 ````  
 
 We can toggle visibility of cell inputs and outputs:
 
 ```mdx-code-block
 <div className=""video-container"">
 <LiteYouTubeEmbed
         id=""espyD47Ny58""
         title=""Cell Visibility""
 />
 </div>
 <br />
 ````
 
 We can even create interactive plots:
 
 ```mdx-code-block
 <div className=""video-container"">
 <LiteYouTubeEmbed
         id=""cheKxuQhQts""
         title=""Interactive Plots""
 />
 </div>
 <br />
 ```
 
 Finally, we make testing our documentation easy. We embed tests directly into notebooks, right next to the code snippets or examples. When then programmatically run all of the code and the tests (which can be configured for fine grained rules around execution):
 
 ```mdx-code-block
 <div className=""video-container"">
 <LiteYouTubeEmbed
         id=""OFRVfjWKnGU""
         title=""Testing Docs""
 />
 </div>
 ```",H2,https://outerbounds.com/blog/technical-docs-with-docusaurus-and-notebooks#authoring,False,3194.0,484.0
943,API Documentation,"In addition to code snippets for tutorials and how-to guides, we wanted to to provide API documentation. Instead of writing this API documentation from scratch, we wanted to render it from existing docstrings, but still maintain the flexibility to interleave code snippets and prose where necessary.  We were able to use nbdev and docusaurus to introspect and render docstrings for various objects.  For example, here the [API docs for the Metaflow client](https://docs.metaflow.org/api/client) are created from [this notebook](https://github.com/Netflix/metaflow-docs/blob/master/docs/api/client.ipynb).",H3,https://outerbounds.com/blog/technical-docs-with-docusaurus-and-notebooks#api-documentation,False,604.0,79.0
944,Content Management,"A key aspect of documentation is collaboration. We wanted to enable people with different skill sets such as SEO specialists, marketers and copy editors to  participate in content authoring. A popular tool for authoring and analyzing content that is popular among non developers is [Wordpress](https://wordpress.com/). Wordpress offers features for SEO analysis as well as a review and editing workflow that is approachable for many. 
 
 In order to preserve the ability to author content in wordpress, we created [wp2md](https://outerbounds.github.io/wp2md/), a simple CLI tool that allows you to export blog articles from Wordpress into Docusaurus-compliant Markdown, along with assets like embedded images and videos.  Below is an example of how [wp2md](https://outerbounds.github.io/wp2md/) works:
 
 ![](_technical_documentation_data/wp2md.png)
 
 
 Another piece of functionality we wanted to mirror that Wordpress offered is SEO analysis with [Yoast](https://yoast.com/). Yoast analyzes your content and helps to flag issues in your site such as:
 
 * Missing metadata such as open graph tags or authors
 * Titles, descriptions and content with inappropriate length
 * Duplicate entities such as  titles, slugs, etc
 * Broken links
 
 Since many of these checks are rule based, we created [mdseo](https://github.com/outerbounds/mdseo/tree/master/), a CLI tool that analyzes Markdown content for these same rules.  The upshot of having a CLI tool rather than a GUI interface is that we can run these checks in CI alongside our other tests to spot issues.  Furthermore, we were able to add customized flags to allow users to optionally ignore certain rules per article.  
 
 Here is a screenshot of mdseo running in GitHub Actions letting us know that a specific article has a description that is less than 50 characters long:
 
 ![](_technical_documentation_data/mdseo_action.png)",H2,https://outerbounds.com/blog/technical-docs-with-docusaurus-and-notebooks#content-management,False,1886.0,282.0
945,Feedback,"This authoring flow has been central to allowing us to author high documentation quickly.  We’ve open sourced, [nbdoc](https://github.com/outerbounds/nbdoc-docusaurus), a reference example of how we use nbdev and Docusaurus. Besides us, a few other companies have adopted the framework successfully:
 
 
 **[David Berg](https://www.linkedin.com/in/david-j-berg/), Senior Software Engineer at Netflix**
 
 _Prior to using nbdev, documentation was the most cumbersome aspect of our software development process… Using nbdev allows us to spend more time creating rich prose around the many code snippets guaranteeing the whole experience is robust. nbdev has turned what was once a chore into a natural extension of the notebook-based testing we were already doing._
 
 **[Roxanna Pourzand](https://www.linkedin.com/in/roxanna-pourzand-445a3035/), Product Manager at [Transform](https://transform.co/)**
 
 _We’re so excited about using nbdev. Our product is technical so our resulting documentation includes a lot of code-based examples. Before nbdev, we had no way of maintaining our code examples and ensuring that it was up-to-date for both command inputs and outputs. It was all manual. [Now, we] have this under control in a sustainable way. Since we’ve deployed these docs, we also had a situation where we were able to identify a bug in one of our interfaces, which we found by seeing the error that was output in the documentation!_
 
 It should be noted that most users may want to follow the development of [nbdev](https://github.com/fastai/nbdev) rather than trying to use nbdoc, as many of these features (documenting existing codebases, custom directives, freedom to use any static site generators, etc.) have been upstreamed into [nbdev](https://github.com/fastai/nbdev).",H3,https://outerbounds.com/blog/technical-docs-with-docusaurus-and-notebooks#feedback,False,1783.0,253.0
946,Try This at Home,"Most of the features described in this article have been upstreamed into [nbdev](https://nbdev.fast.ai/), a community maintained project from [fastai](https://github.com/fastai).  Furthermore, [wp2md](https://outerbounds.github.io/wp2md/) and [mdseo](https://outerbounds.github.io/mdseo/) are open source projects.  We recommend following nbdev for those interested in a community maintained general authoring tool documentation, blogs, and writing python packages.",H2,https://outerbounds.com/blog/technical-docs-with-docusaurus-and-notebooks#try-this-at-home,False,465.0,49.0
947,Get In Touch,"If your company needs well-documented, open-source infrastructure for data science and machine learning [join our Slack for support and feedback](http://slack.outerbounds.co) and check out [Metaflow](https://github.com/netflix/metaflow).",H2,https://outerbounds.com/blog/technical-docs-with-docusaurus-and-notebooks#get-in-touch,False,237.0,24.0
948,Introduction,"Recent advances in large language models (LLMs) have fundamentally transformed how consumers and developers perceive machine-learning-driven products, making it possible to leverage natural language as a user interface to various systems in novel ways.
 People are actively incorporating LLM-driven tools and applications, such as OpenAI's [ChatGPT](https://chat.openai.com/), Microsoft's [Bing](https://www.bing.com/?/ai), and Google's [Bard](https://bard.google.com/), in their professional and personal lives; meanwhile, organizations are making [substantial](https://techcrunch.com/2023/06/12/salesforce-pledges-to-invest-500m-in-generative-ai-startups) [investments](https://www.gartner.com/en/newsroom/press-releases/2023-05-03-gartner-poll-finds-45-percent-of-executives-say-chatgpt-has-prompted-an-increase-in-ai-investment) and betting on the continued expansion of the [generative AI ecosystem](https://www.forbes.com/sites/petercohan/2023/05/30/generative-ai-7-trillion-ecosystem-invest-in-nvidia-microsoft-adobe-and-more).
 
 In this post, we delve into a practical exploration of LLM concepts and their applications, driven by a demonstration of a powerful new fine-tuning method called **instruction tuning**. We describe how data scientists can use instruction tuning to ensure language models adhere to specific instructions and generate appropriate responses for the desired use case. Next, we briefly introduce LLMs and the [LLaMA](https://arxiv.org/pdf/2302.13971v1.pdf) family of models and then motivate instruction tuning.",H2,https://outerbounds.com/blog/custom-llm-tuning#introduction,False,1544.0,145.0
949,Large language models,"From a statistical perspective, the recently trending language models (LM) have a structure that estimates the probability distribution of sequences of tokens, where tokens can refer to single characters, subwords, or whole words.
 
 In essence, LMs are predictors for the subsequent token in a sequence. To achieve this, most LMs undergo an extremely resource-intensive training process, in which they learn to predict the next token by packaging extensive text collections such as books, Wikipedia articles, or even the entire Internet as training datasets. 
 
 The ""largeness"" of a language model refers to its [parameter](https://machine-learning.paperspace.com/wiki/weights-and-biases) count and the scale of its training corpus. Typically, the term “large language model” is reserved for models with billions of parameters and internet-scale training datasets.
 
 ![](./_llm-instruction-tuning/llm-io.png)
 <Caption> A language model is trained to predict the next word of a sentence, meaning that at “inference time” we can use it to generate the next tokens for questions and tasks, optimizing to get the next sensible tokens. </Caption>
 
 The language modeling process, at scale, enables the model to mimic human linguistic behavior, answer questions, and classify text into different categories (e.g., classify if a news article is about sports or politics). 
 
 Increasingly, recent LLMs such as Codex, PaLM, ChatGPT, and GPT-4  even appear to demonstrate emergent capabilities (see [Wei et al., 2022](https://arxiv.org/abs/2206.07682) and [Suzgun et al., 2022](https://arxiv.org/abs/2210.09261)) such as medical and legal knowledge (see [Nori et al., 2023](https://www.microsoft.com/en-us/research/publication/capabilities-of-gpt-4-on-medical-challenge-problems/) and [Katz et al., 2023](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4389233)) and [human-level reasoning capacity on professional benchmark exams](https://arstechnica.com/information-technology/2023/03/openai-announces-gpt-4-its-next-generation-ai-language-model/).
 
 Language models have shown surprising abilities at different scales, so it is crucial to be mindful that while the first large language models we used a couple of years ago had millions of parameters, modern models now have (sometimes hundreds of) billions of parameters. Thus far, the bigger the models are, the better they appear at solving complex tasks that require emergent behavior.",H3,https://outerbounds.com/blog/custom-llm-tuning#large-language-models,False,2442.0,315.0
950,A rise in open-source LLMs,"While language models have a long history that spans many decades, only recently, thanks to the availability of large computing and large datasets, along with hardware developments (see  [Hooker, 2020](https://arxiv.org/abs/2009.06489)), have we been able to train them at billion parameter scales on large amounts of data. Much of the capacity to source these datasets and to access the compute required to train state-of-the-art LLMs is limited to organizations with large budgets for experimental AI projects. Thus, commercial entities such as OpenAI, ChatGPT, Google, and Microsoft have driven the recent hype in the language modeling space.
 
 Indeed, the ChatGPT product and the GPT-3/GPT-4 models that power it may be the [fastest-growing product in history](https://time.com/6253615/chatgpt-fastest-growing/). A core observation of the reasoning behind this post is that adapting what these commercial models do with instruction tuning workflows has proven a critical success factor in using these models in products; for example, in the wave of chatbot and personal assistant products that have emerged over the last year. 
 
 Meanwhile, there has been a notable surge in open-source LLM development of late, with advancements occurring within hours and public code repositories swiftly attracting thousands of enthusiastic developers on GitHub. **With the increased speed of such improvements, practitioners and companies can increasingly experiment with owning more of their LLM stack.**",H4,https://outerbounds.com/blog/custom-llm-tuning#a-rise-in-open-source-llms,False,1498.0,218.0
951,What is instruction tuning?,"Instruction tuning is a specific kind of fine-tuning that has been introduced in different ways in different papers (see [Ouyang et al., 2022](https://arxiv.org/abs/2203.02155) but also [Sanhm, Webson, Raffel, and Bach et al., 2022](https://arxiv.org/pdf/2110.08207.pdf) and [Wei et al., 2022](https://arxiv.org/pdf/2109.01652.pdf)). The idea, at least in the [modern adaptation](https://crfm.stanford.edu/2023/03/13/alpaca.html), is simple: we fine-tune the models on a new language modeling task, with examples that have an additional structure built into how the model prompts.
 
 The instruction-tuned model receives pairs of inputs and outputs that describe tasks that guide the model. For example,
 
 > **Instruction**: Write a list of fun things to do on weekends
 
 > **Output**: Hikes, spend a day at the park, picnics, movie night
 
 Behind the scenes, we ask the model to learn to do language modeling on the entire string of text “Instruction: Write a list of fun things to do on weekends Output: Hikes, spend a day at the park, picnics, movie night.” **This change to the structure of data samples the model sees is the main methodological difference between generic single-task fine-tuning (see [BERT](https://arxiv.org/pdf/1810.04805.pdf)) and instruction tuning.**
 
 Instructions can be any text, such as writing emails, editing sentences, etc. Since we can make very general and diverse instructions, we expect the model to generalize well on many instruction-driven tasks.
 
 Training in this way, when compared with purely computing a loss function on the next token prediction of entire documents, **models learn to follow the pattern of instructions and outputs and become much more valuable in answering questions like those humans are often interested in.** In other words, instruction training improves the quality of the answers given by language models by training the model to respond in a format aligned with how humans tend to give and receive instructions.
 
 ![](./_llm-instruction-tuning/tuning-llm.gif)
 <Caption> The process that happens while fine-tuning a large language model using instructions; the model receives input text that describes both the instruction and the output. At test time, we will use the same format to generate an output from the model. </Caption>
 
 While instruction tuning was popularized by and has been in the works at Google, Meta, and OpenAI for a while, we have had a revolution in the field in the last couple of months. Meta AI released the [LLaMA models](https://ai.facebook.com/blog/large-language-model-llama-meta-ai/), a first set of high-quality and open-source large language models of different sizes. After LLaMA, the Stanford NLP group released [Alpaca](https://crfm.stanford.edu/2023/03/13/alpaca.html): an instruction-tuned model that starts from LLaMA and uses [instructions generated by GPT-3](https://raw.githubusercontent.com/tatsu-lab/stanford_alpaca/main/alpaca_data.json) as a dataset. These are some examples of instructions used to train Alpaca:
 
 | Instruction | Output | 
 | ----------- | ------ |
 | Give three tips for staying healthy. | 1. Eat a balanced and nutritious diet: Make sure your meals are inclusive of a variety of fruits and vegetables, lean protein, whole grains, and healthy fats. This helps to provide your body with the essential nutrients to function at its best and can help prevent chronic diseases. <br></br><br></br> 2. Engage in regular physical activity: Exercise is crucial for maintaining strong bones, muscles, and cardiovascular health. Aim for at least 150 minutes of moderate aerobic exercise or 75 minutes of vigorous exercise each week. <br></br><br></br> 3. Get enough sleep: Getting enough quality sleep is crucial for physical and mental well-being. It helps to regulate mood, improve cognitive function, and supports healthy growth and immune function. Aim for 7-9 hours of sleep each night. |
 | Give two examples of a liquid. | Here are two examples of a liquid:  <br></br><br></br> 1. Water: Water is a clear, colorless and odorless liquid that is vital for all forms of life. It exists in lakes, rivers, oceans, and even underground. <br></br><br></br> 2. Oil: Oil is a viscous liquid that is derived from petroleum. It is typically used as a fuel or lubricant in various industries, such as transportation and manufacturing. | 
 
 Many derivative models have been released in response to LLaMA and Alpaca. Even if [smaller models cannot directly compare with large scale models](https://arxiv.org/pdf/2305.15717.pdf) like ChatGPT/GPT, the quality of their output makes them very useful for many applications.",H3,https://outerbounds.com/blog/custom-llm-tuning#what-is-instruction-tuning,False,4644.0,687.0
952,Why instruction tuning?,"Language models produce fascinating results, but getting meaningful output from an LLM can be challenging. A prompt completion may initially make sense, then quickly diverge from a natural response, not following the intended guide and veering further off track with each prediction.
 
 It is the nature of [autoregressive models](https://en.wikipedia.org/wiki/Autoregressive_model) - which next-token predicting LLMs are - that what the model sees as the next best token to predict can be seen by an observer as an error that propagates and is exacerbated each time the previous model output becomes the next time step’s input.",H3,https://outerbounds.com/blog/custom-llm-tuning#why-instruction-tuning,False,628.0,95.0
953,Is instruction tuning right for my use case?,"If you want to use an LLM to power a product experience, you will likely need an approach to the challenge raised in the previous section. There are ongoing conversations around whether these specific, task-driven LLM use cases require fine-tuning (as demonstrated in our follow-up [post](/blog/llm-tuning-metaflow)) or if skillful prompt engineering of foundation LLMs like OpenAI’s GPT family is sufficient for guiding model responses.",H4,https://outerbounds.com/blog/custom-llm-tuning#is-instruction-tuning-right-for-my-use-case,False,437.0,64.0
954,When to prompt,"When starting a new project, consider using GPT-4 - or any model already trained to respond to instructions - to solve the task(s). **Starting with an external API will help you get going quickly and see whether out-of-the-box performance is good enough for your use case.** Prompt engineering is the process of designing prompts that guide an LLM to solve a particular language task. A key consideration is that the original training datasets for LLMs come from a general sampling of text on the internet. So if you expect the model’s training set to cover your instruction-like use cases well, then fine-tuning may not necessarily add many benefits to how the model performs. In this case, clever prompt engineering can take you very far and reduce your operational burden significantly. 
 
 However, this might not always be the case, as a foundation model’s training dataset might not cover the desired instruction-driven data likely to appear in your task domain, or API latency and costs may block your desired use patterns. You may also want to use proprietary data in fine-tuning workflows that you don’t want to expose to external APIs. In these cases, fine-tuning may be a better approach.
 
 For more on the prompt engineering decision space, Chip Huyen recently provided an [overview](https://huyenchip.com/2023/04/11/llm-engineering.html) of LLMs in production with a focus on the continuum between prompt engineering and fine-tuning.",H4,https://outerbounds.com/blog/custom-llm-tuning#when-to-prompt,False,1447.0,230.0
955,When to apply instruction tuning,"Increasingly many companies have proprietary data and desire LLM use cases that require the model to perform better on tasks than off-the-shelf foundation models. Moreover, despite prompt engineering efforts with the foundation model, you may want to own the fine-tuning process and possibly even the complete training lifecycle. Importantly, instruction tuning is one path to giving developers greater control over LLM behavior and makes it possible to **build uniquely functional and differentiated product experiences in a world where many users are increasingly familiar with generic ChatGPT-level results.**
 
 While tuning requires a higher initial investment, it can lower operational costs by reducing inference costs around expensive API calls and making operations more predictable by removing a dependency on a 3rd party vendor.
 
 To summarize, instruction tuning is fine-tuning with a particular training dataset containing examples that prepend context to inputs the model sees about the task we want the LLM to perform as it predicts token sequences. Thus, working with instruction-tuning workflows is similar to general LLM workflows, and we can generalize learnings.
 
 The choice between instruction-tuning and prompt engineering doesn’t have to be either-or. You can use prompting to additively improve the performance and constrain the output space of your instruction-tuned large language model on the fly.",H4,https://outerbounds.com/blog/custom-llm-tuning#when-to-apply-instruction-tuning,False,1427.0,210.0
956,Considerations for applying LLMs,"Despite the fun demos that generative AI has saturated the internet with, working with LLMs in an organization with an established product line and brand is a serious design and engineering endeavor, particularly if you want to move beyond making generic, external API calls. This section highlights three categories of issues we are paying attention to and desire to explore with our customers.",H2,https://outerbounds.com/blog/custom-llm-tuning#considerations-for-applying-llms,False,395.0,63.0
957,Hardware access,"If you only want to run inference or are just experimenting with models, open-source tools that help you do this with limited hardware are emerging rapidly. For example, the [GPT4All](https://gpt4all.io/index.html) project allows you to prompt some billion-parameter LLMs on your laptop, drastically reducing access barriers to learning how to work with these models in your programs.
 
 However, this post and our [subsequent hands-on post](/blog/llm-tuning-metaflow)
 focus on workflows where you can own the infrastructure stack and observe the contents of datasets, model weights, and code. In this realm, GPUs are typically a hard requirement. Moreover, despite advances in [reduced precision](https://github.com/TimDettmers/bitsandbytes) and [parameter-efficient](https://github.com/huggingface/peft) training, efficiently operating LLM workflows will most likely require on-device GPU memory beyond what all but the [latest](https://www.nvidia.com/en-us/data-center/a100/) [GPUs](https://www.nvidia.com/en-us/data-center/h100/) afford. 
 
 **Organizations with ambitions to use LLMs in this way must stay aware of GPU market conditions and have an ongoing GPU accessibility plan.** Without a plan, you will likely encounter moments that negatively affect a data scientist’s productivity.
 
 Consider the recent all-too-common scenario where a data scientist’s workflow depends on accessing a cloud service provider’s GPUs, and their workflow is blocked because some other customer of the cloud service provider decided to consume all the VM instances with the latest GPUs on them. What do you do in this situation? Shifting workflows to another resource provider can take days or more. It may also require slow processes where DevOps and business leaders need to get involved in approving new infrastructure and cost drivers while data scientists do other work or remain idle on the project.",H3,https://outerbounds.com/blog/custom-llm-tuning#hardware-access,False,1898.0,260.0
958,Dedicated GPU clouds,"At Outerbounds, we encounter this problem too and recently have been using a mixture of GPU providers since generative AI has [upended the market](https://www.fierceelectronics.com/electronics/ask-nvidia-ceo-gtc-gpu-shortage-looming). In addition to [all the major clouds, AWS, Azure, and GCP that Metaflow supports](/blog/metaflow-on-all-major-clouds/), we have been exploring dedicated GPU clouds such as [CoreWeave](http://www.coreweave.com). 
 
 A benefit of CoreWeave is that they let you compose granular resource settings, such as memory and disk space, on each GPU instance you ask for. CoreWeave’s supply of state-of-the-art machines, and a large selection of various GPU types, multiplies the benefits of granular resource selection on VM instances, allowing you to access GPUs with 80GB of on-device memory. Access to the latest hardware advances can drastically simplify the required software stack, as huge models can fit in 80GB of on-device memory, obviating the need for extremely complex distributed training approaches. 
 
 For example, when working with LLMs, you may want to select only one or two accelerators at a time and attach a lot of extra RAM and disk space to this instance to deal with huge datasets and in-memory requirements of model parameters. CoreWeave makes it easy for us to customize such resources and saves us money by letting us make granular choices about the hardware our workflows need, compared to reserving either zero or [eight A100s at a time](https://aws.amazon.com/ec2/instance-types/p4/) and letting most of them go unused. In our [example](https://github.com/outerbounds/metaflow-instruction-tuning/tree/main), we found the training to work well up to 33B parameters on an instance with two A100 processors, and also on an instance with two A6000 processors.",H4,https://outerbounds.com/blog/custom-llm-tuning#dedicated-gpu-clouds,False,1810.0,260.0
959,Learning to operate LLMs to power real-world products,"Much of the applied content on the internet about LLMs centers on flashy but brittle demos that cannot be used in production contexts as such. These demos also tend to rely on external APIs, and companies with established brands realize they cannot risk outsourcing a core part of their product’s supply chain to such external services that can change their behavior on a whim. 
 
 Also, you don’t only adopt an LLM once. Like other technical product features, they should be subject to constant maintenance and improvement as the world around them evolves. **LLMs may be new and shiny in the spring of 2023, but LLMs adopted today will turn into table stakes or, worse, legacy technical debt in a few years.**
 
 In a [previous post](/blog/llm-infrastructure-stack/), we went into detail about how we expect LLMs will affect these issues and the future of the ML infrastructure stack more broadly.
 
 ![](_llm-infrastructure-stack_data/dolly-stack2.png)
 
  A core observation is that although the recent discussion has much focused on the models themselves and the data to train them, operationally custom LLMs require similar infrastructure and tooling as any other ML model: You need to be able to feed them with data from your databases, run them at scale, orchestrate them as a part of larger systems, and track, version, and experiment with them confidently. And, importantly, make the whole production-grade toolchain easily accessible to your ML engineers so that they can iterate quickly.",H3,https://outerbounds.com/blog/custom-llm-tuning#learning-to-operate-llms-to-power-real-world-products,False,1498.0,242.0
960,Ethical issues,"Besides engineering challenges, there are complex ethical considerations when deploying large language models in production. There is a lot of concern among AI researchers and the general public around issues that might arise from LLMs and their usage. The models, such as many mentioned in this post, have known biases ([Abid et al., 2021](https://www.nature.com/articles/s42256-021-00359-2)), and their use can be harmful in practice if you are not careful to constrain their input and output spaces. These issues are common to all the generative AI landscape, including text-to-image generation models such as DallE and Stable Diffusion ([Bianchi et al., 2023](https://arxiv.org/abs/2211.03759)). We hope to cover this deep topic in detail in future posts and conversations.",H3,https://outerbounds.com/blog/custom-llm-tuning#ethical-issues,False,777.0,112.0
961,Next steps,"In this post, we covered an introduction to instruction tuning. In [the following article](/blog/llm-tuning-metaflow), we will describe a small experiment we ran using HuggingFace and Metaflow. Join us on [Slack](http://slack.outerbounds.co/) about your experiences and goals in working with LLMs, and stay tuned to the blog for more posts like this one.",H2,https://outerbounds.com/blog/custom-llm-tuning#next-steps,False,354.0,51.0
962,Acknowledgments,"We would like to thank Mert Yuksekgonul, Mirac Suzgun, Patrick John Chia, Silvia Terragni, Giuseppe Attanasio, and Jacopo Tagliabue for feedback and suggestions on a previous version of this blog post.",H2,https://outerbounds.com/blog/custom-llm-tuning#acknowledgments,False,201.0,31.0
963,Single sources of truth for data and OSS tool chains vs proprietary platforms?,"> _If you go with the open source route, you are always going to have more choices at your disposal, less lock-in, compared to a vertical proprietary stack… such as bleeding edge generative AI tools._
 
 ```mdx-code-block
 <div className=""video-container"">
 <LiteYouTubeEmbed
         id=""-U_qG2NHOeY""
         title=""Single sources of truth for data and OSS toolchains vs proprietary platforms?""
 />
 </div>
 ```
 <br></br>
 
 **HBA**: How do you think about having a single source of truth for all your data needs?
 
 
 **JR**: Historically that hasn’t really been technically feasible, besides for large companies like Netflix and Apple, which could invest a lot of engineering effort into building these single source of truth systems. As open-source projects mature and commercial products emerge around them, that architecture becomes available to the vast majority of companies. It is a very powerful setup ultimately.
 
 Netflix open-sourced Iceberg as they didn't want to be on the hook internally for continuing to keep all the integrations like Spark and Trino up to date or integrate to other tools like Druid. It was a much better thing to share with the community, and then get the whole community involved in building new connectivity and maintaining. It does take a village to keep everything going in the right direction.
 
 **HBA**: Why and when is a tool chain of open-source tools a more appropriate choice for your data stack than a single proprietary platform?
 
 
 **JR**: Historically, the choice was a bit more straightforward. You were either fully in a proprietary platform like Oracle or Teradata - they owned all of your compute and all of your storage, and you could do only what those systems allowed you to do.
 
 Or, you could choose open-source, make it a choose-your-own adventure, and stitch technologies together yourself, which needed a lot of engineering resources. It was expensive, but you gained a lot of power, flexibility, and long-term cost mechanics because you weren't tied into the expensive part from licensing fees.
 
 As an example, Netflix was a big Teradata shop early on. It was incredibly expensive and could really only do SQL analytics, but we wanted to do more. We moved this to OpenStack which was a painful engineering process.
 
 
 **HBA**: How has the open-source landscape changed since?
 
 
 **JR**: Now we're in this world where we have more, even proprietary systems, that have more optionality and more integration, so you can take something like Snowflake, and that actually has support for Iceberg storage tables.
 
 In today’s hybrid world, we can use a combination of open-source and proprietary solutions, so there’s more optionality. If you go with the open source route, you are always going to have more choices at your disposal, less lock-in, compared to a vertical proprietary stack. And the question then becomes is that optionality that you gain worth it for?
 
 On the one hand, you can have an easy-to-use integrated platform where everything works relatively well together, but you're limited in your choices. And you're tied to their cost model. Or, you can go with an open route, have multiple tools and optionality and more control over costs, but with the challenges that come through integrating multiple tools. There are trade-offs to be had for sure.
 
 You can make a choice today but what about next year when the next new tool comes along? How quickly can you integrate that? And are you going to fall behind competitors, because they're able to integrate a new technology? For instance, they start using generative AI tools, but you can't, because you're still waiting for your vendor to provide something in that space.
 
 I think that optionality is also about speed and agility and time to market. Companies like Tabular and Outerbounds built on open-source tooling are making this option more lucrative.",H2,https://outerbounds.com/blog/oss-modern-data-stack-recap#single-sources-of-truth-for-data-and-oss-tool-chains-vs-proprietary-platforms,False,3900.0,655.0
964,"Data pipelines, security, and regulatory requirements","> _It goes back to being able to have a really good lineage and understand how these data products are built, how they're connected, the connectivity across all these things._
 
 ```mdx-code-block
 <div className=""video-container"">
 <LiteYouTubeEmbed
         id=""lbMW4mmRfis""
         title=""Data pipelines, security, and regulatory requirements (LLMs and extraction attacks!)""
 />
 </div>
 ```
 <br></br>
 
 **HBA**: How can you make sure that your data and pipelines meet security and regulatory requirements?
 
 **JR**: This is a bit of a mindset shift architecturally. In a world where your compute and your storage were bundled together, you could secure that. That became really messy when we started collecting data in these open formats.
 
 How do you effectively do security, especially when you've got multiple different ways to access data? And we're in this architecture, in which we all want a single source of truth and read it from multiple different tools. How do we successfully lock that down or even know who has access to what? How do we audit it? Who's accessing the data, like really difficult stuff? 
 
 Now we move to these distributed data architectures when you start to move the security layer down to the physical storage and make those compute layers basically come with some sort of authentication and authorization to the store to say like, Hey, I'm Jason, here's my authorization that says, I should have read access to this data, can you please let me write, and no matter if I'm coming from a Python process, or I'm coming from a Spark job, or I'm coming from a Trino query, that same exchange has to happen. 
 
 So I think that's where we're going to move to if we're going to be successful in this architecture. And it's definitely a challenge, and it's definitely a shift that would get us to at least security, and then we can audit it? And all that security is happening at that storage layer, we can go back again, a nice clean audit. Then there are the secondary concerns… What about GDPR? And those kinds of things when customers say, Hey, forget that you have data on me. And that's yet another suite of capabilities that we need to build out in this architecture. We're still in the early stages of doing that kind of stuff.",H2,https://outerbounds.com/blog/oss-modern-data-stack-recap#data-pipelines-security-and-regulatory-requirements,False,2269.0,402.0
965,"Model governance, LLMs, and extraction attacks!","**HBA**: How do we even think about this when we have data but also feature stores and metric stores and all of these different types of things?
 
 **JR**: I think those are great questions. I think this is where the regulator is always going to be behind the technology. What if you take all of my search history, and you turn that into some embedding, and use it in a model, and then I tell you, hey, you need to forget about me?
 
 Okay, you can drop the records but you use all that data to build your model that is up for grabs in these large language models that have been built on, you know, a corpus of texts that have been what did people give permission for that to be used in the model? This is uncharted territory! 
 
 Again, it goes back to being able to have a really good lineage and understand how these data products are built, how they're connected, the connectivity across all these things.
 
 **HBA**: I'm glad you mentioned the existence of models that are trained on data as well. There's a whole fascinating area of research on extraction attacks of taking models and being able to extract training data from the models, right?
 
 
 **JR**: It is, and it's a new vector of attack. As good architects, we should try and anticipate these future requirements. And technical architecture is a lot about having two-way doors and optionality, right? Don't design yourself into corners that you can't get out of, or there will be big migration efforts, you know, big tech debt holes that you've created for yourself or your company down the line because you didn't think about possible outcomes.",H2,https://outerbounds.com/blog/oss-modern-data-stack-recap#model-governance-llms-and-extraction-attacks,False,1611.0,293.0
966,"Iceberg and Metaflow for principled, robust, and reproducible data science and ML","> _As a data scientist, again, I want to focus on building models or doing data science, I don't want to have to be an expert on Parquet or ORC file formats, or dealing with partitions manually._
 
 ```mdx-code-block
 <div className=""video-container"">
 <LiteYouTubeEmbed
         id=""_2zmZ6L7gDs""
         title=""Iceberg and Metaflow for principled, robust, and reproducible data science and ML""
 />
 </div>
 ```
 <br></br>
 
 **HBA**: Tell me more about why data scientists would want to use Apache Iceberg for working with data.
 
 
 **JR**: Perfect, this is where I can hopefully make some connections with Metaflow and Outerbounds: If you're a data scientist, why should you care about Iceberg? Let’s say I have these Parquet files on S3. Why do I think Iceberg tables are preferred? Here’s a summary:
 
 1. **Schema**: You get to address a data set as just not a data set, but the schema. You don't have to be concerned about the intricacies of Parquet or ORC or any of these formats, or dealing with partitions manually.
 2. **Persist results**: You can write results back to Iceberg tables, since now you've just made it that much easier for anybody else in the ecosystem to take advantage of that result that you produced. 
 3. **Constantly updating data**: Imagine that data updates constantly. You don’t want to be working with old Parquet files. Let's just talk about tables instead of files. That's a big win for interoperability for yourself and your ecosystem.
 4. **Reproducibility**: This is where Iceberg is really fantastic for data science use cases, because every change that is made to an Iceberg table, results in an immutable snapshot of that data set. As a data scientist, I can say, I'm training this model against these very specific versions of my underlying source data, and I produced this specific version of output.",H2,https://outerbounds.com/blog/oss-modern-data-stack-recap#iceberg-and-metaflow-for-principled-robust-and-reproducible-data-science-and-ml,False,1846.0,320.0
967,Join us for more chats!,"After our fireside chats, we have AMAs with our guests on Outerbounds [community slack](http://slack.outerbounds.co/). A lot tends to happen there so join us if you’re interested in such conversations!
 
 You can join us on [slack](http://slack.outerbounds.co/) for more such conversations and can view the other fireside chats [here](https://www.youtube.com/playlist?list=PLUsOvkBBnJBeL8tsNGMdDKQjjOdU29sME). 
 
 You can also sign up for our next Fireside Chat _Kubernetes for Data Scientists_ [here](https://www.eventbrite.com/e/kubernetes-for-data-scientists-tickets-652605078357?aff=datarecap)!
 <br></br>
 
 <a href=""https://www.eventbrite.com/e/kubernetes-for-data-scientists-tickets-652605078357?aff=datarecap"">
 
 ![](../static/assets/k8s-for-data-scienctists-event-promo.png)
 
 </a>
 <br></br>",H2,https://outerbounds.com/blog/oss-modern-data-stack-recap#join-us-for-more-chats,False,803.0,74.0
968,What is Whisper?,"Whisper is the latest [open-source](https://github.com/openai/whisper) system from OpenAI. It performs near or above state-of-the-art on several audio-to-text and text-to-text learning tasks. The model is a sequence-to-sequence transformer trained to do many speech tasks at once on 680,000 hours of audio data. You can read more about the Whisper model architecture and training process in the [paper](https://cdn.openai.com/papers/whisper.pdf). 
 
 
 Whisper follows recent [foundation models](https://research.ibm.com/blog/what-are-foundation-models) like [BLOOM](https://huggingface.co/docs/transformers/model_doc/bloom), [Stable Diffusion](https://stability.ai/blog/stable-diffusion-public-release), and many other impressive AI models that have been open-sourced in the 2020s. You can read more about using Metaflow to scale Stable Diffusion in our [recent article](https://outerbounds.com/blog/parallelizing-stable-diffusion-production-use-cases/).
 
 | Model | Number of Parameters | Domain | Type | Source | 
 | ----- | -------------------- | ------ | ---- | ------ | 
 | BERT | 110M | general NLP | Transformer | Google | 
 | Stable Diffusion | 890M | text-to-image | Diffusion | Stability AI |
 | **Whisper Large** | **1.5B**  | **speech-to-text** | **Transformer** | **OpenAI** |
 | Imagen | 4.5B | text-to-image | Diffusion | Google |
 | Dall-E 2 | 5B | text-to-image | Diffusion | OpenAI | 
 | Dall-E | 12B | text-to-image | Diffusion | OpenAI |
 | GPT-3 | 175B | general NLP | Transformer | OpenAI |
 | OPT-175B | 175B | general NLP | Transformer | Meta AI | 
 | BLOOM | 176B | general NLP | Transformer | Hugging Face |
 
 One useful aspect of Whisper to leverage is that there are five sizes of the model trained (this is a common characteristic of foundation models). Having small versions available makes it easy to get started working with Whisper in any Python code or terminal. This post will show you how to leverage these properties of Whisper by pairing them with Metaflow. You’ll write one workflow to run the small models locally, use larger versions of the model efficiently by leveraging the cloud, as well as making the whole workflow production-ready through highly available workflow orchestration. Note that Whisper is still an experimental model, and that you should take care to study its [failure modes](https://twitter.com/lunixbochs/status/1574848899897884672) before using Whisper in production systems.",H2,https://outerbounds.com/blog/mlops-whisper-and-metaflow#what-is-whisper,False,2442.0,357.0
969,Getting Started with Whisper,"All code referenced in this post is accessible [here](https://github.com/outerbounds/whisper-metaflow). You should be able to run everything on a laptop with the `tiny` or `base` Whisper model, and potentially the larger model versions depending on your machine. You can install local dependencies using [Miniconda](https://docs.conda.io/en/latest/miniconda.html) and instructions in the repository if you want to run the code as you read this post. If desired, you can also follow instructions in the repository to run steps of the `YouTubeVideoTranscription` flow on cloud resources using your [Metaflow deployment](https://outerbounds.com/docs/engineering-welcome/).   
 
 To get started with Whisper on a laptop or your own infrastructure, you can install the latest package using [pip](https://pypi.org/project/pip/):
 
 ```shell
 pip install git+https://github.com/openai/whisper.git
 ```
 
 If you do not already have it, you will need [ffmpeg](https://ffmpeg.org/) installed on your machine for Whisper to work. See these [instructions](https://github.com/openai/whisper#setup) to install on your machine.
 
 One benefit of using Whisper with Metaflow is that you can use Metaflow to [manage compute environments with dependencies](https://docs.metaflow.org/api/step-decorators/conda) like `ffmpeg` packaged. This saves development time when moving between compute environments, whether they are local or remote.
 
 After installing Whisper you can use it as a command line tool or Python module. For example, this Python snippet shows how to transcribe any `audio.mp3` file using the `tiny` version of Whisper:
 
 ```python
 import whisper
 model = whisper.load_model('tiny')
 result = model.transcribe('audio.mp3')
 print(result['text'])
 ```
 
 Let’s use the [pytube](https://pytube.io/en/latest/) package to extend this code. The `transcribe_video` function will extract the audio of a YouTube video using pytube and then transcribe the speech using Whisper:
 
 ```python
 def transcribe_video(url, output_path = './', filename = 'audio.mp3'):
     import whisper
     from pytube import YouTube
     audio = YouTube(url).streams.get_audio_only()
     audio.download(output_path, filename)
     model = whisper.load_model('tiny')
     result = model.transcribe(output_path + filename)
     return result['text']
 ```
 
 In eight lines of Python code, you have a function to transcribe any YouTube video! You can use the function from a [notebook](https://github.com/outerbounds/whisper-metaflow/blob/main/youtube_video_to_text_demo.ipynb) or Python script to make the magic happen. Let’s transcribe the famous [Charlie bit my finger!](https://www.youtube.com/watch?v=0EqSXDwTq6U) video:
 
 
 ```python
 url = 'https://www.youtube.com/watch?v=0EqSXDwTq6U'
 transcription = transcribe_video(url)
 print(transcription)
 ```
 
 
 > I’m pretty upset. Go! Ha ha! Charlie! Charlie bit me! I don’t see you. I don’t see you. Ha ha! Oh! Ouch! Ouch! Ouch! You’re out! Charlie! Ouch! Charlie! That really hurt! No, Charlie bit me! And that really hurt Charlie. I’m still hurt Charlie.
 > 
 > Whisper tiny interpreting the Charlie bit my finger! video
 
 
 The tiny model achieves decent results quickly, yet the transcription quality is lacking. To improve the model’s performance you can change the `model_type` to one of the bigger [versions of Whisper](https://github.com/openai/whisper#available-models-and-languages).",H2,https://outerbounds.com/blog/mlops-whisper-and-metaflow#getting-started-with-whisper,False,3422.0,460.0
970,Effects of Whisper Model Size,"The CLI and Python module for Whisper make changing between these model sizes one argument. Having various model sizes is relevant to MLOps because it affords easy ways to evaluate and compare the performance and cost of different models. Developers can then more easily experiment with the trade-off between infrastructure costs required to operate bigger models with the benefits of more precise models.
 
 
 In the case of Whisper, there are significant performance improvements as the model size increases. This increases the importance of being able to experiment and study the size and performance trade-off across environments.
 
 
 ![](_mlops-whisper-and-metaflow_data/3_img)
 <Caption>Whisper Performance Improves Across Tasks with Bigger Models – https://cdn.openai.com/papers/whisper.pdf</Caption>
 
 For a specific inference example, compare these two Whisper-generated transcriptions of “[Fly Me to the Moon](https://www.youtube.com/watch?v=ZEcqHA7dbwM)” by Frank Sinatra. 
 
 
 **Running the `tiny` Whisper model with the `YouTubeTranscriptionFlow`**
 
 ```bash
 python youtube_video_transcriber.py run \
     --url 'https://www.youtube.com/watch?v=ZEcqHA7dbwM' \
     --model tiny
 ```
 
 > Let me play among the stars. Let me see what spring is like on. As you put a mask, in other words, hold my hand. In other words, baby kiss me. Fill my heart with song and let me sing forevermore. You are all I long for, all I worship and the dawn. In other words, please be true. In other words, I love you. Fill my heart with song and let me sing forevermore. You are all I long for, all I worship and the dawn. In other words, please be true. In other words, in other words, I love you. You.
 > 
 > Whisper tiny transcribing “Fly Me to the Moon”
 
 **Running the `large` Whisper model with the `YouTubeTranscriptionFlow`**
 
 ```bash
 python youtube_video_transcriber.py run \
     --url 'https://www.youtube.com/watch?v=ZEcqHA7dbwM' \
     --model large
 ```
 
 
 
 > Fly me to the moon, let me play among the stars Let me see what spring is like on Jupiter and Mars In other words, hold my hand In other words, baby, kiss me Fill my heart with song and let me sing forever more You are all I long for, all I worship and adore In other words, please be true In other words, I love you Fill my heart with song and let me sing forever more You are all I long for, all I worship and adore In other words, please be true In other words, in other words, I love you.
 > 
 > Whisper large transcribing “Fly Me to the Moon”",H2,https://outerbounds.com/blog/mlops-whisper-and-metaflow#effects-of-whisper-model-size,False,2524.0,426.0
971,Whisper Operations with Metaflow,"Metaflow and Whisper make it easy to develop workflows and test with smaller, cheaper models locally and then run an experiment or move to production with a larger, better optimized model on whatever hardware it requires. In the case of Whisper, the difference in the number of parameters between the `large` and `tiny` models is about 50-60x. This makes it crucial from a cost-effectiveness point-of-view to iterate on workflows with the `tiny` version and only scale to the `large` version when necessary. These benefits come together to increase the focus of the developer on building expertise around the data, iterating quickly, and designing experiments; while Metaflow ensures they have access to infrastructure and that workflows are:
 
 
 * scalable [horizontally](https://outerbounds.com/docs/cross-validation-in-parallel/) and [vertically](https://outerbounds.com/docs/scale-model-training-and-tuning/) – including to access GPUs in the cloud,
 * able to be [scheduled](https://docs.metaflow.org/production/scheduling-metaflow-flows/introduction) and triggered on production-grade workflow orchestrators like Argo Workflows or AWS Step Functions,
 * [versioned](https://outerbounds.com/docs/versioning-code-and-models/),
 * [reproducible](https://outerbounds.com/docs/reproducible-ml/),
 * easy to [visualize](https://docs.metaflow.org/api/cards),
 * and most importantly can be used in a production-grade, highly-available application.
 
 
 To highlight an application of Whisper, we set up [this repository](https://github.com/outerbounds/whisper-metaflow) to help you transcribe YouTube videos. Depending on your use cases, you can readily adapt the flow code for a transcription and/or translation service in the 96 languages Whisper has learned. 
 
 
 Like all Metaflow code, the workflow is defined as a [directed acyclic graph](https://outerbounds.com/docs/dags-in-data-science/). It consists of the following steps:
 
 
 * `start` processes the Flow parameter values to create a list called `self.pending_transcription_task`. This list is what determines the `foreach` call.
 * `transcribe` runs inference with Whisper for each task in the list created during the `start` step.
 * `postprocess` collects results in a dataframe. It also creates a [word cloud](https://amueller.github.io/word_cloud/) and embeds the text of concatenated transcriptions using [word2vec](https://radimrehurek.com/gensim/models/word2vec.html). This is then visualized in 2-dimensions using [PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html).
 * `end` prints to console how many transcriptions were recorded.
 
 
 ![](_mlops-whisper-and-metaflow_data/og.png)
 <Caption>Visualization of the YouTubeVideoTranscription Flow</Caption>",H2,https://outerbounds.com/blog/mlops-whisper-and-metaflow#whisper-operations-with-metaflow,False,2763.0,322.0
972,Run the Code,"You can use the flow to transcribe one video, each video in a playlist, or a file with a list of video URLs in it. When you pass multiple videos via a playlist URL or file the flow will run each `transcribe` task in parallel.
 
 
 ![](_mlops-whisper-and-metaflow_data/5_img)
 
 
 In an example where GPUs provided us significant speedup, we used this workflow to transcribe a number of [Fireside Chats](https://www.youtube.com/watch?v=7zB6ESFto_U&list=PLUsOvkBBnJBeL8tsNGMdDKQjjOdU29sME&index=6), an interview series with ML experts that is hosted by Outerbounds. We transcribed each episode of the Fireside Chat using the following command to run the `YouTubeVideoTranscription` flow:
 
 ```",H2,https://outerbounds.com/blog/mlops-whisper-and-metaflow#run-the-code,False,692.0,100.0
973,Whisper in the Cloud,"To produce Fireside Chat transcriptions with Whisper we used [Metaflow’s AWS Batch integration](https://docs.metaflow.org/api/step-decorators/batch) to run the tasks on a P3 instance with 16 GB of memory, 1 GPU, and 8 CPUs. Metaflow makes these settings easy to configure through decorators like `@batch` and `@kubernetes`:
 
 ![](_mlops-whisper-and-metaflow_data/6_img.png)
 
 The total time to transcribe all five Fireside chats – totaling about 6½ hours of video – was about 30 minutes, at least an order of magnitude faster than running locally with the large model. 
 
 Also, by using the [resume](https://docs.metaflow.org/metaflow/debugging#how-to-debug-failed-flows) command you can speed up your development cycles by avoiding repeated computation of expensive steps. This is beneficial in many machine learning development contexts, where it is desirable to develop downstream steps of a flow iteratively without rerunning earlier, expensive steps like `transcribe`.   
 
 Another dimension of MLOps that Metaflow helps with is moving data scientists’ workflows from the prototyping environment to a production orchestrator. Metaflow allows you to [schedule](https://docs.metaflow.org/api/flow-decorators/schedule) and trigger flows with orchestrators like [Argo](https://docs.metaflow.org/production/scheduling-metaflow-flows/scheduling-with-argo-workflows) and [AWS Step Functions](https://docs.metaflow.org/production/scheduling-metaflow-flows/scheduling-with-aws-step-functions). Consider a scenario where you want to run a flow like `YouTubeVideoTranscription` on a daily cadence on [Argo](https://argoproj.github.io/argo-workflows/). What is the easiest way to move your Python code to the production environment and ensure it runs when you want? With Metaflow this only requires adding a `@schedule` decorator above the flow.
 
 ![](_mlops-whisper-and-metaflow_data/7_img)
 
 Then to run the flow reliably in production – without human supervision – you can deploy the flow to Argo:
 
 
 ```bash
 python youtube_video_transcriber.py --with retry argo-workflows create
 ```",H2,https://outerbounds.com/blog/mlops-whisper-and-metaflow#whisper-in-the-cloud,False,2089.0,254.0
974,"Store, Visualize, and Access Results with Metaflow","Metaflow stores every value assigned to `self` in a common [data store](https://docs.metaflow.org/internals/technical-overview#datastore), such as S3 when deploying Metaflow on AWS. A shared data store makes results produced in one step accessible from steps that run in different compute environments. This enables Metaflow to automatically track and version variables measured during your flow’s runtime, regardless of the execution environment. Then you can inspect all results and intermediate states, such as transcriptions, easily in a notebook. In our example, these state variables include [Whisper transcriptions](https://github.com/outerbounds/whisper-metaflow/blob/main/youtube_video_transcriber.py#L106). Writing your Metaflow code to leverage `self` enables access to the artifacts in a [notebook](https://github.com/outerbounds/whisper-metaflow/blob/main/youtube_video_to_text_demo.ipynb) or any Python script using the Metaflow Client API. This means we can now look up the transcriptions any time we want, and trust that Metaflow has them versioned in the data store!  
 
 In the `postprocess` step Metaflow also stored the text in a [Metaflow card](https://docs.metaflow.org/metaflow/visualizing-results/effortless-task-inspection-with-default-cards). You can see sample card output for `YouTubeVideoTranscription` [here](https://ui.oleg2.dev.outerbounds.xyz/YouTubeVideoTranscription/185594/postprocess/1006372?direction=asc&group=true&order=startTime&section=17b19a95be84835c21efd32840bf94020bc5e3da). After you run the flow, you can access cards associated with the `postprocess` step like this:
 
 ```bash
 python youtube_video_transcriber.py card view postprocess
 ```
 
 An added benefit of Metaflow for teams is that once the ML stack is shared, data scientists and engineers can easily generate and share notebooks, UI views, and cards with data produced in any tasks.",H2,https://outerbounds.com/blog/mlops-whisper-and-metaflow#store-visualize-and-access-results-with-metaflow,False,1893.0,216.0
975,Conclusion,"This post gave a quick overview of the key features of using Metaflow to power a modern foundation model like Whisper:
 
 
 * It is easy to structure the whole application as a workflow, making the code more robust and readable.
 * You can parametrize the workflow to use different sizes of the model, making it quick to develop the workflow without sacrificing the quality in production.
 * You can leverage parallelism to speed up processing.
 * The workflow can be readily executed in the cloud, including cloud GPU instances.
 * You can deploy the workflow to run in production without human supervision with a single command.
 
 
 If you would like to discuss any related topics in more detail with us, or if you want to get started with your own Metaflow deployment today, please join us in the #ask-metaflow channel on [our community slack](http://slack.outerbounds.co/).",H2,https://outerbounds.com/blog/mlops-whisper-and-metaflow#conclusion,False,878.0,148.0
976,How is Metaflow deployed at MoneyLion?,"![](_how-moneylion-customized-metaflow_data/0_img.png)
 At MoneyLion our AI team operates within two AWS accounts: *Staging* and *Production*. An important aspect of the design not included in the diagram above is that across both environments we allow users read access to data in Production (such as access to a Production Snowflake Warehouse). 
 
 
 With the entire design we wanted to accomplish the following:
 
 
 1. Have the main data store be shared across both environments:
 	1. AWS S3 and PostgreSQL are the main parts that are deployed within the Production account.
 	2. Due to each account restricting access via a VPC, we had to deploy an AWS RDS Proxy in the Staging account (more information can be found [here](https://aws.amazon.com/blogs/database/use-amazon-rds-proxy-to-provide-access-to-rds-databases-across-aws-accounts/)).
 	3. We keep separate AWS DynamoDB instances as it is only used for saving states within an AWS Step Functions run and enabling CrossAccount access to an AWS DynamoDB table was causing some issues for our Metaflow IAM Group.
 	4. This setup has the added benefit of deduplicating data across both environments within a flow. If we went with a data store for each environment then data that is used within a Staging Run and a Production Run would not be deduplicated, thus increasing storage costs (more information about deduplication within the data store can be found [here](https://docs.metaflow.org/internals/technical-overview#datastore)).
 2. Enforce strict control within flows that are run in Production while giving data scientists complete freedom to prototype and run flows within Staging.
 	1. Staging acts as a playground environment in which data scientists can run ad-hoc or prototype their flows quickly, but the moment they need to officially deploy something to Production they would have to deploy it to the Production account.
 	2. Flows in Staging are automatically tagged with a branch of the same name, whereas flows in Production are deployed with the **–production** flag.",H2,https://outerbounds.com/blog/how-moneylion-customized-metaflow#how-is-metaflow-deployed-at-moneylion,False,2044.0,303.0
977,What tools have you built on top of Metaflow and what do they do?,"We’ve built 3 main tools on top of Metaflow:
 
 
 1. **Watcher**, for model monitoring and observability, along with it being an alerting platform;
 2. **Soteria**, which is a CI/CD tool;
 3. **Metaflow Cookiecutter Template**, a templated code generator.",H2,https://outerbounds.com/blog/how-moneylion-customized-metaflow#what-tools-have-you-built-on-top-of-metaflow-and-what-do-they-do,False,255.0,40.0
978,"Tell us about Watcher (your Monitoring, Observability & Alerting Platform)","Metaflow Watcher consists of the following microservices:
 
 
 1. Entropy Kaos System,
 2. Overseer, and
 3. Argus.",H2,https://outerbounds.com/blog/how-moneylion-customized-metaflow#tell-us-about-watcher-your-monitoring-observability-alerting-platform,False,115.0,18.0
979,Entropy Kaos System,"When working on a POC for Metaflow I faced an issue with the AWS Batch Jobs being stuck in a RUNNABLE state due to incorrect resources being set within a flow. 
 
 
 ![](_how-moneylion-customized-metaflow_data/1_img.png)
 In the image above, I have two flows running on a schedule. In the PreprocessingExtractDataFlow, I purposely tried provisioning resources that did not exist within the AWS Batch Compute Environment. This resulted in jobs that were added to the AWS Batch Job Queue that would never start. After about a day or two of running it eventually, the AWS Batch Job Queue was filled with these jobs and caused other jobs from other flows not to run as the AWS Batch Job Queue was still awaiting to run these corrupted jobs. This issue can arise from other reasons as well, AWS has a guide on debugging this issue [here](https://aws.amazon.com/premiumsupport/knowledge-center/batch-job-stuck-runnable-status/). 
 
 
 The solution was actually pretty simple, which was to periodically check if a job is in RUNNABLE for a period greater than a fixed threshold, and cancel it. The image below shows an example of it in action, by canceling a job that was stuck for too long. I am still tuning the threshold to use, but so far 2 hours works well for my team. I will go into the system design within the section on the Metaflow Overseer. 
 
 
 ![](_how-moneylion-customized-metaflow_data/2_img.png)",H3,https://outerbounds.com/blog/how-moneylion-customized-metaflow#entropy-kaos-system,False,1405.0,229.0
980,Overseer,"When running scheduled pipelines it is important to be alerted if there are any pipelines that fail, this is a feature that Metaflow lacks so I created the Metaflow Overseer to solve this. Below is an example of an alert sent to a Slack channel tagging the flow owner about the failure of their flow with important information about the failure (Log Url is a link to the flow Failure on the Metaflow UI).
 
 
 ![](_how-moneylion-customized-metaflow_data/3_img.png)
 Since the Entropy Kaos System and Overseer are connected, the design below showcases how they both work together where an AWS Batch Job was stuck in a RUNNABLE state for too long trigger its cancellation and sending an alert to the Slack Channel. 
 
 
 ![](_how-moneylion-customized-metaflow_data/4_img.png)
 One flaw with this design that I later found out was that if you have a large foreach with many jobs running and some of them fail, it will send an alert for each of those failed jobs causing alert fatigue. I initially chose to trigger the alerting on the Batch Job State Change instead of the Step Functions Execution Status Change due to the Step Functions lacking sufficient information to create the message body. A fix that I am working on is to trigger it on the Step Functions Execution Status Change and then drill down to all the AWS Batch Jobs within the flow and create the message body with the information from the AWS Batch Jobs but I have not completed this as of yet.",H3,https://outerbounds.com/blog/how-moneylion-customized-metaflow#overseer,False,1458.0,251.0
981,Argus,"While managing the infrastructure of a Machine Learning platform, having relevant information about the overall health of the underlying infrastructure via monitoring of the key system and business metrics is essential. Savin Goyal, the CTO of Outerbounds, shared [this repository](https://github.com/aws-samples/aws-batch-runtime-monitoring) that solves this issue by creating AWS CloudWatch Dashboards leveraging information about an AWS Batch Job State Change and AWS ECS Instance (De)registration, which I had to port over to Terraform. Here is an example dashboard that tracks the AWS Batch Job State Changes:
 
 
 ![](_how-moneylion-customized-metaflow_data/5_img.png)
 The repository provides a detailed overview of the architecture [here](https://github.com/aws-samples/aws-batch-runtime-monitoring#architecture-diagram) but I made a simpler version of it below:
 
 
 ![](_how-moneylion-customized-metaflow_data/6_img.png)
 Processing and sending the data from the data sources is accomplished via a Step Functions State Machine, below is an example of a State Machine for one of the dashboards:
 
 
 ![](_how-moneylion-customized-metaflow_data/7_img.png)",H3,https://outerbounds.com/blog/how-moneylion-customized-metaflow#argus,False,1163.0,142.0
982,Tell us about Soteria (your CI/CD tool),"At MoneyLion we have a strict separation of Staging and Production AWS Accounts which is aided by CI/CD systems to automate the process of running and deploying programs into either account. I decided to give our data scientists the ability to have full control over running workflows within a Staging environment but enforce the use of a CI/CD system when pushing workflows to Production. With that in mind, I created Metaflow Soteria which automates each stage in the diagram below once a pull request is made to push a flow to production. Metaflow Soteria is a CLI application that gets run within our CI/CD pipeline.
 
 
 ![](_how-moneylion-customized-metaflow_data/8_img.png)",H2,https://outerbounds.com/blog/how-moneylion-customized-metaflow#tell-us-about-soteria-your-ci-cd-tool,False,680.0,108.0
983,Demo:,"1) When a Pull Request is made, the first part of the pipeline which is the Sanity Checks is run. Here Metaflow Soteria lints the flow and ensures that the latest Staging run of the flow was successful.
 
 
 ![](_how-moneylion-customized-metaflow_data/9_img.png)
 
 2) The Latest Staging Run Successful check fails, after fixing it and ensuring that the latest Staging run was indeed successful we can rerun the sanity checks via a command **run sanity checks,** which now passes**.**
 
 
 ![](_how-moneylion-customized-metaflow_data/10_img.png)
 
 3) The flow is ready to be deployed to AWS Step Functions. Using another command **productionize flow,** Soteria creates the State Machine and triggers it to run.
 
 
 ![](_how-moneylion-customized-metaflow_data/11_img.png)",H3,https://outerbounds.com/blog/how-moneylion-customized-metaflow#demo,False,772.0,112.0
984,Tell us about Metaflow Cookiecutter Template (your templated code generator),"The Metaflow Cookiecutter Template was created with the following goals:
 
 
 1. Make it as easy as possible for a data scientist to create a Metaflow project
 2. Enable consistent and strict naming conventions of flows, projects, and tags",H2,https://outerbounds.com/blog/how-moneylion-customized-metaflow#tell-us-about-metaflow-cookiecutter-template-your-templated-code-generator,False,239.0,40.0
985,Demo:,"1) Initialize the Repository with some project values using [Cruft](https://cruft.github.io/cruft/) (enables updating):
 ![](_how-moneylion-customized-metaflow_data/12_img.png)
   
 a) View produced templated repository:
 ![](_how-moneylion-customized-metaflow_data/13_img.png)
 	
 b) View the templated Flow code:
 ![](_how-moneylion-customized-metaflow_data/14_img.png)
 
 2) Run a flow via a bash script with an easy interface to run a particular command:
 ![](_how-moneylion-customized-metaflow_data/15_img.png)
 
 3) View the flow in the Metaflow UI (Easily trackable Project Name and the flow is tagged):
 ![](_how-moneylion-customized-metaflow_data/16_img.png)",H3,https://outerbounds.com/blog/how-moneylion-customized-metaflow#demo,False,667.0,68.0
986,Join our community,"If you are interested in customizing Metaflow for your environment, join Yudhiesh and over 2000 other data scientists and engineers on our [community slack](http://slack.outerbounds.co/)! Also, to hear experiences from other experts in the field, take a look at [our Fireside Chats](https://www.youtube.com/watch?v=Dr6DsWa6Dhg&list=PLUsOvkBBnJBeL8tsNGMdDKQjjOdU29sME&index=1).
 
 
 If you have already built interesting customizations or infrastructure around Metaflow that you’d like to share, please ping @hugo on our Slack so we can help spread the word!",H2,https://outerbounds.com/blog/how-moneylion-customized-metaflow#join-our-community,False,557.0,72.0
987,Motivation,"Consider a Metaflow flow that needs to access an external resource, say, a database requiring authentication such as a username and password. Cases like this are common.
 
 Thus far, there have been two main ways to handle this:
 
 1. Delegating authentication to the execution environment, e.g. to an IAM user executing the code.
 
 2. Accessing credentials from a file or an environment variable.
 
 The first option can be secure, easy to manage centrally, and hence preferable in many cases. Unfortunately, it is mainly applicable to a handful of services like S3 which work with IAM natively. If you want to connect to 3rd party services like Snowflake, you need another approach.
 
 The second option works with any service but storing secrets in local files is considered bad practice for many good reasons. Locally stored secrets are hard to manage - what happens if the database password changes - and they can leak both easily and inconveniently.
 
 Secret managers, such as [AWS Secrets Manager](https://aws.amazon.com/secrets-manager/), provide a third option that combines the best of the two approaches. They allow arbitrary secrets to be stored and managed centrally. Accessing secrets is controlled through IAM roles that are available through the execution environment. Additionally, secrets are never stored in any environment outside the manager.",H2,https://outerbounds.com/blog/metaflow-secrets#motivation,False,1365.0,216.0
988,The new `@secrets` decorator,"Earlier, there was a small speed bump if you wanted to use a secrets manager: Accessing a secret e.g. using the `boto` library takes 15-20 lines of boilerplate infastructure code which, as a data scientist, you would rather not worry about.
 
 To make it easier to write production-ready code without cognitive overhead, Metaflow now provides a `@secrets` decorator that handles this with one line. Besides being a convenient abstraction, `@secrets` provides a standardized way to access secrets in all projects across environments.
 
 Here is an example that uses the `@secrets` decorator to access a secret named `db-credentials`. The secret contains four key-value pairs that specify everything needed to connect to a Postgres database:
 
 
 ```python
 from metaflow import FlowSpec, step, secrets
 import os
 from psycopg import connect
 
 class DBFlow(FlowSpec):
 
     @secrets(sources=['db-credentials'])
     @step
     def start(self):
         with connect(user=os.environ['DB_USER'],  
                      password=os.environ['DB_PASSWORD'],
                      dbname=os.environ['DB_NAME'],
                      host=os.environ['DB_HOST']) as conn:
 
             with conn.cursor() as cur:
                 cur.execute(""SELECT * FROM data"")
                 print(cur.fetchall())
 
         self.next(self.end)
 
     @step
     def end(self):
         pass
 
 if __name__ == '__main__':
     DBFlow()
 ```
 
 
 Assuming you have `db-credentials` stored in AWS Secrets Manager, you can execute the flow on your workstation:
 ```bash
 python dbflow.py run
 ```
 
 or [run it at scale](https://docs.metaflow.org/scaling/introduction) on `@batch` or `@kubernetes` as usual:
 
 ```bash
 python dbflow.py run --with kubernetes
 ```",H2,https://outerbounds.com/blog/metaflow-secrets#the-new-secrets-decorator,False,1744.0,370.0
989,Keeping many `@secrets`,"Often, data scientists develop and test their code and models using a non-production dataset. The `@secrets` decorator supports this scenario smoothly.
 
 Consider the above code snippet featuring `DBFlow` but with the `@secrets` line removed. If you have a test database deployed locally, you can simply set the environment variables manually without having to use a secrets manager. This is ok, as the local database is not accessible to anyone outside your workstation (and it shouldn't contain sensitive data, in any case):
 
 ```bash
 export DB_HOST=localhost
 export DB_USER=me
 export DB_NAME=testdb
 export DB_PASSWORD=not_a_secret
 
 python dbflow.py run
 ```
 
 Alternatively, your company may have a shared database containing test data. In this case, you can access its credentials in a secret, say, `test-db-credentials`, and run the flow like this:
 
 ```bash
 python dbflow.py
    –with 'secrets:sources=[“test-db-credentials”]'
    run
 ```
 
 As usual in Metaflow, the `--with` option attaches the decorator to all steps without having to hardcode it
 in the code.
 
 [To deploy the flow in production](https://docs.metaflow.org/production/introduction), you can have a CI/CD pipeline with a separate IAM role that has exclusive access to production credentials. It can deploy the flow to production like this:
 
 ```bash
 python dbflow.py
   –with 'secrets:sources=[""prod-db-credentials”]'
   argo-workflows create
 ```
 
 In this scenario, the IAM roles assigned to data scientists may disallow access to the `prod-db-credentials` altogether. The production credentials and the database are only accessible to [production tasks running on Argo Workflows](https://docs.metaflow.org/production/scheduling-metaflow-flows/scheduling-with-argo-workflows). 
 
 Crucially, in all these cases you don't have to change anything in the code as you moved from prototype to production.",H2,https://outerbounds.com/blog/metaflow-secrets#keeping-many-secrets,False,1892.0,268.0
990,`@secrets` to success,"You can start using the `@secrets` decorator today! For additional features and setup instructions, [read the documentation for `@secrets`](https://docs.metaflow.org/scaling/secrets).
 
 If you need help getting started or if you have any other questions, join us and thousands of other data scientists and engineers on [the Metaflow community Slack](http://slack.outerbounds.co)! In particular, we would like to hear from you if you would like to see support for other backends besides AWS Secrets Manager.",H2,https://outerbounds.com/blog/metaflow-secrets#secrets-to-success,False,507.0,71.0
991,Basic Tagging,"Let's start with a basic use case that is familiar from many other tools: Manually attaching tags as human-readable annotations to objects, in our case to Metaflow runs. In this case, tags are used to organize, find, and share results.
 
 In the illustrations below, gray boxes refer to Metaflow runs, 👀 to people taking actions,
 ⚙️ to production systems that work without human supervision, and yellow labels to tags.
 
 ![Basic tagging](_five-ways-to-use-the-new-metaflow-tags/tag_blog_basics.png)
 
 Here's the idea:
 
 1. Run Metaflow runs as usual.
 2. Inspect results using [`@card`](https://docs.metaflow.org/metaflow/visualizing-results),
    [notebooks](https://docs.metaflow.org/metaflow/client), and [Metaflow GUI](https://netflixtechblog.com/open-sourcing-a-monitoring-gui-for-metaflow-75ff465f0d60).
 3. Use [the new `tag add` command](https://docs.metaflow.org/metaflow/client#adding-removing-and-replacing-tags) to attach tags to runs based on the analysis.
 
 See the steps in action:
 
 ```mdx-code-block
 import LiteYouTubeEmbed from 'react-lite-youtube-embed';
 import 'react-lite-youtube-embed/dist/LiteYouTubeEmbed.css';
 
 <div className=""video-container"">
 <LiteYouTubeEmbed
         id=""DEmKaTI3MG4""
         title=""Basic Tagging""
 />
 </div>
 ```",H2,https://outerbounds.com/blog/five-ways-to-use-the-new-metaflow-tags#basic-tagging,False,1272.0,152.0
992,Programmatic Tagging,"Tags allow you to express opinions about results of your and your colleagues' work, and,
 importantly, change those assessments at any time. In contrast to runs and artifacts that represent immutable facts (history shouldn't be rewritten), the way how you interpret those facts may change over time, which is reflected in tags.
 
 Tagging doesn't have to happen manually. This example shows how to use the [tagging methods in the Client API](https://docs.metaflow.org/metaflow/client#adding-removing-and-replacing-tags) to attach tags to runs automatically. Later, after you have done deeper analysis of
 the results, you can remove inaccurate tags either via the same API or with the `tag remove` command on the command line.
 
 ![Programmatic tagging](_five-ways-to-use-the-new-metaflow-tags/tag_blog_programmatic.png)
 
 In the video below, we use a code snippet like this
 
 ```python
 from metaflow import Flow, current
 run = Flow(current.flow_name)[current.run_id]
 run.add_tag('promising_model')
 ```
 
 to attach a tag, `promising_model`, to the currently executing run programmatically. After we have iterated on the model for a while, we analyze the promising models in a notebook and use the `Run.remove_tag` call to remove the tag from runs that don't deserve the label after a deeper investigation.
 
 Take a look:
 
 ```mdx-code-block
 <div className=""video-container"">
 <LiteYouTubeEmbed
         id=""25Hqp43J37I""
         title=""Programmatic Tagging""
 />
 </div>
 ```",H2,https://outerbounds.com/blog/five-ways-to-use-the-new-metaflow-tags#programmatic-tagging,False,1484.0,216.0
993,Tags and Namespaces,"Tags are not only for eyeballing. Internally, Metaflow [uses tags to define namespaces](https://docs.metaflow.org/metaflow/tagging#tags-as-namespaces). Namespaces are a lightweight
 way to define boundaries around projects, helping you avoid interfering with colleagues'
 projects inadvertently, and making it easier to [organize your experiments](https://docs.metaflow.org/metaflow/tagging), as illustrated by the following example.
 
 ![Tags and Namespaces](_five-ways-to-use-the-new-metaflow-tags/tag_blog_namespaces.png)
 
 It is common to have a project that spans multiple flows. In cases like this, we often want to fetch results of a flow from another flow. Using _an absolute reference_ like `Run('HelloFlow/32')` has limited utility, since new runs get produced constantly and fresh results are typically preferred. A better solution is to use _a relative reference_ like `Flow('HelloFlow').latest_run` to refer to another flow.
 
 However, when we say `latest_run` - what `Run` does it refer to exactly? My latest run, yours, or something else? There is not a single correct answer. To make it possible to frame the answer based on your specific situation, Metaflow returns the latest run in the currently chosen namespace. As namespaces are just sets of runs that share a tag, tagging can be used creatively to organize work across people and projects.
 
 This example walks through a scenario where Anne and Will work together: Anne is responsible for a feature
 transformation flow and Will for modeling. They experiment and iterate on their flows independently and use tagging to coordinate sharing of results.
 
 ```mdx-code-block
 <div className=""video-container"">
 <LiteYouTubeEmbed
         id=""ifARsmiSNhE""
         title=""Tags and Namespaces""
 />
 </div>
 ```",H2,https://outerbounds.com/blog/five-ways-to-use-the-new-metaflow-tags#tags-and-namespaces,False,1780.0,257.0
994,Tags in CI/CD,"Tags come in handy in production use cases too. In this example, we show how to leverage tags in the
 context of CI/CD, specifically GitHub Actions.
 
 ![Tags in CI/CD](_five-ways-to-use-the-new-metaflow-tags/tag_blog_cicd.png)
 
 The video demonstrates the following steps:
 
 1. The data scientist runs experiments locally as usual.
 2. When they are happy with the results, they prepare a pull request.
 3. A pull request triggers a GitHub Action which executes the new flow version as a part of the test suite. Small-scale tests can be run with worker instances provided by the CI/CD system while larger tests can be [farmed out to the cloud](https://docs.metaflow.org/metaflow/scaling-out-and-up) as usual with Metaflow.
 4. If tests are successful, the action tags the test run with a tag, `gh_test_ok`, either using the Client API or the `add tag` CLI.
 5. The `gh_test_ok` tags can be used to observe the status of PRs in Metaflow UI and to trigger deployments to production systems.
 
 You can set up a GitOps workflow like this easily at home:
 
 ```mdx-code-block
 <div className=""video-container"">
 <LiteYouTubeEmbed
         id=""hIiDXPHqEFM""
         title=""Tags in CI/CD""
 />
 </div>
 ```",H2,https://outerbounds.com/blog/five-ways-to-use-the-new-metaflow-tags#tags-in-ci-cd,False,1202.0,197.0
995,Tags and Continuous Training,"This may be one the most impactful use cases for tagging: Consider a production model that should update, say, monthly. Instead of training the model monthly, or triggering retraining reactively based on shifts in data, a more robust pattern is to retrain the model continuously, e.g. daily. Similar to [the benefits of Continuous Delivery](https://aws.amazon.com/devops/continuous-delivery/) in software engineering, this pattern minimizes surprises between trainings which are caused by changes in code, data, the execution environment, and human errors.
 
 ![Tags in CI/CD](_five-ways-to-use-the-new-metaflow-tags/tag_blog_continuous.png)
 
 Metaflow makes it easy to [schedule workflows](https://docs.metaflow.org/going-to-production-with-metaflow/scheduling-metaflow-flows) that retrain the model without any human supervision. When it is time to decide whether a new model should be promoted to production, the data scientist can evaluate latest model candidates using `@card`, notebooks, Metaflow UI, or even other automated workflows. If the model passes production gates, it is tagged with a tag `production`, triggering an actual deployment to e.g. a model hosting environment.
 
 See how we use the pattern with [AWS Step Functions](https://docs.metaflow.org/going-to-production-with-metaflow/scheduling-metaflow-flows/scheduling-with-aws-step-functions):
 
 ```mdx-code-block
 <div className=""video-container"">
 <LiteYouTubeEmbed
         id=""lZhwhuG0AN8""
         title=""Tags and Continuous Training""
 />
 </div>
 ```",H2,https://outerbounds.com/blog/five-ways-to-use-the-new-metaflow-tags#tags-and-continuous-training,False,1530.0,193.0
996,Conclusion,"It is beneficial to allow people to change their minds about many things, including how they view a Metaflow run. This is now possible with mutable tags, which makes them a foundational building block for various workflows that involve a human in the loop. Hopefully these five use cases inspire you to design and deploy patterns that work well for your needs.
 
 We'd love to hear from you and learn about your patterns, use cases, and pain points. Don't be a stranger - join [the Metaflow Slack](http://slack.outerbounds.co) today!",H2,https://outerbounds.com/blog/five-ways-to-use-the-new-metaflow-tags#conclusion,False,533.0,89.0
997,Why Metaflow on Microsoft Azure?,"Imagine a data scientist who is tasked to build an application powered by data science or ML, say, to forecast demand or to use state-of-the-art computer vision to recognize defects in manufacturing. Consider the infrastructure components, with the foundational layers at the bottom and the application-specific ones at the top of the stack, which are required to build such an application:
 
 
 ![](_metaflow-azure-machine-learning_data/0_img.png)
 
 1. The data scientist needs to determine what data to use and they need to develop pipelines to access, load, and transform **data**.
 2. They need a way to train models and perform inference, which requires access to **compute resources**. In the case of computationally demanding models, they may require a cluster of hundreds or even thousands of instances.
 3. Data pipelines, training code, and all other business logic need to be architected as a production-ready application, which is often structured as a workflow. For the fastest speed of development and frictionless deployment, it would be optimal if the data scientist could develop and test the workflow autonomously, using a robust **workflow orchestrator** which supports both rapid local development as well as highly-available production deployments.
 4. All the work needs to be tracked, **versioned**, and made easily observable, allowing the project to be improved systematically over time, potentially by multiple data scientists.
 5. The application needs to be **deployed** and integrated into the surrounding systems to produce the desired real-world impact.
 6. Finally, the data scientist should be able to leverage their expertise and domain knowledge to the fullest, leveraging the best off-the-shelf libraries, as they **develop models** powering the application.
 
 
 In the past, addressing the six layers has required a fully staffed team consisting of infrastructure engineers, data engineers, and data scientists, which has made the development of ML-powered applications slow and costly. This, in turn, has curtailed the appetite of many organizations to experiment with new ML ideas, which is counterproductive in the long term.",H2,https://outerbounds.com/blog/metaflow-azure-machine-learning#why-metaflow-on-microsoft-azure,False,2167.0,324.0
998,What is Metaflow on Azure?,"Metaflow provides a radically simple, user-friendly Python API that covers the full stack of data science and ML infrastructure outlined above. On the backend side, it relies on a stack of battle-hardened, enterprise-ready components for data, compute, and production orchestration, provided by the Azure and Kubernetes ecosystems. The figure below illustrates the new stack on Azure in the context of a simple Metaflow flow:
 
 
 ![](_metaflow-azure-machine-learning_data/1_img.png)
 
 Note how the Metaflow code itself doesn’t have any hard-coded references to Azure or any other cloud. The integration with Azure, as well as any security and other policies specific to your company, lives as configuration separate from code. For more details on how this works in practice, see [Metaflow resources for engineering](https://outerbounds.com/docs/engineering-welcome/). Or, if you don’t want to manage this infrastructure by yourself, [reach out to us to learn more about our managed offering](https://meetings.hubspot.com/ville-tuulos), deployed in your account.
 
 
 Thanks to this architecture, Metaflow also supports bursting to the public cloud from on-prem Kubernetes-based hybrid cloud deployments, as well as migration between clouds in a straightforward manner, without requiring tedious rewrites of the business logic.",H2,https://outerbounds.com/blog/metaflow-azure-machine-learning#what-is-metaflow-on-azure,False,1328.0,185.0
999,User-friendly and production-ready,"Knowing that companies use Metaflow to power their business-critical ML and data science applications, we spent over a year developing and testing the Metaflow stack on Azure. We have ensured that the Azure integration is at parity with the corresponding stack on AWS, which has been used to power thousands of applications across hundreds of companies, such as [23andMe](https://medium.com/23andme-engineering/machine-learning-eeee69d40736), [CNN](https://medium.com/cnn-digital/accelerating-ml-within-cnn-983f6b7bd2eb), and [Realtor.com](https://medium.com/realtor-com-innovation-blog/improving-data-science-processes-to-speed-innovation-at-realtor-com-b6b90fa530dc).
 
 
 We are [thankful to early adopters who have helped us test the stack on Azure](https://softlandia.fi/en/blog/data-science-on-azure-with-metaflow), including [Softlandia](https://softlandia.fi/), a group of consultants with a decade of experience with Azure-based ML. Today, we are confident that Metaflow on Azure provides one of the most user-friendly data science and ML experiences on Azure, both for data scientists as well as engineers who operate the infrastructure.",H2,https://outerbounds.com/blog/metaflow-azure-machine-learning#user-friendly-and-production-ready,False,1147.0,126.0
1000,Getting started with Metaflow on Azure,"If you are a data scientist or engineer, check out [the documentation for open-source Metaflow](https://docs.metaflow.org/getting-started/infrastructure), which we have updated recently to reflect the multi-cloud nature of Metaflow. You may also want to take a look at a new book covering Metaflow, [Effective Data Science Infrastructure](https://www.manning.com/books/effective-data-science-infrastructure), the lessons of which can be readily applied on Azure.
 
 
 If you want to get a taste of Metaflow before you install anything locally, you can do so by signing up for a [Metaflow Sandbox](https://outerbounds.com/sandbox/) that allows you to test the full stack in the browser. No matter which path you choose, make sure to join the [Metaflow support Slack](http://slack.outerbounds.co) for feedback, support, and to chat with 1500+ like-minded engineers and data scientists. We’d love to hear from you!",H2,https://outerbounds.com/blog/metaflow-azure-machine-learning#getting-started-with-metaflow-on-azure,False,911.0,126.0
1001,Welcome to Metaflow,"Metaflow makes it easy to build and manage real-life data science and machine learning
 projects.
 
 <div className=""tocList"">",H1,https://docs.metaflow.org/#welcome-to-metaflow,False,126.0,18.0
1002,Motivation,,H2,https://docs.metaflow.org/#motivation,False,0.0,1.0
1003,Getting Started,,H2,https://docs.metaflow.org/#getting-started,False,0.0,1.0
1004,I. Local Development,,H2,https://docs.metaflow.org/#i-local-development,False,0.0,1.0
1005,II. Scalable Flows,,H2,https://docs.metaflow.org/#ii-scalable-flows,False,0.0,1.0
1006,III. Production Deployments,,H2,https://docs.metaflow.org/#iii-production-deployments,False,0.0,1.0
1007,API Reference,,H2,https://docs.metaflow.org/#api-reference,False,0.0,1.0
1008,Internals of Metaflow,,H2,https://docs.metaflow.org/#internals-of-metaflow,False,0.0,1.0
1009,Old,,H2,https://docs.metaflow.org/#old,False,0.0,1.0
1010,,</div>,H2,https://docs.metaflow.org/#,False,6.0,1.0
1011,What is Metaflow,"Metaflow is a human-friendly Python library that makes it straightforward to develop,
 deploy, and operate various kinds of data-intensive applications, in particular those
 involving data science and ML. Metaflow was originally developed at Netflix to boost the
 productivity of data scientists who work on a wide variety of projects, from classical
 statistics to state-of-the-art deep learning.
 
 Metaflow is available as open-source under the [Apache License, Version
 2.0](https://github.com/Netflix/metaflow/blob/master/LICENSE).",H1,https://docs.metaflow.org/introduction/what-is-metaflow#what-is-metaflow,False,536.0,68.0
1012,What does Metaflow do exactly?,"Metaflow provides a unified API to the whole [infrastructure
 stack](/introduction/why-metaflow) that is required to execute data science projects
 from prototype to production. Take a look at this simple Metaflow flow that illustrates
 the concepts:
 
 ![image](/assets/what-is-metaflow.svg)
 
  - **Modeling**: You can use any Python libraries with Metaflow. Metaflow helps [make
    them available in all environments reliably](/scaling/dependencies).
  - **Deployment**: Metaflow supports highly available, production-grade [workflow
    orchestration and other deployment
    patterns](/production/scheduling-metaflow-flows/introduction/).
  - **Versioning**: Metaflow [keeps track of all flows, experiments, and artifacts
    automatically](/metaflow/basics#artifacts).
  - **Orchestration**: Metaflow makes it easy to [construct workflows and test them
    locally](/metaflow/basics).
  - **Compute**: Metaflow leverages your [cloud account and Kubernetes clusters for
    scalability](/scaling/introduction).
  - **Data**: Besides managing the data flow inside the workflow, Metaflow provides
    patterns for [accessing data from data warehouses and lakes](/scaling/data).
 
 You could use a separate tool for each of these layers but many data scientists prefer
 using a unified, thoughtfully designed library. This also minimizes the operational
 burden for engineers who manage the infrastructure.",H2,https://docs.metaflow.org/introduction/what-is-metaflow#what-does-metaflow-do-exactly,False,1409.0,189.0
1013,How does Metaflow support prototyping and production use cases?,"Based on our experiences with hundreds of data science and ML projects, we believe that
 projects should grow gradually from quick experiments on the laptop, conducted by a
 single data scientist, to business-critical production deployments developed by a team
 of experts.
 
 In contrast to traditional software engineering, it is hard to assess the value of a
 DS/ML project without running realistic experiments, like backtesting models at scale,
 exposing a prototype to business stakeholders, or running a live A/B test. This should
 be doable without a high upfront cost. When the value of the project has been proven, it
 shouldn't be too hard to take the project all the way to large-scale production.
 
 Here is how Metaflow supports projects throughout this journey (click links in the image
 for more information about specific topics):
 
 <object type=""image/svg+xml"" data=""/assets/metaflow-lifecycle.svg""></object>
 
 A typical project advances through these three stages:
 
  1. [**Prototyping**](/metaflow/introduction): You can [develop and test Metaflow
  workflows locally](/getting-started/install) without having to deploy any
  infrastructure. Making the local development experience fast and smooth is a key to
  productivity.
  2. [**Scaling**](/scaling/introduction): There is only so much data and compute you can
  manage on a laptop. It is very convenient to be able to [test workflows at
  scale](/scaling/introduction) as early as possible. This gives you a good idea of how
  the system is going to behave in production, at no risk.
  3. [**Production**](/production/introduction): A defining feature of any production
  deployment is *high availability*. No matter how the project is deployed to production,
  it shouldn't require any human attention. Metaflow supports this through
  [production-grade workflow
  orchestrators](/production/scheduling-metaflow-flows/introduction), deploying to which
  require no changes in the code. Also, the team needs to be able to continue development
  of [new versions of the project alongside any production
  deployments](/production/coordinating-larger-metaflow-projects) and be able to A/B test
  them easily. And, as you needs grow, you may want to start building larger, [reactive
  systems using individual flows as building blocks](/production/event-triggering).
 
 You can assess the value of the project at every stage before deciding to invest more. A
 core tenet of Metaflow is to make simple things simple and demanding
 production-deployments possible. Read more about the journey from prototype to
 production in [Introduction to Developing with Metaflow](/metaflow/introduction).
 
 Also - don't consider only the happy path! Any real-world project should [account for
 failures](/scaling/failures), consider how the system is
 [monitored](https://github.com/Netflix/metaflow-ui), and provide clear playbooks for
 [debugging issues](/metaflow/debugging) occurring at any point in the project's
 lifecycle, production issues in particular.",H2,https://docs.metaflow.org/introduction/what-is-metaflow#how-does-metaflow-support-prototyping-and-production-use-cases,False,3026.0,425.0
1014,Is Metaflow easy to deploy and operate?,"You can get started with local development [by `pip` installing
 Metaflow](/getting-started/install) like any other Python library. To benefit from
 scaling and production features of Metaflow, you need to [deploy Metaflow to your cloud
 account or Kubernetes cluster](/getting-started/infrastructure) which can be done using
 [our Terraform or Cloudformation
 templates](https://outerbounds.com/docs/engineering-welcome/) which shouldn't take more
 than 15-30 minutes, unless you want to customize the setup. Take a look at [an overview
 of Metaflow infrastructure](/getting-started/infrastructure) for more information.
 
 In general, engineering teams like Metaflow as it integrates well with the company's
 existing infrastructure instead of introducing major new components. Metaflow works with
 all the major cloud providers: AWS, Azure, and GCP. It provides native,
 thoroughly-tested integrations to popular systems like [AWS
 Batch](https://aws.amazon.com/batch/), [AWS Step
 Functions](https://aws.amazon.com/step-functions/),
 [Kubernetes](https://kubernetes.io/), [Argo
 Workflows](https://argoproj.github.io/argo-workflows/), and [Apache
 Airflow](https://airflow.apache.org/).
 
 Many teams find it comforting to know that Metaflow has been used for serious,
 large-scale production use cases involving tens of thousands of flows and millions of
 runs for more than four years, so the codebase is extremely well tested and
 battle-hardened. Also, we provide [a strong guarantee of backwards compatibility for the
 user-facing API](/api), so you get to stand on a solid foundation.",H2,https://docs.metaflow.org/introduction/what-is-metaflow#is-metaflow-easy-to-deploy-and-operate,False,1594.0,192.0
1015,Should I Use Metaflow?,"If you are working on an existing project dealing with data and compute, or you are
 planning to start a new one, consider the following questions:
 
 1. **Scalability**: Do you need more than one laptop-size computer in the project?
 2. **Criticality**: Is it important that results are produced correctly and in a timely
    manner?
 3. **Complexity**: Does the project have many moving pieces or many people working
    together?
 
 If you answered ""yes"" to any of the above, keep on reading - Metaflow can help you! If
 the answer is ""no"" to all of the above, Metaflow doesn't provide much benefit. This is a
 valid scenario, for instance, when you are hacking small-scale experiments in a
 notebook. You should come back when the project is ready to move to the next stage.
 
 When the time is right, moving from notebooks-only to
 [notebooks-with-Metaflow](/metaflow/client) should be a smooth sailing. Metaflow is
 designed to be user-friendly and welcoming to all data scientists, novice and experts
 alike, who want to start building end-to-end applications more independently.",H2,https://docs.metaflow.org/introduction/what-is-metaflow#should-i-use-metaflow,False,1086.0,180.0
1016,Metaflow Resources,"Here's an incomplete overview of Metaflow resources outside this documentation. [Let us
 know on Slack](http://slack.outerbounds.co) or [open a pull
 request](https://github.com/netflix/metaflow-docs) if you find or, even better, create a
 resource that should be listed here 🤗",H1,https://docs.metaflow.org/introduction/metaflow-resources#metaflow-resources,False,277.0,35.0
1017,Community,"- Join [the Metaflow Slack](http://slack.outerbounds.co) to meet the developers and
    users of Metaflow across hundreds of companies.
  - Report [issues on GitHub](https://github.com/netflix/metaflow/issues).
  - You can also email us at [help@metaflow.org](mailto://help@metaflow.org).",H2,https://docs.metaflow.org/introduction/metaflow-resources#community,False,288.0,35.0
1018,Deployment & Operations,"- A comprehensive guide to [deploying and operating the infrastructure for
    Metaflow](https://outerbounds.com/docs/engineering-welcome/).",H2,https://docs.metaflow.org/introduction/metaflow-resources#deployment-operations,False,140.0,15.0
1019,Tutorials,"- [Getting started with Metaflow tutorial](/getting-started/tutorials), a part of this
    documentation.
  - [Additional tutorials](https://outerbounds.com/docs/intro-tutorial-overview/),
    provided by Outerbounds.
  - [Metaflow how-to guides](https://outerbounds.com/docs/data-science-welcome/) for tips
    & tricks.",H2,https://docs.metaflow.org/introduction/metaflow-resources#tutorials,False,321.0,36.0
1020,Books,"- [Effective Data Science
    Infrastructure](https://www.manning.com/books/effective-data-science-infrastructure):
    Learn to design and develop the full infrastructure stack for data science using
    Metaflow.",H2,https://docs.metaflow.org/introduction/metaflow-resources#books,False,214.0,28.0
1021,Videos,"- [Metaflow on YouTube](https://www.youtube.com/results?search_query=metaflow+ml).
  - You can start with [this recent
    overview](https://www.youtube.com/watch?v=gZnhSHvhuFQ).",H2,https://docs.metaflow.org/introduction/metaflow-resources#videos,False,178.0,16.0
1022,Blogs,"- 23andMe: [Developing safe and reliable ML products at
    23andMe](https://medium.com/23andme-engineering/machine-learning-eeee69d40736)
  - AWS: [Getting started with the open source data science tool Metaflow on
    AWS](https://aws.amazon.com/blogs/opensource/getting-started-with-the-open-source-data-science-tool-metaflow-on-aws/)
  - CNN: [Accelerating ML within
    CNN](https://medium.com/cnn-digital/accelerating-ml-within-cnn-983f6b7bd2eb)
  - Latana: [Brand Tracking with Bayesian Statistics and AWS
    Batch](https://aws.amazon.com/blogs/startups/brand-tracking-with-bayesian-statistics-and-aws-batch/)
  - Netflix: [Open-Sourcing Metaflow, a Human-Centric Framework for Data
    Science](https://netflixtechblog.com/open-sourcing-metaflow-a-human-centric-framework-for-data-science-fa72e04a5d9)
  - Netflix: [Unbundling Data Science Workflows with Metaflow and AWS Step
    Functions](https://netflixtechblog.com/unbundling-data-science-workflows-with-metaflow-and-aws-step-functions-d454780c6280)
  - Netflix: [Open-Sourcing a Monitoring GUI for Metaflow, Netflix’s ML
    Platform](https://netflixtechblog.com/open-sourcing-a-monitoring-gui-for-metaflow-75ff465f0d60)
  - Netflix: [Supporting content decision makers with machine
    learning](https://netflixtechblog.com/supporting-content-decision-makers-with-machine-learning-995b7b76006f)
  - Netflix: [Scaling Media Machine Learning at
    Netflix](https://netflixtechblog.com/scaling-media-machine-learning-at-netflix-f19b400243)
  - Outerbounds: [Various articles about Metaflow](https://outerbounds.com/blog/)
  - REA: [Accelerating experimentation with
    MLOps](https://www.rea-group.com/about-us/news-and-insights/blog/accelerating-experimentation-with-mlops/)
  - Realtor.com: [Improving Data Science Processes to Speed Innovation at
    Realtor.com](https://medium.com/realtor-com-innovation-blog/improving-data-science-processes-to-speed-innovation-at-realtor-com-b6b90fa530dc)
  - SAP: [Train your model in SAP AI Core using the Metaflow-Argo
    plugin](https://blogs.sap.com/2022/04/20/train-your-model-in-sap-ai-core-using-the-metaflow-argo-plugin/)
  - Softlandia: [Distributed data science with Metaflow and Dask in Azure Kubernetes
    Service](https://softlandia.fi/en/blog/distributed-data-science-with-metaflow-and-dask)",H2,https://docs.metaflow.org/introduction/metaflow-resources#blogs,False,2313.0,191.0
1023,Why Metaflow,,H1,https://docs.metaflow.org/introduction/why-metaflow#why-metaflow,False,0.0,1.0
1024,1. Modern businesses are eager to utilize data science and ML,"In the past, data scientists and ML engineers had to rely on a medley of point solutions
 and custom systems to build ML and data science applications.
 
 ![Many data science opportunities](/assets/mf-intro-01.png)",H3,https://docs.metaflow.org/introduction/why-metaflow#1-modern-businesses-are-eager-to-utilize-data-science-and-ml,False,214.0,32.0
1025,2. What is common in DS/ML applications?,"Applications can be built quicker and more robustly if they stand on a common,
 human-friendly foundation. But what should the foundation cover?
 
 ![A solid foundation for all use cases](/assets/mf-intro-02.png)",H3,https://docs.metaflow.org/introduction/why-metaflow#2-what-is-common-in-ds-ml-applications,False,212.0,30.0
1026,3. All DS/ML applications use data,"**Data** may come in different shapes and sizes and may be loaded from various data
 stores. However, no matter what data is used, accessing and processing it shouldn't be
 too cumbersome.
 
 ![Data](/assets/mf-intro-03.png)",H3,https://docs.metaflow.org/introduction/why-metaflow#3-all-ds-ml-applications-use-data,False,224.0,33.0
1027,4. DS/ML applications need to perform computation,"Some applications require a tremendous amount of compute power - think computer vision -
 while some do with less. Regardless of the scale, all applications need to perform
 **computation** reliably. Thanks to cloud computing, data scientists and ML engineers
 should be able to utilize elastic compute resources without friction.
 
 ![Compute](/assets/mf-intro-04.png)",H3,https://docs.metaflow.org/introduction/why-metaflow#4-ds-ml-applications-need-to-perform-computation,False,369.0,51.0
1028,5. DS/ML applications consists of multiple interconnected parts,"Consider an application that loads data, transforms it, trains a bunch of models,
 chooses the best performing one, runs inference, and writes the results to a database.
 Multi-steps workflows like this are a norm in ML. **A workflow orchestrator** is needed
 to make sure all steps get executed in order, on time.
 
 ![Orchestration](/assets/mf-intro-05.png)",H3,https://docs.metaflow.org/introduction/why-metaflow#5-ds-ml-applications-consists-of-multiple-interconnected-parts,False,359.0,54.0
1029,6. DS/ML applications evolve over time incrementally,"Rarely a real-world application is built and deployed only once. Instead, a typical
 application is built gradually, through contributions by many people. The project needs
 to be tracked, organized, and **versioned**, which enables systematic and continuous
 improvement over time.
 
 ![Versioning](/assets/mf-intro-06.png)",H3,https://docs.metaflow.org/introduction/why-metaflow#6-ds-ml-applications-evolve-over-time-incrementally,False,324.0,41.0
1030,7. DS/ML applications produce business value in various ways,"To produce real business value, DS/ML applications can't live in a walled garden. They
 must be integrated with the surrounding systems seamlessly: Some applications enhance
 data in a database, some power internal dashboards or microservices, whereas some power
 user-facing products. There are many such ways to **deploy** ML in production. The more
 valuable the application, the more carefully it needs to be operated and monitored as
 well.
 
 ![Deployment](/assets/mf-intro-07.png)",H3,https://docs.metaflow.org/introduction/why-metaflow#7-ds-ml-applications-produce-business-value-in-various-ways,False,487.0,69.0
1031,8. DS/ML applications should leverage the best tools available,"For many data scientists and ML engineers, the most rewarding part of the project is
 **modeling**. Using their domain knowledge and expertise, the modeler should be able to
 choose the best tool for the job amongst off-the-shelf libraries, such as PyTorch,
 XGBoost, Scikit Learn, and many others. Or, if necessary, they should be able to use a
 wholly custom approach.
 
 ![Modeling](/assets/mf-intro-08.png)",H3,https://docs.metaflow.org/introduction/why-metaflow#8-ds-ml-applications-should-leverage-the-best-tools-available,False,410.0,62.0
1032,9. Metaflow covers the full stack of DS/ML infrastructure,"Metaflow was originally created at Netflix, motivated by the realization that data
 scientists and ML engineers need help with all these concerns: Any gaps or friction in
 the stack can slow down the project drastically. Thanks to a common foundation provided
 by Metaflow, data scientists can iterate on ideas quickly and deploy them confidently by
 relying on a well-defined architecture and best practices, shared by everyone in the
 team.
 
 ![Full-stack Metaflow](/assets/mf-intro-09.png)",H3,https://docs.metaflow.org/introduction/why-metaflow#9-metaflow-covers-the-full-stack-of-ds-ml-infrastructure,False,493.0,72.0
1033,"10. Metaflow takes care of the plumbing, so you can focus on the fun parts","Metaflow provides a robust and user-friendly foundation for a wide spectrum of
 data-intensive applications, including most data science and ML use cases. Data
 scientists and ML engineers who know the basics of Python can build their own
 applications, models, and policies on top of it, while Metaflow takes care of the
 low-level infrastructure: data, compute, orchestration, and versioning.
 
 ![Full stack triangles](/assets/mf-intro-10.png)",H3,https://docs.metaflow.org/introduction/why-metaflow#10-metaflow-takes-care-of-the-plumbing-so-you-can-focus-on-the-fun-parts,False,446.0,62.0
1034,11. Metaflow relies on systems that engineers know and trust,"Metaflow was designed at Netflix to serve the needs of business-critical ML/DS
 applications. It relies on proven and scalable infrastructure which works for small and
 large organizations alike. Metaflow integrates with all the top clouds as well as with
 Kubernetes and systems around them in a responsible manner. It respects the security and
 other policies of your company, making engineering teams happy too.
 
 ![Existing infrastructure](/assets/mf-intro-11.png)",H3,https://docs.metaflow.org/introduction/why-metaflow#11-metaflow-relies-on-systems-that-engineers-know-and-trust,False,469.0,66.0
1035,12. Metaflow is used by hundreds of innovative companies,"Today, Metaflow powers thousands of ML/DS applications at innovative companies such as
 [Netflix](https://netflixtechblog.com/supporting-content-decision-makers-with-machine-learning-995b7b76006f),
 [CNN](https://medium.com/cnn-digital/accelerating-ml-within-cnn-983f6b7bd2eb),
 [SAP](https://blogs.sap.com/2022/04/20/train-your-model-in-sap-ai-core-using-the-metaflow-argo-plugin/),
 [23andMe](https://medium.com/23andme-engineering/machine-learning-eeee69d40736),
 [Realtor.com](https://medium.com/realtor-com-innovation-blog/improving-data-science-processes-to-speed-innovation-at-realtor-com-b6b90fa530dc),
 [REA](https://www.rea-group.com/about-us/news-and-insights/blog/accelerating-experimentation-with-mlops/),
 [Coveo](https://outerbounds.com/blog/dataops-mlops-reasonable-organizations/),
 [Latana](https://aws.amazon.com/blogs/startups/brand-tracking-with-bayesian-statistics-and-aws-batch/),
 and hundreds of others across industries. Commercial support for Metaflow is provided by
 [Outerbounds](https://outerbounds.com). To hear first-hand experiences from these
 companies and many others, [join the Metaflow Slack](http://slack.outerbounds.co).",H3,https://docs.metaflow.org/introduction/why-metaflow#12-metaflow-is-used-by-hundreds-of-innovative-companies,False,1160.0,48.0
1036,Release Notes,"Read below how Metaflow has improved over time.
 
 We take backwards compatibility very seriously. In the vast majority of cases, you can
 upgrade Metaflow without expecting changes in your existing code. In the rare cases when
 breaking changes are absolutely necessary, usually, due to bug fixes, you can take a
 look at minor breaking changes below before you upgrade.
 
 :::info
 
 For the most recent release notes, see [release notes on
 Github](https://github.com/Netflix/metaflow/releases)
 
 :::",H1,https://docs.metaflow.org/internals/release-notes#release-notes,False,504.0,76.0
1037,"[2.6.1 (May 13, 2022)](https://github.com/Netflix/metaflow/releases/tag/2.6.1)",The Metaflow 2.6.1 release is a minor release.,H2,https://docs.metaflow.org/internals/release-notes#261-may-13-2022,False,46.0,8.0
1038,"[2.6.0 (Apr 25, 2022)](https://github.com/Netflix/metaflow/releases/tag/2.6.0)","The Metaflow 2.6.0 release is a minor release and introduces Metaflow's integration with
 [Kubernetes](https://docs.metaflow.org/scaling/introduction/effortless-scaling-with-kubernetes)
 and [Argo",H2,https://docs.metaflow.org/internals/release-notes#260-apr-25-2022,False,196.0,16.0
1039,Add capability to launch Metaflow tasks on Kubernetes and schedule Metaflow flows with Argo Workflows.,"This release enables brand new capabilities for [Metaflow on top of
 Kubernetes](https://outerbounds.com/blog/human-centric-data-science-on-kubernetes-with-metaflow/).
 You can now [`run --with
 kubernetes`](https://docs.metaflow.org/scaling/introduction/effortless-scaling-with-kubernetes)
 all or parts of any Metaflow flow on top of _any_ Kubernetes cluster from your
 workstation. To execute your flow asynchronously, you can deploy the flow to Argo
 Workflows (a Kubernetes-native workflow scheduler) with a single command -
 [`argo-workflows
 create`](https://docs.metaflow.org/production/scheduling-metaflow-flows/introduction/scheduling-with-argo-workflows).
 
 To get started, take a look at the [deployment guide for
 Kubernetes](https://outerbounds.com/docs/engineering-welcome/). Your feedback and
 feature requests are highly appreciated! - please reach out to us at
 slack.outerbounds.co
 
 PR #992 addressed issue #50.",H4,https://docs.metaflow.org/internals/release-notes#add-capability-to-launch-metaflow-tasks-on-kubernetes-and-schedule-metaflow-flows-with-argo-workflows,False,933.0,93.0
1040,Expose `tags` in `current` object.,"Metaflow tags are now available as part of the `current` singleton object.
 
 ```
 @step
 def my_step(self):
     from metaflow import current
     tags = current.tags
     ...
 ```
 
 PR #1019 fixed issue #1007.",H4,https://docs.metaflow.org/internals/release-notes#expose-tags-in-current-object,False,212.0,44.0
1041,"[2.5.4 (Mar 24, 2022)](https://github.com/Netflix/metaflow/releases/tag/2.5.4)",The Metaflow 2.5.4 release is a minor release.,H2,https://docs.metaflow.org/internals/release-notes#254-mar-24-2022,False,46.0,8.0
1042,"[2.5.3 (Mar 7, 2022)](https://github.com/Netflix/metaflow/releases/tag/2.5.3)",The Metaflow 2.5.3 release is a minor release.,H2,https://docs.metaflow.org/internals/release-notes#253-mar-7-2022,False,46.0,8.0
1043,"[2.5.2 (Feb 16, 2022)](https://github.com/Netflix/metaflow/releases/tag/2.5.2)",The Metaflow 2.5.2 release is a minor release.,H2,https://docs.metaflow.org/internals/release-notes#252-feb-16-2022,False,46.0,8.0
1044,"[2.5.1 (Feb 15, 2022)](https://github.com/Netflix/metaflow/releases/tag/2.5.1)",The Metaflow 2.5.1 release is a minor release.,H2,https://docs.metaflow.org/internals/release-notes#251-feb-15-2022,False,46.0,8.0
1045,"[2.5.0 (Jan 25, 2022)](https://github.com/Netflix/metaflow/releases/tag/2.5.0)",The Metaflow 2.5.0 release is a minor release.,H2,https://docs.metaflow.org/internals/release-notes#250-jan-25-2022,False,46.0,8.0
1046,"[2.4.9 (Jan 18, 2022)](https://github.com/Netflix/metaflow/releases/tag/2.4.9)","The Metaflow 2.4.9 release is a patch release.
 
 * Improvements
   * Store information about the DAG being executed in an artifact. This will allow to
     render execution DAG in a `@card` (
     [#822](https://github.com/Netflix/metaflow/pull/822) )
 * Bug Fixes
   * Fixed cli command when task_id provided (
     [#890](https://github.com/Netflix/metaflow/pull/890) )
   * Fix with metadata syncing on AWS Batch when running without remote metadata service
     ( [#902](https://github.com/Netflix/metaflow/pull/902) )
   * Fix default resource math. Previously we sometimes computed vCPU and memory settings
     incorrectly, in cases when they were set to something less than the default value (
     [#810](https://github.com/Netflix/metaflow/pull/810) , fixes
     [#467](https://github.com/Netflix/metaflow/issues/467) )",H2,https://docs.metaflow.org/internals/release-notes#249-jan-18-2022,False,830.0,134.0
1047,"[2.4.8 (Jan 10, 2022)](https://github.com/Netflix/metaflow/releases/tag/2.4.8)",The Metaflow 2.4.8 release is a patch release.,H2,https://docs.metaflow.org/internals/release-notes#248-jan-10-2022,False,46.0,8.0
1048,"[2.4.7 (Dec 16, 2021)](https://github.com/Netflix/metaflow/releases/tag/2.4.7)",The Metaflow 2.4.7 release is a patch release.,H2,https://docs.metaflow.org/internals/release-notes#247-dec-16-2021,False,46.0,8.0
1049,"[2.4.6 (Dec 16, 2021)](https://github.com/Netflix/metaflow/releases/tag/2.4.6)",This version was skipped due to technical reasons,H2,https://docs.metaflow.org/internals/release-notes#246-dec-16-2021,False,49.0,8.0
1050,"[2.4.5 (Dec 8, 2021)](https://github.com/Netflix/metaflow/releases/tag/2.4.5)",The Metaflow 2.4.5 release is a patch release.,H2,https://docs.metaflow.org/internals/release-notes#245-dec-8-2021,False,46.0,8.0
1051,"[2.4.4 (Nov 29, 2021)](https://github.com/Netflix/metaflow/releases/2.4.4)",The Metaflow 2.4.4 release is a patch release.,H2,https://docs.metaflow.org/internals/release-notes#244-nov-29-2021,False,46.0,8.0
1052,"Improvements <a href=""user-content-v2.4.4_improvements"" id=""user-content-v2.4.4_improvements""></a>",,H3,https://docs.metaflow.org/internals/release-notes#improvements-a-href-user-content-v244-improvements-id-user-content-v244-improvements-a,False,0.0,1.0
1053,Add default image config option as described in [489](https://github.com/Netflix/metaflow/issues/489) ([813](https://github.com/Netflix/metaflow/pull/813)),"We're moving to a more consistent scheme for naming options related to docker images.
 You can read the details in [#489](https://github.com/Netflix/metaflow/issues/489), but
 this release introduces new config options `DEFAULT_CONTAINER_IMAGE` and
 `DEFAULT_CONTAINER_REGISTRY` that can be used to specify docker image in addition to
 plugin-specific options like `KUBERNETES_CONTAINER_IMAGE`",H4,https://docs.metaflow.org/internals/release-notes#add-default-image-config-option-as-described-in-489,False,393.0,46.0
1054,Read default k8s namespace from config ([823](https://github.com/Netflix/metaflow/pull/823)),"This adds a new configuration option to set the default namespace for the Kubernetes
 plugin",H4,https://docs.metaflow.org/internals/release-notes#read-default-k8s-namespace-from-config-823,False,92.0,15.0
1055,[2.4.3 (Nov 3rd 2021)](https://github.com/Netflix/metaflow/releases/tag/2.4.3),The Metaflow 2.4.3 release is a patch release,H2,https://docs.metaflow.org/internals/release-notes#243-nov-3rd-2021,False,45.0,8.0
1056,"Bug Fixes <a href=""user-content-v2.4.3_bugs"" id=""user-content-v2.4.3_bugs""></a>",,H3,https://docs.metaflow.org/internals/release-notes#bug-fixes-a-href-user-content-v243-bugs-id-user-content-v243-bugs-a,False,0.0,1.0
1057,"Fix a race condition when accessing artifacts of a running task ([789](https://github.com/Netflix/metaflow/pull/789)) <a href=""user-content-789"" id=""user-content-789""></a>","When accessing artifacts of a running task using `Task(...).artifacts`, a race condition
 existed and the call could return a difficult to understand error message. This release
 fixes this issue and making this call will either return the artifacts present or no
 artifacts at all if none are present yet.",H4,https://docs.metaflow.org/internals/release-notes#fix-a-race-condition-when-accessing-artifacts-of-a-running-task-789,False,306.0,49.0
1058,"Fix an issue when using a combination of `@catch` and `@retry` decorators ([776](https://github.com/Netflix/metaflow/pull/776)) <a href=""user-content-776"" id=""user-content-776""></a>","A step as below:
 
 ```python
 @retry(times=2)
 @catch(var='exception')
 @step
 def my_step(self):
     raise ValueError()
 ```
 
 would not retry 2 times as expected but instead the exception would be caught the first
 time around. This release fixes this issue and the step will now execute a total of 3
 times and the exception will be caught on the third time.",H4,https://docs.metaflow.org/internals/release-notes#fix-an-issue-when-using-a-combination-of-catch-and-retry-decorators-776,False,364.0,63.0
1059,"Upgrade Pandas in tutorials ([707](https://github.com/Netflix/metaflow/pull/707)) <a href=""user-content-707"" id=""user-content-707""></a>","On macOS Big Sur, certain tutorials were broken due to using an older version of Pandas.
 This updates the tutorials to use 1.3.3 to solve this issue",H4,https://docs.metaflow.org/internals/release-notes#upgrade-pandas-in-tutorials-707,False,149.0,27.0
1060,"[2.4.2 (Oct 25th, 2021)](https://github.com/Netflix/metaflow/releases/2.4.2)",The Metaflow 2.4.2 release is a patch release,H2,https://docs.metaflow.org/internals/release-notes#242-oct-25th-2021,False,45.0,8.0
1061,Bug Fixes,,H3,https://docs.metaflow.org/internals/release-notes#bug-fixes,False,0.0,1.0
1062,Fix a bug with accessing legacy logs through `metaflow.client` ([779](https://github.com/Netflix/metaflow/pull/779)),"Metaflow `v2.4.1` introduced a bug (due to a typo) in accessing legacy task logs through
 `metaflow.client`
 
 ```
 Task(""pathspec/to/task"").stdout
 ```
 
 This release fixes this issue.",H4,https://docs.metaflow.org/internals/release-notes#fix-a-bug-with-accessing-legacy-logs-through-metaflowclient-779,False,186.0,26.0
1063,Fix a bug with task datastore access when no task attempt has been recorded ([780](https://github.com/Netflix/metaflow/pull/780)),"A subtle bug was introduced in Metaflow `2.4.0` where the task datastore access fails
 when no task attempt was recorded. This release fixes this issue.",H4,https://docs.metaflow.org/internals/release-notes#fix-a-bug-with-task-datastore-access-when-no-task-attempt-has-been-recorded-780,False,152.0,25.0
1064,"[2.4.1 (Oct 18th, 2021)](https://github.com/Netflix/metaflow/releases/2.4.1)",The Metaflow 2.4.1 release is a patch release,H2,https://docs.metaflow.org/internals/release-notes#241-oct-18th-2021,False,45.0,8.0
1065,Bug Fixes,,H3,https://docs.metaflow.org/internals/release-notes#bug-fixes,False,0.0,1.0
1066,Expose non-pythonic dependencies inside the conda environment on AWS Batch ([735](https://github.com/Netflix/metaflow/pull/735)),"Prior to this release, non-pythonic dependencies in a conda environment were not
 automatically visible to a Metaflow task executing on AWS Batch (see
 [#734](https://github.com/Netflix/metaflow/issues/734)) (they were available for tasks
 that were executed locally). For example
 
 ```python
 import os
 from metaflow import FlowSpec, step, conda, conda_base, batch
 
 class TestFlow(FlowSpec):
 
     @step
     def start(self):
         self.next(self.use_node)
 
     @batch
     @conda(libraries={""nodejs"": "">=16.0.0""})
     @step
     def use_node(self):
         print(os.system(""node --version""))
         self.next(self.end)
 
     @step
     def end(self):
         pass
 
 
 if __name__ == ""__main__"":
     TestFlow()
 ```
 
 would print an error. This release fixes the issue with the incorrect `PATH`
 configuration.",H4,https://docs.metaflow.org/internals/release-notes#expose-non-pythonic-dependencies-inside-the-conda-environment-on-aws-batch-735,False,830.0,161.0
1067,New Features,,H3,https://docs.metaflow.org/internals/release-notes#new-features,False,0.0,1.0
1068,Introduce size properties for artifacts and logs in metaflow.client ([752](https://github.com/Netflix/metaflow/pull/752)),"This release exposes size properties for artifacts and logs (stderr and stdout) in
 metaflow.client. These properties are relied upon by the Metaflow UI ([open-sourcing
 soon!](https://www.eventbrite.fi/e/netflix-data-science-metaflow-gui-pre-release-meetup-tickets-185523605097)).",H4,https://docs.metaflow.org/internals/release-notes#introduce-size-properties-for-artifacts-and-logs-in-metaflowclient-752,False,281.0,25.0
1069,Expose attempt level task properties ([725](https://github.com/Netflix/metaflow/pull/725)),"In addition to the above mentioned properties, now users of Metaflow can access attempt
 specific Task metadata using the client
 
 ```
 Task('42/start/452', attempt=1)
 ```",H4,https://docs.metaflow.org/internals/release-notes#expose-attempt-level-task-properties-725,False,173.0,25.0
1070,Introduce @kubernetes decorator for launching Metaflow tasks on Kubernetes ([644](https://github.com/Netflix/metaflow/pull/644)),"This release marks the alpha launch of `@kubernetes` decorator that allows farming off
 Metaflow tasks onto Kubernetes. The functionality works in exactly the same manner as
 [`@batch`](/scaling/remote-tasks/introduction) -
 
 ```python
 from metaflow import FlowSpec, step, resources
 
 class BigSum(FlowSpec):
 
     @resources(memory=60000, cpu=1)
     @step
     def start(self):
         import numpy
         import time
         big_matrix = numpy.random.ranf((80000, 80000))
         t = time.time()
         self.sum = numpy.sum(big_matrix)
         self.took = time.time() - t
         self.next(self.end)
 
     @step
     def end(self):
         print(""The sum is %f."" % self.sum)
         print(""Computing it took %dms."" % (self.took * 1000))
 
 if __name__ == '__main__':
     BigSum()
 ```
 
 ```
 python big_sum.py run --with kubernetes
 ```
 
 will run all steps of this workflow on your existing EKS cluster (which can be
 configured with `metaflow configure eks`) and provides all the goodness of Metaflow!
 
 To get started follow [this
 guide](https://docs.google.com/document/d/1L_4Fws1KoGg_dtSTaRlAcREX1F8FPS4ZaYk7eJyu_jA/edit)!
 We would appreciate your early feedback at
 [http://slack.outerbounds.co](htps://slack.outerbounds.co).",H4,https://docs.metaflow.org/internals/release-notes#introduce-kubernetes-decorator-for-launching-metaflow-tasks-on-kubernetes-644,False,1256.0,237.0
1071,"[2.4.0 (Oct 4th, 2021)](https://github.com/Netflix/metaflow/releases/tag/2.4.0)",The Metaflow 2.4.0 release is a minor release and includes a _breaking change_,H2,https://docs.metaflow.org/internals/release-notes#240-oct-4th-2021,False,78.0,13.0
1072,Breaking Changes,,H3,https://docs.metaflow.org/internals/release-notes#breaking-changes,False,0.0,1.0
1073,Change return type of created_at/finished_at in the client ([692](https://github.com/Netflix/metaflow/pull/692)),"Prior to this release, the return type for `created_at` and `finished_at` properties in
 the Client API was a timestamp string. This release changes this to a `datetime` object,
 as the old behavior is considered an unintentional mis-feature (see below for details).
 
 _How to retain the old behavior_
 
 To keep the old behavior, append an explicit string conversion,
 `.strftime('%Y-%m-%dT%H:%M:%SZ')`, to the `created_at` and `finished_at` calls, e.g.
 
 ```
 run.created_at.strftime('%Y-%m-%dT%H:%M:%SZ')
 ```
 
 _Background_
 
 The first versions of Metaflow (internal to Netflix) returned a `datetime` object in all
 calls dealing with timestamps in the Client API to make it easier to perform operations
 between timestamps. Unintentionally, the return type was changed to string in the
 initial open-source release. This release introduces a number of internal changes,
 removing all remaining discrepancies between the legacy version of Metaflow that was
 used inside Netflix and the open-source version.
 
 The timestamp change is the only change affecting the user-facing API. While Metaflow
 continues to make a strong promise of backwards compatibility of user-facing features
 and APIs, the benefits of one-time unification outweigh the cost of this relatively
 minor breaking change.",H4,https://docs.metaflow.org/internals/release-notes#change-return-type-of-created-at-finished-at-in-the-client-692,False,1299.0,187.0
1074,Bug Fixes,,H3,https://docs.metaflow.org/internals/release-notes#bug-fixes,False,0.0,1.0
1075,Better error messages in case of a Conda issue ([706](https://github.com/Netflix/metaflow/pull/706)),"Conda errors printed to `stderr` were not surfaced to the user; this release addresses
 this issue.",H4,https://docs.metaflow.org/internals/release-notes#better-error-messages-in-case-of-a-conda-issue-706,False,99.0,16.0
1076,Fix error message in Metadata service ([712](https://github.com/Netflix/metaflow/pull/712)),"The code responsible for printing error messages from the metadata service had a problem
 that could cause it to be unable to print the correct error message and would instead
 raise another error that obfuscated the initial error. This release addresses this issue
 and errors from the metadata service are now properly printed.",H4,https://docs.metaflow.org/internals/release-notes#fix-error-message-in-metadata-service-712,False,329.0,53.0
1077,New Features,,H3,https://docs.metaflow.org/internals/release-notes#new-features,False,0.0,1.0
1078,S3 retry counts are now configurable ([700](https://github.com/Netflix/metaflow/pull/700)),"This release allows you to set the number of times S3 access are retried (the default is
 7). The relevant environment variable is: `METAFLOW_S3_RETRY_COUNT`.",H4,https://docs.metaflow.org/internals/release-notes#s3-retry-counts-are-now-configurable-700,False,158.0,24.0
1079,New datastore implementation resulting in improved performance ([580](https://github.com/Netflix/metaflow/pull/580)),"The datastore implementation was reworked to make it easier to extend in the future. It
 also now uploads artifacts in parallel to S3 (as opposed to sequentially) which can lead
 to better performance. The changes also contribute to a notable improvement in the speed
 of `resume` which can now start resuming a flow twice as fast as before. Documentation
 can be found [here](https://github.com/Netflix/metaflow/blob/master/docs/datastore.md).",H4,https://docs.metaflow.org/internals/release-notes#new-datastore-implementation-resulting-in-improved-performance-580,False,444.0,63.0
1080,S3 datatools performance improvements ([697](https://github.com/Netflix/metaflow/pull/697)),"The S3 datatools better handles small versus large files by using the `download_file`
 command for larger files and using `get_object` for smaller files to minimize the number
 of calls made to S3.",H4,https://docs.metaflow.org/internals/release-notes#s3-datatools-performance-improvements-697,False,197.0,32.0
1081,"2.3.6 (Sep 8th, 2021)",The Metaflow 2.3.6 release is a patch release.,H2,https://docs.metaflow.org/internals/release-notes#236-sep-8th-2021,False,46.0,8.0
1082,Bug Fixes,,H3,https://docs.metaflow.org/internals/release-notes#bug-fixes,False,0.0,1.0
1083,[Fix recursion error when `METAFLOW_DEFAULT_ENVIRONMENT` is set to `conda`](https://github.com/Netflix/metaflow/releases673),"Prior to this release, setting default execution environment to `conda` through
 `METAFLOW_DEFAULT_ENVIRONMENT` would result in a recursion error.
 
 ```
 METAFLOW_DEFAULT_ENVIRONMENT=conda python flow.py run
 ```
 
 ```
   File ""/Users/savin/Code/metaflow/metaflow/cli.py"", line 868, in start
     if e.TYPE == environment][0](ctx.obj.flow)
   File ""/Users/savin/Code/metaflow/metaflow/plugins/conda/conda_environment.py"", line 27, in __init__
     if e.TYPE == DEFAULT_ENVIRONMENT][0](self.flow)
   File ""/Users/savin/Code/metaflow/metaflow/plugins/conda/conda_environment.py"", line 27, in __init__
     if e.TYPE == DEFAULT_ENVIRONMENT][0](self.flow)
   File ""/Users/savin/Code/metaflow/metaflow/plugins/conda/conda_environment.py"", line 27, in __init__
     if e.TYPE == DEFAULT_ENVIRONMENT][0](self.flow)
   [Previous line repeated 488 more times]
   File ""/Users/savin/Code/metaflow/metaflow/plugins/conda/conda_environment.py"", line 24, in __init__
     from ...plugins import ENVIRONMENTS
 RecursionError: maximum recursion depth exceeded
 ```
 
 This release fixes this bug.",H4,https://docs.metaflow.org/internals/release-notes#fix-recursion-error-when-metaflow-default-environment-is-set-to-conda,False,1083.0,127.0
1084,[Allow dots in `host_volumes` attribute for `@batch` decorator](https://github.com/Netflix/metaflow/releases676),"Dots in volume names - `@batch(host_volumes='/path/with/.dot')` weren't being sanitized
 properly resulting in errors when a Metaflow task launched on AWS Batch. This release
 fixes this bug.",H4,https://docs.metaflow.org/internals/release-notes#allow-dots-in-host-volumes-attribute-for-batch-decorator,False,191.0,26.0
1085,"2.3.5 (Aug 23rd, 2021)",The Metaflow 2.3.5 release is a patch release.,H2,https://docs.metaflow.org/internals/release-notes#235-aug-23rd-2021,False,46.0,8.0
1086,Features,,H3,https://docs.metaflow.org/internals/release-notes#features,False,0.0,1.0
1087,[Enable mounting host volumes in AWS Batch](https://github.com/Netflix/metaflow/issues/441),"With this release, you can now [mount and access instance host
 volumes](https://aws.amazon.com/premiumsupport/knowledge-center/batch-mount-efs/) within
 a Metaflow task running on AWS Batch. To access a host volume, you can add
 `host-volumes` argument to your `@batch` decorator -
 
 ```
 @batch(host_volumes=['/home', '/var/log'])
 ```",H4,https://docs.metaflow.org/internals/release-notes#enable-mounting-host-volumes-in-aws-batch,False,338.0,40.0
1088,Bug Fixes,,H3,https://docs.metaflow.org/internals/release-notes#bug-fixes,False,0.0,1.0
1089,[Fix input values for Parameters of type `list` within a Metaflow Foreach task](https://github.com/Netflix/metaflow/issues/651),"The following flow had a bug where the value for `self.input` was being imputed to
 `None` rather than the dictionary element. This release fixes this issue -
 
 ```python
 from metaflow import FlowSpec, Parameter, step, JSONType
 
 class ForeachFlow(FlowSpec):
     numbers_param = Parameter(
         ""numbers_param"",
         type=JSONType,
         default='[1,2,3]'
     )
 
     @step
     def start(self):
         # This works, and passes each number to the run_number step:
         #
         # self.numbers = self.numbers_param
         # self.next(self.run_number, foreach='numbers')
 
         # But this doesn't:
         self.next(self.run_number, foreach='numbers_param')
 
     @step
     def run_number(self):
         print(f""number is {self.input}"")
         self.next(self.join)
 
     @step
     def join(self, inputs):
         self.next(self.end)
 
     @step
     def end(self):
         pass
 
 if __name__ == '__main__':
     ForeachFlow()
 ```",H4,https://docs.metaflow.org/internals/release-notes#fix-input-values-for-parameters-of-type-list-within-a-metaflow-foreach-task,False,971.0,250.0
1090,"2.3.4 (Aug 11th, 2021)",The Metaflow 2.3.4 release is a patch release.,H2,https://docs.metaflow.org/internals/release-notes#234-aug-11th-2021,False,46.0,8.0
1091,Bug Fixes,,H3,https://docs.metaflow.org/internals/release-notes#bug-fixes,False,0.0,1.0
1092,[Fix execution of `step-functions create` when using an `IncludeFile` parameter](https://github.com/Netflix/metaflow/releases637),"PR [#607](https://github.com/Netflix/metaflow/pull/607) in `Metaflow 2.3.3` introduced a
 bug with `step-functions create` command for `IncludeFile` parameters. This release
 rolls back that PR. A subsequent release will reintroduce a modified version of PR
 [#607](https://github.com/Netflix/metaflow/pull/607).",H4,https://docs.metaflow.org/internals/release-notes#fix-execution-of-step-functions-create-when-using-an-includefile-parameter,False,312.0,32.0
1093,"2.3.3 (Jul 29th, 2021)",The Metaflow 2.3.3 release is a patch release.,H2,https://docs.metaflow.org/internals/release-notes#233-jul-29th-2021,False,46.0,8.0
1094,Features,,H3,https://docs.metaflow.org/internals/release-notes#features,False,0.0,1.0
1095,[Support resource tags for Metaflow's integration with AWS Batch](https://github.com/Netflix/metaflow/releases632),"Metaflow now supports setting [resource tags for AWS Batch
 jobs](https://docs.aws.amazon.com/batch/latest/userguide/using-tags.html) and
 propagating them to the underlying ECS tasks. The following tags are attached to the AWS
 Batch jobs now -",H4,https://docs.metaflow.org/internals/release-notes#support-resource-tags-for-metaflow-s-integration-with-aws-batch,False,245.0,30.0
1096,Bug Fixes,,H3,https://docs.metaflow.org/internals/release-notes#bug-fixes,False,0.0,1.0
1097,[Properly handle `None` as defaults for parameters for AWS Step Functions execution](https://github.com/Netflix/metaflow/releases630),"Prior to this release, a parameter specification like -
 
 ```
 Parameter(name=""test_param"", type=int, default=None)
 ```
 
 will result in an error even though the default has been specified
 
 ```
 Flow failed:
     The value of parameter test_param is ambiguous. It does not have a default and it is not required.
 ```
 
 This release fixes this behavior by allowing the flow to execute as it would locally.",H4,https://docs.metaflow.org/internals/release-notes#properly-handle-none-as-defaults-for-parameters-for-aws-step-functions-execution,False,410.0,71.0
1098,[Fix return value of `IncludeFile` artifacts](https://github.com/Netflix/metaflow/releases607),"The `IncludeFile` parameter would return JSONified metadata about the file rather than
 the file contents when accessed through the `Metaflow Client`. This release fixes that
 behavior by returning instead the file contents, just like any other Metaflow data
 artifact.",H4,https://docs.metaflow.org/internals/release-notes#fix-return-value-of-includefile-artifacts,False,269.0,39.0
1099,"2.3.2 (Jun 29th, 2021)",The Metaflow 2.3.2 release is a minor release.,H2,https://docs.metaflow.org/internals/release-notes#232-jun-29th-2021,False,46.0,8.0
1100,Features,"**`step-functions trigger` command now supports `--run-id-file` option**
 
 Similar to `run` , you can now pass `--run-id-file` option to `step-function trigger`.
 Metaflow then will write the triggered run id to the specified file. This is useful if
 you have additional scripts that require the run id to examine the run or wait until it
 finishes.",H3,https://docs.metaflow.org/internals/release-notes#features,False,350.0,55.0
1101,"2.3.1 (Jun 23rd, 2021)",The Metaflow 2.3.1 release is a minor release.,H2,https://docs.metaflow.org/internals/release-notes#231-jun-23rd-2021,False,46.0,8.0
1102,Features,"[**Performance optimizations for
 `merge_artifacts`**](https://github.com/Netflix/metaflow/releases/tag/2.3.1#556)
 
 Prior to this release, `FlowSpec.merge_artifacts` was loading all of the merged
 artifacts into memory after doing all of the consistency checks with hashes. This
 release now avoids the memory and compute costs of decompressing, de-pickling,
 re-pickling, and recompressing each merged artifact - resulting in improved performance
 of `merge_artifacts`.",H3,https://docs.metaflow.org/internals/release-notes#features,False,472.0,53.0
1103,"2.3.0 (May 27th, 2021)",The Metaflow 2.3.0 release is a minor release.,H2,https://docs.metaflow.org/internals/release-notes#230-may-27th-2021,False,46.0,8.0
1104,Features,,H3,https://docs.metaflow.org/internals/release-notes#features,False,0.0,1.0
1105,[Coordinate larger Metaflow projects with `@project`](../production/coordinating-larger-metaflow-projects),"It's not uncommon for multiple people to work on the same workflow simultaneously.
 Metaflow makes it possible by keeping executions [isolated through independently stored
 artifacts and namespaces](../scaling/tagging). However, by default, [all AWS Step
 Functions deployments](../production/scheduling-metaflow-flows/introduction) are bound
 to the name of the workflow. If multiple people call `step-functions create`
 independently, each deployment will overwrite the previous one. In the early stages of a
 project, this simple model is convenient but as the project grows, it is desirable that
 multiple people can test their own AWS Step Functions deployments without interference.
 Or, as a single developer, you may want to experiment with multiple independent AWS Step
 Functions deployments of their workflow. This release introduces a `@project` decorator
 to address this need. The `@project` decorator is used at the `FlowSpec`-level to bind a
 Flow to a specific project. All flows with the same project name belong to the same
 project.
 
 ```python
 from metaflow import FlowSpec, step, project, current
 
 @project(name='example_project')
 class ProjectFlow(FlowSpec):
 
     @step
     def start(self):
         print('project name:', current.project_name)
         print('project branch:', current.branch_name)
         print('is this a production run?', current.is_production)
         self.next(self.end)
 
     @step
     def end(self):
         pass
 
 if __name__ == '__main__':
     ProjectFlow()
 ```
 
 ```
 python flow.py run
 ```
 
 The flow works exactly as before when executed outside AWS Step Functions and introduces
 `project_name`, `branch_name` & `is_production` in the
 [`current`](../scaling/tagging#accessing-current-ids-in-a-flow) object.
 
 On AWS Step Functions, however, `step-functions create` will create a new workflow
 `example_project.user.username.ProjectFlow` (where `username` is your username) with a
 user-specific [isolated namespace](../scaling/tagging) and a [separate production
 token](../scaling/tagging#production-tokens).
 
 For deploying experimental (test) versions that can run in parallel with production, you
 can deploy custom branches with `--branch`
 
 ```
 python flow.py --branch foo step-functions create
 ```
 
 To deploy a production version, you can deploy with `--production` flag (or pair it up
 with `--branch` if you want to run multiple variants in production)
 
 ```
 python project_flow.py --production step-functions create
 ```
 
 Note that the isolated namespaces offered by `@project` work best when your code is
 designed to respect these boundaries. For instance, when writing results to a table, you
 can use current.branch_name to choose the table to write to or you can disable writes
 outside production by checking current.is_production.",H4,https://docs.metaflow.org/internals/release-notes#coordinate-larger-metaflow-projects-with-project-production-coordinating-larger-metaflow-projects,False,2832.0,421.0
1106,Hyphenated-parameters support in AWS Step Functions,"Prior to this release, hyphenated parameters in AWS Step Functions weren't supported
 through CLI.
 
 ```python
 from metaflow import FlowSpec, Parameter, step
 
 class ParameterFlow(FlowSpec):
     foo_bar = Parameter('foo-bar',
                       help='Learning rate',
                       default=0.01)
 
     @step
     def start(self):
         print('foo_bar is %f' % self.foo_bar)
         self.next(self.end)
 
     @step
     def end(self):
         print('foo_bar is still %f' % self.foo_bar)
 
 if __name__ == '__main__':
     ParameterFlow()
 ```
 
 Now, users can create their flows as usual on AWS Step Functions (with `step-functions
 create`) and trigger the deployed flows through CLI with hyphenated parameters -
 
 ```
 python flow.py step-functions trigger --foo-bar 42
 ```",H4,https://docs.metaflow.org/internals/release-notes#hyphenated-parameters-support-in-aws-step-functions,False,800.0,186.0
1107,State Machine execution history logging for AWS Step Functions,"Metaflow now logs [State Machine execution history in AWS CloudWatch
 Logs](https://docs.aws.amazon.com/step-functions/latest/dg/cw-logs.html) for deployed
 Metaflow flows. You can enable it by specifying `--log-execution-history` flag while
 creating the state machine
 
 ```
 python flow.py step-functions create --log-execution-history
 ```
 
 Note that you would need to set the environment variable (or alternatively in your
 Metaflow config) `METAFLOW_SFN_EXECUTION_LOG_GROUP_ARN` to your AWS CloudWatch Logs Log
 Group ARN to pipe the execution history logs to AWS CloudWatch Logs",H4,https://docs.metaflow.org/internals/release-notes#state-machine-execution-history-logging-for-aws-step-functions,False,587.0,72.0
1108,"2.2.13 (May 19th, 2021)",The Metaflow 2.2.13 release is a minor patch release.,H2,https://docs.metaflow.org/internals/release-notes#2213-may-19th-2021,False,53.0,9.0
1109,Bug Fixes,,H3,https://docs.metaflow.org/internals/release-notes#bug-fixes,False,0.0,1.0
1110,[Handle regression with `@batch` execution on certain docker images](https://github.com/Netflix/metaflow/releases/tag/2.2.13534),"Certain [docker
 images](https://docs.aws.amazon.com/sagemaker/latest/dg/pre-built-containers-frameworks-deep-learning.html)
 override the entrypoint by executing `eval` on the user-supplied command. The `2.2.10`
 release impacted these docker images where we modified the entrypoint to support
 datastore based logging. This release fixes that regression.",H4,https://docs.metaflow.org/internals/release-notes#handle-regression-with-batch-execution-on-certain-docker-images,False,356.0,35.0
1111,"2.2.12 (May 18th, 2021)",The Metaflow 2.2.12 release is a minor patch release.,H2,https://docs.metaflow.org/internals/release-notes#2212-may-18th-2021,False,53.0,9.0
1112,Features,,H3,https://docs.metaflow.org/internals/release-notes#features,False,0.0,1.0
1113,[Add capability to override AWS Step Functions state machine name while deploying flows to AWS Step Functions](https://github.com/Netflix/metaflow/releases/tag/2.2.12532),"Prior to this release, the State Machines created by Metaflow while deploying flows to
 AWS Step Functions had the same name as that of the flow. With this release, Metaflow
 users can now override the name of the State Machine created by passing in a `--name`
 argument : `python flow.py step-functions --name foo create` or `python flow.py
 step-functions --name foo trigger`.",H4,https://docs.metaflow.org/internals/release-notes#add-capability-to-override-aws-step-functions-state-machine-name-while-deploying-flows-to-aws-step-functions,False,378.0,61.0
1114,[Introduce heartbeats for Metaflow flows](https://github.com/Netflix/metaflow/releases/tag/2.2.12333),"Metaflow now registers heartbeats at the run level and the task level for all flow
 executions (with the exception of flows running on AWS Step Functions where only
 task-level heartbeats are captured). This provides the necessary metadata to ascertain
 if a run/task has been lost. Subsequent releases of Metaflow will expose this
 information through the client.",H4,https://docs.metaflow.org/internals/release-notes#introduce-heartbeats-for-metaflow-flows,False,364.0,56.0
1115,Bug Fixes,,H3,https://docs.metaflow.org/internals/release-notes#bug-fixes,False,0.0,1.0
1116,[Handle regression with `Click >=8.0.x`](https://github.com/Netflix/metaflow/releases/tag/2.2.12526),"The latest release of Click (8.0.0) broke certain idempotency assumptions in Metaflow
 which PR [#526](https://github.com/Netflix/metaflow/pull/526) addresses.",H4,https://docs.metaflow.org/internals/release-notes#handle-regression-with-click-80x,False,159.0,16.0
1117,"2.2.11 (Apr 30th, 2021)",The Metaflow 2.2.11 release is a minor patch release.,H2,https://docs.metaflow.org/internals/release-notes#2211-apr-30th-2021,False,53.0,9.0
1118,Bug Fixes,,H3,https://docs.metaflow.org/internals/release-notes#bug-fixes,False,0.0,1.0
1119,Fix regression that broke compatibility with Python 2.7,"`shlex.quote`, introduced in #493, is not compatible with Python 2.7. `pipes.quote` is
 now used for Python 2.7.",H4,https://docs.metaflow.org/internals/release-notes#fix-regression-that-broke-compatibility-with-python-27,False,112.0,17.0
1120,"2.2.10 (Apr 22nd, 2021)",The Metaflow 2.2.10 release is a minor patch release.,H2,https://docs.metaflow.org/internals/release-notes#2210-apr-22nd-2021,False,53.0,9.0
1121,Features,,H3,https://docs.metaflow.org/internals/release-notes#features,False,0.0,1.0
1122,"AWS Logs Group, Region and Stream are now available in metadata for tasks executed on AWS Batch","For tasks that execute on AWS Batch, Metaflow now records the location where the AWS
 Batch instance writes the container logs in AWS Logs. This can be handy in locating the
 logs through the client API -
 
 ```
 Step('Flow/42/a').task.metadata_dict['aws-batch-awslogs-group']
 Step('Flow/42/a').task.metadata_dict['aws-batch-awslogs-region']
 Step('Flow/42/a').task.metadata_dict['aws-batch-awslogs-stream']
 ```",H4,https://docs.metaflow.org/internals/release-notes#aws-logs-group-region-and-stream-are-now-available-in-metadata-for-tasks-executed-on-aws-batch,False,413.0,43.0
1123,Execution logs are now available for all tasks in Metaflow universe,"All Metaflow runtime/task logs are now published via a sidecar process to the datastore.
 The user-visible logs on the console are streamed directly from the datastore. For
 Metaflow's integrations with the cloud (AWS at the moment), the compute tasks logs (AWS
 Batch) are directly written by Metaflow into the datastore (Amazon S3) independent of
 where the flow is launched from (User's laptop or AWS Step Functions). This has multiple
 benefits",H4,https://docs.metaflow.org/internals/release-notes#execution-logs-are-now-available-for-all-tasks-in-metaflow-universe,False,448.0,70.0
1124,Bug Fixes,,H3,https://docs.metaflow.org/internals/release-notes#bug-fixes,False,0.0,1.0
1125,Fix regression with `ping/` endpoint for Metadata service,"Fix a regression introduced in `v2.2.9` where the endpoint responsible for ascertaining
 the version of the deployed Metadata service was erroneously moved to `ping/` from
 `ping`",H4,https://docs.metaflow.org/internals/release-notes#fix-regression-with-ping-endpoint-for-metadata-service,False,179.0,26.0
1126,[Fix the behaviour of `--namespace=` CLI args when executing a flow](https://gitter.im/metaflow_org/community?at=605decca68921b62f48a4190),"`python flow.py run --namespace=` now correctly makes the global namespace visible
 within the flow execution.",H4,https://docs.metaflow.org/internals/release-notes#fix-the-behaviour-of-namespace-cli-args-when-executing-a-flow-https-gitterim-metaflow-org-community-at605decca68921b62f48a4190,False,110.0,15.0
1127,"2.2.9 (Apr 19th, 2021)",The Metaflow 2.2.9 release is a minor patch release.,H2,https://docs.metaflow.org/internals/release-notes#229-apr-19th-2021,False,52.0,9.0
1128,Bugs,,H3,https://docs.metaflow.org/internals/release-notes#bugs,False,0.0,1.0
1129,[Remove pinned pylint dependency](https://gitter.im/metaflow_org/community?at=60622af8940f1d555e277c12),"Pylint dependency was unpinned and made floating. See PR
 [#462](https://github.com/Netflix/metaflow/pull/462).",H4,https://docs.metaflow.org/internals/release-notes#remove-pinned-pylint-dependency-https-gitterim-metaflow-org-community-at60622af8940f1d555e277c12,False,111.0,10.0
1130,[Improve handling of `/` in image parameter for batch](https://gitter.im/metaflow_org/community?at=5f80e21d02e81701b0106c6d),"You are now able to specify docker images of the form `foo/bar/baz:tag` in the batch
 decorator. See PR [#466](https://github.com/Netflix/metaflow/pull/466).",H4,https://docs.metaflow.org/internals/release-notes#improve-handling-of-in-image-parameter-for-batch-https-gitterim-metaflow-org-community-at5f80e21d02e81701b0106c6d,False,157.0,19.0
1131,List custom FlowSpec parameters in the intended order,"The order in which parameters are specified by the user in the FlowSpec is now preserved
 when displaying them with `--help`. See PR
 [#456](https://github.com/Netflix/metaflow/pull/456).",H4,https://docs.metaflow.org/internals/release-notes#list-custom-flowspec-parameters-in-the-intended-order,False,187.0,24.0
1132,"2.2.8 (Mar 15th, 2021)",The Metaflow 2.2.8 release is a minor patch release.,H2,https://docs.metaflow.org/internals/release-notes#228-mar-15th-2021,False,52.0,9.0
1133,Bugs,,H3,https://docs.metaflow.org/internals/release-notes#bugs,False,0.0,1.0
1134,[Fix `@environment` behavior for conflicting attribute values](https://gitter.im/metaflow_org/community?at=604a2bfb44f5a454a46cc7f8),"Metaflow was incorrectly handling environment variables passed through the
 `@environment` decorator in some specific instances. When `@environment` decorator is
 specified over multiple steps, the actual environment that's available to any step is
 the union of attributes of all the `@environment` decorators; which is incorrect
 behavior. For example, in the following workflow -
 
 ```python
 from metaflow import FlowSpec, step, batch, environment
 import os
 class LinearFlow(FlowSpec):
     @environment(vars={'var':os.getenv('var_1')})
     @step
     def start(self):
         print(os.getenv('var'))
         self.next(self.a)
     @environment(vars={'var':os.getenv('var_2')})
     @step
     def a(self):
         print(os.getenv('var'))
         self.next(self.end)
     @step
     def end(self):
         pass
 if __name__ == '__main__':
     LinearFlow()
 ```
 
 ```
 var_1=foo var_2=bar python flow.py run
 ```
 
 will result in
 
 ```
 Metaflow 2.2.7.post10+gitb7d4c48 executing LinearFlow for user:savin
 Validating your flow...
     The graph looks good!
 Running pylint...
     Pylint is happy!
 2021-03-12 20:46:04.161 Workflow starting (run-id 6810):
 2021-03-12 20:46:04.614 [6810/start/86638 (pid 10997)] Task is starting.
 2021-03-12 20:46:06.783 [6810/start/86638 (pid 10997)] foo
 2021-03-12 20:46:07.815 [6810/start/86638 (pid 10997)] Task finished successfully.
 2021-03-12 20:46:08.390 [6810/a/86639 (pid 11003)] Task is starting.
 2021-03-12 20:46:10.649 [6810/a/86639 (pid 11003)] foo
 2021-03-12 20:46:11.550 [6810/a/86639 (pid 11003)] Task finished successfully.
 2021-03-12 20:46:12.145 [6810/end/86640 (pid 11009)] Task is starting.
 2021-03-12 20:46:15.382 [6810/end/86640 (pid 11009)] Task finished successfully.
 2021-03-12 20:46:15.563 Done!
 ```
 
 Note the output for the step `a` which should have been `bar`. PR
 [#452](https://github.com/Netflix/metaflow/pull/452) fixes the issue.",H4,https://docs.metaflow.org/internals/release-notes#fix-environment-behavior-for-conflicting-attribute-values-https-gitterim-metaflow-org-community-at604a2bfb44f5a454a46cc7f8,False,1926.0,291.0
1135,[Fix `environment is not callable` error when using `@environment`](https://gitter.im/metaflow_org/community?at=6048a07d823b6654d296d62d),"Using `@environment` would often result in an error from `pylint` - `E1102: environment
 is not callable (not-callable)`. Users were getting around this issue by launching their
 flows with `--no-pylint`. PR [#451](https://github.com/Netflix/metaflow/pull/451) fixes
 this issue.",H4,https://docs.metaflow.org/internals/release-notes#fix-environment-is-not-callable-error-when-using-environment-https-gitterim-metaflow-org-community-at6048a07d823b6654d296d62d,False,279.0,34.0
1136,"2.2.7 (Feb 8th, 2021)",The Metaflow 2.2.7 release is a minor patch release.,H2,https://docs.metaflow.org/internals/release-notes#227-feb-8th-2021,False,52.0,9.0
1137,Bugs,,H3,https://docs.metaflow.org/internals/release-notes#bugs,False,0.0,1.0
1138,[Handle for-eaches properly for AWS Step Functions workflows running on AWS Fargate](https://gitter.im/metaflow_org/community?at=601f56d955359c58bf28ef1a),"Workflows orchestrated by AWS Step Functions were failing to properly execute `for-each`
 steps on AWS Fargate. The culprit was lack of access to instance metadata for ECS.
 Metaflow instantiates a connection to Amazon DynamoDB to keep track of `for-each`
 cardinality. This connection requires knowledge of the region that the job executes in
 and is made available via [instance
 metadata](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-metadata.html)
 on EC2; which unfortunately is not available on ECS (for AWS Fargate). This fix
 introduces the necessary checks for inferring the region correctly for tasks executing
 on AWS Fargate. Note that after the recent changes to [Amazon S3's consistency
 model](https://aws.amazon.com/blogs/aws/amazon-s3-update-strong-read-after-write-consistency/),
 the Amazon DynamoDB dependency is no longer needed and will be done away in a subsequent
 release. PR: [#436](https://github.com/Netflix/metaflow/pull/436)",H4,https://docs.metaflow.org/internals/release-notes#handle-for-eaches-properly-for-aws-step-functions-workflows-running-on-aws-fargate-https-gitterim-metaflow-org-community-at601f56d955359c58bf28ef1a,False,974.0,118.0
1139,"2.2.6 (Jan 26th, 2021)",The Metaflow 2.2.6 release is a minor patch release.,H2,https://docs.metaflow.org/internals/release-notes#226-jan-26th-2021,False,52.0,9.0
1140,Features,,H3,https://docs.metaflow.org/internals/release-notes#features,False,0.0,1.0
1141,Support AWS Fargate as compute backend for Metaflow tasks launched on AWS Batch,"At [AWS re:invent 2020, AWS announced support for AWS
 Fargate](https://aws.amazon.com/blogs/aws/new-fully-serverless-batch-computing-with-aws-batch-support-for-aws-fargate/)
 as a compute backend (in addition to EC2) for AWS Batch. With this feature, Metaflow
 users can now submit their Metaflow jobs to AWS Batch Job Queues which are connected to
 AWS Fargate Compute Environments as well. By setting the environment variable -
 `METAFLOW_ECS_FARGATE_EXECUTION_ROLE`, users can configure the ecsTaskExecutionRole for
 the AWS Batch container and AWS Fargate agent.",H4,https://docs.metaflow.org/internals/release-notes#support-aws-fargate-as-compute-backend-for-metaflow-tasks-launched-on-aws-batch,False,567.0,68.0
1142,"Support `shared_memory`, `max_swap`, `swappiness` attributes for Metaflow tasks launched on AWS Batch","The `@batch` decorator now supports `shared_memory`, `max_swap`, `swappiness` attributes
 for Metaflow tasks launched on AWS Batch to provide a greater degree of control for
 memory management.",H4,https://docs.metaflow.org/internals/release-notes#support-shared-memory-max-swap-swappiness-attributes-for-metaflow-tasks-launched-on-aws-batch,False,193.0,26.0
1143,Support wider very-wide workflows on top of AWS Step Functions,"The tag `metaflow_version:` and `runtime:` is now available for all packaged executions
 and remote executions as well. This ensures that every run logged by Metaflow will have
 `metaflow_version` and `runtime` system tags available.",H4,https://docs.metaflow.org/internals/release-notes#support-wider-very-wide-workflows-on-top-of-aws-step-functions,False,233.0,33.0
1144,Bug Fixes,,H3,https://docs.metaflow.org/internals/release-notes#bug-fixes,False,0.0,1.0
1145,Assign tags to `Run` objects generated through AWS Step Functions executions,"`Run` objects generated by flows executed on top of AWS Step Functions were missing the
 tags assigned to the flow; even though the tags were correctly persisted to tasks. This
 release fixes and brings inline the tagging behavior as observed with local flow
 executions.",H4,https://docs.metaflow.org/internals/release-notes#assign-tags-to-run-objects-generated-through-aws-step-functions-executions,False,271.0,44.0
1146,Pipe all workflow set-up logs to `stderr`,Execution set-up logs for `@conda` and `IncludeFile` were being piped to `stdout` which,H4,https://docs.metaflow.org/internals/release-notes#pipe-all-workflow-set-up-logs-to-stderr,False,87.0,13.0
1147,made manipulating the output of commands like `python flow.py step-functions create,,H2,https://docs.metaflow.org/internals/release-notes#made-manipulating-the-output-of-commands-like-python-flowpy-step-functions-create,False,0.0,1.0
1148,Handle null assignment to `IncludeFile` properly,"A workflow executed without a required `IncludeFile` parameter would fail when the
 parameter was referenced inside the flow. This release fixes the issue by assigning a
 null value to the parameter in such cases.",H4,https://docs.metaflow.org/internals/release-notes#handle-null-assignment-to-includefile-properly,False,213.0,34.0
1149,"2.2.5 (Nov 11th, 2020)",The Metaflow 2.2.5 release is a minor patch release.,H2,https://docs.metaflow.org/internals/release-notes#225-nov-11th-2020,False,52.0,9.0
1150,Features,,H3,https://docs.metaflow.org/internals/release-notes#features,False,0.0,1.0
1151,Log `metaflow_version:` and `runtime:` tag for all executions,"The tag `metaflow_version:` and `runtime:` is now available for all packaged executions
 and remote executions as well. This ensures that every run logged by Metaflow will have
 `metaflow_version` and `runtime` system tags available.",H4,https://docs.metaflow.org/internals/release-notes#log-metaflow-version-and-runtime-tag-for-all-executions,False,233.0,33.0
1152,Bug Fixes,,H3,https://docs.metaflow.org/internals/release-notes#bug-fixes,False,0.0,1.0
1153,Handle inconsistently cased file system issue when creating @conda environments on macOS for linux-64,"Conda fails to correctly set up environments for linux-64 packages on macOS at times due
 to inconsistently cased filesystems. Environment creation is needed to collect the
 necessary metadata for correctly setting up the conda environment on AWS Batch. This fix
 simply ignores the error-checks that conda throws while setting up the environments on
 macOS when the intended destination is AWS Batch.",H4,https://docs.metaflow.org/internals/release-notes#handle-inconsistently-cased-file-system-issue-when-creating-conda-environments-on-macos-for-linux-64,False,401.0,61.0
1154,"2.2.4 (Oct 28th, 2020)",The Metaflow 2.2.4 release is a minor patch release.,H2,https://docs.metaflow.org/internals/release-notes#224-oct-28th-2020,False,52.0,9.0
1155,Features,,H3,https://docs.metaflow.org/internals/release-notes#features,False,0.0,1.0
1156,Metaflow is now compliant with AWS GovCloud & AWS CN regions,"AWS GovCloud & AWS CN users can now enjoy all the features of Metaflow within their
 region partition with no change on their end. PR: #364",H4,https://docs.metaflow.org/internals/release-notes#metaflow-is-now-compliant-with-aws-govcloud-aws-cn-regions,False,139.0,26.0
1157,Bug Fixes,,H3,https://docs.metaflow.org/internals/release-notes#bug-fixes,False,0.0,1.0
1158,Address a bug with overriding the default value for [IncludeFile](../scaling/datadata-in-local-files),"Metaflow v2.1.0 introduced a bug in [IncludeFile
 functionality](../scaling/data#data-in-local-files) which prevented users from
 overriding the default value specified.",H4,https://docs.metaflow.org/internals/release-notes#address-a-bug-with-overriding-the-default-value-for-includefile-scaling-datadata-in-local-files,False,169.0,17.0
1159,Port AWS region check for AWS DynamoDb from `curl` to `requests`,"Metaflow's AWS Step Functions' integration relies on AWS DynamoDb to manage
 [foreach](../metaflow/basics#foreach) constructs. Metaflow was leveraging `curl` at
 runtime to detect the region for AWS DynamoDb. Some docker images don't have `curl`
 installed by default; moving to `requests` (a metaflow dependency) fixes the issue.",H4,https://docs.metaflow.org/internals/release-notes#port-aws-region-check-for-aws-dynamodb-from-curl-to-requests,False,330.0,44.0
1160,"2.2.3 (Sept 8th, 2020)",The Metaflow 2.2.3 release is a minor patch release.,H2,https://docs.metaflow.org/internals/release-notes#223-sept-8th-2020,False,52.0,9.0
1161,Bug Fixes,,H3,https://docs.metaflow.org/internals/release-notes#bug-fixes,False,0.0,1.0
1162,Fix issue [305](https://github.com/Netflix/metaflow/issues/305) : Default 'help' for parameters was not handled properly,"Fix the issue where default `help` for parameters was not handled properly. Issue
 [#305](https://github.com/Netflix/metaflow/issues/305): flow fails because
 `IncludeFile`'s default value for the `help` argument is None. PR:
 [#318](https://github.com/Netflix/metaflow/pull/318)",H4,https://docs.metaflow.org/internals/release-notes#fix-issue-305,False,279.0,28.0
1163,Pin the conda library versions for Metaflow default dependencies based on the Python version,"The previously pinned library version does not work with python 3.8. Now we have two
 sets of different version combinations which should work for python 2.7, 3.5, 3.6, 3.7,
 and 3.8. PR: [#308](https://github.com/Netflix/metaflow/pull/308)",H4,https://docs.metaflow.org/internals/release-notes#pin-the-conda-library-versions-for-metaflow-default-dependencies-based-on-the-python-version,False,240.0,33.0
1164,Add conda bin path to the PATH environment variable during Metaflow step execution,"Previously the executable installed in conda environment was not visible inside Metaflow
 steps. Fixing this issue by appending conda bin path to the PATH environment variable.
 PR: [#307](https://github.com/Netflix/metaflow/pull/307)
 
 PRs: [#307](https://github.com/Netflix/metaflow/pull/307),
 [#308](https://github.com/Netflix/metaflow/pull/308),
 [#310](https://github.com/Netflix/metaflow/pull/310),
 [#314](https://github.com/Netflix/metaflow/pull/314),
 [#317](https://github.com/Netflix/metaflow/pull/317),
 [#318](https://github.com/Netflix/metaflow/pull/318)",H4,https://docs.metaflow.org/internals/release-notes#add-conda-bin-path-to-the-path-environment-variable-during-metaflow-step-execution,False,570.0,36.0
1165,"2.2.2 (Aug 20th, 2020)",The Metaflow 2.2.2 release is a minor patch release.,H2,https://docs.metaflow.org/internals/release-notes#222-aug-20th-2020,False,52.0,9.0
1166,Bug Fixes,,H3,https://docs.metaflow.org/internals/release-notes#bug-fixes,False,0.0,1.0
1167,Fix a regression with Conda,"Metaflow 2.2.1 included a commit which was merged too early and broke the use of Conda.
 This release reverses this patch.",H4,https://docs.metaflow.org/internals/release-notes#fix-a-regression-with-conda,False,122.0,21.0
1168,Clarify Pandas version needed for Episode 04,"Recent versions of Pandas are not backward compatible with the one used in the tutorial;
 a small comment was added to warn of this fact.",H4,https://docs.metaflow.org/internals/release-notes#clarify-pandas-version-needed-for-episode-04,False,137.0,25.0
1169,Fix an issue with the metadata service,"In some cases, the metadata service would not properly create runs or tasks.
 
 PRs [#296](https://github.com/Netflix/metaflow/pull/296),
 [#297](https://github.com/Netflix/metaflow/pull/297),
 [#298](https://github.com/Netflix/metaflow/pull/298)",H4,https://docs.metaflow.org/internals/release-notes#fix-an-issue-with-the-metadata-service,False,246.0,18.0
1170,"2.2.1 (Aug 17th, 2020)",The Metaflow 2.2.1 release is a minor patch release.,H2,https://docs.metaflow.org/internals/release-notes#221-aug-17th-2020,False,52.0,9.0
1171,Features,,H3,https://docs.metaflow.org/internals/release-notes#features,False,0.0,1.0
1172,Add `include` parameter for `merge_artifacts`,"You can now specify the artifacts to be merged explicitly by the `merge_artifacts`
 method as opposed to just specifying the ones that should _not_ be merged.",H4,https://docs.metaflow.org/internals/release-notes#add-include-parameter-for-merge-artifacts,False,158.0,26.0
1173,Bug Fixes,,H3,https://docs.metaflow.org/internals/release-notes#bug-fixes,False,0.0,1.0
1174,Fix a regression with datatools,"Fixes the regression described in
 [#285](https://github.com/Netflix/metaflow/issues/285).",H4,https://docs.metaflow.org/internals/release-notes#fix-a-regression-with-datatools,False,90.0,6.0
1175,Fix an issue with Conda in certain environments,"In some cases, Conda is installed system wide and the user cannot write to its
 installation directory. This was causing issues when trying to use the Conda
 environment. Fixes [#179](https://github.com/Netflix/metaflow/issues/179).",H4,https://docs.metaflow.org/internals/release-notes#fix-an-issue-with-conda-in-certain-environments,False,232.0,30.0
1176,Fix an issue with the S3 datastore in case of retries,"Retries were not properly handled when uploading artifacts to the S3 datastore. This fix
 addresses this issue.
 
 PRs [#282](https://github.com/Netflix/metaflow/pull/282),
 [#286](https://github.com/Netflix/metaflow/pull/286),
 [#287](https://github.com/Netflix/metaflow/pull/287),
 [#288](https://github.com/Netflix/metaflow/pull/288),
 [#289](https://github.com/Netflix/metaflow/pull/289),
 [#290](https://github.com/Netflix/metaflow/pull/290),
 [#291](https://github.com/Netflix/metaflow/pull/291)",H4,https://docs.metaflow.org/internals/release-notes#fix-an-issue-with-the-s3-datastore-in-case-of-retries,False,501.0,26.0
1177,"2.2.0 (Aug 4th, 2020)","The Metaflow 2.2.0 release is a minor release and introduces [Metaflow's support for R
 lang](../../v/r/).",H2,https://docs.metaflow.org/internals/release-notes#220-aug-4th-2020,False,106.0,15.0
1178,Features,,H3,https://docs.metaflow.org/internals/release-notes#features,False,0.0,1.0
1179,Support for R lang.,"This release provides an [idiomatic API to access Metaflow in R lang](../../v/r/). It
 piggybacks on the Pythonic implementation as the backend providing most of the
 functionality previously accessible to the Python community. With this release, R users
 can structure their code as a metaflow flow. Metaflow will [snapshot the code, data, and
 dependencies](../../v/r/metaflow/basics#the-structure-of-metaflow-code) automatically in
 a content-addressed datastore allowing for [resuming of
 workflows](../../v/r/metaflow/debugging#how-to-debug-failed-flows), [reproducing past
 results, and inspecting anything about the workflow](../../v/r/metaflow/client) e.g. in
 a notebook or RStudio IDE. Additionally, without any changes to their workflows, users
 can now [execute code on AWS Batch and interact with Amazon S3
 seamlessly](../../v/r/metaflow/scaling).
 
 PR [#263](https://github.com/Netflix/metaflow/pull/263) and PR
 [#214](https://github.com/Netflix/metaflow/pull/214)",H4,https://docs.metaflow.org/internals/release-notes#support-for-r-lang,False,981.0,106.0
1180,"2.1.1 (Jul 30th, 2020)",The Metaflow 2.1.1 release is a minor patch release.,H2,https://docs.metaflow.org/internals/release-notes#211-jul-30th-2020,False,52.0,9.0
1181,Bug Fixes,,H3,https://docs.metaflow.org/internals/release-notes#bug-fixes,False,0.0,1.0
1182,Handle race condition for `/step` endpoint of metadata service.,"The `foreach` step in AWS Step Functions launches multiple AWS Batch tasks, each of
 which tries to register the step metadata if it already doesn't exist. This can result
 in a race condition and cause the task to fail. This patch properly handles the 409
 response from the service.
 
 PR [#258](https://github.com/Netflix/metaflow/pull/258) & PR
 [#260](https://github.com/Netflix/metaflow/pull/260)",H4,https://docs.metaflow.org/internals/release-notes#handle-race-condition-for-step-endpoint-of-metadata-service,False,402.0,55.0
1183,"2.1.0 (Jul 29th, 2020)","The Metaflow 2.1.0 release is a minor release and introduces [Metaflow's integration
 with AWS Step Functions](../production/scheduling-metaflow-flows/introduction).",H2,https://docs.metaflow.org/internals/release-notes#210-jul-29th-2020,False,165.0,16.0
1184,Features,,H3,https://docs.metaflow.org/internals/release-notes#features,False,0.0,1.0
1185,Add capability to schedule Metaflow flows with AWS Step Functions.,"Netflix uses an [internal DAG
 scheduler](https://medium.com/@NetflixTechBlog/unbundling-data-science-workflows-with-metaflow-and-aws-step-functions-d454780c6280)
 to orchestrate most machine learning and ETL pipelines in production. Metaflow users at
 Netflix can seamlessly deploy and schedule their flows to this scheduler. Now, with this
 release, we are introducing a similar integration with [AWS Step
 Functions](https://aws.amazon.com/step-functions/) where Metaflow users can [easily
 deploy & schedule their flows](../production/scheduling-metaflow-flows/introduction) by
 simply executing
 
 ```
 python myflow.py step-functions create
 ```
 
 which will create an AWS Step Functions state machine for them. With this feature,
 Metaflow users can now enjoy all the features of Metaflow along with a highly available,
 scalable, maintenance-free production scheduler without any changes in their existing
 code.
 
 We are also introducing a new decorator -
 [`@schedule`](../production/scheduling-metaflow-flows/introduction#scheduling-a-flow),
 which allows Metaflow users to instrument time-based triggers via [Amazon
 EventBridge](https://aws.amazon.com/eventbridge/) for their flows deployed on AWS Step
 Functions.
 
 With this integration, Metaflow users can [inspect](../metaflow/client) their flows
 deployed on AWS Step Functions as before and [debug and
 reproduce](../metaflow/debugging#reproducing-production-issues-locally) results from AWS
 Step Functions on their local laptop or within a notebook.
 
 [Documentation](../production/scheduling-metaflow-flows/introduction)\
 [Launch Blog
 Post](https://medium.com/@NetflixTechBlog/unbundling-data-science-workflows-with-metaflow-and-aws-step-functions-d454780c6280)
 
 PR [#211](https://github.com/Netflix/metaflow/pull/211) addresses Issue
 [#2](https://github.com/Netflix/metaflow/issues/2).",H4,https://docs.metaflow.org/internals/release-notes#add-capability-to-schedule-metaflow-flows-with-aws-step-functions,False,1867.0,179.0
1186,Improvements,,H3,https://docs.metaflow.org/internals/release-notes#improvements,False,0.0,1.0
1187,Fix log indenting in Metaflow.,"Metaflow was inadvertently removing leading whitespace from user-visible logs on the
 console. Now Metaflow presents user-visible logs with the correct formatting.
 
 PR [#244](https://github.com/Netflix/metaflow/pull/244) fixed issue
 [#223](https://github.com/Netflix/metaflow/issues/223).",H4,https://docs.metaflow.org/internals/release-notes#fix-log-indenting-in-metaflow,False,291.0,27.0
1188,Throw exception properly if fetching code package from Amazon S3 on AWS Batch fails.,"Due to malformed permissions, AWS Batch might not be able to fetch the code package from
 Amazon S3 for user code execution. In such scenarios, it wasn't apparent to the user,
 where the code package was being pulled from, making triaging any permission issue a bit
 difficult. Now, the Amazon S3 file location is part of the exception stack trace.
 
 PR [#243](https://github.com/Netflix/metaflow/pull/243) fixed issue
 [#232](https://github.com/Netflix/metaflow/issues/232).",H4,https://docs.metaflow.org/internals/release-notes#throw-exception-properly-if-fetching-code-package-from-amazon-s3-on-aws-batch-fails,False,476.0,66.0
1189,Remove millisecond information from timestamps returned by Metaflow client.,"Metaflow uses `time` to store the `created_at` and `finished_at` information for the
 `Run` object returned by Metaflow client. `time` unfortunately does not support the
 [`%f` directive](https://docs.python.org/3/library/time.html#time.strftime), making it
 difficult to parse these fields by `datetime` or `time`. Since Metaflow doesn't expose
 timings at millisecond grain, this PR drops the `%f` directive.
 
 PR [#227](https://github.com/Netflix/metaflow/pull/227) fixed issue
 [#224](https://github.com/Netflix/metaflow/issues/224).",H4,https://docs.metaflow.org/internals/release-notes#remove-millisecond-information-from-timestamps-returned-by-metaflow-client,False,538.0,57.0
1190,Handle CloudWatchLogs resource creation delay gracefully.,"When launching jobs on AWS Batch, the CloudWatchLogStream might not be immediately
 created (and may never be created if say we fail to pull the docker image for any reason
 whatsoever). Metaflow will now simply retry again next time.
 
 PR [#209](https://github.com/Netflix/metaflow/pull/209).",H4,https://docs.metaflow.org/internals/release-notes#handle-cloudwatchlogs-resource-creation-delay-gracefully,False,294.0,42.0
1191,"2.0.5 (Apr 30th, 2020)",The Metaflow 2.0.5 release is a minor patch release.,H2,https://docs.metaflow.org/internals/release-notes#205-apr-30th-2020,False,52.0,9.0
1192,"Improvements <a href=""2-0-5-improvements"" id=""2-0-5-improvements""></a>",,H3,https://docs.metaflow.org/internals/release-notes#improvements-a-href-2-0-5-improvements-id-2-0-5-improvements-a,False,0.0,1.0
1193,Fix logging of prefixes in datatools.S3.\_read_many_files,"Avoid a cryptic error message when `datatools.S3._read_many_files` is unsuccessful by
 converting `prefixes` from a generator to a list.",H4,https://docs.metaflow.org/internals/release-notes#fix-logging-of-prefixes-in-datatoolss3-read-many-files,False,136.0,18.0
1194,Increase retry count for AWS Batch logs streaming.,"Modify the retry behavior for log fetching on AWS Batch by adding jitters to exponential
 backoffs as well as reset the retry counter for every successful request.
 
 Additionally, fail the Metaflow task when we fail to stream the task logs back to the
 user's terminal even if AWS Batch task succeeds.",H4,https://docs.metaflow.org/internals/release-notes#increase-retry-count-for-aws-batch-logs-streaming,False,302.0,52.0
1195,Upper-bound pylint version to < 2.5.0.,"`pylint` version `2.5.0` would mark Metaflow's `self.next()` syntax as an error. As a
 result, `python helloworld.py run` would fail at the pylint check step unless we run
 with `--no-pylint`. This version upper-bound is supposed to automatically downgrade
 `pylint` during `metaflow` installation if `pylint==2.5.0` has been installed.",H4,https://docs.metaflow.org/internals/release-notes#upper-bound-pylint-version-to-250,False,336.0,46.0
1196,"2.0.4 (Apr 28th, 2020)",The Metaflow 2.0.4 release is a minor patch release.,H2,https://docs.metaflow.org/internals/release-notes#204-apr-28th-2020,False,52.0,9.0
1197,"Improvements <a href=""2-0-4-improvements"" id=""2-0-4-improvements""></a>",,H3,https://docs.metaflow.org/internals/release-notes#improvements-a-href-2-0-4-improvements-id-2-0-4-improvements-a,False,0.0,1.0
1198,Expose `retry_count` in `Current`,"You can now use the [`current`](../scaling/tagging#accessing-current-ids-in-a-flow)
 singleton to access the `retry_count` of your task. The first attempt of the task will
 have `retry_count` as 0 and subsequent retries will increment the `retry_count`. As an
 example:
 
 ```python
 @retry
 @step
 def my_step(self):
     from metaflow import current
     print(""retry_count: %s"" % current.retry_count)
     self.next(self.a)
 ```",H4,https://docs.metaflow.org/internals/release-notes#expose-retry-count-in-current,False,431.0,63.0
1199,Mute superfluous `ThrottleExceptions` in AWS Batch job logs,"The AWS Logs API for `get_log_events` has a global hard limit on 10 requests per sec.
 While we have retry logic in place to respect this limit, some of the
 `ThrottleExceptions` usually end up in the job logs causing confusion to the end-user.
 This release addresses this issue (also documented in #184).",H4,https://docs.metaflow.org/internals/release-notes#mute-superfluous-throttleexceptions-in-aws-batch-job-logs,False,306.0,52.0
1200,"Bug Fixes <a href=""2-0-4-bug-fixes"" id=""2-0-4-bug-fixes""></a>",,H3,https://docs.metaflow.org/internals/release-notes#bug-fixes-a-href-2-0-4-bug-fixes-id-2-0-4-bug-fixes-a,False,0.0,1.0
1201,Set proper thresholds for retrying `DescribeJobs` API for AWS Batch,"The AWS Batch API for `describe_jobs` throws `ThrottleExceptions` when managing a flow
 with a very wide `for-each` step. This release adds retry behavior with backoffs to add
 proper resiliency (addresses #138).",H4,https://docs.metaflow.org/internals/release-notes#set-proper-thresholds-for-retrying-describejobs-api-for-aws-batch,False,212.0,31.0
1202,Explicitly override `PYTHONNOUSERSITE` for `conda` environments,"In certain user environments, to properly isolate `conda` environments, we have to
 explicitly override `PYTHONNOUSERSITE` rather than simply relying on `python -s`
 (addresses #178).",H4,https://docs.metaflow.org/internals/release-notes#explicitly-override-pythonnousersite-for-conda-environments,False,183.0,24.0
1203,Preempt AWS Batch job log collection when the job fails to get into a `RUNNING` state,"Fixes a bug where if the AWS Batch job crashes before entering the `RUNNING` state
 (often due to incorrect IAM perms), the previous log collection behavior would fail to
 print the correct error message making it harder to debug the issue (addresses #185).",H4,https://docs.metaflow.org/internals/release-notes#preempt-aws-batch-job-log-collection-when-the-job-fails-to-get-into-a-running-state,False,257.0,43.0
1204,"2.0.3 (Mar 6th, 2020)",The Metaflow 2.0.3 release is a minor patch release.,H2,https://docs.metaflow.org/internals/release-notes#203-mar-6th-2020,False,52.0,9.0
1205,Improvements,,H3,https://docs.metaflow.org/internals/release-notes#improvements,False,0.0,1.0
1206,Parameter listing,"You can now use the `current` singleton (documented
 [here](../scaling/tagging#accessing-current-ids-in-a-flow)) to access the names of the
 parameters passed into your flow. As an example:
 
 ```python
 for var in current.parameter_names:
     print(""Parameter %s has value %s"" % (var, getattr(self, var))
 ```
 
 This addresses [#137](https://github.com/Netflix/metaflow/issues/137).",H4,https://docs.metaflow.org/internals/release-notes#parameter-listing,False,385.0,47.0
1207,Usability improvements,"A few issues were addressed to improve the usability of Metaflow. In particular, `show`
 now properly respects indentation making the description of steps and flows more
 readable. This addresses [#92](https://github.com/Netflix/metaflow/issues/92).
 Superfluous print messages were also suppressed when executing on AWS batch with the
 local metadata provider ([#152](https://github.com/Netflix/metaflow/pull/152)).",H4,https://docs.metaflow.org/internals/release-notes#usability-improvements,False,416.0,47.0
1208,Performance,,H3,https://docs.metaflow.org/internals/release-notes#performance,False,0.0,1.0
1209,Conda,"A smaller, newer and standalone Conda installer is now used resulting in faster and more
 reliable Conda bootstrapping ([#123](https://github.com/Netflix/metaflow/pull/123)).",H4,https://docs.metaflow.org/internals/release-notes#conda,False,174.0,19.0
1210,Bug Fixes,,H3,https://docs.metaflow.org/internals/release-notes#bug-fixes,False,0.0,1.0
1211,Executing on AWS Batch,"We now check for the command line `--datastore-root` prior to using the environment
 variable `METAFLOW_DATASTORE_SYSROOT_S3` when determining the S3 root
 ([#134](https://github.com/Netflix/metaflow/pull/134)). This release also fixes an issue
 where using the local Metadata provider with AWS batch resulted in incorrect directory
 structure in the `.metaflow` directory
 ([#141](https://github.com/Netflix/metaflow/pull/141)).",H4,https://docs.metaflow.org/internals/release-notes#executing-on-aws-batch,False,429.0,46.0
1212,"2.0.2 (Feb 11th, 2020)",Bug Fixes,H2,https://docs.metaflow.org/internals/release-notes#202-feb-11th-2020,False,9.0,2.0
1213,"2.0.1 (Dec 16th, 2019)",Enhancements,H2,https://docs.metaflow.org/internals/release-notes#201-dec-16th-2019,False,12.0,1.0
1214,"2.0.0 (Dec 3rd, 2019)",,H2,https://docs.metaflow.org/internals/release-notes#200-dec-3rd-2019,False,0.0,1.0
1215,Hello World!,,H4,https://docs.metaflow.org/internals/release-notes#hello-world,False,0.0,1.0
1216,Releases pre-2.0.0 were internal to Netflix,,H2,https://docs.metaflow.org/internals/release-notes#releases-pre-200-were-internal-to-netflix,False,0.0,1.0
1217,Technical Overview,"Make sure you have read [Basics of Metaflow](../metaflow/basics) before diving into
 technical details below. You can find more technical details at the infrastructure level
 in [Administrator's Guide to Metaflow](https://outerbounds.com/docs/admin). This
 document focuses on the Metaflow codebase.
 
 We wanted to build a data science platform that can make data science code usable,
 scalable, reproducible, and production-ready, as described in the [Why
 Metaflow](../introduction/why-metaflow) section. There are many ways to achieve these
 high-level goals. We took an approach designed around the following four core functions:
 
 1. Provide a highly usable API for structuring the code as a workflow, i.e. as a
    directed graph of steps (**usability**).
 2. Persist an immutable snapshot of data, code, and external dependencies required to
    execute each step (**reproducibility**).
 3. Facilitate execution of the steps in various environments, from development to
    production (**scalability**, **production-readiness**).
 4. Record metadata about previous executions and make them easily accessible
    (**usability**, **reproducibility**).
 
 This document gives an overview of how the core functionality is implemented.",H1,https://docs.metaflow.org/internals/technical-overview#technical-overview,False,1239.0,175.0
1218,Architecture,"Here is a high-level architecture diagram of Metaflow:
 
 ![](/assets/Untitled_presentation.png)
 
 Below, we will describe the components in detail. To highlight the time-dimension which
 is missing from the diagram, we group the descriptions by the following phases in the
 development lifecycle:
 
 1. Development-time, i.e. when the code gets written.
 2. Runtime, i.e. when the code gets run.
 3. Result-time, i.e. when the results of the run get used.
 
 Every component includes links to source files where the functionality is implemented.",H2,https://docs.metaflow.org/internals/technical-overview#architecture,False,547.0,82.0
1219,Development-Time Components,"The core development-time concept in Metaflow is a _flow_. It represents the business
 logic of what needs to be computed.
 
 How to intertwine the business logic with the framework in the most **usable** manner is
 a central design concern of Metaflow. We want to encourage the user to structure the
 code in a way that enables **reproducibility** and **scalability**.
 
 In contrast, we would like to minimize concerns related to production-readiness during
 development time. Optimally, the user can write idiomatic Python code focusing on the
 logic itself and the guard rails of the framework will automatically make the code
 production-ready.",H2,https://docs.metaflow.org/internals/technical-overview#development-time-components,False,649.0,101.0
1220,**Flow**,"A flow is the smallest unit of computation that can be scheduled for execution.
 Typically, a flow defines a workflow that pulls data from an external source as input,
 processes it in several steps, and produces output data.
 
 User implements a flow by subclassing `FlowSpec` and implementing steps as methods.
 Besides steps, a flow can define other attributes relevant for scheduling, such as
 parameters and data triggers.",H3,https://docs.metaflow.org/internals/technical-overview#flow,False,427.0,68.0
1221,Graph,"Metaflow infers a directed (typically acyclic) graph based on the transitions between
 step functions.
 
 Metaflow requires the transitions to be defined so that the graph can be parsed from the
 source code of the flow statically. This makes it possible to translate the graph for
 execution by runtimes that support only statically defined graphs, such as Meson.",H3,https://docs.metaflow.org/internals/technical-overview#graph,False,364.0,58.0
1222,Step,"A step is the smallest resumable unit of computation. It is implemented by the user as a
 method that is decorated with the `@step` decorator in a flow class.
 
 A step is [a checkpoint](https://en.wikipedia.org/wiki/Application_checkpointing).
 Metaflow takes a snapshot of the data produced by a step which in turn is used as input
 to the subsequent steps. Hence, if a step fails, it can be resumed without rerunning the
 preceding steps.
 
 Being able to resume execution is a powerful feature. It would be convenient to be able
 to resume execution at any arbitrary line of code. The main reason why checkpointing is
 done at the step level instead of line level is the overhead of saving state. The user
 is encouraged to keep the steps small but not so small that the overhead becomes
 noticeable.",H3,https://docs.metaflow.org/internals/technical-overview#step,False,804.0,136.0
1223,Decorators,"The behavior of a step can be modified with decorators. Tags are the main mechanism for
 extending Metaflow. For instance, a decorator can catch exceptions, implement a timeout,
 or define resource requirements for a step.
 
 A step may have arbitrary many decorators, implemented as Python decorators.",H3,https://docs.metaflow.org/internals/technical-overview#decorators,False,302.0,47.0
1224,Step Code,"Step code is the body of a step. It implements the actual business logic of flow.
 
 It is possible to implement various language bindings, e.g. R, for Metaflow so that only
 the language of the step code is changed while all the core functionality, implemented
 in Python, stays intact.
 
 All instance variables, e.g. `self.x`, used in the step code become _data artifacts_
 that are persisted automatically. Stack variables, e.g. `x`, are not persisted. This
 dichotomy allows the user to control the overhead of checkpointing by explicitly
 choosing between persistent vs. non-persistent variables in the step code.",H3,https://docs.metaflow.org/internals/technical-overview#step-code,False,619.0,98.0
1225,Runtime Components,"The core runtime concept in Metaflow is a _run_, that is, an execution of a user-defined
 flow. A run happens when the user executes `python myflow.py run` on the command line.
 
 A key design decision of Metaflow is to make the framework runtime-agnostic. The same
 code should be runnable in various environments, such as on a laptop during development
 or on a **production-ready** workflow orchestrator during production.
 
 Similarly, we want to provide seamless **scalability** by allowing the same code run on
 a laptop in parallel over multiple processes or in the cloud over multiple batch jobs.",H2,https://docs.metaflow.org/internals/technical-overview#runtime-components,False,604.0,98.0
1226,**Task**,"The runtime counterpart of a step is a _task_. In runtime, a normal step spawns one task
 for execution. A foreach split step may spawn multiple tasks which are identified by a
 unique _foreach stack_.",H3,https://docs.metaflow.org/internals/technical-overview#task,False,201.0,35.0
1227,Code Package,"In order to be able to **reproduce** the results of a run, we need to snapshot the code
 that was run.
 
 Code package is an immutable snapshot of the relevant code in the working directory,
 stored in the datastore, at the time when the run was started. A convenient side effect
 of the snapshot is that it also works as a code distribution mechanism for runs that
 happen in the cloud.",H3,https://docs.metaflow.org/internals/technical-overview#code-package,False,387.0,72.0
1228,**Environment**,"Unfortunately, just snapshotting the working directory of the flow code is not
 sufficient for reproducibility. The code often depends on external libraries which also
 need to be included in the snapshot.
 
 The concept of an _environment_ is closely related to code packages. The environment
 encapsulates both the flow code and its external dependencies, so that the exact
 execution environment can be reproduced on a remote system accurately.",H3,https://docs.metaflow.org/internals/technical-overview#environment,False,447.0,68.0
1229,Runtime,"A run of a flow is executed by executing tasks defined by steps in a topological order.
 It is the job of a runtime to orchestrate this execution. A better name for ""runtime""
 might be a scheduler.
 
 For quick local iterations, Metaflow comes with a built-in runtime which executes tasks
 as separate processes. However, this is not intended as a production-grade scheduler.
 
 For production runs, one should use a runtime that supports retries, error reporting,
 logging, is highly available, scalable, and preferably comes with a user-friendly UI. At
 Netflix,
 [Meson](https://medium.com/netflix-techblog/meson-workflow-orchestration-for-netflix-recommendations-fc932625c1d9)
 is such a runtime. It is well-supported by Metaflow.
 
 A key feature of Metaflow is that it is agnostic of the runtime. The same code can be
 executed both with the local runtime and with production runtime, which enables a rapid
 development-deploy-debug cycle.",H3,https://docs.metaflow.org/internals/technical-overview#runtime,False,945.0,136.0
1230,Datastore,"Metaflow requires an object store where both code snapshots and data artifacts can be
 persisted. This data store should be accessible by all environments where Metaflow code
 is executed. The AWS S3 is a perfect solution for this need. Secondarily, Metaflow
 supports using a local disk as a data store, which is mainly useful during the
 development of Metaflow itself.
 
 An important feature of Metaflow is that the data store is used as a content-addressed
 storage. Both code and data are identified by a hash of their contents, similar to Git,
 so equal copies of data are deduplicated automatically. Note that this deduplication is
 limited in scope however; data across different flows will not be deduplicated.",H3,https://docs.metaflow.org/internals/technical-overview#datastore,False,720.0,117.0
1231,Metadata Provider,"A centralized Metadata Provider keeps track of runs. Strictly speaking, this
 functionality is not required by Metaflow, but it makes the system much more **usable.**
 The service also helps to make data artifacts and other metadata about runs more
 discoverable during result-time, as explained below.",H3,https://docs.metaflow.org/internals/technical-overview#metadata-provider,False,302.0,45.0
1232,Result-time Components,"Flows are defined and run for their results. Metaflow supports a number of different
 ways to consume outputs of runs: Results can be written to Hive tables for consumption
 by downstream systems and dashboards, they can be accessed in a notebook for further
 analysis, or in a hosted web service (this last functionality is not yet available in
 Open Source).",H2,https://docs.metaflow.org/internals/technical-overview#result-time-components,False,360.0,60.0
1233,Metaflow Client,"Metaflow provides a highly **usable** Python API to access results of previous runs,
 called `metaflow.client`. A typical way to use `metaflow.client` is to access data
 artifacts of past runs in a Jupyter notebook. It is extremely convenient to be able to
 examine the internal state of production runs or perform further ad-hoc analysis of the
 results in a notebook.",H3,https://docs.metaflow.org/internals/technical-overview#metaflow-client,False,369.0,59.0
1234,Testing Philosophy,"Watch this talk for motivation: [Autonomous Testing and the Future of Software
 Development by Will Wilson](https://www.youtube.com/watch?v=fFSPwJFXVlw).",H1,https://docs.metaflow.org/internals/testing-philosophy#testing-philosophy,False,153.0,16.0
1235,Metaflow Test Suite,"The integration test harness for the core Metaflow at `test/core`, generates and
 executes synthetic Metaflow flows, exercising all aspects of Metaflow. The test suite is
 executed using [tox](http://tox.readthedocs.io) as configured in `tox.ini`. You can run
 the tests by hand using `pytest` or `run_tests.py` as described below.
 
 What happens when you execute `python helloworld.py run`? The execution involves
 multiple layers of the Metaflow stack. The stack looks like following, starting from the
 most fundamental layer all the way to the user interface:
 
 1. Python interpreter \(`python2`, `python3`\)
 2. Metaflow core \(`task.py`, `runtime.py`, `datastore`, etc.\)
 3. Metaflow plugins \(`@timeout`, `@catch`, `metadata.py` etc.\)
 4. User-defined graph
 5. User-defined step functions
 6. User interface \(`cli.py`, `metaflow.client`\)
 
 We could write unit tests for functions in the layers 2, 3, and 6, which would capture
 some bugs. However, a much larger superset of bugs is caused by unintended interactions
 across the layers. For instance, exceptions caught by the `@catch` tag \(3\) inside a
 deeply nested foreach graph \(4\) might not be returned correctly in the client API
 \(6\) when using Python 3 \(1\).
 
 The integration test harness included in the `core` directory tries to surface bugs like
 this by generating test cases automatically using _specifications_ provided by the
 developer.",H2,https://docs.metaflow.org/internals/testing-philosophy#metaflow-test-suite,False,1424.0,207.0
1236,Specifications,"The test harness allows you to customize behavior in four ways that correspond to the
 layers above:
 
 1. You define the execution environment, including environment variables, the version of
    the Python interpreter, and the type of datastore used as _contexts_ in
    `contexts.json` \(layers 1 and 2\).
 2. You define the step functions, the decorators used, and the expected results as
    `MetaflowTest` templates, stored in the `tests` directory \(layers 3 and 5\).
 3. You define various graphs that match the step functions as simple JSON descriptions
    of the graph structure, stored in the `graphs` directory \(layer 4\).
 4. You define various ways to check the results that correspond to the different user
    interfaces of Metaflow as `MetaflowCheck` classes, stored in the `metaflow_test`
    directory \(layer 6\). You can customize which checkers get used in which contexts in
    `context.json`.
 
 The test harness takes all `contexts`, `graphs`, `tests`, and `checkers` and generates a
 test flow for every combination of them, unless you explicitly set constraints on what
 combinations are allowed. The test flows are then executed, optionally in parallel, and
 results are collected and summarized.",H2,https://docs.metaflow.org/internals/testing-philosophy#specifications,False,1226.0,204.0
1237,**Contexts**,"Contexts are defined in `contexts.json`. The file should be pretty self-explanatory.
 Most likely you do not need to edit the file unless you are adding tests for a new
 command-line argument.
 
 Note that some contexts have `disabled: true`. These contexts are not executed by
 default when tests are run by a CI system. You can enable them on the command line for
 local testing, as shown below.",H3,https://docs.metaflow.org/internals/testing-philosophy#contexts,False,397.0,68.0
1238,**Tests**,"Take a look at `tests/basic_artifact.py`. This test verifies that artifacts defined in
 the first step are available in all steps downstream. You can use this simple test as a
 template for new tests.
 
 Your test class should derive from `MetaflowTest`. The class variable `PRIORITY` denotes
 how fundamental the exercised functionality is to Metaflow. The tests are executed in
 the ascending order of priority, to make sure that foundations are solid before
 proceeding to more sophisticated cases.
 
 The step functions are decorated with the `@steps` decorator. Note that in contrast to
 normal Metaflow flows, these functions can be applied to multiple steps in a graph. A
 core idea behind this test harness is to decouple graphs from step functions, so various
 combinations can be tested automatically. Hence, you need to provide step functions that
 can be applied to various step types.
 
 The `@steps` decorator takes two arguments. The first argument is an integer that
 defines the order of precedence between multiple `steps` functions, in case multiple
 step function templates match. A typical pattern is to provide a specific function for a
 specific step type, such as joins and give it a precedence of `0`. Then another
 catch-all can be defined with `@steps(2, ['all'])`. As the result, the special function
 is applied to joins and the catch-all function for all other steps.
 
 The second argument gives a list of _qualifiers_ specifying which types of steps this
 function can be applied to. There is a set of built-in qualifiers: `all`, `start`,
 `end`, `join`, `linear` which match to the corresponding step types. In addition to
 these built-in qualifiers, graphs can specify any custom qualifiers.
 
 By specifying `required=True` as a keyword argument to `@steps`, you can require that a
 certain step function needs to be used in combination with a graph to produce a valid
 test case. By creating a custom qualifier and setting `required=True` you can control
 how tests get matched to graphs.
 
 In general, it is beneficial to write test cases that do not specify overly restrictive
 qualifiers and `required=True`. This way you cast a wide net to catch bugs with many
 generated test cases. However, if the test is slow to execute and/or does not benefit
 from a large number of matching graphs, it is a good idea to make it more specific.",H3,https://docs.metaflow.org/internals/testing-philosophy#tests,False,2373.0,387.0
1239,**Assertions**,"The test case is not very useful unless it verifies its results. There are two ways to
 assert that the test behaves as expected.
 
 You can use a function `assert_equals(expected, got)` inside step functions to confirm
 that data inside the step functions is valid. Secondly, you can define a method
 `check_results(self, flow, checker)` in your test class, which verifies the stored
 results after the flow has been executed successfully.
 
 Use
 
 ```text
 checker.assert_artifact(step_name, artifact_name, expected_value)
 ```
 
 to assert that steps contain the expected data artifacts.
 
 Take a look at existing test cases in the `tests` directory to get an idea how this
 works in practice.",H4,https://docs.metaflow.org/internals/testing-philosophy#assertions,False,698.0,109.0
1240,**Graphs**,"Graphs are simple JSON representations of directed graphs. They list every step in a
 graph and transitions between them. Every step can have an optional list of custom
 qualifiers, as described above.
 
 You can take a look at the existing graphs in the `graphs` directory to get an idea of
 the syntax.",H3,https://docs.metaflow.org/internals/testing-philosophy#graphs,False,304.0,53.0
1241,**Checkers**,"Currently, the test harness exercises two types of user interfaces: The command-line
 interface, defined in `cli_check.py`, and the Python API, defined in `mli_check.py`.
 
 Currently, you can use these checkers to assert values of data artifacts or log output.
 If you want to add tests for new types of functionality in the CLI and/or the Python
 API, you should add a new method in the `MetaflowCheck` base class and corresponding
 implementations in `mli_check.py` and `cli_check.py`. If certain functionality is only
 available in one of the interfaces, you can provide a stub implementation returning
 `True` in the other checker class.",H3,https://docs.metaflow.org/internals/testing-philosophy#checkers,False,642.0,99.0
1242,Usage,"The test harness is executed by running `run_tests.py`. By default, it executes all
 valid combinations of contexts, tests, graphs, and checkers. This mode is suitable for
 automated tests run by a CI system.
 
 When testing locally, it is recommended to run the test suite as follows:
 
 ```text
 cd metaflow/test/core
 PYTHONPATH=`pwd`/../../ python run_tests.py --debug --contexts dev-local
 ```
 
 This uses only the `dev_local` context, which does not depend on any over-the-network
 communication like `--metadata=service` or `--datastore=s3`. The `--debug` flag makes
 the harness fail fast when the first test case fails. The default mode is to run all
 test cases and summarize all failures in the end.
 
 You can run a single test case as follows:
 
 ```text
 cd metaflow/test/core
 PYTHONPATH=`pwd`/../../ python run_tests.py --debug --contexts dev-local --graphs single-linear-step --tests BasicArtifactTest
 ```
 
 This chooses a single context, a single graph, and a single test. If you are developing
 a new test, this is the fastest way to test the test.",H2,https://docs.metaflow.org/internals/testing-philosophy#usage,False,1070.0,161.0
1243,Coverage report,"The test harness uses the `coverage` package in Python to produce a test coverage
 report. By default, you can find a comprehensive test coverage report in the `coverage`
 directory after the test harness has finished.
 
 After you have developed a new feature in Metaflow, use the line-by-line coverage report
 to confirm that all lines related the new feature are touched by the tests.",H3,https://docs.metaflow.org/internals/testing-philosophy#coverage-report,False,387.0,64.0
1244,Contributing to Metaflow,"First off, thanks for taking the time!
 
 If you are interested in contributing to Metaflow, we'd love to hear from you! Drop us a
 line in our [chatroom](http://chat.metaflow.org/).",H1,https://docs.metaflow.org/internals/contributing#contributing-to-metaflow,False,182.0,29.0
1245,Contributing Code and Issues,"We are proud of our philosophy and human-centric development style, which means that we
 value thoughtfulness and a polished user experience more than the number of features or
 lines of code.
 
 When Metaflow was developed internally at Netflix, we spent a considerable amount of
 time, often months, to hone the design for features before we implemented them. We are
 not sure what’s the best way to facilitate the design process in the open yet - your
 ideas are welcome. We have outlined our current suggested way below -
 
 Please make sure there is an open issue discussing your contribution. Before opening a
 new issue, please check for [existing
 issues](https://github.com/Netflix/metaflow/issues?q=is%3Aissue). If you find an
 existing issue that matches closely with yours, please thumbs-up or comment on it, so we
 know that the issue is relevant to many people.
 
 If you hit a bug that is not covered by an existing issue, please open a new issue. If
 you are able to fix the bug by yourself with a few lines of code, we welcome your PR.
 However, if fixing the bug requires more effort, please wait for our response on the
 issue. We try to respond as quickly as possible.
 
 If you have a proposal for an enhancement or a new feature, please open an issue and
 engage with us on the issue before devoting significant time and resources to it. We
 care deeply about the ergonomics of Metaflow, and as such, for any new user-visible
 enhancement, please expect multiple design iterations.
 
 We will do our best to respond to and review your PR, and we hope you will stay engaged
 with us throughout the process.
 
 We'd appreciate [issue reports](https://github.com/Netflix/metaflow/issues) if you run
 into trouble using Metaflow.",H3,https://docs.metaflow.org/internals/contributing#contributing-code-and-issues,False,1747.0,292.0
1246,Community,"Everyone is welcome to join us in our [chatroom](http://chat.metaflow.org/)!
 
 Please maintain appropriate, professional conduct while participating in our community.
 This includes all channels of communication. We take reports of harassment or
 unwelcoming behavior very seriously. To report such behavior, please contact us via
 [email](mailto:help@metaflow.org).",H3,https://docs.metaflow.org/internals/contributing#community,False,367.0,45.0
1247,Accessing Secrets,"If your flow needs to access an external service (e.g. a database) that requires
 authentication, you need to supply credentials to the flow. If security wasn't
 a concern, you could easily achieve this using
 [Metaflow parameters](/metaflow/basics#how-to-define-parameters-for-flows). However,
 when it comes to credentials and other sensitive information, security is a top concern.
 
 The industry-standard best practice is to store credentials in a secrets
 manager, such as [AWS Secrets Manager](https://aws.amazon.com/secrets-manager/).
 Once secrets are managed by such a system, Metaflow provides a decorator, `@secrets`,
 which makes it easy to access them securely in a flow.
 
 For more background, see [the `@secrets` launch blog post](https://outerbounds.com/blog/metaflow-secrets/).
 Also, take a look at [the API docs for `@secrets`](/api/step-decorators/secrets).
 
 :::info
 
 Currently, `@secrets` supports only AWS Secrets Manager. Contact us on
 [Metaflow support Slack](http://chat.metaflow.org) if you are interested in
 using another secrets manager.
 
 :::",H1,https://docs.metaflow.org/scaling/secrets#accessing-secrets,False,1080.0,140.0
1248,Basic usage,"This video gives a one-minute overview of how to store a secret and access it in
 Metaflow (no sound):
 
 import ReactPlayer from 'react-player'
 
 <ReactPlayer controls url=""https://www.youtube.com/watch?v=tGRc8tWTzoQ"" />
 <br/>
 
 The secrets manager stores secrets as a set of key-value pairs that are
 identified by a name. Given a name, the `@secrets` decorator fetches the
 key-value pairs - assuming your IAM user is allowed to read the secret - and
 exposes them through environment variables.
 
 Here is the simple example featured in the video:
 
 ```python
 from metaflow import FlowSpec, step, secrets
 import os
 
 class SecretFlow(FlowSpec):
 
     @secrets(sources=['metaflow-example-password'])
     @step
     def start(self):
         print(""Here's the password:"", os.environ['password'])
         self.next(self.end)
 
     @step
     def end(self):
         pass
 
 if __name__ == '__main__':
     SecretFlow()
 ```
 
 In this case, `metaflow-example-password` is the name of the secret which
 contains a key `password`. The `sources` attribute, which defines the secret
 sources, could contain multiple names, in which case the union of all secret
 sets is exposed through environment variables.",H2,https://docs.metaflow.org/scaling/secrets#basic-usage,False,1216.0,211.0
1249,Configuring a secrets backend,"To use `@secrets`, you need to inform Metaflow which secrets manager you want to
 use. Currently, the choice is easy since the only supported backend is AWS
 Secrets Manager.
 
 Make sure your Metaflow configuration contains the following line:
 
 ```json
 ""METAFLOW_DEFAULT_SECRETS_BACKEND_TYPE"": ""aws-secrets-manager""
 ```",H3,https://docs.metaflow.org/scaling/secrets#configuring-a-secrets-backend,False,324.0,44.0
1250,Defining secrets on the command line,"Note that you can define `@secrets` on the command line using the `--with`
 option like any other decorator. This comes especially handy when moving
 between prototype and production: For instance, you can access a different
 database during development and production.
 
 Consider this example that connects to a Postgres database:
 
 ```python
 from metaflow import FlowSpec, step, secrets
 import os
 from psycopg import connect
 
 class DBFlow(FlowSpec):
 
     @step
     def start(self):
         with connect(user=os.environ['DB_USER'],  
                      password=os.environ['DB_PASSWORD'],
                      dbname=os.environ['DB_NAME'],
                      host=os.environ['DB_HOST']) as conn:
 
             with conn.cursor() as cur:
                 cur.execute(""SELECT * FROM data"")
                 print(cur.fetchall())
 
         self.next(self.end)
 
     @step
     def end(self):
         pass
 
 if __name__ == '__main__':
     DBFlow()
 ```
 
 During development, you can run the flow locally, maybe reading credentials to a
 local database from environment variables - no need to use a secrets manager
 during early prototyping.
 
 To read data from a test database, you can fetch credentials from a secrets
 manager by running the flow like this:
 
 ```bash
 python dbflow.py
    –with 'secrets:sources=[“test-db-credentials”]'
    run
 ```
 
 And you can [deploy to production](/production/introduction) using a production
 database like this:
 
 ```bash
 python dbflow.py
   –with 'secrets:sources=[""prod-db-credentials”]'
   argo-workflows create
 ```",H3,https://docs.metaflow.org/scaling/secrets#defining-secrets-on-the-command-line,False,1589.0,347.0
1251,Advanced topics,"The following topics come up occasionally when running flows in serious
 production environments.",H2,https://docs.metaflow.org/scaling/secrets#advanced-topics,False,97.0,13.0
1252,Controlling access to secrets,"A major benefit of using a secrets manager is that you can control closely who
 gets to access which secrets. In the case of AWS Secrets Manager, access
 control is accomplished through IAM policies. For more details, [consult the
 section about access control in the AWS Secrets Manager
 documentation](https://docs.aws.amazon.com/secretsmanager/latest/userguide/auth-and-access.html).
 
 For instance, you can set up IAM policies so that only a test database is
 accessible to users directly, while production database can be only accessed
 by [tasks running on a production scheduler](/production/introduction).",H3,https://docs.metaflow.org/scaling/secrets#controlling-access-to-secrets,False,614.0,83.0
1253,Using an alternative role,"By default, `@secrets` accesses secrets using the default IAM role available in
 the execution environment. For local runs, this is typically the role attached
 to the IAM user.
 
 If the default role doesn't have access to the specified secrets, you can define
 an alternative role through the `role` attribute:
 
 ```python
 @secrets(sources=['metaflow-example-password'],
          role='arn:aws:iam::123456789012:role/SecretsAccess')
 ```
 
 The default role needs to be able to assume the specified `role` for this
 to work.",H3,https://docs.metaflow.org/scaling/secrets#using-an-alternative-role,False,529.0,81.0
1254,Accessing secrets from a non-default location,"AWS Secrets Manager is an account- and region-specific service. By default, when
 you specify a secret name in the `sources` list, `@secrets` assumes that the
 name is available in the current AWS account, in the current default region.
 
 If this is not the case, you can specify a full secrets ARN (available on the
 AWS Secrets Manager console) as a source:
 
 ```python
 @secrets(sources=['arn:aws:secretsmanager:us-west-2:001234556000:secret:some-secret'])
 ```",H3,https://docs.metaflow.org/scaling/secrets#accessing-secrets-from-a-non-default-location,False,466.0,66.0
1255,Dealing with Failures,"Failures are a natural, expected part of data science workflows. Here are some typical
 reasons why you can expect your workflow to fail:
 
 1. **Misbehaving code:** no code is perfect. Your code may fail to handle edge cases or
    libraries behave differently than what you expected.
 2. **Unanticipated issues with data:** data is harder than science. Data is how Metaflow
    workflows interact with the chaotic, high entropy, outside world. It is practically
    impossible to anticipate all possible ways the input data can be broken.
 3. **Platform issues:** the best infrastructure is invisible. Unfortunately, every now
    and then platforms that Metaflow relies on, or Metaflow itself, make their existence
    painfully obvious by failing in creative ways.
 
 Metaflow provides straightforward tools for you to handle all these scenarios. If you
 are in a hurry, see [a quick summary of the tools](failures.md#summary).",H1,https://docs.metaflow.org/scaling/failures#dealing-with-failures,False,931.0,156.0
1256,Retrying Tasks with the `retry` Decorator,"Retrying a failed task is the simplest way to try to handle errors. It is a particularly
 effective strategy with platform issues which are typically transient in nature.
 
 You can enable retries for a step simply by adding `retry` decorator in the step, like
 here:
 
 ```python
 from metaflow import FlowSpec, step, retry
 
 class RetryFlow(FlowSpec):
 
     @retry
     @step
     def start(self):
         import time
         if int(time.time()) % 2 == 0:
             raise Exception(""Bad luck!"")
         else:
             print(""Lucky you!"")
         self.next(self.end)
 
     @step
     def end(self):
         print(""Phew!"")
 
 if __name__ == '__main__':
     RetryFlow()
 ```
 
 When you run this flow you will see that sometimes it succeeds without a hitch, but
 sometimes the `start` step raises an exception and needs to be retried. By default,
 `retry` retries the step three times. Thanks to `retry`, this workflow will almost
 always succeed.
 
 It is recommended that you use `retry` every time you [run tasks
 remotely](/scaling/remote-tasks/introduction). Instead of annotating every step with a
 retry decorator, you can also automatically add a retry decorator to all steps that do
 not have one as follows:
 
 ```python
 python RetryFlow.py run --with retry
 ```",H2,https://docs.metaflow.org/scaling/failures#retrying-tasks-with-the-retry-decorator,False,1288.0,272.0
1257,How to Prevent Retries,"If retries are such a good idea, why not enable them by default for all steps? First,
 retries only help with transient errors, like sporadic platform issues. If the input
 data or your code is broken, retrying will not help anything. Secondly, not all steps
 can be retried safely.
 
 Imagine a hypothetical step like this:
 
 ```python
 @step
 def withdraw_money_from_account(self):
     requests.post('bank.com/account/123/withdraw', data={'amount': 1000})
 ```
 
 If you run this code with:
 
 ```bash
 python MoneyFlow.py run --with retry
 ```
 
 you may end up withdrawing up to $4000 instead of the intended $1000. To make sure no
 one will accidentally retry a step with _destructive side effects_ like this, you should
 add `times=0` in the code:
 
 ```python
 @retry(times=0)
 @step
 def withdraw_money_from_account(self):
     requests.post('bank.com/account/123/withdraw', data={'amount': 1000})
 ```
 
 Now the code can be safely rerun, even using `--with retry`. All other steps will be
 retried as usual.
 
 Most data science workflows do not have to worry about this. As long as your step only
 reads and writes Metaflow artifacts and/or performs only read-only operations with
 external systems \(e.g. performs only `SELECT` queries, no `INSERT`s etc.\), your step
 is [idempotent](https://en.wikipedia.org/wiki/Idempotence#Computer_science_meaning) and
 can be retried without concern.",H3,https://docs.metaflow.org/scaling/failures#how-to-prevent-retries,False,1403.0,204.0
1258,Maximizing Safety,"By default, `retry` will retry the step for three times before giving up. It waits for 2
 minutes between retries for [remote tasks](/scaling/remote-tasks/introduction). This
 means that if your code fails fast, any transient platform issues need to get resolved
 in less than 10 minutes or the whole run will fail. 10 minutes is typically more than
 enough, but sometimes you want both a belt and suspenders.
 
 If you have a sensitive production workflow which should not fail easily, there are four
 things you can do:
 
 1. You can increase the number of retries to `times=4`, which is the maximum number of
    retries currently.
 2. You can make the time between retries arbitrarily long, e.g. `times=4,
    minutes_between_retries=20.` This will give the task over an hour to succeed.
 3. You can use `catch`, described below, as a way to continue even after all retries
    have failed.
 4. You can use `timeout`, also described below, to make sure your code will not get
    stuck.
 
 You can use any combination of these four techniques, or all of them together, to build
 rock-solid workflows.",H3,https://docs.metaflow.org/scaling/failures#maximizing-safety,False,1104.0,194.0
1259,Results of Retries,"If the same code is executed multiple times by `retry`, are there going to be duplicate
 artifacts? No, Metaflow manages retries so that only artifacts from the last retry are
 visible. If you use [the Client API ](/metaflow/client.md)to inspect results, you don't
 have to do anything special to deal with retries that may have happened. Each task will
 have only one set of results. Correspondingly, the logs returned by `task` show the
 output of the last attempt only.
 
 If you want to know if a task was retried, you can retrieve retry timestamps from `Task`
 metadata:
 
 ```python
 from metaflow import Run, namespace
 
 namespace(None)
 task = Run('RetryFlow/10')['start'].task
 attempts = [m for m in task.metadata if m.type == 'attempt']
 ```",H3,https://docs.metaflow.org/scaling/failures#results-of-retries,False,753.0,121.0
1260,Catching Exceptions with the `catch` Decorator,"As mentioned above, `retry` is an appropriate tool for dealing with transient issues.
 What about issues that are not transient? Metaflow has another decorator, `catch` that
 catches any exceptions that occur during the step and then continues execution of
 subsequent steps.
 
 The main upside of `catch` is that it can make your workflows extremely robust: it will
 handle all error scenarios from faulty code and faulty data to platform issues. The main
 downside is that your code needs to be modified so that it can tolerate faulty steps.
 
 Let's consider issues caused by your code versus everything surrounding it separately.",H2,https://docs.metaflow.org/scaling/failures#catching-exceptions-with-the-catch-decorator,False,633.0,102.0
1261,Exceptions Raised by Your Code,"By default, Metaflow stops execution of the flow when a step fails. It can't know what
 to do with failed steps automatically.
 
 You may know that some steps are error-prone. For instance, this can happen with a step
 inside a foreach loop that iterates over unknown data, such as the results of a query or
 a parameter matrix. In this case, it may be desirable to let some tasks fail without
 crashing the whole flow.
 
 Consider this example that is structured like a hyperparameter search:
 
 ```python
 from metaflow import FlowSpec, catch, step
 
 class CatchFlow(FlowSpec):
 
     @step
     def start(self):
         self.params = range(3)
         self.next(self.compute, foreach='params')
 
     @catch(var='compute_failed')
     @step
     def compute(self):
         self.div = self.input
         self.x = 5 / self.div
         self.next(self.join)
 
     @step
     def join(self, inputs):
         for input in inputs:
             if input.compute_failed:
                 print('compute failed for parameter: %d' % input.div)
         self.next(self.end)
 
     @step
     def end(self):
         pass
 
 if __name__ == '__main__':
     CatchFlow()
 ```
 
 As you can guess, the above flow raises an error. Normally, this would crash the whole
 flow. However, in this example the `catch` decorator catches the exception and stores it
 in an instance variable called `compute_failed`, and lets the execution continue. The
 next step, `join`, contains logic to handle the exception.
 
 The `var` argument is optional. The exception is not stored unless you specify it. You
 can also specify `print_exception=False` to prevent the `catch` decorator from printing
 out the caught exception on stdout.",H3,https://docs.metaflow.org/scaling/failures#exceptions-raised-by-your-code,False,1713.0,369.0
1262,Platform Exceptions,"You could have dealt with the above error by wrapping the whole step in a `try ...
 except` block. In effect, this is how `catch` deals with errors raised in the user code.
 
 In contrast, platform issues happen outside of your code, so you can't handle them with
 a `try ... except` block.
 
 Let's simulate a platform issue like these with the following flow that kills itself
 without giving Python a chance to recover:
 
 ```python
 from metaflow import FlowSpec, step, retry, catch
 
 class SuicidalFlow(FlowSpec):
 
     @catch(var='start_failed')
     @retry
     @step
     def start(self):
         import os, signal
         # kill this process with the KILL signal
         os.kill(os.getpid(), signal.SIGKILL)
         self.next(self.end)
 
     @step
     def end(self):
         if self.start_failed is not None:
             print(""It seems 'start' did not survive."")
 
 if __name__ == '__main__':
     SuicidalFlow()
 ```
 
 Note that we use both `retry` and `catch` above. `retry` attempts to run the step three
 times, hoping that the issue is transient. The hope is futile. The task kills itself
 every time.
 
 After all retries are exhausted, `catch` takes over and records an exception in
 `start_failed`, notifying that all attempts to run `start` failed. Now it is up to the
 subsequent steps, `end` in this case, to deal with the scenario that `start` produced no
 results whatsoever. They can choose an alternative code path using the variable assigned
 by `catch`, `start_failed` in this example.",H3,https://docs.metaflow.org/scaling/failures#platform-exceptions,False,1523.0,307.0
1263,Timing out with the `timeout` Decorator,"By default, there is no timeout for steps. If you cause an infinite loop accidentally or
 query an external service that hangs, the step will block the flow forever. This is
 undesirable especially in production runs that should not require human intervention.
 
 Metaflow provides a `timeout` decorator to address this issue:
 
 ```python
 from metaflow import FlowSpec, timeout, step
 import time
 
 class TimeoutFlow(FlowSpec):
 
     @timeout(seconds=5)
     @step
     def start(self):
         for i in range(100):
             print(i)
             time.sleep(1)
         self.next(self.end)
 
     @step
     def end(self):
         pass
 
 if __name__ == '__main__':
     TimeoutFlow()
 ```
 
 Here, the `start` step times out after five seconds. Besides `seconds`, you can specify
 `minutes` and/or `hours` as the timeout value. Note that all values specified are
 cumulative so specifying 10 seconds and 5 minutes will result in a timeout of 5 minutes
 and 10 seconds.
 
 The above example raises an exception if the step does not finish in the given time
 period. This is a good pattern if the timeout is a genuine error condition.
 
 In some cases you can handle a timeout in subsequent steps. Similar to `SuicidalFlow`
 above, you can use the `catch` decorator to catch the timeout exception:
 
 ```python
 from metaflow import FlowSpec, timeout, step, catch
 import time
 
 class CatchTimeoutFlow(FlowSpec):
 
     @catch(print_exception=False, var='timeout')
     @timeout(seconds=5)
     @step
     def start(self):
         for i in range(100):
             print(i)
             time.sleep(1)
         self.next(self.end)
 
     @step
     def end(self):
         if self.timeout:
             print('the previous step timed out')
         else:
             print('all ok!')
 
 if __name__ == '__main__':
     CatchTimeoutFlow()
 ```
 
 This example handles a timeout in `start` gracefully without showing any exceptions.",H2,https://docs.metaflow.org/scaling/failures#timing-out-with-the-timeout-decorator,False,1941.0,436.0
1264,Summary,"Here is a quick summary of failure handling in Metaflow:
 
 * Use `retry` to deal with transient platform issues. You can do this easily on the
   command line with the `--with retry` option.
 * Use `retry` **with** `catch` for extra robustness if you have modified your code to
   deal with faulty steps which are handled by `catch`.
 * Use `catch` **without** `retry` to handle steps [that can't be retried
   safely](failures.md#how-to-prevent-retries). It is a good idea to use `times=0` for
   `retry` in this case.
 * Use `timeout` with any of the above if your code can get stuck.",H2,https://docs.metaflow.org/scaling/failures#summary,False,587.0,106.0
1265,Managing External Libraries,"What if your step code wants to import an external library? When you run Metaflow code
 locally, it behaves as any other Python code, so all libraries available to your Python
 interpreter can be imported in steps.
 
 However, a core benefit of Metaflow is that the same code can be run in different
 environments without modifications. Clearly this promise does not hold if a step code
 depends on locally installed libraries. The topic of providing isolated and encapsulated
 execution environments is a surprisingly complex one. We recommend the following
 approaches for handling external libraries, in order of preference:
 
 1. If your code needs an additional Python module, for instance, a library module that
    you wrote by yourself, you can place the file in the same directory with your flow
    file. When Metaflow packages your code for remote execution, any `.py` files in the
    directory are included in the distribution automatically. In this case, you can
    simply `import mymodule` in the step code. This works even with packages with
    multiple files which can be included as subdirectories.
 2. If you need a custom package that is too complex to include in the flow directory,
    one approach is to install it on the fly in your step code:
 
    ```
    os.system('pip install my_package')
    import my_package
    ```
 
    This approach is, however, not encouraged for multiple reasons:
 
    1. It makes the results harder to reproduce since the version of `my_package` may
       change;
    2. `pip install`ing packages on the fly is brittle, especially if performed in tasks
       running in parallel.
 
    Instead, to define external dependencies for a step, you can instead use the `@conda`
    decorator which uses [conda](https://docs.conda.io/en/latest/) behind the scenes.",H1,https://docs.metaflow.org/scaling/dependencies#managing-external-libraries,False,1817.0,338.0
1266,Managing dependencies with `@conda` decorator,"Reproducibility is a core value of Machine Learning Infrastructure. It is hard to
 collaborate on data science projects effectively without being able to reproduce past
 results reliably. Metaflow tries to solve several questions related to reproducible
 research, principle amongst them, dependency management: how can you, the data
 scientist, specify libraries that your code needs so that the results are reproducible?
 
 Note that reproducibility and dependency management are related but separate topics. We
 could solve either one individually. A classic `os.system(‘pip install pandas’)` is an
 example of dependency management without reproducibility (what if the version of
 `pandas` changes?). On the other hand, we could make code perfectly reproducible by
 forbidding external libraries - reproducibility without dependency management.
 
 Metaflow aims at solving both the questions at once: how can we handle dependencies so
 that the results are reproducible? Specifically, it addresses the following three
 issues:
 
 1. How to make external dependencies available locally during development?
 2. How to execute code remotely on Kubernetes or AWS Batch with external dependencies?
 3. How to ensure that anyone can reproduce past results even months later?
 
 Metaflow provides an execution environment context, `--environment=conda`, which runs
 every step in a separate environment that only contains dependencies that are explicitly
 listed as requirements for that step. The solution relies on
 [Conda](https://conda.io/docs/), a language-agnostic open-source package manager by the
 authors of Numpy.
 
 For instance:
 
 ```python
 @conda(libraries={""pandas"": ""0.22.0""})
 def fit_model(self):
     ...
 ```
 
 The above code snippet will execute the `fit_model` step in an automatically created
 conda environment that contains only specific pinned versions of `Python`, `Pandas`, and
 `Metaflow`(and its dependencies `boto3`, `click` and `requests`). No unspecified
 libraries outside of the standard Python library would be available. This isolates your
 code from any external factors, such as automatically upgrading libraries.
 
 Internally, Metaflow handles automatic dependency resolution, cross-platform support,
 environment snapshotting and caching in Amazon S3 (if enabled). We require that all
 dependencies are pinned to a specific version. This avoids any ambiguity about the
 version used and helps make deployments fully immutable; in other words, once you deploy
 a version in production, nothing will inadvertently change its behavior without explicit
 action.",H2,https://docs.metaflow.org/scaling/dependencies#managing-dependencies-with-conda-decorator,False,2599.0,362.0
1267,Local Execution,"Let's look at the [LinearFlow](/metaflow/basics.md#linear) from before:
 
 ```python
 from metaflow import FlowSpec, step
 
 class LinearFlow(FlowSpec):
 
     @step
     def start(self):
         self.my_var = 'hello world'
         self.next(self.a)
 
     @step
     def a(self):
         print('the data artifact is: %s' % self.my_var)
         self.next(self.end)
 
     @step
     def end(self):
         print('the data artifact is still: %s' % self.my_var)
 
 if __name__ == '__main__':
     LinearFlow()
 ```
 
 You can execute this flow in a conda environment by executing:
 
 ```bash
 $ python LinearFlow.py --environment=conda run
 ```
 
 Metaflow will bootstrap a dedicated conda environment for each of the steps of the
 workflow, executing each of them in that isolated environment, resulting in an output
 like this -
 
 ```bash
 2019-11-27 20:04:27.579 Bootstrapping conda environment...(this could take a few minutes)
 2019-11-27 20:05:13.623 Workflow starting (run-id 164):
 2019-11-27 20:05:14.273 [164/start/4222215 (pid 14011)] Task is starting.
 2019-11-27 20:05:16.426 [164/start/4222215 (pid 14011)] Task finished successfully.
 2019-11-27 20:05:17.246 [164/a/4222216 (pid 14064)] Task is starting.
 2019-11-27 20:05:19.014 [164/a/4222216 (pid 14064)] the data artifact is: hello world
 2019-11-27 20:05:19.484 [164/a/4222216 (pid 14064)] Task finished successfully.
 2019-11-27 20:05:20.192 [164/end/4222217 (pid 14117)] Task is starting.
 2019-11-27 20:05:21.756 [164/end/4222217 (pid 14117)] the data artifact is still: hello world
 2019-11-27 20:05:22.436 [164/end/4222217 (pid 14117)] Task finished successfully.
 2019-11-27 20:05:22.512 Done!
 ```
 
 You might notice some delay when you execute this flow for the first time, as Metaflow
 performs dependency resolution, creates the environments and caches the results.
 Subsequent executions rely on this cache to reduce this overhead going forward.
 
 Let’s import the module `sklearn` in one of the steps -
 
 ```python
 from metaflow import FlowSpec, step
 
 class LinearFlow(FlowSpec):
 
     @step
     def start(self):
         import sklearn
         self.my_var = 'hello world'
         self.next(self.a)
 
     @step
     def a(self):
         print('the data artifact is: %s' % self.my_var)
         self.next(self.end)
 
     @step
     def end(self):
         print('the data artifact is still: %s' % self.my_var)
 
 if __name__ == '__main__':
     LinearFlow()
 ```
 
 Let's execute this flow in a conda environment, again, by executing:
 
 ```bash
 $ python LinearFlow.py --environment=conda run
 ```
 
 You will notice that the execution progresses fairly quickly compared to before since
 all the specified dependencies are already cached locally, but the flow fails at step
 `start`, with the error `` `ModuleNotFoundError: No module named ‘sklearn’` ``, even
 though you might have the module installed locally already.
 
 You can add an explicit dependency on the module `sklearn` by using the `@conda`
 decorator in the `start` step -
 
 ```python
 from metaflow import FlowSpec, step, conda
 
 class LinearFlow(FlowSpec):
 
     @conda(libraries={'scikit-learn':'0.21.1'})
     @step
     def start(self):
         import sklearn
         self.my_var = 'hello world'
         self.next(self.a)
 
     @step
     def a(self):
         print('the data artifact is: %s' % self.my_var)
         self.next(self.end)
 
     @step
     def end(self):
         print('the data artifact is still: %s' % self.my_var)
 
 if __name__ == '__main__':
     LinearFlow()
 ```
 
 Let’s execute this flow, in the conda environment again -
 
 ```bash
 $ python LinearFlow.py --environment=conda run
 ```
 
 You will notice that bootstrapping takes a little bit longer than before as we pull in
 the new set of dependencies (`scikit-learn` `0.21.1` and its dependencies) and the flow
 succeeds. `scikit-learn 0.21.1` is only available to the step `start` and no other step.
 
 Every subsequent execution of your flow is guaranteed to execute in the same environment
 unless you explicitly make a change to the contrary. Behind the scenes, we resolve the
 dependencies you have specified in your steps and cache both the resolution order and
 dependencies (stated and transitive) locally and on Amazon S3 to be used for subsequent
 executions. We do this to isolate your code from changes not related to your code. This
 also allows for isolation between runs, you should be able to use a different version of
 tensorflow for different flows and even across different steps of the same flow if that
 suits your use-case.",H3,https://docs.metaflow.org/scaling/dependencies#local-execution,False,4602.0,843.0
1268,Remote Execution,"You can execute your flow on Kubernetes or AWS Batch, like before -
 
 ```bash
 $ python LinearFlow.py --environment=conda run --with kubernetes
 ```
 
 ```bash
 $ python LinearFlow.py --environment=conda run --with batch
 ```
 
 Since we cache the exact set of dependencies (stated and transitive) for your flow in
 Amazon S3, you are not at the mercy of an upstream package repository and can avoid
 overwhelming it, particularly while running multiple parallel tasks, while being
 guaranteed the same execution environment locally, on Kubernetes and on AWS Batch.
 
 Note that, the exact set of dependencies and their behavior might differ between an
 execution on macOS (darwin) and on Kubernetes/AWS Batch (linux).",H3,https://docs.metaflow.org/scaling/dependencies#remote-execution,False,719.0,111.0
1269,`@conda` Tips and Tricks,,H3,https://docs.metaflow.org/scaling/dependencies#conda-tips-and-tricks,False,0.0,1.0
1270,"Can I use an alternate dependency manager, given that conda can be slow at resolving dependencies?","By default, Metaflow relies on conda for dependency resolution but for many data science
 packages, conda can be quite slow for [a variety of different
 reasons](https://docs.conda.io/projects/conda/en/latest/user-guide/concepts/conda-performance.html#conda-performance).
 [Mamba](https://mamba.readthedocs.io/en/latest/) is another cross-platform package
 manager that is fully compatible with conda packages and [offers better performance and
 reliability compared to conda](https://stackoverflow.com/a/68043228/6510628). You can
 use mamba instead of conda by setting the environment variable
 `METAFLOW_CONDA_DEPENDENCY_RESOLVER=mamba` either in your execution environment or
 inside your metaflow config (usually located at `~/.metaflowconfig/`).",H4,https://docs.metaflow.org/scaling/dependencies#can-i-use-an-alternate-dependency-manager-given-that-conda-can-be-slow-at-resolving-dependencies,False,751.0,75.0
1271,How do I specify the version of Python interpreter?,"By default, we take the version of the Python interpreter with which you invoke your
 flow. You can override it whatever version you choose, e.g, `@conda(python='3.6.5')`.",H4,https://docs.metaflow.org/scaling/dependencies#how-do-i-specify-the-version-of-python-interpreter,False,171.0,26.0
1272,What about storing and retrieving data artifacts between steps in my flow?,"Metaflow relies on pickle for object serialization/deserialization. Make sure you have
 compatible versions (ideally the same version) of the object module as a dependency in
 your steps which rely on interacting with this object artifact.",H4,https://docs.metaflow.org/scaling/dependencies#what-about-storing-and-retrieving-data-artifacts-between-steps-in-my-flow,False,239.0,35.0
1273,How do I specify flow-level dependencies?,"Using the flow-level `@conda_base` decorator you can specify, for the flow, explicit
 library dependencies, python version and also if you want to exclude all steps from
 executing within a conda environment. Some examples -
 
 ```python
 @conda_base(libraries={'numpy':'1.15.4'}, python=’3.6.5’)
 class LinearFlow(FlowSpec):
     ...
 ```
 
 ```python
 @conda_base(disabled=True)
 class LinearFlow(FlowSpec):
     ...
 ```
 
 Step-level `@conda` decorators, for the step, will directly update the explicit library
 dependencies, python version, and conda environment exclusion as specified by the
 `@conda_base` decorator. Some examples:
 
 ```python
 from metaflow import FlowSpec, step, conda, conda_base
 
 @conda_base(libraries={'numpy':'1.15.4'}, python='3.6.5')
 class MyFlow(FlowSpec):
     @step
     def a(self):
        ...
 
     @conda(libraries={'numpy':'1.16.3'})
     @step
     def b(self):
        ...
 
     @conda(disabled=True)
     @step
     def c(self):
        ...
 ```
 
 In this example, step `a` executes in the environment `libraries={'numpy':'1.15.4'},
 python=’3.6.5’`, step `b` executes in the environment `libraries={'numpy':'1.16.3'},
 python=’3.6.5’`, while step `c` executes outside the conda environment in the user
 environment.",H4,https://docs.metaflow.org/scaling/dependencies#how-do-i-specify-flow-level-dependencies,False,1266.0,201.0
1274,Loading and Storing Data,"This chapter describes tools and patterns for moving data in and out of your Metaflow
 flows.
 
 Besides the mundane concern of loading data, there is also the question of how to
 organize code related to model-specific data transformations, such as feature
 engineering. Short answer: [keep data access separate from feature
 engineering](http://en.wikipedia.org/wiki/Separation\_of\_concerns).
 
 In a perfect world, the data scientist could design and test features without having to
 concern themselves with the underlying mechanics of data transfer and processing.
 Unfortunately the larger the dataset, the more intermingled the two concerns become.
 
 Metaflow can not make the world perfect yet. However, we recommend that data science
 workflows try to keep the two concerns as separate as possible. In practice, you should
 use the solutions presented in this chapter purely to load a clean dataset in your
 workflow. Then, you should perform any model-specific data transformations in your
 Python code. In particular, we recommend that you use SQL only for data access, not for
 model-specific data manipulation.
 
 There are multiple benefits in keeping data access separate from model-specific data
 manipulation:
 
 * It is easier to keep a model and its features in sync when they are computed together.
   [Metaflow's built-in versioning](/scaling/tagging.md#tagging) makes it easy to iterate
   on multiple concurrent versions of the model safely. However, Metaflow can't protect
   you against stale input data. It is frustrating to troubleshoot bad model results that
   are caused by out-of-sync features.
 * It is quicker to iterate on your model. Testing and debugging Python is easier than
   testing and debugging SQL.
 * You can request [arbitrary amount of resources](/scaling/remote-tasks/introduction)
   for your data manipulation needs.
 * Instead of having data manipulation code in two places (SQL and Python), all code can
   be clearly laid out in a single place, in a single language, for maximum readability.
 * It is easier to optimize your code for performance when IO bottlenecks can be profiled
   separately from CPU bottlenecks.
 
 Keep this guideline in mind when choosing the right data access method below.&#x20;",H1,https://docs.metaflow.org/scaling/data#loading-and-storing-data,False,2258.0,352.0
1275,Data in Tables,"Accessing data in tables (most often Hive) is by far the most common way to load input
 data to Metaflow workflows. A common paradigm is to issue arbitrary SQL queries against
 the data warehouse to fetch data.
 
 :::info
 
 See [Accessing Secrets](/scaling/secrets) if your database or query engine requires
 authentication.
 
 :::
 
 However, depending on the data volume and the complexity of the query, queries
 can be slow to execute and can potentially congest the query engine. It is not
 uncommon for a data science workflow to hit these limitations. Even if your
 data set is not huge, you may want to build multiple models in parallel, e.g.
 one per country. In this case, each model needs to load a shard of data. If you
 used SQL to load the shards, it will very quickly overload your query engine.
 
 As a solution, [`metaflow.S3`](data.md#data-in-s3-metaflows3) provides a way to load
 data directly from S3, bypassing any query engines such as Spark. Combined with a
 [metadata catalog](https://github.com/Netflix/metacat), it is easy to write shims on top
 of `metaflow.S3` to directly interface with data files on S3 backing your tables. Since
 data is loaded directly from S3, there is no limitation to the number of parallel
 processes. The size of data is only limited by the size of your instance, which can be
 easily controlled with [the `@resources`
 decorator](/scaling/remote-tasks/introduction#requesting-resources-with-resources-decorator).
 The best part is that this approach is blazingly fast compared to executing SQL.
 
 The main downside of this approach is that the table needs to have partitions that match
 your access pattern. For small and medium-sized tables, this isn't necessarily an issue
 as you can afford loading extra data. Additional filtering can be performed in your
 Python code. With larger tables this approach is not feasible, so you may need to run an
 extra SQL query to repartition data properly.",H2,https://docs.metaflow.org/scaling/data#data-in-tables,False,1953.0,309.0
1276,Use cases,"* Workflows that need to process large amounts of data.
 * Workflows that build many models in parallel.
 * Performance-oriented workflows.",H3,https://docs.metaflow.org/scaling/data#use-cases,False,139.0,21.0
1277,Data in S3: `metaflow.S3`,"*This section contains an overview of `metaflow.S3`. For a complete API, see [the API
 reference for the S3 class](/api/s3).*
 
 It is not always appropriate to store data in a table. For instance, Netflix has many
 systems that communicate via JSON files in S3. Or, there is little benefit in storing a
 large Keras model serialized with
 [`model.save()`](https://keras.io/getting-started/faq/#how-can-i-save-a-%20keras-model)
 in a table.
 
 When you assign anything to `self` in your Metaflow flow, the object gets automatically
 persisted in S3 as [a Metaflow artifact](/metaflow/basics.md#linear). Hence, in most
 cases you do not need to worry about saving data or models to S3 explicitly. We
 recommend that you use Metaflow artifacts whenever possible, since they are easily
 accessible through [the Client API](/metaflow/client.md) by you, by other people, and by
 other workflows.
 
 However, there are valid reasons for interacting with S3 directly. For instance, you may
 need to consume or produce data to a 3rd party system that knows nothing about Metaflow.
 For use cases like this, we provide a high-performance S3 client, `metaflow.S3`.
 
 The sole benefit of `metaflow.S3` over Metaflow artifacts is that you get to see and
 control the S3 locations for data. Also, you must take care of object serialization by
 yourself: `metaflow.S3` only deals with objects of type `str`, `unicode`, and `bytes`.
 
 Compared to other S3 clients `metaflow.S3` provides two key benefits: First, when used
 in Metaflow flows, it can piggyback on Metaflow versioning, which makes it easy to track
 the lineage of an object back to the Metaflow run that produced it. Secondly,
 `metaflow.S3` provides better throughput than any other S3 client that we are aware of.
 In other words, it is very fast at loading and storing large amounts of data in S3.",H2,https://docs.metaflow.org/scaling/data#data-in-s3-metaflows3,False,1851.0,288.0
1278,**Pros**,"* Load and store data to/from arbitrary S3 locations.
 * Built-in support for lineage and versioning.
 * Maximum throughput between S3 and a compute instance.",H4,https://docs.metaflow.org/scaling/data#pros,False,158.0,25.0
1279,**Cons**,"* Don't use `metaflow.S3` if you can use Metaflow artifacts instead. In contrast to
   Metaflow artifacts, `metaflow.S3` is more tedious to use, uses space more wastefully,
   and it is less suitable for [moving data between Metaflow steps
   reliably](data.md#caution-overwriting-data-in-s3).",H4,https://docs.metaflow.org/scaling/data#cons,False,293.0,44.0
1280,Use cases,"* Communication with external systems through files in S3.
 * Special corner cases where you need more control over object serialization than what
   Metaflow artifacts provide by default.
 
 We recommend that you use `metaflow.S3` in a `with` scope in Python. Objects retrieved
 from S3 are stored in local temporary files for the lifetime of the `with` scope, not in
 memory. You can use `metaflow.S3` without `with` but in this case you need to call
 `s3.close()` to get rid of the temporary files. See examples of this below.
 
 Note that in order to get the maximum performance out of `metaflow.S3`, you need to set
 your `@resources` properly. However, don't request more resources than what your
 workload actually needs.",H3,https://docs.metaflow.org/scaling/data#use-cases,False,728.0,121.0
1281,Choosing the context,"To benefit from the built-in support for versioning, first you need to tell
 `metaflow.S3` whether it is being used in the context of a Metaflow run. A run can refer
 to a currently running flow (`run=self`) or a past run, `run=Run(...)`. If `run` is not
 specified, `metaflow.S3` can be used to access data without versioning in arbitrary S3
 locations.",H3,https://docs.metaflow.org/scaling/data#choosing-the-context,False,354.0,59.0
1282,**Store and load objects in a Metaflow flow**,"We expect that the most common use case for `metaflow.S3` is to store auxiliary data in
 a Metaflow flow. Here is an example:
 
 ```python
 from metaflow import FlowSpec, step, S3
 import json
 
 class S3DemoFlow(FlowSpec):
 
     @step
     def start(self):
         with S3(run=self) as s3:
             message = json.dumps({'message': 'hello world!'})
             url = s3.put('example_object', message)
             print(""Message saved at"", url)
         self.next(self.end)
 
     @step
     def end(self):
         with S3(run=self) as s3:
             s3obj = s3.get('example_object')
             print(""Object found at"", s3obj.url)
             print(""Message:"", json.loads(s3obj.text))
 
 if __name__ == '__main__':
     S3DemoFlow()
 ```
 
 Running the flow produced the following output:
 
 ```bash
 Workflow starting (run-id 3):
 [3/start/646436 (pid 30559)] Task is starting.
 [3/start/646436 (pid 30559)] Message saved at s3://my-bucket/metaflow/userdata/v1/S3DemoFlow/3/example_object
 [3/start/646436 (pid 30559)] Task finished successfully.
 [3/end/646437 (pid 30619)] Task is starting.
 [3/end/646437 (pid 30619)] Object found at s3://my-bucket/metaflow/userdata/v1/S3DemoFlow/3/example_object
 [3/end/646437 (pid 30619)] Message: {'message': 'hello world!'}
 [3/end/646437 (pid 30619)] Task finished successfully.
 ```
 
 Now you could share the URL,
 `s3://my-bucket/metaflow/userdata/v1/S3DemoFlow/3/example_object`, with external
 systems. Note that the URL includes both the flow name, `S3DemoFlow`, as well as its
 unique run id, `3`, which allow us to track the lineage of the object back to the run
 that produced it.
 
 Note that `metaflow.S3` provides a default S3 location for storing data. You could
 change the location by defining `S3(bucket='my-bucket', prefix='/my/prefix')` for the
 constructor. Metaflow versioning information would be concatenated to the `prefix`.",H4,https://docs.metaflow.org/scaling/data#store-and-load-objects-in-a-metaflow-flow,False,1905.0,337.0
1283,**Load external objects produced by a Metaflow run**,"What if you want to inspect S3 data produced by a flow afterwards? Just use [the Client
 API](/metaflow/client.md) as usual to locate the desired `Run` and use it to initialize
 an `S3` object:
 
 ```python
 from metaflow import S3
 with S3(run=Flow('S3DemoFlow').latest_run) as s3:
     print(s3.get('example_object').text)
 
 {""message"": ""hello world!""}
 ```
 
 This pattern is particularly convenient for notebooks.",H4,https://docs.metaflow.org/scaling/data#load-external-objects-produced-by-a-metaflow-run,False,418.0,61.0
1284,**Store and load objects to/from a known S3 location**,"The above examples inferred the S3 location based on the current or an existing Metaflow
 run. What if you want to load data that has nothing to do with Metaflow? Easy:
 
 ```python
 from metaflow import S3
 with S3() as s3:
     res = s3.get('s3://my-bucket/savin/tmp/external_data')
     print('an alien message: %s' % res.text)
 
 an alien message: I know nothing about Metaflow
 ```
 
 If `S3` is initialized without any arguments, all operations require a full S3 URL.
 
 If you need to operate on multiple files, it may be more convenient to specify a custom
 S3 prefix with the `s3root` argument:
 
 ```python
 from metaflow import S3
 with S3(s3root='s3://my-bucket/savin/tmp/s3demo/') as s3:
     s3.put('fruit', 'pineapple')
     s3.put('animal', 'mongoose')
 with S3() as s3:
     s3.get('s3://my-bucket/savin/tmp/s3demo/fruit').text
 
 pineapple
 ```
 
 If the requested URL does not exist, the `get` call will raise an exception. You can
 call `get` with `return_missing=True` if you want to return a missing URL as an ordinary
 result object, as described in the section below.
 
 By default, `put_*` calls will overwrite existing keys in S3. To avoid this behavior you
 can invoke your `put_*` calls with `overwrite=False`. Refer to [this
 section](data.md#caution-overwriting-data-in-s3) for some of the pitfalls involved with
 overwriting keys in S3.",H4,https://docs.metaflow.org/scaling/data#store-and-load-objects-to-from-a-known-s3-location,False,1367.0,219.0
1285,The S3 result object,"All `get` operations return an `S3Object`, backed by a temporary file on local disk,
 which exposes a number of attributes about the object:
 
 ```python
 with S3(s3root='s3://my-bucket/savin/tmp/s3demo/') as s3:
     s3obj = s3.get('fruit')
     print('location', s3obj.url)
     print('key', s3obj.key)
     print('size', s3obj.size)
     print('local path', s3obj.path)
     print('bytes', s3obj.blob)
     print('unicode', s3obj.text)
     print('metadata', s3obj.metadata)
     print('content-type', s3obj.content_type)
     print('downloaded', s3obj.downloaded)
 
 location s3://my-bucket/savin/tmp/s3demo/fruit
 key fruit
 size 9
 local path /data/metaflow/metaflow.s3.5agi129m/metaflow.s3.one_file.pih_iseg
 bytes b'pineapple'
 unicode pineapple
 metadata None
 content-type application/octet-stream
 downloaded True
 ```
 
 Note that you can not access data behind `s3obj` outside the `with` scope as the
 temporary file pointed at `s3obj.path` will get deleted as the scope exits.
 
 The `S3Object` may also refer to an S3 URL that does not correspond to an object in S3.
 These objects have `exists` property set to `False`. Non-existent objects may be
 returned by a `list_path` call, if the result refers to an S3 prefix, not an object.
 Listing operations also set `downloaded` property to `False`, to distinguish them from
 operations that download data locally. Also `get` and `get_many` may return non-existent
 objects if you call these methods with an argument `return_missing=True`.",H3,https://docs.metaflow.org/scaling/data#the-s3-result-object,False,1502.0,221.0
1286,**Querying objects without downloading them**,"The above information about an object, like `size` and `metadata`, can be useful even
 without downloading the file itself. To just get the metadata, use the `info` and
 `info_many` calls that work like `get` and `get_many` but avoid the potentially
 expensive downloading part. The info calls set `downloaded=False` in the result object.",H4,https://docs.metaflow.org/scaling/data#querying-objects-without-downloading-them,False,338.0,52.0
1287,Operations on multiple objects,"After you have instantiated the object given the right context information, all `get`
 and `put` operations work equally. The context is only used to construct an appropriate
 S3 URL.
 
 Besides loading individual files with `.get()` and `.put()` as shown above,
 `metaflow.S3` really shines at operating multiple files at once.
 
 It is guaranteed that the list of `S3Objects` returned is always in the same order as
 long as the underlying data does not change. This can be important e.g. if you use
 `metaflow.S3` to feed data for a model. The input data will be in a deterministic order
 so results should be easily reproducible.",H3,https://docs.metaflow.org/scaling/data#operations-on-multiple-objects,False,633.0,105.0
1288,**Load multiple objects in parallel**,"Use `get_many()` to load arbitrarily many objects at once:
 
 ```python
 from metaflow import S3
 with S3(s3root='s3://my-bucket/savin/tmp/s3demo/') as s3:
     s3.get_many(['fruit', 'animal'])
 
 [<S3Object s3://my-bucket/savin/tmp/s3demo/fruit (9 bytes)>,
  <S3Object s3://my-bucket/savin/tmp/s3demo/animal (8 bytes)>]
 ```
 
 Here, `get_many()` loads objects in parallel, which is much faster than loading
 individual objects sequentially. You can achieve the optimal throughput with S3 only
 when you operate on many files in parallel.
 
 If one of the requested URLs doesn't exist, the `get_many` call will raise an exception.
 If you don't want to fail all objects because of missing URLs, call `get_many` with
 `return_missing=True`. This will make `get_many` return missing URLs amongst other
 results. You can distinguish between the found and not found URLs using the `exists`
 property of `S3Object`.",H4,https://docs.metaflow.org/scaling/data#load-multiple-objects-in-parallel,False,911.0,127.0
1289,**Load all objects recursively under a prefix**,"We can load all objects under a given prefix:
 
 ```python
 from metaflow import S3
 with S3() as s3:
     s3.get_recursive(['s3://my-bucket/savin/tmp/s3demo'])
 
 [<S3Object s3://my-bucket/savin/tmp/s3demo/animal (8 bytes)>,
  <S3Object s3://my-bucket/savin/tmp/s3demo/fruit (9 bytes)>]
 ```
 
 Note that `get_recursive` takes a list of prefixes. This is useful for achieving the
 maximum level of parallelism when retrieving data under multiple prefixes.
 
 If you have specified a custom `s3root`, you can use `get_all()` to get all files
 recursively under the given prefix.",H4,https://docs.metaflow.org/scaling/data#load-all-objects-recursively-under-a-prefix,False,578.0,81.0
1290,Loading parts of files,"A performance-sensitive application may want to read only a part of a large file.
 Instead of a string, the `get` and `get_many` calls also accept an object with `key`,
 `offset`, `length` attributes that specify a part of a file to download. You can use an
 object called `S3GetObject` provided by Metaflow for this purpose.
 
 This example loads two 1KB chunks of a file in S3:
 
 ```python
 from metaflow import S3
 from metaflow.datatools.s3 import S3GetObject
 
 URL = 's3://ursa-labs-taxi-data/2014/12/data.parquet'
 
 with S3() as s3:
     res = s3.get_many([S3GetObject(key=URL, offset=0, length=1024),
                        S3GetObject(key=URL, offset=1024, length=1024)])
 
     for obj in res:
         print(obj.path, obj.size)
 ```",H4,https://docs.metaflow.org/scaling/data#loading-parts-of-files,False,746.0,140.0
1291,**Store multiple objects or files**,"If you need to store multiple objects, use `put_many`:
 
 ```python
 from metaflow import S3
 many = {'first_key': 'foo', 'second_key': 'bar'}
 with S3(s3root='s3://my-bucket/savin/tmp/s3demo_put/') as s3:
     s3.put_many(many.items())
 
 [('first_key', 's3://my-bucket/savin/tmp/s3demo_put/first_key'),
  ('second_key', 's3://my-bucket/savin/tmp/s3demo_put/second_key')]
 ```
 
 You may want to store more data to S3 than what you can fit in memory at once. This is a
 good use case for `put_files`:
 
 ```python
 from metaflow import S3
 with open('/tmp/1', 'w') as f:
     f.write('first datum')
 with open('/tmp/2', 'w') as f:
     f.write('second datum')
 with S3(s3root='s3://my-bucket/savin/tmp/s3demo_put/') as s3:
     s3.put_files([('first_file', '/tmp/1'), ('second_file', '/tmp/2')])
 
 [('first_file', 's3://my-bucket/savin/tmp/s3demo_put/first_file'),
  ('second_file', 's3://my-bucket/savin/tmp/s3demo_put/second_file')]
 ```
 
 Objects are stored in S3 in parallel for maximum throughput.",H4,https://docs.metaflow.org/scaling/data#store-multiple-objects-or-files,False,1005.0,122.0
1292,**Listing objects in S3**,"To get objects with `get` and `get_many`, you need to know the exact names of the
 objects to download. S3 is optimized for looking up specific names, so it is preferable
 to structure your code around known names. However, sometimes this is not possible and
 you need to check first what is available in S3.
 
 Metaflow provides two ways to list objects in S3: `list_paths` and `list_recursive`. The
 first method provides the next level of prefixes (directories) in S3, directly under the
 given prefix. The latter method provides all objects under the given prefix. Since
 `list_paths` returns a subset of prefixes returned by `list_recursive`, it is typically
 a much faster operation.
 
 Here's an example: First, let's create files in S3 in a hierarchy like this:
 
 ```
 first/a/object1
 first/b/x/object2
 second/c/object3
 ```
 
 ```python
 from metaflow import S3
 many = {'first/a/object1': 'data',
         'first/b/x/object2': 'data',
         'second/c/object3': 'data'}
 with S3(s3root='s3://my-bucket/savin/tmp/s3demo_list/') as s3:
     s3.put_many(many.items())
 ```
 
 Next, let's list all directories using `list_paths`:
 
 ```python
 from metaflow import S3
 with S3(s3root='s3://my-bucket/savin/tmp/s3demo_list/') as s3:
     for key in s3.list_paths():
         print key.key
 
 first
 second
 ```
 
 You can list multiple prefixes in parallel by giving `list_paths` a list of prefixes:
 
 ```python
 from metaflow import S3
 with S3(s3root='s3://my-bucket/savin/tmp/s3demo_list/') as s3:
     for key in s3.list_paths(['first', 'second']):
         print key.key
 
 a
 b
 c
 ```
 
 Listing may return either prefixes (directories) or objects. To distinguish between the
 two, use the `.exists` property of the returned `S3Object`:
 
 ```python
 from metaflow import S3
 with S3(s3root='s3://my-bucket/savin/tmp/s3demo_list/') as s3:
     for key in s3.list_paths(['first/a', 'first/b']):
         print key.key, 'object' if key.exists else 'prefix'
 
 object1 object
 x prefix
 ```
 
 If you want all objects under the given prefix, use the `list_recursive` method:
 
 ```python
 from metaflow import S3
 with S3(s3root='s3://my-bucket/savin/tmp/s3demo_list/') as s3:
     for key in s3.list_recursive():
         print key.key
 
 first/a/object1
 first/b/x/object2
 second/c/object3
 ```
 
 Similar to `list_paths`, `list_recursive` can take a list of prefixes to process in
 parallel.
 
 A common pattern is to list objects using either `list_paths` or `list_recursive`,
 filter out some keys from the listing, and provide the pruned list to `get_many` for
 fast parallelized downloading.",H4,https://docs.metaflow.org/scaling/data#listing-objects-in-s3,False,2614.0,417.0
1293,Caution: Overwriting data in S3,"You should avoid overwriting data in the same key (URL) in S3. S3 guarantees that new
 keys always reflect the latest data. In contrast, when you overwrite data in an existing
 key, there is a short period of time when a reader may see either the old version or the
 new version of the data.
 
 In particular, when you use `metaflow.S3` in your Metaflow flows, make sure that every
 task and step writes to a unique key. Otherwise you may find results unpredictable and
 inconsistent.
 
 Note that specifying `overwrite=False` in your `put_*` calls changes the behavior of S3
 slightly compared to the default mode of `overwrite=True`. There may be a small delay
 (typically in the order of milliseconds) before the key becomes available for reading.
 
 This is an important reason to rely on Metaflow artifacts, which handle this
 complication for you, whenever possible. If you absolutely need to handle this by
 yourself, one way to guarantee uniqueness is to use `current.task_id` from [the
 `current` module](/scaling/tagging.md#accessing-current-ids-in-a-flow) as a part of your
 S3 keys.",H3,https://docs.metaflow.org/scaling/data#caution-overwriting-data-in-s3,False,1094.0,175.0
1294,Maximizing S3 performance,"S3 can provide massive download speeds, tens of gigabits per second on large instances,
 when using `metaflow.S3`. In order to achieve the maximum throughput, pay attention to
 the following dimensions:",H2,https://docs.metaflow.org/scaling/data#maximizing-s3-performance,False,202.0,30.0
1295,Using `metaflow.S3` for in-memory processing,"For maximum performance, ensure that
 [the `@resources(memory=)`
 setting](/scaling/remote-tasks/introduction#requesting-resources-with-resources-decorator)
 is higher than the amount of data you are downloading with `metaflow.S3`.
 
 If the amount of data is higher than the available disk space, you can use the
 `use_tmpfs=True` with [`@batch`](/api/step-decorators/batch) an
 [`@kubernetes`](/api/step-decorators/kubernetes) to create an in-memory
 filesystem which `metaflow.S3` will use automatically.
 
 These options are available for `tmpfs`:
 
  - `use_tmpfs=True` enabled a `tmpfs` mountpoint and instructs `metaflow.S3` to use it
    as a destination for downloads. Note that you must ensure that the `tmpfs` size is
    large enough for all data downloaded.
 
  - `tmpfs_tempdir=False` will instruct `metaflow.S3` **to not use** the `tmpfs`. Use
    this option if you want to reserve the `tmpfs` mount for your own use only.
 
  - `tmpfs_size=N` allocates at most `N` megabytes for `tmpfs`. Note that unused space
    doesn't count towards actual memory usage, so you can safely overallocate space. By
    default, 50% of the available memory is made available for `tmpfs`.
 
  - `tmpfs_path=P` allows you to use an alternative mount point for `tmpfs`.
 
 You can access the current `tmpfs` mountpoint in your tasks with
 [`current.tempdir`](/api/current#current.tempdir). You can use it as fast
 temporary disk space for your own needs as well.",H3,https://docs.metaflow.org/scaling/data#using-metaflows3-for-in-memory-processing,False,1459.0,218.0
1296,Data in Local Files,"Similarly to [Parameters](/metaflow/basics.md#how-to-define-parameters-for-flows), you
 can define a data file to include as input for your flow. Metaflow will version the file
 and make it accessible to all the steps directly through the `self` object in your flow.
 
 This example allows the user to include a data file and compute its hash:
 
 ```python
 from metaflow import FlowSpec, step, IncludeFile
 
 class HashFileFlow(FlowSpec):
     myfile = IncludeFile(
         'myfile',
         is_text=False,
         help='My input',
         default='/Users/bob/myinput.bin')
 
     @step
     def start(self):
         import hashlib
         print('Hello from start')
         print('Hash of file is %s' % \
             str(hashlib.sha1(self.myfile).hexdigest()))
         self.next(self.end)
 
     @step
     def end(self):
         print('Goodbye')
 
 if __name__ == '__main__':
     HashFileFlow()
 ```
 
 You can specify the file to use by using:
 
 ```bash
 python hash_flow.py run --myfile '/path/to/input/file'
 ```",H2,https://docs.metaflow.org/scaling/data#data-in-local-files,False,1029.0,227.0
1297,Scalable Compute and Data,"After you have [prototyped a flow locally](/metaflow/introduction) and iterated with it
 for a while, you may face questions like these: How can I test the flow with more data
 without running out of memory? Or, how can I make the model train faster? You could try
 to optimize the code to work better on your laptop, but such [premature
 optimization](https://xkcd.com/1691/) is probably not the best use of your time.
 
 Instead, you can leverage the cloud to get a bigger laptop or more laptops (virtually,
 not literally). This is the Stage II in Metaflow development: Scaling flows with the
 cloud. Luckily Metaflow makes this trivially easy - no changes in the code required -
 after you have done the initial legwork to [configure infrastructure for
 Metaflow](/getting-started/infrastructure).
 
 ![](/assets/intro-cartoon-2.svg)",H1,https://docs.metaflow.org/scaling/introduction#scalable-compute-and-data,False,837.0,128.0
1298,Supersizing Flows,"Here's how Metaflow can help make your project more scalable:
 
 1. You can make your existing flows more scalable just by adding a line of code,
 `@resources`. This way you can request more CPUs, memory, or GPUs in your flows. Or, you
 can parallelize processing over multiple instances, even thousands of them.
 
 2. Once your project starts showing promise with realistic-size workloads, it may start
 attracting interest from colleagues too. Metaflow contains a number of features, such as
 [namespaces](/scaling/tagging), which make collaboration smoother by allowing many
 people contribute without interfering with each other's work accidentally.",H2,https://docs.metaflow.org/scaling/introduction#supersizing-flows,False,653.0,97.0
1299,Toolbox of Scalability,"There is no single magic formula for scalability. Instead of proposing a novel paradigm
 to make your Python code faster, Metaflow provides a set of pragmatic tools, leveraging
 the best off-the-shelf components and services, which help you make code more scalable
 and performant depending on your specific needs. 
 
 The scalability tools fall into three categories:",H3,https://docs.metaflow.org/scaling/introduction#toolbox-of-scalability,False,368.0,57.0
1300,What You Will Learn,"In this section, you will learn how to make your flows capable of handling more data and
 execute faster. You will also learn how to scale projects over multiple people by
 organizing results better. We cover five topics:
 
 1. [Executing tasks remotely with Kubernetes or AWS
    Batch](/scaling/remote-tasks/introduction)
 2. [Dealing with failures](/scaling/failures)
 3. [Managing execution environments](/scaling/dependencies)
 4. [Loading and storing data efficiently](/scaling/data)
 5. [Organizing results for smoother collaboration](/scaling/tagging)
 
 Before you proceed, make sure to [configure infrastructure for
 Metaflow](/getting-started/infrastructure) or sign up for a [Metaflow
 Sandbox](https://outerbounds.com/sandbox/).",H2,https://docs.metaflow.org/scaling/introduction#what-you-will-learn,False,741.0,89.0
1301,Organizing Results,"A boring, under-appreciated part of high-quality science (or any project work in
 general), is keeping results organized. This is the key to effective collaboration,
 versioning of parallel lines of work, and reproducibility.
 
 The good news is that Metaflow does 80% of this work for you without you having to do
 anything. This document explains how Metaflow keeps things organized with a concept
 called **namespaces** and how you can optionally make results even neater with **tags**.",H1,https://docs.metaflow.org/scaling/tagging#organizing-results,False,489.0,76.0
1302,Namespaces,"As explained in [Creating Flows](/metaflow/basics), Metaflow persists all runs and all
 the data artifacts they produce. Every run gets a unique run ID, e.g. `HelloFlow/546`,
 which can be used to refer to a specific set of results. You can access these results
 with the [Client API](/metaflow/client).
 
 Many users can use Metaflow concurrently. Imagine that Anne and Will are collaborating
 on a project that consists of two flows, `PredictionFlow` and `FeatureFlow`. As they,
 amongst other people, run their versions independently they end up with the following
 runs:
 
 ![](/assets/assets_metaflow_-lpjn0yp7r49jrnxca_5_-lpjryuuy7v5kovmxtsv_namespace1.png)
 
 Anne could analyze her latest `PredictionFlow` results in a notebook by remembering that
 her latest run is `PredictionFlow/8`. Fortunately, Metaflow makes this even easier
 thanks to **namespaces**:
 
 ![](/assets/assets_metaflow_-lpjn0yp7r49jrnxca_5_-lpjryuvqmspdu9w5imb_namespace2.png)
 
 When Anne runs `PredictionFlow`, her runs are automatically **tagged** with her
 username, prefixed with `user:`. By default, when Anne uses the [Client
 API](/metaflow/client) in a notebook or in a Python script, the API only returns results
 that are tagged with `user:anne`. Instead of having to remember the exact ID of her
 latest run, she can simply say:
 
 ```python
 from metaflow import Flow
 
 run = Flow('PredictionFlow').latest_run
 ```
 
 For Anne, this will return `'PredictionFlow/8'`. For Will, this will return
 `'PredictionFlow/5'`.",H2,https://docs.metaflow.org/scaling/tagging#namespaces,False,1509.0,201.0
1303,Switching Namespaces,"Namespaces are not about security or access control. They help you to keep results
 organized. During development, organizing results by the user who produced them is a
 sensible default.
 
 You can freely explore results produced by other people. In a notebook (for example),
 Anne can write
 
 ```python
 from metaflow import Flow, namespace
 
 namespace('user:will')
 run = Flow('PredictionFlow').latest_run
 ```
 
 to see Will's latest results, in this case, `'PredictionFlow/5'`.
 
 You can also access a specific run given its ID directly:
 
 ```python
 from metaflow import Flow, namespace
 
 run = Run('PredictionFlow/5')
 ```
 
 However, this will fail for Anne, since `PredictionFlow/5` is in Will's namespace. An
 important feature of namespaces is to make sure that you can't accidentally use someone
 else's results, which could lead to hard-to-debug, incorrect analyses.
 
 If Anne wants to access Will's results, she must do so explicitly by switching to Will's
 namespace:
 
 ```python
 from metaflow import Flow, namespace
 
 namespace('user:will')
 run = Run('PredictionFlow/5')
 ```
 
 In other words, you can use the Client API freely without having to worry about using
 incorrect results by accident.
 
 If you use the Client API in your flows to access results of other flows, you can use
 the `--namespace` flag on the command line to switch between namespaces. This is a
 better approach than hardcoding a `namespace()` function call in the code that defines
 your Metaflow workflow.",H3,https://docs.metaflow.org/scaling/tagging#switching-namespaces,False,1508.0,232.0
1304,Global Namespace,"What if you know a run ID, but you don't know whose namespace it belongs to? No worries,
 you can access all results in the Metaflow universe in the **global namespace**:
 
 ```python
 from metaflow import Flow, namespace
 
 namespace(None)
 run = Run('PredictionFlow/5')
 ```
 
 Setting `namespace(None)` allows you to access all results without limitations. Be
 careful though: relative references like `latest_run` make little sense in the global
 namespace since anyone can produce a new run at any time.",H3,https://docs.metaflow.org/scaling/tagging#global-namespace,False,508.0,79.0
1305,Production Namespaces,"During development, namespacing by the username feels natural. However, when you
 [schedule your flow to run
 automatically](../production/scheduling-metaflow-flows/introduction/), runs are not
 related to a specific user anymore. It is typical for multiple people to collaborate on
 a project that has a canonical production version. It is not obvious which user ""owns""
 the production version.
 
 Moreover, it is critical that you, and all other people, can keep experimenting on the
 project without having to worry about breaking the production version. If the production
 flow ran in the namespace of any individual, relative references like `latest_run` could
 break the production easily as the user keeps executing experimental runs.
 
 As a solution, by default the production namespace is made separate from the
 usernamespace:
 
 ![](</assets/namespace4_(3).png>)
 
 Isolated production namespaces have three main benefits:
 
 1. Production tokens allow all users of Metaflow to **experiment freely** with any
    project without having to worry about accidentally breaking a production deployment.
    Even if they ran step-functions create, they could not overwrite a production version
    without explicit consent, via a shared production token, by the person who did the
    previous authorized deployment.
 2. An isolated production namespace makes it easy to **keep production results separate
    from any experimental runs** of the same project running concurrently. You can rest
    assured that when you switch to a production namespace, you will see only results
    related to production - nothing more, nothing less.
 3. By having control over the production namespace, you can **alter data that is seen by
    production flows**. For instance, if you have separate training and prediction flows
    in production, the prediction flow can access the previously built model as long as
    one exists in the same namespace. By changing the production namespace, you can make
    sure that a new deployment isn't tainted by old results.
 
 If you are a single developer working on a new project, you don't have to do anything
 special to deal with production namespaces. You can rely on the default behavior of
 `step-functions create`.",H2,https://docs.metaflow.org/scaling/tagging#production-namespaces,False,2258.0,367.0
1306,Production tokens,"When you deploy a Flow to production for the first time, Metaflow creates a new,
 isolated production namespace for your production flow. This namespace is identified by
 a **production token**, which is a random identifier that identifies a production
 deployment, e.g. production:PredictionFlow3 above. You can examine production results in
 a notebook by switching to the production namespace.
 
 If another person wants to deploy a new version of the flow to production, they must use
 the same production token. You, or whoever has the token, are responsible for sharing it
 with users who are authorized to deploy new versions to production. This manual step
 should prevent random users from deploying versions to production inadvertently.
 
 After you have shared the production token with another person, they can deploy a new
 version on AWS Step Functions with
 
 ```bash
 python production_flow.py step-functions create --authorize TOKEN_YOU_SHARED_WITH_THEM
 ```
 
 or on Argo Workflows with
 
 ```bash
 python production_flow.py argo-workflows create --authorize TOKEN_YOU_SHARED_WITH_THEM
 ```
 
 or on Airflow with
 
 ```bash
 python production_flow.py airflow create --authorize TOKEN_YOU_SHARED_WITH_THEM
 ```
 
 They need to use the `--authorize` option only once. Metaflow stores the token for them
 after the first deployment, so they need to do this only once.",H3,https://docs.metaflow.org/scaling/tagging#production-tokens,False,1382.0,202.0
1307,Resetting a production namespace,"If you call `step-functions create` (or `argo-workflow create`) again, it will deploy an
 updated version of your code in the existing production namespace of the flow.
 
 Sometimes the code has changed so drastically that you want to recreate a fresh
 namespace for its results. You can do this as follows for AWS Step Functions:
 
 ```bash
 python production_flow.py step-functions create --generate-new-token
 ```
 
 and equivalently for Argo Workflows:
 ```bash
 python production_flow.py argo-workflows create --generate-new-token
 ```
 or Airflow:
 ```bash
 python production_flow.py airflow create --generate-new-token
 ```
 
 This will deploy a new version in production using a fresh, empty namespace.",H3,https://docs.metaflow.org/scaling/tagging#resetting-a-production-namespace,False,710.0,99.0
1308,Resuming across namespaces,"[The `resume` command](/metaflow/debugging#how-to-use-the-resume-command) is smart
 enough to work across production and personal namespaces. You can `resume` a production
 workflow without having to do anything special with namespaces.
 
 You can resume runs of other users, and you can resume any production runs. The results
 of your resumed runs are always created in your personal namespace.",H2,https://docs.metaflow.org/scaling/tagging#resuming-across-namespaces,False,396.0,55.0
1309,Tagging,"The `user:` tag is assigned by Metaflow automatically. In addition to automatically
 assigned tags, you can add and remove arbitrary tags in objects. Tags are an excellent
 way to add extra annotations to runs, tasks etc., which makes it easier for you and
 other people to find and retrieve results of interest.",H2,https://docs.metaflow.org/scaling/tagging#tagging,False,312.0,52.0
1310,"If you know a tag to be attached before a run starts, you can add it using the `run","instance, this will annotate a `HelloFlow` run with a tag `crazy_test`.
 
 ```bash
 python helloworld.py run --tag crazy_test
 ```
 
 Often, you may want to add or change tags after a run has completed. In contrast to
 artifacts, tags can be mutated any time: Consider them as mutating interpretations of
 immutable (arti)facts. You can mutate tags either [using the Client
 API](/metaflow/client#adding-removing-and-replacing-tags) or the command line.
 
 Add a tag on the command line like this:
 
 ```
 python helloworld.py tag add --run-id 2 crazy_test
 ```
 
 Remove works symmetrically:
 ```
 python helloworld.py tag remove --run-id 2 crazy_test
 ```
 
 You can see the current set of tags with
 ```
 python helloworld.py tag list
 ```
 
 Note that the above command lists also [system tags](/metaflow/client#system-tags) that
 can not be mutated, but they can be used for filtering.",H2,https://docs.metaflow.org/scaling/tagging#if-you-know-a-tag-to-be-attached-before-a-run-starts-you-can-add-it-using-the-run,False,890.0,136.0
1311,Filtering by tags,"You can access runs (or steps or tasks) with a certain tag easily using the Client API:
 
 ```python
 from metaflow import Flow
 run = list(Flow('HelloFlow').runs('crazy_test'))[0]
 ```
 
 This will return the latest run of `HelloFlow` with a tag `crazy_test` in your
 namespace. Filtering is performed both based on the current `namespace()` and the tag
 filter.
 
 You can also filter by multiple tags:
 
 ```python
 from metaflow import Flow
 run = list(Flow('HelloFlow').runs('crazy_test', 'date:20180301'))[0]
 ```
 
 This requires that all the tags listed, and the current namespace, are present in the
 object.
 
 You can see the set of tags assigned to an object with the `.tags` property. In the
 above case, `run.tags` would return a set with a string `crazy_test` amongst other
 automatically assigned tags.",H3,https://docs.metaflow.org/scaling/tagging#filtering-by-tags,False,818.0,126.0
1312,Tags as Namespaces,"Let's consider again the earlier example with Anne and Will. They are working on their
 own versions of `PredictionFlow` but they want to collaborate on `FeatureFlow`. They
 could add a descriptive tag, say `xyz_features`, to `FeatureFlow` runs.
 
 ![](/assets/namespace3.png)
 
 Now, they can easily get the latest results of `FeatureFlow` regardless of the user who
 ran the flow:
 
 ```python
 from metaflow import Flow
 namespace('xyz_features')
 run = Flow('FeatureFlow').latest_run
 ```
 
 This will return `FeatureFlow/34` which happened to be run by Anne. If Will runs the
 flow again, his results will be the latest results in this namespace.
 
 We encourage you to use a combination of namespaces, domain-specific tags, and filtering
 by tags to design a workflow that works well for your project.",H3,https://docs.metaflow.org/scaling/tagging#tags-as-namespaces,False,807.0,123.0
1313,Accessing Current IDs in a Flow,"*This section contains an overview of `current`. For a complete API, see [the API
 reference for `current`](/api/current).*
 
 Tagging and namespaces, together with the [Client API](/metaflow/client), are the main
 ways for accessing results of past runs. Metaflow uses these mechanisms to organize and
 isolate results automatically, so in most cases you don't have to do anything.
 
 However, in some cases you may need to deal with IDs explicitly. For instance, if your
 flow interacts with external systems, it is a good idea to inform the external system
 about the identity of the run, so you can trace back any issues to a specific run. Also
 IDs can come in handy if you need to version externally stored data.
 
 For this purpose, Metaflow provides a singleton object `current` that represents the
 identity of the currently running task. Use it in your `FlowSpec` to retrieve current
 IDs of interest:
 
 ```python
 from metaflow import FlowSpec, step, current
 
 class CurrentFlow(FlowSpec):
 
     @step
     def start(self):
         print(""flow name: %s"" % current.flow_name)
         print(""run id: %s"" % current.run_id)
         print(""origin run id: %s"" % current.origin_run_id)
         print(""step name: %s"" % current.step_name)
         print(""task id: %s"" % current.task_id)
         print(""pathspec: %s"" % current.pathspec)
         print(""namespace: %s"" % current.namespace)
         print(""username: %s"" % current.username)
         print(""flow parameters: %s"" % str(current.parameter_names))
         self.next(self.end)
 
     @step
     def end(self):
         print(""end has a different step name: %s"" % current.step_name)
         print(""end has a different task id: %s"" % current.task_id)
         print(""end has a different pathspec: %s"" % current.pathspec)
 
 if __name__ == '__main__':
     CurrentFlow()
 ```
 
 In particular, the value of `current.pathspec` is convenient as an unambiguous
 identifier of a task. For instance, the above script printed out
 
 ```
 pathspec: CurrentFlow/1/start/550539
 ```
 
 Now you can inspect this particular task using[ the Client API](/metaflow/client) by
 instantiating a `Task` object as follows:
 
 ```python
 from metaflow import Task
 task = Task('CurrentFlow/1/start/550539')
 print task.stdout
 ```
 
 This prints out the output of the task identified by the `pathspec`.
 
 The `current` singleton also provides programmatic access to the CLI option
 `--origin-run-id` used by the
 [resume](/metaflow/debugging#how-to-use-the-resume-command) within your flow code.
 
 If a user explicitly overrides the CLI option `--origin-run-id`, the `current` singleton
 would reflect that value.
 
 If not, it would be the id of the last invocation of `run` (successful or not).
 
 :::info
 
 This value would remain the same even after multiple successful `resume` invocations. If
 you don't want this behavior, you can always override the CLI option `origin-run-id` and
 `resume` a specific run.
 
 :::
 
 For regular `run` invocations, the value of `current.origin_run_id` is `None`.
 
 Suppose we invoked `resume` for the above script to re-run everything from `start`
 without explicitly overriding the CLI option `origin-run-id`, we can see the value
 chosen by Metaflow using the `current` singleton:
 
 ```bash
 python current_flow.py resume start
 ```
 
 You should see the `origin_run_id` used by the `resume` in the output (the exact value
 for you might be different):
 
 ```
 origin run id: 4
 ```",H2,https://docs.metaflow.org/scaling/tagging#accessing-current-ids-in-a-flow,False,3478.0,611.0
1314,Using Kubernetes,"Here are some useful tips and tricks related to running Metaflow on Kubernetes. See our
 engineering resources for additional information about [setting up and operating
 Kubernetes for Metaflow](https://outerbounds.com/docs/engineering-welcome/).",H1,https://docs.metaflow.org/scaling/remote-tasks/kubernetes#using-kubernetes,False,247.0,28.0
1315,What value of `@timeout` should I set?,"Metaflow sets a default timeout of 5 days so that you tasks don't get stuck infinitely
 while running on Kubernetes. For more details on how to use `@timeout` please read
 [this.](../failures.md#timing-out-with-the-timeout-decorator)",H2,https://docs.metaflow.org/scaling/remote-tasks/kubernetes#what-value-of-timeout-should-i-set,False,233.0,31.0
1316,How much `@resources` can I request?,"Here are the current defaults for different resource types:
 
 * `cpu`: 1
 * `memory`: 4096 \(4GB\)
 * `disk`: 10240 \(10GB\)
 
 When setting `@resources`, keep in mind the configuration of your Kubernetes cluster.
 Your pod will be stuck in an unschedulable state if Kubernetes is unable to provision
 the requested resources. Additionally, as a good measure, don't request more resources
 than what your workflow actually needs. On the other hand, never optimize resources
 prematurely.
 
 You can place your Kubernetes pod in a specific namespace by using the `namespace`
 argument. By default, all pods execute on a vanilla [python docker
 image](https://hub.docker.com/_/python/) corresponding to the version of Python
 interpreter used to launch the flow and can be overridden using the `image` argument.
 
 You can also specify the resource requirements on command line as well:
 
 ```bash
 $ python BigSum.py run --with kubernetes:cpu=4,memory=10000,namespace=foo,image=ubuntu:latest
 ```",H2,https://docs.metaflow.org/scaling/remote-tasks/kubernetes#how-much-resources-can-i-request,False,996.0,144.0
1317,Accessing Kubernetes logs,"As a convenience feature, you can also see the logs of any past step as follows:
 
 ```bash
 $ python bigsum.py logs 15/end
 ```",H2,https://docs.metaflow.org/scaling/remote-tasks/kubernetes#accessing-kubernetes-logs,False,128.0,24.0
1318,Disk space,"You can request higher disk space for pods by using the `disk` attribute of
 `@kubernetes`.",H2,https://docs.metaflow.org/scaling/remote-tasks/kubernetes#disk-space,False,91.0,15.0
1319,Executing Tasks Remotely,"There are two ways to handle larger amounts of data and compute:
 
 1. _Scale up_ by running your code on a larger machine with more memory, CPU cores, and
    GPUs, or
 2. _Scale out_ by using more machines in parallel.
 
 As described below, Metaflow supports the former through the `@resources` decorator and
 the latter through [foreach](/metaflow/basics#foreach) when flows are run on Kubernetes
 or AWS Batch.
 
 Everything described on this page applies to all compute platforms supported by
 Metaflow. The data scientist can write their flows using foreaches and the `@resource`
 decorator knowing that the code will execute on any supported platforms. For additional
 tips and tricks related to specific systems, see [Using AWS Batch](aws-batch) and [Using
 Kubernetes](kubernetes).",H1,https://docs.metaflow.org/scaling/remote-tasks/introduction#executing-tasks-remotely,False,791.0,123.0
1320,Requesting resources with `resources` decorator,"Consider the following example:
 
 ```python
 from metaflow import FlowSpec, step, resources
 
 class BigSum(FlowSpec):
 
     @resources(memory=60000, cpu=1)
     @step
     def start(self):
         import numpy
         import time
         big_matrix = numpy.random.ranf((80000, 80000))
         t = time.time()
         self.sum = numpy.sum(big_matrix)
         self.took = time.time() - t
         self.next(self.end)
 
     @step
     def end(self):
         print(""The sum is %f."" % self.sum)
         print(""Computing it took %dms."" % (self.took * 1000))
 
 if __name__ == '__main__':
     BigSum()
 ```
 
 This example creates a huge 80000x80000 random matrix, `big_matrix`. The matrix requires
 about 80000^2 \* 8 bytes = 48GB of memory. 
 
 If you attempt to run this on your local machine, it is likely that the following will
 happen:
 
 ```bash
 $ python BigSum.py run
 
 2019-11-29 02:43:39.689 [5/start/21975 (pid 83812)] File ""BugSum.py"", line 11, in start
 2018-11-29 02:43:39.689 [5/start/21975 (pid 83812)] big_matrix = numpy.random.ranf((80000, 80000))
 2018-11-29 02:43:39.689 [5/start/21975 (pid 83812)] File ""mtrand.pyx"", line 856, in mtrand.RandomState.random_sample
 2018-11-29 02:43:39.689 [5/start/21975 (pid 83812)] File ""mtrand.pyx"", line 167, in mtrand.cont0_array
 2018-11-29 02:43:39.689 [5/start/21975 (pid 83812)] MemoryError
 2018-11-29 02:43:39.689 [5/start/21975 (pid 83812)]
 2018-11-29 02:43:39.844 [5/start/21975 (pid 83812)] Task failed.
 2018-11-29 02:43:39.844 Workflow failed.
     Step failure:
     Step start (task-id 21975) failed.
 ```
 
 This fails quickly due to a `MemoryError` on most laptops as we are unable to allocate
 48GB of memory. 
 
 The `@resources` decorator suggests resource requirements for a step. The `memory`
 argument specifies the amount of RAM in megabytes and `cpu` the number of CPU cores
 requested. It does not produce the resources magically, which is why the run above
 failed. The `@resources` decorator takes effect only when combined with another
 decorator that describes what compute platform, like Kubernetes or AWS Batch, to use.
 
 Let's use the `--with` option to attach a desired decorator to all steps on the command
 line. Choose one of the commands in the tabs below corresponding to whichever you use-
 Kubernetes or AWS Batch. This assumes that you have [configured one of these systems
 work with Metaflow](/getting-started/infrastructure).
 
 import Tabs from '@theme/Tabs';
 import TabItem from '@theme/TabItem';
 
 <Tabs>
 <TabItem value=""k8s"" label=""Kubernetes"">
 
 ```batch
 $ python BigSum.py run --with kubernetes
 ```
 
 </TabItem>
 
 <TabItem value=""batch"" label=""AWS Batch"">
 
 ```k8s
 $ python BigSum.py run --with batch
 ```
 
 </TabItem>
 </Tabs>
 
 The `--with batch` or `--with kubernetes` option instructs Metaflow to run all tasks as
 separate jobs on the chosen compute platform, instead of using a local process for each
 task. It has the same effect as adding the decorator above all steps in the source code.
 
 This time the run should succeed thanks to the large enough instance, assuming a large
 enough instance is available in your compute environment. In this case the `resources`
 decorator is used as a prescription for the size of the instance that the job should run
 on. Make sure that this resource requirement can be met. If a large enough instance is
 not available, the task won't start executing.
 
 You should see an output like this:
 
 ```bash
 The sum is 3200003911.795288.
 Computing it took 4497ms.
 ```
 
 In addition to `cpu` and `memory` you can specify `gpu=N` to request N GPUs for the
 instance.",H2,https://docs.metaflow.org/scaling/remote-tasks/introduction#requesting-resources-with-resources-decorator,False,3641.0,621.0
1321,Running only specific steps remotely,"The `resources` decorator is an annotation that signals how much resources are required
 by a step. By itself, it does not force the step to be executed on any particular
 platform. This is convenient as you can make the choice later, executing the same flow
 on different environments without changes.
 
 Sometimes it is useful to make sure that a step always executes on a certain compute
 platform, maybe using a platform-specific configuration. You can achieve this by adding
 either `@batch` or `@kubernetes` above steps that should be executed remotely. The
 decorators accept the same keyword arguments as `@resources` as well as
 platform-specific arguments that you can find listed in [the API
 reference](/api/step-decorators).
 
 For instance, in the example above, replace `@resources` with `@batch` or `@kubernetes`
 and run it as follows:
 
 ```bash
 $ python BigSum.py run
 ```
 
 You will see that the `start` step gets executed on a remote instance but the `end`
 step, which does not need special resources, is executed locally. You could even mix
 decorators so that some steps execute on `@kubernetes`, some on `@batch`, and some
 locally.",H3,https://docs.metaflow.org/scaling/remote-tasks/introduction#running-only-specific-steps-remotely,False,1159.0,183.0
1322,Parallelization over multiple cores,"When running locally, tasks are executed as separate processes. The operating system
 takes care of allocating them to separate CPU cores, so they will actually execute in
 parallel assuming that enough CPU cores are available. Hence, your flow can utilize
 multiple cores without you having to do anything special besides defining branches in
 the flow.
 
 When running remotely on `@batch` or `@kubernetes`, branches are mapped to separate jobs
 that are executed in parallel, allowing you to *scale horizontally* to any number of
 parallel tasks. In addition, you may take advantage of multiple CPU cores inside a task.
 This may happen automatically if you use a modern ML library like PyTorch or Scikit
 Learn, or you may parallelize functions explicitly, as explained below.",H3,https://docs.metaflow.org/scaling/remote-tasks/introduction#parallelization-over-multiple-cores,False,780.0,123.0
1323,Parallel map,"Metaflow provides a utility function called `parallel_map` that helps take advantage of
 multiple CPU cores. This function is almost equivalent to `Pool().map` in the Python's
 built-in
 [multiprocessing](https://docs.python.org/2/library/multiprocessing.html#multiprocessing.pool.multiprocessing.Pool.map)
 library. The main differences are the following:
 
 * `parallel_map` supports lambdas and any other callables of Python.
 * `parallel_map` does not suffer from bugs present in `multiprocessing`.
 * `parallel_map` can handle larger amounts of data.
 
 You may also use `parallel_map` to parallelize simple operations that might be too
 cumbersome to implement as separate steps.
 
 Here is an extension of our previous example that implements a multicore `sum()` by
 partitioning the matrix by row:
 
 ```python
 from metaflow import FlowSpec, step, batch, parallel_map
 
 class BigSum(FlowSpec):
 
     @resources(memory=60000, cpu=8)
     @step
     def start(self):
         import numpy
         import time
         big_matrix = numpy.random.ranf((80000, 80000))
         t = time.time()
         parts = parallel_map(lambda i: big_matrix[i:i+10000].sum(),
                              range(0, 80000, 10000))
         self.sum = sum(parts)
         self.took = time.time() - t
         self.next(self.end)
 
     @step
     def end(self):
         print(""The sum is %f."" % self.sum)
         print(""Computing it took %dms."" % (self.took * 1000))
 
 if __name__ == '__main__':
     BigSum()
 ```
 
 Note that we use `cpu=8` to request enough CPU cores, so our `parallel_map` can benefit
 from optimal parallelism. Disappointingly, in this case the parallel `sum` is not faster
 than the original simple implementation due to the overhead of launching separate
 processes in `parallel_map`. A less trivial operation might see a much larger
 performance boost.",H4,https://docs.metaflow.org/scaling/remote-tasks/introduction#parallel-map,False,1871.0,362.0
1324,**Safeguard flags**,"It is almost too easy to execute tasks remotely using Metaflow. Consider a foreach loop
 defined as follows:
 
 ```python
 self.params = range(1000)
 self.next(self.fanned_out, foreach='params')
 ```
 
 When run with `--with batch` or `--with kubernetes`, this code would launch up to 1000
 parallel instances which may turn out to be quite expensive.
 
 To safeguard against inadvertent launching of many parallel jobs, the `run` and `resume`
 commands have a flag `--max-num-splits` which fails the task if it attempts to launch
 more than 100 splits by default. Use the flag to increase the limit if you actually need
 more tasks.
 
 ```bash
 $ python myflow.py run --max-num-splits 200
 ```
 
 Another flag, `--max-workers`, limits the number of tasks run in parallel. Even if a
 foreach launched 100 splits, `--max-workers` would make only 16 \(by default\) of them
 run in parallel at any point in time. If you want more parallelism, increase the value
 of `--max-workers`.
 
 ```bash
 $ python myflow.py run --max-workers 32
 ```",H2,https://docs.metaflow.org/scaling/remote-tasks/introduction#safeguard-flags,False,1036.0,163.0
1325,Big Data,"Thus far, we have focused on CPU and memory-bound steps. Loading and processing big data
 is often an IO-bound operation which requires a different approach. Read [Loading and
 Storing Data](/scaling/data) for more details about how to build efficient data
 pipelines in Metaflow.",H2,https://docs.metaflow.org/scaling/remote-tasks/introduction#big-data,False,280.0,42.0
1326,Using AWS Batch,"Here are some useful tips and tricks related to running Metaflow on AWS Batch. See our
 engineering resources for additional information about [setting up and operating AWS
 Batch for Metaflow](https://outerbounds.com/docs/engineering-welcome/).",H1,https://docs.metaflow.org/scaling/remote-tasks/aws-batch#using-aws-batch,False,245.0,30.0
1327,What value of `@timeout` should I set?,"Metaflow sets a default timeout of 5 days so that you tasks don't get stuck infinitely
 while running on AWS Batch. For more details on how to use `@timeout` please read
 [this.](../failures.md#timing-out-with-the-timeout-decorator)",H2,https://docs.metaflow.org/scaling/remote-tasks/aws-batch#what-value-of-timeout-should-i-set,False,232.0,32.0
1328,How much `@resources` can I request?,"Here are the current defaults for different resource types:
 
 * `cpu`: 1
 * `memory`: 4000 \(4GB\)
 
 When setting `@resources`, keep in mind the configuration of your AWS Batch Compute
 Environment. Your job will be stuck in a `RUNNABLE` state if AWS is unable to provision
 the requested resources. Additionally, as a good measure, don't request more resources
 than what your workflow actually needs. On the other hand, never optimize resources
 prematurely.
 
 You can place your AWS Batch task in a specific queue by using the `queue` argument. By
 default, all tasks execute on a vanilla [python docker
 image](https://hub.docker.com/_/python/) corresponding to the version of Python
 interpreter used to launch the flow and can be overridden using the `image` argument.
 
 You can also specify the resource requirements on command line as well:
 
 ```bash
 $ python BigSum.py run --with batch:cpu=4,memory=10000,queue=default,image=ubuntu:latest
 ```",H2,https://docs.metaflow.org/scaling/remote-tasks/aws-batch#how-much-resources-can-i-request,False,958.0,143.0
1329,My job is stuck in `RUNNABLE` state. What do I do?,"Consult [this
 article](https://docs.aws.amazon.com/batch/latest/userguide/troubleshooting.html#job_stuck_in_runnable).",H2,https://docs.metaflow.org/scaling/remote-tasks/aws-batch#my-job-is-stuck-in-runnable-state-what-do-i-do,False,119.0,3.0
1330,Listing and killing AWS Batch tasks,"If you interrupt a Metaflow run, any AWS Batch tasks launched by the run get killed by
 Metaflow automatically. Even if something went wrong during the final cleanup, the tasks
 will finish and die eventually, at the latest when they hit the maximum time allowed for
 an AWS Batch task.
 
 If you want to make sure you have no AWS Batch tasks running, or you want to manage them
 manually, you can use the `batch list` and `batch kill` commands.
 
 You can easily see what AWS Batch tasks were launched by your latest run with
 
 ```bash
 $ python myflow.py batch list
 ```
 
 You can kill the tasks started by the latest run with
 
 ```bash
 $ python myflow.py batch kill
 ```
 
 If you have started multiple runs, you can make sure there are no orphaned tasks still
 running with
 
 ```bash
 $ python myflow.py batch list --my-runs
 ```
 
 You can kill the tasks started by the latest run with
 
 ```bash
 $ python myflow.py batch kill --my-runs
 ```
 
 If you see multiple runs running, you can cherry-pick a specific job, e.g. 456, to be
 killed as follows
 
 ```bash
 $ python myflow.py batch kill --run-id 456
 ```
 
 If you are working with another person, you can see and kill their tasks related to this
 flow with
 
 ```bash
 $ python myflow.py batch kill --user willsmith
 ```
 
 Note that all the above commands only affect the flow defined in your script. You can
 work on many flows in parallel and be confident that `kill` kills tasks only related to
 the flow you called `kill` with.",H2,https://docs.metaflow.org/scaling/remote-tasks/aws-batch#listing-and-killing-aws-batch-tasks,False,1499.0,273.0
1331,Accessing AWS Batch logs,"As a convenience feature, you can also see the logs of any past step as follows:
 
 ```bash
 $ python bigsum.py logs 15/end
 ```",H2,https://docs.metaflow.org/scaling/remote-tasks/aws-batch#accessing-aws-batch-logs,False,128.0,24.0
1332,Disk space,"You can request higher disk space on AWS Batch instances by using an unmanaged Compute
 Environment with a custom AMI.",H2,https://docs.metaflow.org/scaling/remote-tasks/aws-batch#disk-space,False,118.0,20.0
1333,Installing Metaflow,"Metaflow is available as a Python package for macOS and Linux. You can visit our [GitHub
 repository](https://github.com/Netflix/metaflow) or get the latest version from
 [PyPI](https://pypi.org/):
 
 ```bash
 pip install metaflow
 ```
 
 We highly recommend using **Python 3** for new projects. Metaflow supports Python 2.7
 for legacy applications, but Python 3 has fewer bugs and is better supported than [the
 deprecated Python 2.7](http://pythonclock.org).
 
 :::info
 
 If you want to get a feel of Metaflow and the infrastructure behind it without having to
 install anything locally, you can do in the browser by signing up for [a Metaflow
 Sandbox](https://outerbounds.com/sandbox/).
 
 :::
 
 
 Now you are ready to get your hands dirty with the [Tutorials](tutorials/).",H1,https://docs.metaflow.org/getting-started/install#installing-metaflow,False,780.0,114.0
1334,Upgrading Metaflow,"If you have installed Metaflow previously, you can upgrade to the latest version with:
 
 ```bash
 pip install --upgrade metaflow
 ```",H2,https://docs.metaflow.org/getting-started/install#upgrading-metaflow,False,134.0,21.0
1335,Deploying Infrastructure for Metaflow,"While you can [get started with Metaflow easily](/getting-started/install) on your
 laptop, the main benefits of Metaflow lie in its ability to [scale out to external
 compute clusters](/scaling/introduction) and to [deploy to production-grade workflow
 orchestrators](/production/introduction). To benefit from these features, you need to
 configure Metaflow and the infrastructure behind it appropriately. A separate guide,
 [Metaflow Resources for Engineers](https://outerbounds.com/engineering/welcome/) covers
 everything related to such deployments. This page provides a quick overview.",H1,https://docs.metaflow.org/getting-started/infrastructure#deploying-infrastructure-for-metaflow,False,592.0,69.0
1336,Supported infrastructure components,"Since modern data science / ML applications are powered by a number of interconnected
  systems, it is useful to organize them as an infrastructure stack like the one
 illustrated below ([Why? See here](/introduction/why-metaflow)). You can see logos of
 all supported systems which you can use to enable each layer.
 
 Consider this illustration as a menu that allows you to build your own pizza: You get to
 customize your own crust, sauce, toppings, and cheese. You can make the choices based on
 your existing business infrastructure and the requirements and preferences of your
 organization. Fortunately, Metaflow provides a consistent API for all these
 combinations, so you can even change the choices later without having to rewrite your
 flows.
 
 <object style={{width: 700}} type=""image/svg+xml""
 data=""/assets/infra-stack.svg""></object>
 
 The table below explains the five major deployment options for Metaflow and what
 components of the stack are supported in each. You can choose to deploy Metaflow on:
 
 1. **Only local** environment - [just `pip install metaflow` on any
    workstation](/getting-started/install).
 2. **AWS** either on [EKS as a Kubernetes
    platform](https://outerbounds.com/engineering/deployment/aws-k8s/deployment/) or
    [using AWS-managed
    services](https://outerbounds.com/engineering/deployment/aws-managed/introduction/).
 3. **Azure** on [AKS as a Kubernetes
    platform](https://outerbounds.com/engineering/deployment/azure-k8s/deployment/).
 4. **Google Cloud** on [GKE as a Kubernetes
    platform](https://outerbounds.com/engineering/deployment/gcp-k8s/deployment/).
 4. [Any **Kubernetes**
    cluster](https://outerbounds.com/engineering/deployment/aws-k8s/deployment/)
    including on-premise deployments.
 
 | Layer         | Component                                                                                                    | Description                                        | Only Local | AWS | Azure | GCP | K8s |
 | ------------- | ------------------------------------------------------------------------------------------------------------ | -------------------------------------------------- | ---------- | --- | ----- | --- | --- |
 | Modeling      | <img src=""/assets/infra-python.png"" width="" 30"" style={{verticalAlign:""middle""}}/> **Python libraries**      | Any Python libraries                               | 🟢          | 🟢   | 🟢     | 🟢   | 🟢   |
 | Deployment    | <img src=""/assets/infra-argo.png"" width="" 30"" style={{verticalAlign:""middle""}}/> **Argo Workflows**          | Open-source production-grade workflow orchestrator |            | 🟢   | 🟢     | 🟢   | 🟢   |
 | Deployment    | <img src=""/assets/infra-sfn.png"" width="" 30"" style={{verticalAlign:""middle""}}/> **Step Functions**           | AWS-managed production-grade workflow orchestrator |            | 🟢   |       |     |     |
 | Deployment    | <img src=""/assets/infra-airflow.png"" width="" 30"" style={{verticalAlign:""middle""}}/> **Apache Airflow**       | Popular open-source workflow orchestrator          |            | 🟢   | 🟢     | 🟢   | 🟢   |
 | Versioning    | <img src=""/assets/infra-mflocal.png"" width="" 30"" style={{verticalAlign:""middle""}}/> **Local Metadata**       | Metaflow's tracking in local files                 | 🟢          | 🟢   | 🟢     | 🟢   | 🟢   |
 | Versioning    | <img src=""/assets/infra-metaflow.png"" width="" 30"" style={{verticalAlign:""middle""}}/> **Metadata Service**    | Metaflow's tracking in a central database          |            | 🟢   | 🟢     | 🟢   | 🟢   |
 | Orchestration | <img src=""/assets/infra-mflocal.png"" width="" 30"" style={{verticalAlign:""middle""}}/> **Local Orchestrator**   | Metaflow's local workflow orchestrator             | 🟢          | 🟢   | 🟢     | 🟢   | 🟢   |
 | Compute       | <img src=""/assets/infra-mflocal.png"" width="" 30"" style={{verticalAlign:""middle""}}/> **Local Processes**      | Metaflow tasks as local processes                  | 🟢          | 🟢   | 🟢     | 🟢   | 🟢   |
 | Compute       | <img src=""/assets/infra-batch.png"" width="" 30"" style={{verticalAlign:""middle""}}/> **AWS Batch**              | AWS-managed batch compute service                  |            | 🟢   |       |     |
 | Compute       | <img src=""/assets/infra-k8s.png"" width="" 30"" style={{verticalAlign:""middle""}}/> **Kubernetes**               | Open-source batch compute platform                 |            | 🟢   | 🟢     | 🟢   | 🟢   |
 | Data          | <img src=""/assets/infra-mflocal.png"" width="" 30"" style={{verticalAlign:""middle""}}/> **Local Datastore**      | Metaflow artifacts in local files                  | 🟢          | 🟢   | 🟢     | 🟢   | 🟢   |
 | Data          | <img src=""/assets/infra-s3.png"" width="" 30"" style={{verticalAlign:""middle""}}/> **AWS S3**                    | Metaflow artifacts in AWS-managed storage          |            | 🟢   |       |     | 🟢   |
 | Data          | <img src=""/assets/infra-azureblob.png"" width="" 30"" style={{verticalAlign:""middle""}}/> **Azure Blob Storage** | Metaflow artifacts in Azure-managed storage        |            |     | 🟢     |     | 🟢   |
 | Data          | <img src=""/assets/infra-gcs.png"" width="" 30"" style={{verticalAlign:""middle""}}/> **Google Cloud Storage**     | Metaflow artifacts in Google-managed storage       |            |     |       | 🟢   | 🟢   |
 Note that fast prototyping with the Local Orchestrator is supported in all these
 options, but the **only local** option doesn't support scalability with an external
 compute layer, nor production-grade deployments.
 
 :::info
 
 You can test the AWS/Azure/GCP/Kubernetes stack easily in your browser for free
 by signing up for a [Metaflow Sandbox](https://outerbounds.com/sandbox/).
 
 :::",H2,https://docs.metaflow.org/getting-started/infrastructure#supported-infrastructure-components,False,5709.0,1458.0
1337,Example stacks,Here are some typical deployments that we have seen in action:,H2,https://docs.metaflow.org/getting-started/infrastructure#example-stacks,False,62.0,11.0
1338,Local: Effortless prototyping,"**Just `pip install metaflow` to deploy this stack**
 
 This is the stack you get by default when you [install Metaflow
 locally](/getting-started/install). It's main benefit is zero configuration and
 maintenance - it works out of the box. It is a great way to get started with Metaflow.
 
 <object style={{width: 700}} type=""image/svg+xml""
 data=""/assets/infra-stack-all-local.svg""></object>
 
 When you want to start [collaborating with multiple people](/scaling/tagging) which
 requires a central metadata service, or you want to start running [larger-scale
 workloads](/scaling/introduction), or you want to [deploy your
 workflows](/production/introduction) so that they run even when your laptop is asleep,
 look into more featureful stacks below.",H3,https://docs.metaflow.org/getting-started/infrastructure#local-effortless-prototyping,False,754.0,100.0
1339,"Low-maintenance scalable prototyping, powered by AWS","**[Click here to deploy this
 stack](https://outerbounds.com/engineering/deployment/aws-managed/introduction/)**
 
 If you are looking for the easiest and the most affordable way to scale out compute to
 the cloud, including cloud-based GPUs, this stack is a great option. Consider the
 benefits:",H3,https://docs.metaflow.org/getting-started/infrastructure#low-maintenance-scalable-prototyping-powered-by-aws,False,296.0,38.0
1340,"Low-maintenance full stack, powered by AWS","**[Click here to deploy this
 stack](https://outerbounds.com/engineering/deployment/aws-managed/introduction/)**
 
 If you need the full stack of data science/ML infrastructure but want to spend a minimal
 amount of effort to set up and manage it, choose this option. You get all the benefits
 of AWS Batch as described above, as well as production deployments on [AWS Step
 Functions](https://aws.amazon.com/step-functions/) which is a highly-available, scalable
 workflow orchestrator managed by AWS. Metaflow tracks everything in a central metadata
 service, making collaboration straightforward.
 
 <object style={{width: 700}} type=""image/svg+xml""
 data=""/assets/infra-stack-aws-native.svg""></object>
 
 Here are the main reasons for not using this stack:",H3,https://docs.metaflow.org/getting-started/infrastructure#low-maintenance-full-stack-powered-by-aws,False,760.0,93.0
1341,"Customizable full stack on AWS, powered by Kubernetes","**[Click here to deploy this
 stack](https://outerbounds.com/engineering/deployment/aws-k8s/deployment/)**
 
 If your engineering team has prior experience with Kubernetes, they might prefer a
 familiar stack that works with their existing security policies, observability tools,
 and deployment mechanisms. In this case, this Kubernetes-native stack featuring compute
 on Kubernetes and deployments on reliable, scalable, open-source [Argo
 Workflows](https://argoproj.github.io/argo-workflows/) is a good option.
 
 This stack can be [easily deployed on EKS on
 AWS](https://outerbounds.com/engineering/deployment/aws-k8s/deployment/), leveraging S3
 as the datastore. Alternatively, some companies run this stack on-premise using
 [Minio](https://min.io/) as an S3-compatible datastore.
 
 <object style={{width: 700}} type=""image/svg+xml""
 data=""/assets/infra-stack-aws-k8s.svg""></object>
 
 This stack requires more maintenance than the AWS-native stack above, although the basic
 setup is quite manageable if your organization is already familiar with Kubernetes.",H3,https://docs.metaflow.org/getting-started/infrastructure#customizable-full-stack-on-aws-powered-by-kubernetes,False,1069.0,117.0
1342,"Customizable full stack on Azure, powered by Kubernetes","**[Click here to deploy this
 stack](https://outerbounds.com/engineering/deployment/azure-k8s/deployment/)**
 
 If you need a full-stack DS/ML platform on Azure, this Kubernetes-based stack is a good
 option. It is the same stack as the one running on EKS on AWS, with the S3-based
 datastore replaced with Azure Blob Storage.
 
 <object style={{width: 700}} type=""image/svg+xml""
 data=""/assets/infra-stack-azure.svg""></object>
 
 This stack incurs a typical maintenance overhead of an AKS-based Kubernetes cluster,
 which shouldn't add much burden if your organization uses AKS already.",H3,https://docs.metaflow.org/getting-started/infrastructure#customizable-full-stack-on-azure-powered-by-kubernetes,False,587.0,75.0
1343,"Customizable full stack on Google Cloud, powered by Kubernetes","**[Click here to deploy this
 stack](https://outerbounds.com/engineering/deployment/gcp-k8s/deployment/)**
 
 If you need a full-stack DS/ML platform on Google Cloud, this Kubernetes-based stack is
 a good option. It is the same stack as the one running on EKS on AWS, with the S3-based
 datastore replaced with Google Cloud Storage.
 
 <object style={{width: 700}} type=""image/svg+xml""
 data=""/assets/infra-stack-gcp.svg""></object>
 
 This stack incurs a typical maintenance overhead of an GKE-based Kubernetes cluster,
 which shouldn't add much burden if your organization uses GKE already.
 
 
 
 If you are unsure about the stacks, just run `pip install metaflow` to install the local
 stack and move on to [the tutorials](/getting-started/tutorials). Flows you create will
 work without changes on any of these stacks.",H3,https://docs.metaflow.org/getting-started/infrastructure#customizable-full-stack-on-google-cloud-powered-by-kubernetes,False,823.0,114.0
1344,Episode 5: Hello Cloud,,H1,https://docs.metaflow.org/getting-started/tutorials/season-2-scaling-out-and-up/episode05#episode-5-hello-cloud,False,0.0,1.0
1345,"Look Mom, We're in the Cloud.","This flow is a simple linear workflow that verifies your cloud configuration. The
 `start` and `end` steps will run locally, while the `hello` step will [run
 remotely](/scaling/remote-tasks/introduction). After [configuring
 Metaflow](/getting-started/infrastructure) to run in the cloud, data and metadata about
 your runs will be stored remotely. This means you can use the client to access
 information about any flow from anywhere.
 
 You can find the tutorial code on
 [GitHub](https://github.com/Netflix/metaflow/tree/master/metaflow/tutorials/05-hello-cloud)
 
 **Showcasing:**",H2,https://docs.metaflow.org/getting-started/tutorials/season-2-scaling-out-and-up/episode05#look-mom-we-re-in-the-cloud,False,585.0,71.0
1346,Episode 8: Autopilot,,H1,https://docs.metaflow.org/getting-started/tutorials/season-2-scaling-out-and-up/episode08#episode-8-autopilot,False,0.0,1.0
1347,Scheduling Compute in the Cloud.,"This example revisits [Episode 06-statistics-redux: Computing in the
 Cloud](episode06.md). Wi th Metaflow, you don't need to make any code changes to
 schedule your flow in the cloud. In this example, we will schedule the `stats.py`
 workflow using the `argo-workflows create` command-line argument. This instructs
 Metaflow to schedule your flow on [Argo
 Workflows](https://argoproj.github.io/argo-workflows/) without changing any code. You
 can execute your flow on Argo Workflows by using the `argo-workflows trigger`
 command-line argument. You can use a notebook to set up a simple dashboard to monitor
 all of your Metaflow flows.
 
 You can find the tutorial code on
 [GitHub](https://github.com/Netflix/metaflow/tree/master/metaflow/tutorials/08-autopilot)
 
 **Showcasing:**",H2,https://docs.metaflow.org/getting-started/tutorials/season-2-scaling-out-and-up/episode08#scheduling-compute-in-the-cloud,False,785.0,100.0
1348,Episode 7: Worldview,,H1,https://docs.metaflow.org/getting-started/tutorials/season-2-scaling-out-and-up/episode07#episode-7-worldview,False,0.0,1.0
1349,Way up here.,"This episode shows how you can use a notebook to set up a simple dashboard to monitor
 all of your Metaflow flows.
 
 You can find the tutorial code on
 [GitHub](https://github.com/Netflix/metaflow/tree/master/metaflow/tutorials/07-worldview)
 
 **Showcasing:**",H2,https://docs.metaflow.org/getting-started/tutorials/season-2-scaling-out-and-up/episode07#way-up-here,False,261.0,33.0
1350,Episode 6: Statistics Redux,,H1,https://docs.metaflow.org/getting-started/tutorials/season-2-scaling-out-and-up/episode06#episode-6-statistics-redux,False,0.0,1.0
1351,Computing in the Cloud.,"This example revisits [Episode 02-statistics: Is this Data
 Science?](../season-1-the-local-experience/episode02). With Metaflow, you don't need to
 make any code changes to scale up your flow by running on remote compute. In this
 example, we re-run the `stats.py` workflow adding the `--with kubernetes` command line
 argument. This instructs Metaflow to run all your steps in the cloud without changing
 any code. You can control the behavior with additional arguments, like
 `--max-workers`**.** For this example, `max-workers` is used to limit the number of
 parallel genre-specific statistics computations. You can then access the data artifacts
 \(even the local CSV file\) from anywhere because the data is being stored in the
 cloud-based datastore.
 
 You can find the tutorial code on
 [GitHub](https://github.com/Netflix/metaflow/tree/master/metaflow/tutorials/06-statistics-redux)
 
 **Showcasing:**",H2,https://docs.metaflow.org/getting-started/tutorials/season-2-scaling-out-and-up/episode06#computing-in-the-cloud,False,912.0,119.0
1352,Episode 1: Playlist,,H1,https://docs.metaflow.org/getting-started/tutorials/season-1-the-local-experience/episode01#episode-1-playlist,False,0.0,1.0
1353,Let's build you a movie playlist.,"This flow loads a movie metadata CSV file and builds a playlist for your favorite movie
 genre. Everything in Metaflow is versioned, so you can run it multiple times and view
 all the historical playlists with the Metaflow client in a Notebook.
 
 You can find the tutorial code on
 [GitHub](https://github.com/Netflix/metaflow/tree/master/metaflow/tutorials/01-playlist)
 
 **Showcasing:**",H2,https://docs.metaflow.org/getting-started/tutorials/season-1-the-local-experience/episode01#let-s-build-you-a-movie-playlist,False,390.0,53.0
1354,Episode 4: Playlist Plus,,H1,https://docs.metaflow.org/getting-started/tutorials/season-1-the-local-experience/episode04#episode-4-playlist-plus,False,0.0,1.0
1355,The Final Showdown.,"Now that we've improved our genre-based playlist generator. We expose a _**hint**_
 parameter allowing the user to suggest a better bonus movie. The bonus movie is chosen
 from the movie that has the most similar name to the _**hint**_.
 
 This is achieved by importing a string edit distance package using Metaflow's conda
 based dependency management feature. Dependency management builds isolated and
 reproducible environments for individual steps.
 
 You can find the tutorial code on
 [GitHub](https://github.com/Netflix/metaflow/tree/master/metaflow/tutorials/04-playlist-plus)
 
 **Showcasing:**",H2,https://docs.metaflow.org/getting-started/tutorials/season-1-the-local-experience/episode04#the-final-showdown,False,603.0,78.0
1356,Episode 0: Hello World,,H1,https://docs.metaflow.org/getting-started/tutorials/season-1-the-local-experience/episode00#episode-0-hello-world,False,0.0,1.0
1357,Metaflow says Hi!,"This flow is a simple linear workflow that verifies your installation by printing out
 _**Metaflow says: Hi!**_ to the terminal.
 
 You can find the tutorial code on
 [GitHub](https://github.com/Netflix/metaflow/tree/master/metaflow/tutorials/00-helloworld)",H2,https://docs.metaflow.org/getting-started/tutorials/season-1-the-local-experience/episode00#metaflow-says-hi,False,257.0,29.0
1358,Showcasing:,,H4,https://docs.metaflow.org/getting-started/tutorials/season-1-the-local-experience/episode00#showcasing,False,0.0,1.0
1359,To play this episode:,"1. `cd metaflow-tutorials`
 2. `python 00-helloworld/helloworld.py show`
 3. `python 00-helloworld/helloworld.py run`
 
 <TutorialsLink link=""../../tutorials""/>",H4,https://docs.metaflow.org/getting-started/tutorials/season-1-the-local-experience/episode00#to-play-this-episode,False,160.0,14.0
1360,Episode 3: Playlist Redux,,H1,https://docs.metaflow.org/getting-started/tutorials/season-1-the-local-experience/episode03#episode-3-playlist-redux,False,0.0,1.0
1361,Follow the Money.,"Use Metaflow to load the statistics generated from [Episode 2](episode02) and improve
 our playlist generator by only recommending top box office grossing movies.
 
 You can find the tutorial code on
 [GitHub](https://github.com/Netflix/metaflow/tree/master/metaflow/tutorials/03-playlist-redux)
 
 **Showcasing:**",H2,https://docs.metaflow.org/getting-started/tutorials/season-1-the-local-experience/episode03#follow-the-money,False,314.0,34.0
1362,Episode 2: Statistics,,H1,https://docs.metaflow.org/getting-started/tutorials/season-1-the-local-experience/episode02#episode-2-statistics,False,0.0,1.0
1363,Is this Data Science?,"Use metaflow to load the movie metadata CSV file into a dataframe and compute some movie
 genre-specific statistics. These statistics are then used in later examples to improve
 our playlist generator. You can optionally use the Metaflow client to eyeball the
 results in a Notebook, and make some simple plots using the Matplotlib library.
 
 You can find the tutorial code on
 [GitHub](https://github.com/Netflix/metaflow/tree/master/metaflow/tutorials/02-statistics)
 
 **Showcasing:**",H2,https://docs.metaflow.org/getting-started/tutorials/season-1-the-local-experience/episode02#is-this-data-science,False,488.0,65.0
1364,Production Deployments,"What does *production* mean exactly? Surely the answer depends on who you ask and what
 application they are working on. There are so many different ways to produce business
 value with machine learning and data science that there can't be a single unambiguous
 definition or a way to deploy projects to production.
 
 However, there are characteristics that are common to all production deployments:
 
  - Production deployments should **run without human intervention**. It is not very
  practical to use results that require you to execute `run` on your laptop to power
  serious business processes or products.
 
  - Production deployments should **run reliably** in a highly available manner. Results
    should appear predictably, even if infrastructure encounters spurious failures.
 
 Consider the Metaflow journey
 
 ![](/assets/intro-cartoon-3.svg)
 
 Thus far the steps have involved a human in the loop, from [local
 development](/metaflow/introduction) to [scalable flows](/scaling/introduction). In
 contrast, a defining feature of production deployments is that they are fully automated.
 We achieve this by *scheduling flows* to run automatically on *a production-grade
 workflow orchestrator*, so you don't need to write `run` manually to produce the desired
 results.",H1,https://docs.metaflow.org/production/introduction#production-deployments,False,1285.0,192.0
1365,"Reliably Running, Automated Flows","What about the second characteristic of production deployments - reliability? Firstly, a
 big benefit of [Stage II](/scaling/introduction) is that you can test your workflows at
 scale, and add [reliability-enhancing](/scaling/failures)
 [features](/scaling/dependencies), making sure your flows can cope with production-scale
 workloads. Secondly, your flow needs to be orchestrated by a system that by itself runs
 reliably, which is [harder than it
 sounds](https://netflixtechblog.com/unbundling-data-science-workflows-with-metaflow-and-aws-step-functions-d454780c6280).
 Such a production-grade orchestrator needs to be
 
  - **Highly available**: The orchestrator itself must not crash, even if a server it
    runs on hits a random failure.
 
  - **Highly scalable**: We shouldn't have to worry about the number flows orchestrated
    by the system.
 
  - **Capable of triggering flows based on different conditions**: We should be able to
    automate flow execution flexibly.
 
  - **Easy to monitor and operate**: To minimize the time spent on occasional human
    interventions.
 
 Fortunately, a few systems are able to fulfill these requirements, judging by their
 track record. Metaflow integrates with two of them: [Argo
 Workflows](https://argoproj.github.io/argo-workflows/) that runs on Kubernetes and [AWS
 Step Functions](https://aws.amazon.com/step-functions/), a managed service by AWS. As
 of today, Argo Workflows is the only orchestrator that supports
 [Metaflow's powerful event-triggering functionality](/production/event-triggering),
 which makes it a good default choice.
 
 In addition, Metaflow integrates with a popular open-source workflow orchestrator,
 [Apache Airflow](https://airflow.apache.org/). While Airflow has more limitations than
 the two aforementioned orchestrators, it is a good choice if you have many DAGs deployed
 on it already and you don't want to introduce a new orchestrator in your environment.
 
 While all of these systems are quite complex under the hood, Metaflow makes using them
 trivial: You can deploy your flows literally with a single command - no changes in the
 code required. Also, this means that you can switch between schedulers easily. For
 instance, you can start with Apache Airflow to stay compatible with your existing data
 pipelines and migrate to Argo Workflows over time without having to pay any migration
 tax.",H2,https://docs.metaflow.org/production/introduction#reliably-running-automated-flows,False,2394.0,338.0
1366,Patterns of production deployments,"Once flows run reliably, you can leverage the results - like freshly trained models - on
 various systems:
 
 1. You can use deployed workflows as building blocks to compose larger systems using
    [event triggering](/production/event-triggering).
 2. You can write fresh predictions or other results in a data warehouse, e.g. to power a
    dashboard.
 3. You can populate fresh results in a cache e.g. for a recommendation system.
 4. You can deploy models on a *model hosting* platform of your choosing, e.g.
    [Seldon](https://www.seldon.io/) or [AWS
    Sagemaker](https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-deployment.html).
 
 The exact pattern depends on your use case. Importantly, creating these integrations
 become much easier when you can trust your flows to run reliably. We are happy to help
 you on [the Metaflow support Slack](http://slack.outerbounds.co) to find a pattern that
 works for your needs.",H3,https://docs.metaflow.org/production/introduction#patterns-of-production-deployments,False,939.0,142.0
1367,To Production And Back,"While the journey illustration above looks like a linear path from prototype to
 production, realistically the picture should illustrate loops everywhere. In particular,
 there is constant interaction between local development and production deployments, as
 you troubleshoot production issues (inevitably), as well as keep working on newer
 versions of flows. 
 
 When it comes to troubleshooting, a hugely convenient feature is the ability to
 [`resume` failed production runs
 locally](/metaflow/debugging#reproducing-production-issues-locally). Also, remember that
 you can inspect the state of any production run with
 [cards](/metaflow/visualizing-results) and [notebooks](/metaflow/client), in real-time.
 
 When it comes to working on newer versions, how do you know if a newer version performs
 better than the latest production deployment? Often, the best way to answer the question
 is to deploy the new version to run concurrently with the existing production version
 and compare the results, as in an A/B test. This pattern is enabled by [the `@project`
 decorator](coordinating-larger-metaflow-projects). Also, [Metaflow
 tags](https://outerbounds.com/blog/five-ways-to-use-the-new-metaflow-tags/) come in
 handy when designing processes around production deployments.",H2,https://docs.metaflow.org/production/introduction#to-production-and-back,False,1283.0,162.0
1368,What You Will Learn,":::info
 
 You can get a feel of all these concepts and test them hands-on without having to
 install anything locally by signing up for
 [a Metaflow Sandbox](https://outerbounds.com/sandbox/)!
 
 :::
 
 In this section, you will learn how to make your flows run automatically without any
 human intervention. 
 
 1. The basics of [scheduling Metaflow flows](/production/scheduling-metaflow-flows/introduction):
    - Depending on the [infrastructure you have
      installed](/getting-started/infrastructure), pick a section below:
       - [Scheduling flows with Argo
         Workflows](/production/scheduling-metaflow-flows/scheduling-with-argo-workflows)
         - Choose this if running on Kubernetes.
       - [Scheduling flows with AWS Step
         Functions](/production/scheduling-metaflow-flows/scheduling-with-aws-step-functions)
         - Choose for minimal operational overhead.
       - [Scheduling flows with Apache
         Airflow](/production/scheduling-metaflow-flows/scheduling-with-airflow)
         - Choose to stay compatible with your existing Airflow deployment.
 
  2. [Coordinating larger Metaflow
     projects](/production/coordinating-larger-metaflow-projects) is a more advanced pattern
     that enables multiple parallel production deployments.
 
 3. [Connecting flows via events](/production/event-triggering) shows how you can
    make workflows start automatically based on real-time events. This pattern
    allows you to build reactive systems using flows as building blocks.",H2,https://docs.metaflow.org/production/introduction#what-you-will-learn,False,1517.0,248.0
1369,Coordinating Larger Metaflow Projects,":::info
 
 This page applies equally to all [production
 orchestrators](/production/scheduling-metaflow-flows/introduction) supported by
 Metaflow, i.e. AWS Step Functions, Argo Workflows, and Airflow. Examples below mention
 `step-functions` but you can replace `step-functions` with `argo-workflows` or `airflow`
 to get equivalent behavior on your orchestrator of choice (except the last part about
 event triggering, which applies only to Argo Workflows).
 
 :::
 
 
 Most Metaflow projects start as a simple Python script that is developed by a single
 data scientist. Metaflow takes care of [keeping results organized
 automatically](../scaling/tagging), so you can focus on developing models and the
 business logic around them.
 
 Over time, the project matures to the point that you want to deploy it to [a production
 orchestrator](/production/scheduling-metaflow-flows/introduction) to test how the model
 works with real-life, updating data. In Metaflow, this is a matter of executing a single
 command like `step-functions create`. Having the workflow run automatically with fresh
 data is a great way to surface unforeseen issues in the code.
 
 After a few iterations, the workflow starts to work reliably. If the results are
 promising enough, stakeholders can start relying on the results of your workflow. Often,
 success attracts more developers to join the project. At this point, you will need to
 start thinking about how to coordinate work amongst multiple people and how to iterate
 on new, experimental versions of the workflow while providing stable results to your
 stakeholders. This is where the `@project` decorator comes in.",H1,https://docs.metaflow.org/production/coordinating-larger-metaflow-projects#coordinating-larger-metaflow-projects,False,1655.0,239.0
1370,The `@project` decorator,"During development, multiple people can work on the same workflow simultaneously as
 Metaflow keeps executions isolated through [independently stored artifacts and
 namespaces](../scaling/tagging). However, by default, all production deployments are
 bound to the name of the workflow. If multiple people call `step-functions create`
 independently, each deployment will overwrite the previous one.
 
 In the early stages of a project, this simple model is convenient but as the project
 grows, it is desirable that multiple people can test their own production deployments
 without interference. Or, as a single developer, you may want to experiment with
 multiple independent deployments of your workflow.
 
 Metaflow provides a `@project` decorator to address this need. The `@project` decorator
 is used at the `FlowSpec`-level to bind a Flow to a specific project. All flows with the
 same project name belong to the same project.
 
 You can test this by executing the following flow.
 
 ```python title=""project_flow.py""
 from metaflow import FlowSpec, step, project, current
 
 @project(name='example_project')
 class ProjectFlow(FlowSpec):
 
     @step
     def start(self):
         print('project name:', current.project_name)
         print('project branch:', current.branch_name)
         print('is this a production run?', current.is_production)
         self.next(self.end)
 
     @step
     def end(self):
         pass
 
 if __name__ == '__main__':
     ProjectFlow()
 ```
 
 Save the above snippet in a file, `project_flow.py`. Now you can run the flow as usual:
 
 ```python
 python project_flow.py run
 ```
 
 The `@project` decorator exposes new project-related attributes, `project_name`,
 `branch_name`, and `is_production` in [the current
 object](../scaling/tagging#accessing-current-ids-in-a-flow) which you can use to alter
 the behavior of the flow depending on the execution context. Besides the new attributes
 in current, the flow works exactly as before when executed outside a production
 orchestrator.",H2,https://docs.metaflow.org/production/coordinating-larger-metaflow-projects#the-project-decorator,False,2034.0,323.0
1371,Projects in production,"The main benefit of `@project` relates to deployments on [a production
 orchestrator](/production/scheduling-metaflow-flows/introduction). Below, we will cover
 this case: How to manage a production project with multiple developers collaborating.",H2,https://docs.metaflow.org/production/coordinating-larger-metaflow-projects#projects-in-production,False,246.0,28.0
1372,"Single Flow, multiple developers","If `ProjectFlow` did not have a `@project decorator`, it would get deployed as a
 workflow called `ProjectFlow` on AWS Step Functions by `step-functions create`. Only one
 version of `ProjectFlow` could exist on a production orchestrator at a time. Everyone
 deploying the flow would need to know [the production
 token](../scaling/tagging#production-namespaces) assigned to the deployment.
 
 On the UI of your production orchestrator, you would see one workflow called
 `ProjectFlow`:
 
 ![](/assets/project_old.png)
 
 The `@project` decorator changes this behavior. Let's deploy `ProjectFlow`:
 
 ```python
 python project_flow.py step-functions create
 ```
 
 The `@project` decorator adds a user-specific prefix in the workflow name: the workflow
 gets deployed with a name like `example_project.user.YOURNAME.ProjectFlow` where
 `YOURNAME` is your username. Metaflow gets the username by looking, in order, at the
 following environment variables: `METAFLOW_USER`, `SUDO_USER`, `USERNAME` and `USER`.
 
 This allows multiple developers to deploy their workflows on a production orchestrator
 without fear that they might interfere with someone else's deployment. Imagine Alice,
 Bob, and Carol collaborating on a project. Each one of them can call `step-functions
 create` independently, which results in three separate workflows in production:
 
 ![](/assets/project_user.png)
 
 Note that each one of these deployments gets [an isolated namespace](../scaling/tagging)
 and [a separate production token](../scaling/tagging#production-tokens). This means that
 if your code refers to `Flow('ProjectFlow').latest_run` in production, it is guaranteed
 to refer to a run that corresponds to its own isolated deployment. The deployments don't
 interfere with each other.",H3,https://docs.metaflow.org/production/coordinating-larger-metaflow-projects#single-flow-multiple-developers,False,1773.0,232.0
1373,Main production deployment,"In addition to user-specific deployments, most projects have a single blessed production
 version which represents the official results of the workflow.
 
 The `@project` decorator exposes a new top-level command-line argument, `--production`
 that denotes a production run or deployment. See what happens when you run ProjectFlow
 with `--production`:
 
 ```python
 python project_flow.py --production run
 ```
 
 The `current.branch_name` will be set to prod and `current.is_production` is set to
 True. For instance, you could write results to a production table only if
 `current.is_production`.
 
 You can deploy a production version to AWS Step Functions as follows:
 
 ```python
 python project_flow.py --production step-functions create
 ```
 
 Instead of deploying the flow with a user-specific prefix, this will deploy the flow as
 `example_project.prod.ProjectFlow`. You will get a warning about missing production
 token if you are not authorized to deploy the flow to production.
 
 The production deployment gets a separate, isolated namespace of its own:
 
 ![](/assets/project_prod.png)",H3,https://docs.metaflow.org/production/coordinating-larger-metaflow-projects#main-production-deployment,False,1102.0,154.0
1374,Custom branches,"Imagine that ProjectFlow has a stable version in production. Now, Alice and Bob want to
 start developing a new, experimental version of the flow. They can work on a common
 codebase and run the code locally independently. Eventually, they will want to deploy
 the experimental version to a production orchestrator and let it run in parallel with
 the production version for a while, to see that it works correctly.
 
 Alice and Bob could deploy the experimental version under a user-specific namespace of
 theirs but this would make it hard to keep iterating on the code, as one of the
 usernamespaces would be reserved for the long-running experiment. A better approach is
 to deploy the experimental code under a custom branch.
 
 Try the following:
 
 ```python
 python project_flow.py --branch better_version run
 ```
 
 The flow reports that the branch name is `test.better_version`. You can deploy the
 custom branch to AWS Step Functions:
 
 ```python
 python project_flow.py --branch better_version step-functions create
 ```
 
 which will result in another separate, isolated namespace:
 
 ![](/assets/project_branch.png)
 
 Alice and Bob can share the production token corresponding to the branch, so either of
 them can redeploy the branch when needed.",H3,https://docs.metaflow.org/production/coordinating-larger-metaflow-projects#custom-branches,False,1264.0,196.0
1375,Custom branches for production deployments,"There are scenarios where Alice might need to run multiple variants of ProjectFlow in
 production. Alice can very simply use custom branches to run multiple production
 versions -
 
 Try the following:
 
 ```python
 python project_flow.py --branch better_version --production run
 ```
 
 The flow reports that the branch name is `prod.better_version`. You can deploy the
 custom branch to AWS Step Functions:
 
 ```python
 python project_flow.py --branch better_version --production step-functions create
 ```",H3,https://docs.metaflow.org/production/coordinating-larger-metaflow-projects#custom-branches-for-production-deployments,False,509.0,71.0
1376,`@project` and event triggering,"Importantly, workflows connected through
 [the `@trigger_on_finish` decorator](/production/event-triggering/flow-events)
 respect the `@project` decorator. Besides deploying individual workflows as branches,
 as shown above, you can deploy flows-of-flows as isolated branches. Read more about
 this pattern in
 [Deploying Variants of Event-Triggered Flows](/production/event-triggering/project-events).",H3,https://docs.metaflow.org/production/coordinating-larger-metaflow-projects#project-and-event-triggering,False,402.0,38.0
1377,Summary,"The `@project` decorator makes available three classes of namespaces that will affect
 the behavior of a production deployment:",H2,https://docs.metaflow.org/production/coordinating-larger-metaflow-projects#summary,False,127.0,18.0
1378,Inspecting Events,"Events provide an additional layer of observability in production systems. By
 following a trail of events, you can inspect what was triggered, when, and by
 whom.",H1,https://docs.metaflow.org/production/event-triggering/inspect-events#inspecting-events,False,163.0,26.0
1379,Accessing events through Client API,"You can inspect the event(s) with the Client API that exposes
 [the `MetaflowTrigger` object](/api/client#metaflowtrigger) for every
 event-triggered run. For instance, we can inspect the event that triggered a
 production run `ModelRefreshFlow/argo-modelrefreshflow-rlpgc`:
 
 ```python
 from metaflow import namespace, Run
 namespace(None)
 Run('ModelRefreshFlow/argo-modelrefreshflow-rlpgc').trigger.event
 ```
 
 Remember that `namespace(None)` sets
 [the global namespace](/scaling/tagging#global-namespace) which is
 required to inspect production runs e.g. in a notebook. This will print
 out metadata about the event:
 
 ```
 MetaflowEvent(name='data_updated',
 			  id='ca75a1a4-91de-40c2-944c-0b39436c721e',
 			  timestamp=datetime.datetime(2023, 5, 15, 19, 50, 43),
 			  type='event')
 ```
 
 Find a description of all fields in [the related API documentation](#).",H2,https://docs.metaflow.org/production/event-triggering/inspect-events#accessing-events-through-client-api,False,877.0,98.0
1380,Dealing with multiple triggers,"If a run was triggered by multiple events, you can [inspect specific
 `MetaflowEvent` by name](/api/client#Trigger.__getitem__):
 
 ```
 run.trigger['data_updated'].name
 ```
 
 Or, if a run was triggered by multiple flows when using `@trigger_on_finish`,
 you can inspect a specific triggering `Run` by its flow name:
 
 ```
 run.trigger['FirstFlow'].data.model
 ```",H3,https://docs.metaflow.org/production/event-triggering/inspect-events#dealing-with-multiple-triggers,False,367.0,47.0
1381,Following a trail of runs,"When flows are connected with `@trigger_on_finish`, you can use
 `MetaflowTrigger` to follow a chain of triggered runs. Consider this chain of
 flows:
 
 ```
 ETLFlow → TrainingFlow → PredictionFlow
 ```
 
 We can walk backwards from the latest run of `PredictionFlow` back to the
 triggering run of `ETLFlow`:
 
 ```python
 etl_run = Flow('PredictionFlow').latest_run.trigger.run.trigger.run
 ```",H3,https://docs.metaflow.org/production/event-triggering/inspect-events#following-a-trail-of-runs,False,397.0,54.0
1382,Events in the Metaflow UI,"If you have deployed
 [the Metaflow
 GUI](https://netflixtechblog.com/open-sourcing-a-monitoring-gui-for-metaflow-75ff465f0d60),
 you can view metadata about triggers right in the UI. The circles with arrows
 inside indicate event-triggered runs:
 
 ```mdx-code-block
 import ReactPlayer from 'react-player';
 ```
 
 <ReactPlayer playing controls muted loop url='/assets/mfgui-event.mp4' width='100%' height='100%'/>",H2,https://docs.metaflow.org/production/event-triggering/inspect-events#events-in-the-metaflow-ui,False,416.0,41.0
1383,Deploying Variants of Event-Triggered Flows,"Consider this advanced scenario: You have deployed two flows [linked together
 via `@trigger_on_finish`](/production/event-triggering/flow-events#passing-data-across-flows).
 The flows run happily in production. At some point, you want to experiment with a new
 modeling approach. In order to know if the new approach works better than the
 current production version, you'd like to run them concurrently using the same
 data, maybe powering an A/B test.
 
 It is critical that the experimental variant doesn't interfere with the
 production version. Conceptually, you would like to have two isolated
 deployments like here:
 
 ```mdx-code-block
 import ReactPlayer from 'react-player';
 ```
 
 <ReactPlayer playing controls muted loop url='/assets/et-variants.mp4' width='100%' height='100%'/>
 
 Fortunately, you can achieve such isolated deployments by using [the `@project`
 decorator](/production/coordinating-larger-metaflow-projects) in conjunction
 with `@trigger_on_finish`.",H1,https://docs.metaflow.org/production/event-triggering/project-events#deploying-variants-of-event-triggered-flows,False,983.0,119.0
1384,Using `@project` and `@trigger_on_finish` together,"```python
 from metaflow import FlowSpec, step, current, project
 
 @project(name='variant_demo')
 class FirstProjectFlow(FlowSpec):
 
     @step
     def start(self):
         print(""This deployment is called"", current.project_flow_name)
         self.next(self.end)
 
     @step
     def end(self):
         pass
 
 if __name__ == '__main__':
     FirstProjectFlow()
 ```
 
 and 
 
 ```python
 from metaflow import FlowSpec, step, trigger_on_finish, current, project
 
 @trigger_on_finish(flow='FirstProjectFlow')
 @project(name='variant_demo')
 class SecondProjectFlow(FlowSpec):
 
     @step
     def start(self):
         print(""This deployment is called"", current.project_flow_name)
         print(""This run was triggered by"", current.trigger.event)
         self.next(self.end)
 
     @step
     def end(self):
         pass
 
 if __name__ == '__main__':
     SecondProjectFlow()
 ```
 
 Deploy both the flows on Argo Workflows:
 ```
 python firstproject.py argo-workflows create
 python secondproject.py argo-workflows create
 ```
 
 and trigger the first one manually:
 
 ```
 python firstproject.py argo-workflows trigger
 ```
 
 Thanks to `@project`, the flows are deployed with a special name that includes a
 branch prefix. By default,
 [each user gets their own
 prefix](/production/coordinating-larger-metaflow-projects#single-flow-multiple-developers),
 so you should the output of the `start` step of `FirstProjectFlow` should look
 like:
 
 ```
 The deployment is called variant_demo.user.alice.FirstProjectFlow
 ```
 
 The `SecondProjectFlow` starts automatically when `FirstProjectFlow` completes.
 It should show output like here:
 
 
 ```
 This deployment is called variant_demo.user.alice.SecondProjectFlow
 This run was triggered by
 MetaflowEvent(name='metaflow.variant_demo.user.alice.FirstProjectFlow.end', ...)
 ```
 which indicates that the event triggering the run is specific to Alice.",H2,https://docs.metaflow.org/production/event-triggering/project-events#using-project-and-trigger-on-finish-together,False,1916.0,297.0
1385,Deploying a parallel branch,"To deploy a parallel variant or *a branch* - in the sense of Git branches -
 execute the following commands:
 
 ```
 python firstproject.py --branch new_model argo-workflows create
 python secondproject.py --branch new_model argo-workflows create
 ```
 and trigger the branch like here:
 ```
 python firstproject.py --branch new_model argo-workflows trigger
 ```
 
 You should see a corresponding output for these runs. Importantly, triggering
 the `new_model` branch doesn't have any effect on Alice's deployment, which is
 [fully isolated in its own namespace](/scaling/tagging).",H3,https://docs.metaflow.org/production/event-triggering/project-events#deploying-a-parallel-branch,False,581.0,79.0
1386,Triggering across branches,"As shown above, `@project` guarantees that all flows linked together within the
 same project and branch are isolated from other deployments. However, sometimes
 you may want to depend on an upstream flow that is deployed outside of your
 branch. For instance, you may want to deploy a variant of a downstream
 workflow, like `SecondProjectFlow` above, while reusing results from an
 existing upstream flow, like `FirstProjectFlow`.
 
 You can accomplish this by specifying explicitly the project-branch name that
 you want to depend on. For instance, this line makes a flow depend on Alice's
 deployment regardless of the branch the flow is deployed on:
 
 ```python
 @trigger_on_finish(flow='variant_demo.user.alice.FirstProjectFlow')
 ```",H2,https://docs.metaflow.org/production/event-triggering/project-events#triggering-across-branches,False,741.0,106.0
1387,Triggering Flows Based on Other Flows,"Besides triggering flows based on external events, you can trigger a flow when
 another flow completes. Metaflow provides a special decorator to support the
 pattern, [`@trigger_on_finish`](/api/flow-decorators/trigger_on_finish), which
 allows you to build arbitrarily complex systems of interconnected flows.
 
 Here, the completion of `FirstFlow` triggers a run of `SecondFlow`:
 
 ```mdx-code-block
 import ReactPlayer from 'react-player';
 ```
 
 <ReactPlayer playing controls muted loop url='/assets/et-flows.mp4' width='100%' height='100%'/>
 
 Let's demonstrate the case with two simple flows:
 
 ```python
 from metaflow import FlowSpec, step
 
 class FirstFlow(FlowSpec):
 
     @step
     def start(self):
         print(""This is the first flow"")
         self.next(self.end)
 
     @step
     def end(self):
         pass
 
 if __name__ == '__main__':
     FirstFlow()
 ```
 
 and 
 
 ```python
 from metaflow import FlowSpec, step, trigger_on_finish
 
 @trigger_on_finish(flow='FirstFlow')
 class SecondFlow(FlowSpec):
 
     @step
     def start(self):
         print(""This is the second flow"")
         self.next(self.end)
 
     @step
     def end(self):
         pass
 
 if __name__ == '__main__':
     SecondFlow()
 ```
 
 Deploy both the flows on Argo Workflows:
 ```
 python firstflow.py argo-workflows create
 python secondflow.py argo-workflows create
 ```
 
 Since we didn't specify a trigger or `@schedule` for `FirstFlow`, we must start it manually:
 ```
 python firstflow.py argo-workflows trigger
 ```
 
 After `FirstFlow` completes, you should see `SecondFlow` starting automatically.
 
 :::warning
 
 You can create infinite loops with `@trigger_on_finish`. For instance, if you
 specify `@trigger_on_finish(flow='SecondFlow')` for `FirstFlow` above, the
 flows will trigger each other infinitely, consuming resources on the cluster.
 If this happens, you can open the Argo Workflows UI and delete the workflow.
 
 :::",H1,https://docs.metaflow.org/production/event-triggering/flow-events#triggering-flows-based-on-other-flows,False,1947.0,327.0
1388,Triggering based on multiple flows,"You can also depend on multiple flows completing before starting a flow. Simply define a list of flows:
 ```python
 @trigger_on_finish(flows=['FirstFlow', 'AnotherFlow'])
 ```
 all of the flows need to complete within a configured time windows for the flow to trigger.",H3,https://docs.metaflow.org/production/event-triggering/flow-events#triggering-based-on-multiple-flows,False,268.0,39.0
1389,Passing data across flows,"Consider an extension of `ModelRefreshFlow` that was featured
 [on the previous
 page](/production/event-triggering/external-events#passing-parameters-in-events).
 This time, we want to use the newly trained model to run inference for the
 latest data. This requires passing the model object from `TrainingFlow` to
 `InferenceFlow`:
 
 <ReactPlayer playing controls muted loop url='/assets/et-combo.mp4' width='100%' height='100%'/>
 
 Whenever a flow is triggered by an event, information about the event is made
 available through
 [the `MetaflowTrigger`object](/api/client#metaflowtrigger)
 that is accessible at `current.trigger`. See the
 API documentation for [`MetaflowEvent` for all available event-related
 metadata](/api/client#metaflowevent).
 
 When using `@trigger_on_finish`, you can access information about the triggering
 runs through
 [`current.trigger.run`](/api/current#trigger-and-trigger_on_finish) or
 [`current.trigger.runs`](/api/current#trigger-and-trigger_on_finish) in
 the case of multiple flows, which return one or more
 [`Run` objects](/metaflow/client#properties-related-to-runs). Use the
 `Run` object to access artifacts as you do when
 [using the Client API directly](/metaflow/client).
 
 In this example, we access the `model` artifact created in `ModelRefreshFlow`:
 
 ```python
 from metaflow import FlowSpec, step, trigger_on_finish, current
 
 @trigger_on_finish(flow='ModelRefreshFlow')
 class InferenceFlow(FlowSpec):
 
     @step
     def start(self):
         print(""Triggering run"", current.trigger.run)
         self.model = current.trigger.run.data.model
         print('Model', self.model)
         self.next(self.end)
 
     @step
     def end(self):
         pass
 
 if __name__ == '__main__':
     InferenceFlow()
 ```",H2,https://docs.metaflow.org/production/event-triggering/flow-events#passing-data-across-flows,False,1771.0,238.0
1390,Testing flow triggering locally,"You may have noticed one issue with `InferenceFlow` above. If you `run` it
 locally, it will fail as `current.trigger` is not defined. Obviously, it would
 be convenient to be able to test the flow before deploying to Argo Workflows.
 
 During development, you can manually assign the triggering run on the command line:
 
 ```
 python inferenceflow.py --trigger ModelRefreshFlow/233 run
 ```
 
 This will run the flow as if it was triggered by a run `ModelRefreshFlow/233`.
 This allows you to quickly iterate on the flow locally, testing it with
 different upstream data providers.",H2,https://docs.metaflow.org/production/event-triggering/flow-events#testing-flow-triggering-locally,False,583.0,93.0
1391,Triggering Flows Based on External Events,"You can configure flows
 [deployed on Argo Workflows](/production/scheduling-metaflow-flows/scheduling-with-argo-workflows)
 to start automatically when an event occurs in an external system. For instance, you
 could start a flow whenever new data is available in a data warehouse:
 
 ```mdx-code-block
 import ReactPlayer from 'react-player';
 ```
 
 <ReactPlayer playing controls muted loop url='/assets/et-basic-event.mp4' width='100%' height='100%'/>
 
 All you have to do is to add [a decorator, `@trigger`](/api/flow-decorators/trigger), with
 a desired event name above the flow:
 
 ```python
 from metaflow import FlowSpec, step, trigger
 
 @trigger(event='data_updated')
 class FreshDataFlow(FlowSpec):
 
     @step
     def start(self):
         # load data from the data warehouse
         print('processing fresh data!')
         self.next(self.end)
 
     @step
     def end(self):
         pass
 
 if __name__ == '__main__':
     FreshDataFlow()
 ```
 
 You can develop and test the flow locally as usual: `@trigger` doesn't have any
 effect on local runs. To test triggering, deploy the flow to Argo Workflows:
 
 ```
 python freshdata.py argo-workflows create
 ```
 
 The output should state something along the lines of
 ```
 What will trigger execution of the workflow:
     This workflow triggers automatically when the upstream
     data_updated event is/are published.
 ```
 indicating that the deployment has been linked to the desired event.",H1,https://docs.metaflow.org/production/event-triggering/external-events#triggering-flows-based-on-external-events,False,1464.0,246.0
1392,Defining events,"In the above example, we used `data_updated` as the name of the event that triggers the flow. You
 can choose the name freely. By using different names, you can make flows react to different events.
 
 If you are familiar with streaming systems like Kafka or queues like AWS SQS, you can think of the
 event name as *a topic* in these systems.",H3,https://docs.metaflow.org/production/event-triggering/external-events#defining-events,False,343.0,63.0
1393,Depending on multiple events,"You can require that multiple events must be present before the flow gets
 triggered. Simply define a list of events:
 
 ```python
 @trigger(events=['data_updated', 'phase_of_the_moon'])
 ```
 
 all the events need to be occur within a configured time window for the flow to trigger.",H3,https://docs.metaflow.org/production/event-triggering/external-events#depending-on-multiple-events,False,283.0,43.0
1394,Creating events,"In order to trigger the flow deployed with `@trigger`, we need an event.
 Metaflow comes with a utility class, [`ArgoEvent`](/api/argoevent), which
 makes it easy to create suitable events from any environment. You can call
 it as a part of your ETL pipeline running outside Metaflow, in a microservice,
 or in a notebook - wherever and whenever you want to trigger a Metaflow execution.
 
 ```python
 from metaflow.integrations import ArgoEvent
 
 ArgoEvent(name=""data_updated"").publish()
 ```
 
 This line will create an event that will trigger **all flows** deployed on Argo Workflows that are
 waiting for the event `data_updated`.
 
 Note that `publish()` only publishes an event and returns immediately. It does not guarantee that
 a run will start -- it's possible that no flow is waiting for the particular event. Correspondingly,
 if you call `ArgoEvent` many times, you can trigger arbitrarily many runs of connected flows.
 
 :::info
 
 Before calling `ArgoEvent` make sure that you have a valid Metaflow
 configuration and a connection to the Kubernetes cluster set up in the
 environment where you call `.publish()`. If you call it from systems outside
 Metaflow, make sure that these prerequisites are met.
 
 :::",H2,https://docs.metaflow.org/production/event-triggering/external-events#creating-events,False,1227.0,191.0
1395,Advanced case: Publishing events inside a flow,"It is not common to publish events inside a Metaflow flow, since
 [the `@trigger_on_finish` decorator](/production/event-triggering/flow-events)
 takes care of flow-to-flow
 triggering conveniently. Should you have a more advanced use case that requires
 publishing events inside a flow, it is recommended that you use [the
 `ArgoEvent.safe_publish` method](/api/argoevent#ArgoEvent.safe_publish):
 
 ```python
 from metaflow.integrations import ArgoEvent
 
 ArgoEvent(name=""data_updated"").safe_publish()
 ```
 
 The only difference to `publish()` is that events won't be created during local
 runs. This means that you can include `safe_publish()` in your code safely and
 develop and test it locally as usual, knowing that you won't be causing
 unintended side-effects in surrounding systems that may depend on the event.",H3,https://docs.metaflow.org/production/event-triggering/external-events#advanced-case-publishing-events-inside-a-flow,False,823.0,105.0
1396,Passing parameters in events,"Besides simply starting runs through events, you can change their behavior on
 the fly by letting the event
 [define `Parameters` of the flow](/metaflow/basics#how-to-define-parameters-for-flows).
 
 Consider this typical machine learning system that implements a continuously refreshing model:
 
 <ReactPlayer playing controls muted loop url='/assets/et-model.mp4' width='100%' height='100%'/>
 
 1. An event is created whenever new data is available in the data warehouse.
 2. The event contains information about the latest data available in the warehouse.
 3. Using the information, a model is refreshed with a training set containing the
 last N days of data.
 
 The corresponding flow could look like this, ignoring details of data loading and the actual
 training:
 
 ```python
 from metaflow import FlowSpec, step, trigger, Parameter
 from datetime import datetime, timedelta
 
 @trigger(event=""data_updated"")
 class ModelRefreshFlow(FlowSpec):
     latest = Parameter(""latest"", default=""2023-05-01"")
     window = Parameter(""window"", default=3)
 
     def load_data(self):
         # replace this with an actual data loader
         SQL = f""select * from data where time > to_date('{self.start_date}')""
         print(""loading data since %s"" % self.start_date)
         return [1, 2, 3]
 
     def train_model(self, df):
         # replace this with actual model training
         return df
 
     @step
     def start(self):
         self.latest_date = datetime.fromisoformat(self.latest)
         self.start_date = self.latest_date - timedelta(days=self.window)
         self.next(self.train)
 
     @step
     def train(self):
         df = self.load_data()
         self.model = self.train_model(df)
         self.next(self.end)
 
     @step
     def end(self):
         pass
 
 if __name__ == ""__main__"":
     ModelRefreshFlow()
 ```
 
 To pass in parameters, we can simply define them in the `payload` of `ArgoEvent`:
 ```python
 from metaflow.integrations import ArgoEvent
 from datetime import datetime
 
 ArgoEvent(name=""data_updated"").publish(payload={'latest': datetime.now().isoformat()})
 ```",H2,https://docs.metaflow.org/production/event-triggering/external-events#passing-parameters-in-events,False,2114.0,391.0
1397,Mapping parameter names,"Above, the payload field matches the parameter name `latest` exactly. In certain situations you may want
 to define manually how parameters get their values. For instance, a common event may be used to trigger
 various kinds of flows and it may be hard to coordinate parameter names across all consumers of the event.
 
 In this situation, you can remap payload fields to parameter names through the `parameters` argument:
 ```python
 @trigger(event={'name':'some_event', 'parameters': {'window': 'how_many_days'}})
 ```
 Here, we define that `Parameter('window')` gets its value from the event payload field `how_many_days`.
 Note that you need to remap all `parameters` that you want to assign through the event. Default assignments
 are disabled when `parameters` is specified, which allows you to stay in full control of parameter mappings.
 
 Parameter mapping comes in handy when multiple events are present:
 ```python
 @trigger(events=[{'name':'one_event', 'parameters': {'window': 'how_many_days'}},
                  {'name':'another_event', 'parameters': {'latest': 'timestamp'}}])
 ```
 In this case, `window` gets its value through the event `one_event` and `latest` through `another_event`.",H3,https://docs.metaflow.org/production/event-triggering/external-events#mapping-parameter-names,False,1204.0,177.0
1398,Scheduling Metaflow Flows,"While the `run` command is convenient during development, you can't keep executing it
 manually in production. An old-school solution would be to use a time-based scheduler
 like [Cron](https://en.wikipedia.org/wiki/Cron) to execute the command automatically at
 a set schedule, but this approach has a number of serious downsides:
 
 What if the server running cron fails? If the scheduled command fails, how do you know
 it has failed? How do you see its error logs? Does the Cron server have enough capacity
 to handle another command? And most importantly, how should you orchestrate schedules of
 multiple commands so that their mutual dependencies are handled correctly?
 
 Metaflow currently integrates with [Argo
 Workflows](../scheduling-metaflow-flows/scheduling-with-argo-workflows.md) ([a modern,
 Kubernetes-native workflow orchestrator](https://argoproj.github.io/workflows)), [AWS
 Step Functions](../scheduling-metaflow-flows/scheduling-with-aws-step-functions.md) ([a
 managed general-purpose orchestrator](https://aws.amazon.com/step-functions/)), and
 [Apache Airflow](../scheduling-metaflow-flows/scheduling-with-airflow.md) ([a
 widely-known open-source orchestrator](https://airflow.apache.org/)) which can answer
 these questions.
 
 Learn more about how to deploy your Metaflow flows to these orchestrators in the
 following subsections:",H1,https://docs.metaflow.org/production/scheduling-metaflow-flows/introduction#scheduling-metaflow-flows,False,1361.0,151.0
1399,Scheduling Metaflow Flows with Apache Airflow,"[Apache Airflow](https://airflow.apache.org/) is a popular open-source workflow
 orchestrator. It has a number of limitations compared to [Argo
 Workflows](/production/scheduling-metaflow-flows/scheduling-with-argo-workflows) and
 [AWS Step
 Functions](/production/scheduling-metaflow-flows/scheduling-with-aws-step-functions), so
 we mainly recommend it if you are an existing Airflow user and you want to avoid
 introducing a new orchestrator in your environment.
 
 The Metaflow-Airflow integration is a great way to modernize your Airflow deployment. It
 provides a more user-friendly and productive development API for data scientists and
 data engineers, without needing to change anything in your existing pipelines or
 operational playbooks, as described in [its announcement blog
 post](https://outerbounds.com/blog/better-airflow-with-metaflow/). To learn how to
 deploy and operate the integration, see [Using Airflow with
 Metaflow](https://outerbounds.com/engineering/operations/airflow/).
 
 Here are the main benefits of using Metaflow with Airflow:
 
  - You get to use the human-friendly API of Metaflow to define and test workflows.
    Almost all features of Metaflow work with Airflow out of the box, except nested
    foreaches, which are not supported by Airflow, and `@batch` as the current
    integration only supports `@kubernetes`.
  - You can deploy Metaflow flows to your existing Airflow server without having to
  change anything operationally. From the Airflow's point of view, Metaflow flows look
  like any other Airflow DAG.
  - If you want to consider moving to another orchestrator supported by Metaflow, you can
    test them easily just by changing one command to deploy to [Argo
    Workflows](/production/scheduling-metaflow-flows/scheduling-with-argo-workflows) or
    [AWS Step
    Functions](/production/scheduling-metaflow-flows/scheduling-with-aws-step-functions).
 
 When running on Airflow, Metaflow code works exactly as it does locally: No changes are
 required in the code. All data artifacts produced by steps run on Airflow are available
 using the [Client API](../../metaflow/client.md). All tasks are run on Kubernetes
 respecting the `@resources` decorator, as if the `@kubernetes` decorator was added to
 all steps, as explained in [Executing Tasks
 Remotely](/scaling/remote-tasks/introduction#safeguard-flags).
 
 This document describes the basics of Airflow scheduling. If your project involves
 multiple people, multiple workflows, or it is becoming business-critical, check out the
 section around [coordinating larger Metaflow
 projects](../coordinating-larger-metaflow-projects.md).",H1,https://docs.metaflow.org/production/scheduling-metaflow-flows/scheduling-with-airflow#scheduling-metaflow-flows-with-apache-airflow,False,2647.0,345.0
1400,Pushing a flow to production,"Let's use [the flow from the section about
 parameters](../../metaflow/basics#how-to-define-parameters-for-flows) as an example:
 
 ```python
 from metaflow import FlowSpec, Parameter, step
 
 class ParameterFlow(FlowSpec):
     alpha = Parameter('alpha',
                       help='Learning rate',
                       default=0.01)
 
     @step
     def start(self):
         print('alpha is %f' % self.alpha)
         self.next(self.end)
 
     @step
     def end(self):
         print('alpha is still %f' % self.alpha)
 
 if __name__ == '__main__':
     ParameterFlow()
 ```
 
 Save this script to a file `parameter_flow.py`. To deploy a version to Airflow, simply
 run
 ```
 python parameter_flow.py --with retry airflow create parameter_dag.py 
 ```
 
 This command takes a snapshot of your code in the working directory, as well as the
 version of Metaflow used, and creates an Airflow DAG in `parameter_dag.py` for
 scheduling on Airflow. You should deploy `parameter_dag.py` to your Airflow instance
 like you would do with any other user-written DAG.
 
 Metaflow automatically maps the Parameters of your flow to corresponding parameters on
 Airflow. You can execute your Metaflow flow deployed on Airflow like any other Airflow
 DAG - seamlessly getting all the benefits of Airflow alongside all the benefits of
 Metaflow.",H2,https://docs.metaflow.org/production/scheduling-metaflow-flows/scheduling-with-airflow#pushing-a-flow-to-production,False,1337.0,265.0
1401,Hardening deployments,"It is highly recommended that you [enable
 retries](../../scaling/failures#retrying-tasks-with-the-retry-decorator) when deploying
 to Airflow, which you can do easily with `--with retry` as shown above. However, make
 sure that all your steps are safe to retry before you do this. If some of your steps
 interact with external services in ways that can't tolerate automatic retries, decorate
 them with retry with times set to zero \(times=0\) as described in [How to Prevent
 Retries](../../scaling/failures#how-to-prevent-retries).
 
 
 If you want to test on Airflow without interfering with a production flow, you can
 change the name of your class, e.g. from `ParameterFlow` to `ParameterFlowStaging`, and
 airflow create the dag under a new name or [use the @project
 decorator](../coordinating-larger-metaflow-projects.md).
 
 Note that airflow create creates a new isolated production namespace for your production
 flow. Read [Organizing Results](/scaling/tagging) to learn all about namespace behavior.",H3,https://docs.metaflow.org/production/scheduling-metaflow-flows/scheduling-with-airflow#hardening-deployments,False,1013.0,137.0
1402,Limiting the number of concurrent tasks,"By default, Metaflow configures Airflow to execute at most 100 tasks concurrently within
 a foreach step. This should ensure that most workflows finish quickly without
 overwhelming your Kubernetes cluster, the execution backend.
 
 If your workflow includes a large foreach and you need results faster, you can increase
 the default with the `--max-workers` option. For instance, `airflow create --max-workers
 500` allows 500 tasks to be executed concurrently for every foreach step.",H3,https://docs.metaflow.org/production/scheduling-metaflow-flows/scheduling-with-airflow#limiting-the-number-of-concurrent-tasks,False,485.0,71.0
1403,This option is similar to [`run,limit concurrency outside Airflow.,H2,https://docs.metaflow.org/production/scheduling-metaflow-flows/scheduling-with-airflow#this-option-is-similar-to-run,False,34.0,4.0
1404,Deploy-time parameters,"You can customize Airflow deployments through Parameters that are evaluated at the
 deployment time, i.e. when `airflow create` is executed.
 
 For instance, you can change the default value of a `Parameter` based on who deployed
 the workflow or what Git branch the deployment was executed in. Crucially, the function
 in Parameter is evaluated only once during `airflow create` and not during the execution
 of the flow.
 
 You can run the flow locally as usual. The function inside `Parameter` is called only
 once when the execution starts.
 
 ```python
 from metaflow import FlowSpec, Parameter, step, JSONType
 from datetime import datetime
 import json
 
 def deployment_info(context):
     return json.dumps({'who': context.user_name,
                        'when': datetime.now().isoformat()})
 
 class DeploymentInfoFlow(FlowSpec):
     info = Parameter('deployment_info',
                      type=JSONType,
                      default=deployment_info)
 
     @step
     def start(self):
         print('This flow was deployed at %s by %s'\
               % (self.info['when'], self.info['who']))
         self.next(self.end)
 
     @step
     def end(self):
         pass
 
 if __name__ == '__main__':
     DeploymentInfoFlow()
 ```
 
 When `airflow create` is called, `deployment_info` is evaluated which captures your
 username and the time of deployment. This information remains constant on Airflow
 Workflows, although the user may override the default value.
 
 The `context` object is passed to any function defined in Parameter. It contains various
 fields related to the flow being deployed. By relying on the values passed in context,
 you can create generic deploy-time functions that can be reused by multiple flows.",H3,https://docs.metaflow.org/production/scheduling-metaflow-flows/scheduling-with-airflow#deploy-time-parameters,False,1744.0,354.0
1405,Scheduling a flow,"By default, a flow on Airflow does not run automatically. You need to set up a trigger
 to launch the flow when an event occurs.
 
 On Airflow, Metaflow provides built-in support for triggering Metaflow flows
 through time-based \(cron\) triggers, which, as the name implies, triggers the
 workflow at a certain time. As of today, [event-based triggering]
 (/production/event-triggering) is not supported on Airflow.
 
 Time-based triggers are implemented at the FlowSpec-level using the `@schedule`
 decorator. This flow is triggered hourly:
 
 ```python
 from metaflow import FlowSpec, schedule, step
 from datetime import datetime
 
 @schedule(hourly=True)
 class HourlyFlow(FlowSpec):
 
     @step
     def start(self):
         now = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
         print('time is %s' % now)
         self.next(self.end)
 
     @step
     def end(self):
         pass
 
 if __name__ == '__main__':
     HourlyFlow()
 ```
 
 You can define the schedule with `@schedule` in one of the following ways:",H2,https://docs.metaflow.org/production/scheduling-metaflow-flows/scheduling-with-airflow#scheduling-a-flow,False,1025.0,188.0
1406,Reproducing failed production runs,"Let's use [`DebugFlow` from the debugging
 section](/metaflow/debugging#how-to-use-the-resume-command) as an example. The flow
 contains a bug in the step `b`. When you run it, the failed run will look like this on
 the Airflow UI:
 
 ![](/assets/airflow_debug.png)
 
 The graph visualization shows that step b failed, as expected. First, you should inspect
 the logs of the failed step in the Airflow UI (or the Metaflow UI) to get an idea of why
 it failed.
 
 Notice the Metaflow Run ID of `airflow-ec19e85042a1` that is available from the Rendered
 Template page for the failed task in the Airflow UI (look for the `metaflow_run_id`
 attribute). You can use this Run ID to locate the execution in the Metaflow UI as well
 if needed.
 
 ![](/assets/airflow_debug1.png)
 
 Next, we want to reproduce the above error locally. We do this by resuming the specific
 Airflow run that failed:
 
 ```
 python debug.py resume --origin-run-id airflow-ec19e85042a1
 ```
 This will reuse the results of the start and a step from the Airflow run. It will try to
 rerun the step `b` locally, which fails with the same error as it does in production.
 
 You can fix the error locally, as above. In the case of this simple flow, you can run
 the whole flow locally to confirm that the fix works. After validating the results, you
 would deploy a new version to production with airflow create.
 
 However, this might not be a feasible approach for complex production flow. For
 instance, the flow might process large amounts of data that can not be handled in your
 local instance. We have better approaches for staging flows for production:",H2,https://docs.metaflow.org/production/scheduling-metaflow-flows/scheduling-with-airflow#reproducing-failed-production-runs,False,1627.0,271.0
1407,Staging flows for production,"The easiest approach to test a demanding flow is to run it on Kubernetes. This works
 even with resume:
 ```
 python debug.py resume --origin-run-id airflow-ec19e85042a1 --with kubernetes
 ```
 
 This will resume your flow and run every step on Kubernetes. When you are ready to test
 a fixed flow end-to-end, just run it as follows:
 ```
 python debug.py run --with kubernetes
 ```
 
 Alternatively, you can change the name of the flow temporarily, e.g. from DebugFlow to
 DebugFlowStaging. Then you can run `airflow create` with the new name, which will create
 a separate staging flow on Airflow. You can also use the
 [`@project`](/production/coordinating-larger-metaflow-projects.md#the-project-decorator)
 decorator.
 
 You can test the staging flow freely without interfering with the production flow. Once
 the staging flow runs successfully, you can confidently deploy a new version to
 production.",H3,https://docs.metaflow.org/production/scheduling-metaflow-flows/scheduling-with-airflow#staging-flows-for-production,False,907.0,133.0
1408,Scheduling Metaflow Flows with AWS Step Functions,"[AWS Step Functions](https://aws.amazon.com/step-functions/) is a general-purpose
 workflow orchestrator - you can [read AWS Step Functions documentation to learn all
 about it](https://docs.aws.amazon.com/step-functions/latest/dg/welcome.html). If you
 just want to get your flow in production, this document contains everything you need to
 know.
 
 In Metaflow's point of view, the main benefits of AWS Step Functions are the following:",H1,https://docs.metaflow.org/production/scheduling-metaflow-flows/scheduling-with-aws-step-functions#scheduling-metaflow-flows-with-aws-step-functions,False,439.0,55.0
1409,Pushing a flow to production,"Let's use [the flow from the section about
 parameters](../../metaflow/basics#how-to-define-parameters-for-flows) as an example:
 
 ```python
 from metaflow import FlowSpec, Parameter, step
 
 class ParameterFlow(FlowSpec):
     alpha = Parameter('alpha',
                       help='Learning rate',
                       default=0.01)
 
     @step
     def start(self):
         print('alpha is %f' % self.alpha)
         self.next(self.end)
 
     @step
     def end(self):
         print('alpha is still %f' % self.alpha)
 
 if __name__ == '__main__':
     ParameterFlow()
 ```
 
 Save this script to a file `parameter_flow.py`. To deploy a version to AWS Step
 Functions, simply run
 
 ```bash
 python parameter_flow.py --with retry step-functions create
 ```
 
 This command takes a snapshot of your code in the working directory, as well as the
 version of Metaflow used and exports the whole package to AWS Step Functions for
 scheduling.
 
 It is highly recommended that you [enable
 retries](../../scaling/failures#retrying-tasks-with-the-retry-decorator) when deploying
 to AWS Step Functions, which you can do easily with --with retry as shown above.
 However, make sure that all your steps are safe to retry before you do this. If some of
 your steps interact with external services in ways that can't tolerate automatic
 retries, decorate them with retry with times set to zero \(times=0\) as described in
 [How to Prevent Retries](../../scaling/failures#how-to-prevent-retries).
 
 The command will export your workflow to AWS Step Functions. You can also search for the
 flow by name within the AWS Step Functions UI. You should see a visualization of the
 exported flow, like here:
 
 ![](/assets/image2.png)
 
 ![](/assets/image5.png)
 
 You can click the orange Start Execution button to execute the flow on AWS Step
 Functions. It pops up a dialog asking for an input. You can specify your parameters as
 an escaped JSON string with `Parameters` as the key - \*\*\*\*
 
 ```bash
 {
     ""Parameters"" : ""{\""alpha\"": 0.5}""
 }
 ```
 
 Metaflow automatically maps Parameters of your flow to corresponding parameters on AWS
 Step Functions.
 
 After you click Start Execution on the Input dialog, AWS Step Functions starts running
 the flow:
 
 ![](/assets/image6.png)
 
 In this case, the run should succeed without problems. If there were errors, you could
 reproduce them locally as explained in [Debugging with
 Metaflow](../../metaflow/debugging#reproducing-production-issues-locally).
 
 You can trigger the workflow through command line as well:
 
 ```bash
 python parameter_flow.py step-functions trigger --alpha 0.5
 ```
 
 If you run `step-functions create` again, it will create a new version of your flow on
 AWS Step Functions. The newest version becomes the production version automatically
 \(due to the consistency guarantees provided by AWS Step Functions, it might be a couple
 of seconds before this happens\). If you want to test on AWS Step Functions without
 interfering with a production flow, you can change the name of your class, e.g. from
 ParameterFlow to ParameterFlowStaging, and `step-functions create` the flow under a new
 name or use the
 [@project](../coordinating-larger-metaflow-projects.md/#projects-on-aws-step-functions--argo-workflows)
 decorator.
 
 Note that `step-functions create` creates a new isolated [production
 namespace](../../scaling/tagging#production-namespaces) for your production flow. Please
 read [Organizing Results](../../scaling/tagging) to learn all about namespace behavior.
 
 In case your flow doesn't accept any parameters, and you would like to execute it from
 within the AWS Step Functions UI, you would need to pass in the following in the input
 dialog box:
 
 ```bash
 {
     ""Parameters"" : ""{}""
 }
 ```",H2,https://docs.metaflow.org/production/scheduling-metaflow-flows/scheduling-with-aws-step-functions#pushing-a-flow-to-production,False,3793.0,620.0
1410,Limiting the number of concurrent tasks,"By default, Metaflow configures AWS Step Functions to execute at most 100 tasks
 concurrently within a foreach step. This should ensure that most workflows finish
 quickly without overwhelming your AWS Batch queue, the execution backend.
 
 If your workflow includes a large foreach and you need results faster, you can increase",H3,https://docs.metaflow.org/production/scheduling-metaflow-flows/scheduling-with-aws-step-functions#limiting-the-number-of-concurrent-tasks,False,328.0,51.0
1411,"the default with the `--max-workers` option. For instance, `step-functions create",,H2,https://docs.metaflow.org/production/scheduling-metaflow-flows/scheduling-with-aws-step-functions#the-default-with-the-max-workers-option-for-instance-step-functions-create,False,0.0,1.0
1412,This option is similar to [`run,limit concurrency outside AWS Step Functions.,H2,https://docs.metaflow.org/production/scheduling-metaflow-flows/scheduling-with-aws-step-functions#this-option-is-similar-to-run,False,45.0,6.0
1413,Deploy-time parameters,"You can customize AWS Step Functions deployments through Parameters that are evaluated
 at the deployment time, i.e. when `step-functions create` is executed.
 
 For instance, you can change the default value of a Parameter based on who deployed the
 workflow or what Git branch the deployment was executed in. Crucially, the function in
 Parameter is evaluated only once during `step-functions create` and not during the
 execution of the flow.
 
 You can run the flow locally as usual. The function inside Parameter is called only once
 when the execution starts.
 
 ```python
 from metaflow import FlowSpec, Parameter, step, JSONType
 from datetime import datetime
 import json
 
 def deployment_info(context):
     return json.dumps({'who': context.user_name,
                        'when': datetime.now().isoformat()})
 
 class DeploymentInfoFlow(FlowSpec):
     info = Parameter('deployment_info',
                      type=JSONType,
                      default=deployment_info)
 
     @step
     def start(self):
         print('This flow was deployed at %s by %s'\
               % (self.info['when'], self.info['who']))
         self.next(self.end)
 
     @step
     def end(self):
         pass
 
 if __name__ == '__main__':
     DeploymentInfoFlow()
 ```
 
 When `step-functions create` is called, `deployment_info` is evaluated which captures
 your username and the time of deployment. This information remains constant on AWS Step
 Functions, although the user may override the default value.
 
 The `context` object is passed to any function defined in Parameter. It contains various
 fields related to the flow being deployed. By relying on the values passed in context,
 you can create generic deploy-time functions that can be reused by multiple flows.",H3,https://docs.metaflow.org/production/scheduling-metaflow-flows/scheduling-with-aws-step-functions#deploy-time-parameters,False,1773.0,357.0
1414,Scheduling a flow,"By default, a flow on AWS Step Functions does not run automatically. You need to set up
 a trigger to launch the flow when an event occurs.
 
 On Step Functions, Metaflow provides built-in support for triggering Metaflow
 flows through time-based \(cron\) triggers, which, as the name implies,
 triggers the workflow at a certain time. As of today, [event-based triggering]
 (/production/event-triggering) is not supported on Step Functions.
 
 Time-based triggers are implemented at the FlowSpec-level using the `@schedule`
 decorator. This flow is triggered hourly:
 
 ```python
 from metaflow import FlowSpec, schedule, step
 from datetime import datetime
 
 @schedule(hourly=True)
 class HourlyFlow(FlowSpec):
 
     @step
     def start(self):
         now = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
         print('time is %s' % now)
         self.next(self.end)
 
     @step
     def end(self):
         pass
 
 if __name__ == '__main__':
     HourlyFlow()
 ```
 
 You can define the schedule with `@schedule` in one of the following ways:",H2,https://docs.metaflow.org/production/scheduling-metaflow-flows/scheduling-with-aws-step-functions#scheduling-a-flow,False,1050.0,192.0
1415,Reproducing failed production runs,"Let's use [`DebugFlow` from the debugging
 section](/metaflow/debugging#how-to-use-the-resume-command) as an example. The flow
 contains a bug in the step `b`. When you run it, the failed run will look like this on
 the AWS Step Functions UI:
 
 ![](/assets/image1.png)
 
 ![](</assets/image3_(1).png>)
 
 Notice the execution ID of `5ca85f96-8508-409d-a5f5-b567db1040c5`. When running on AWS
 Step Functions, Metaflow uses the AWS Step Functions execution ID (prefixed with `sfn-`)
 as the run id.
 
 The graph visualization shows that step `b` failed, as expected. First, you should
 inspect the logs of the failed step to get an idea of why it failed. You can access AWS
 Batch step logs in the AWS Step Functions UI by looking for the `JobId` in the `Error`
 blob that can be accessed by clicking on the `Exception` pane on the right side of the
 UI. You can use this `JobId` in the AWS Batch console to check the job logs. This
 `JobId` is also the metaflow task ID for the step.
 
 Next, we want to reproduce the above error locally. We do this by resuming the specific
 AWS Step Functions run that failed:
 
 ```bash
 python debug.py resume --origin-run-id sfn-5ca85f96-8508-409d-a5f5-b567db1040c5
 ```
 
 This will reuse the results of the `start` and `a` step from the AWS Step Functions run.
 It will try to rerun the step `b` locally, which fails with the same error as it does in
 production.
 
 You can fix the error locally as above. In the case of this simple flow, you can run the
 whole flow locally to confirm that the fix works. After validating the results, you
 would deploy a new version to production with `step-functions create`.
 
 However, this might not be a feasible approach for complex production flow. For
 instance, the flow might process large amounts of data that can not be handled in your
 local instance. We have better approaches for staging flows for production:",H2,https://docs.metaflow.org/production/scheduling-metaflow-flows/scheduling-with-aws-step-functions#reproducing-failed-production-runs,False,1901.0,316.0
1416,Staging flows for production,"The easiest approach to test a demanding flow is to run it with AWS Batch. This works
 even with resume:
 
 ```bash
 python debug.py resume --origin-run-id sfn-5ca85f96-8508-409d-a5f5-b567db1040c5 --with batch
 ```
 
 This will resume your flow and run every step on AWS Batch. When you are ready to test a
 fixed flow end-to-end, just run it as follows:
 
 ```bash
 python debug.py run --with batch
 ```
 
 Alternatively, you can change the name of the flow temporarily, e.g. from DebugFlow to
 DebugFlowStaging. Then you can run `step-functions create` with the new name, which will
 create a separate staging flow on AWS Step Functions. You can also use the
 [`@project`](/production/coordinating-larger-metaflow-projects.md#the-project-decorator)
 decorator.
 
 You can test the staging flow freely without interfering with the production flow. Once
 the staging flow runs successfully, you can confidently deploy a new version to
 production.",H3,https://docs.metaflow.org/production/scheduling-metaflow-flows/scheduling-with-aws-step-functions#staging-flows-for-production,False,947.0,139.0
1417,Scheduling Metaflow Flows with Argo Workflows,"[Argo Workflows](https://argoproj.github.io/workflows) is a Kubernetes-native workflow
 orchestrator - you can [read Argo Workflows documentation to learn all about
 it](https://argoproj.github.io/argo-workflows/core-concepts/). If you just want to get
 your flow in production, this document contains everything you need to know.
 
 In Metaflow's point of view, the main benefits of Argo Workflows are the following:",H1,https://docs.metaflow.org/production/scheduling-metaflow-flows/scheduling-with-argo-workflows#scheduling-metaflow-flows-with-argo-workflows,False,417.0,52.0
1418,Pushing a flow to production,"Let's use [the flow from the section about
 parameters](../../metaflow/basics#how-to-define-parameters-for-flows) as an example:
 
 ```python
 from metaflow import FlowSpec, Parameter, step
 
 class ParameterFlow(FlowSpec):
     alpha = Parameter('alpha',
                       help='Learning rate',
                       default=0.01)
 
     @step
     def start(self):
         print('alpha is %f' % self.alpha)
         self.next(self.end)
 
     @step
     def end(self):
         print('alpha is still %f' % self.alpha)
 
 if __name__ == '__main__':
     ParameterFlow()
 ```
 
 Save this script to a file `parameter_flow.py`. To deploy a version to Argo Workflows,
 simply run
 
 ```bash
 python parameter_flow.py --with retry argo-workflows create
 ```
 
 This command takes a snapshot of your code in the working directory, as well as the
 version of Metaflow used and exports the whole package to Argo Workflows for scheduling.
 
 It is highly recommended that you [enable
 retries](../../scaling/failures#retrying-tasks-with-the-retry-decorator) when deploying
 to Argo Workflows, which you can do easily with --with retry as shown above. However,
 make sure that all your steps are safe to retry before you do this. If some of your
 steps interact with external services in ways that can't tolerate automatic retries,
 decorate them with retry with times set to zero \(times=0\) as described in [How to
 Prevent Retries](../../scaling/failures#how-to-prevent-retries).
 
 The command will export your workflow to  Argo Workflows as a _workflow template_. You
 can also search for the _workflow template_ by name within the Argo Workflows UI. 
 
 ![](/assets/argo-ui.png)
 
 You can click on _Submit new workflow_ to submit your generated _Workflow Template_ for
 execution
 
 ![](/assets/argo-ui-0.png)
 
 Metaflow automatically maps Parameters of your flow to corresponding parameters on Argo
 Workflows.
 
 ![](/assets/argo-ui-1.png)
 
 After you click _Submit_, Argo Workflow starts running the flow:
 
 ![](/assets/argo-ui-2.png)
 
 In this case, the run should succeed without problems. If there were errors, you could
 reproduce them locally as explained in [Debugging with
 Metaflow](../../metaflow/debugging#reproducing-production-issues-locally).
 
 You can trigger the workflow through command line as well:
 
 ```bash
 python parameter_flow.py argo-workflows trigger --alpha 0.5
 ```
 
 If you run `argo-workflows create` again, it will create a new version of your flow on
 Argo Workflows. The newest version becomes the production version automatically. If you
 want to test on Argo Workflows without interfering with a production flow, you can
 change the name of your class, e.g. from ParameterFlow to ParameterFlowStaging, and
 `argo-workflows create` the flow under a new name or use the
 [@project](../coordinating-larger-metaflow-projects.md/#projects-on-aws-step-functions--argo-workflows)
 decorator.
 
 Note that `argo-workflows create` creates a new isolated [production
 namespace](../../scaling/tagging#production-namespaces) for your production flow. Please
 read [Organizing Results](../../scaling/tagging) to learn all about namespace behavior.",H2,https://docs.metaflow.org/production/scheduling-metaflow-flows/scheduling-with-argo-workflows#pushing-a-flow-to-production,False,3185.0,496.0
1419,Limiting the number of concurrent tasks,"By default, Metaflow configures Argo Workflows to execute at most 100 tasks concurrently
 within a foreach step. This should ensure that most workflows finish quickly without
 overwhelming your Kubernetes cluster, the execution backend.
 
 If your workflow includes a large foreach and you need results faster, you can increase",H3,https://docs.metaflow.org/production/scheduling-metaflow-flows/scheduling-with-argo-workflows#limiting-the-number-of-concurrent-tasks,False,327.0,49.0
1420,"the default with the `--max-workers` option. For instance, `argo-workflows create",,H2,https://docs.metaflow.org/production/scheduling-metaflow-flows/scheduling-with-argo-workflows#the-default-with-the-max-workers-option-for-instance-argo-workflows-create,False,0.0,1.0
1421,This option is similar to [`run,limit concurrency outside Argo Workflows.,H2,https://docs.metaflow.org/production/scheduling-metaflow-flows/scheduling-with-argo-workflows#this-option-is-similar-to-run,False,41.0,5.0
1422,Deploy-time parameters,"You can customize Argo Workflows deployments through Parameters that are evaluated at
 the deployment time, i.e. when `argo-workflows create` is executed.
 
 For instance, you can change the default value of a Parameter based on who deployed the
 workflow or what Git branch the deployment was executed in. Crucially, the function in
 Parameter is evaluated only once during `argo-workflows create` and not during the
 execution of the flow.
 
 You can run the flow locally as usual. The function inside Parameter is called only once
 when the execution starts.
 
 ```python
 from metaflow import FlowSpec, Parameter, step, JSONType
 from datetime import datetime
 import json
 
 def deployment_info(context):
     return json.dumps({'who': context.user_name,
                        'when': datetime.now().isoformat()})
 
 class DeploymentInfoFlow(FlowSpec):
     info = Parameter('deployment_info',
                      type=JSONType,
                      default=deployment_info)
 
     @step
     def start(self):
         print('This flow was deployed at %s by %s'\
               % (self.info['when'], self.info['who']))
         self.next(self.end)
 
     @step
     def end(self):
         pass
 
 if __name__ == '__main__':
     DeploymentInfoFlow()
 ```
 
 When `argo-workflows create` is called, `deployment_info` is evaluated which captures
 your username and the time of deployment. This information remains constant on Argo
 Workflows, although the user may override the default value.
 
 The `context` object is passed to any function defined in Parameter. It contains various
 fields related to the flow being deployed. By relying on the values passed in context,
 you can create generic deploy-time functions that can be reused by multiple flows.",H3,https://docs.metaflow.org/production/scheduling-metaflow-flows/scheduling-with-argo-workflows#deploy-time-parameters,False,1765.0,355.0
1423,Scheduling a flow,"By default, a flow on Argo Workflows does not run automatically. You need to set up a
 trigger to launch the flow when an event occurs. With Argo Workflows, Metaflow supports
 two kinds of triggering:
 
 1. [Triggering based on events (using `@trigger` and `@trigger_on_finish`)](/production/event-triggering).
 2. Triggering based on time (using `@schedule`), described below.",H2,https://docs.metaflow.org/production/scheduling-metaflow-flows/scheduling-with-argo-workflows#scheduling-a-flow,False,377.0,54.0
1424,Time-based triggering,"Metaflow provides built-in support for triggering Metaflow flows through time-based
 \(cron\) triggers. Use a time-based trigger if you want to trigger the workflow at a
 certain time.
 
 Time-based triggers are implemented at the FlowSpec-level using the `@schedule`
 decorator. This flow is triggered hourly:
 
 ```python
 from metaflow import FlowSpec, schedule, step
 from datetime import datetime
 
 @schedule(hourly=True)
 class HourlyFlow(FlowSpec):
 
     @step
     def start(self):
         now = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
         print('time is %s' % now)
         self.next(self.end)
 
     @step
     def end(self):
         pass
 
 if __name__ == '__main__':
     HourlyFlow()
 ```
 
 You can define the schedule with `@schedule` in one of the following ways:",H3,https://docs.metaflow.org/production/scheduling-metaflow-flows/scheduling-with-argo-workflows#time-based-triggering,False,793.0,152.0
1425,Reproducing failed production runs,"Let's use [`DebugFlow` from the debugging
 section](/metaflow/debugging#how-to-use-the-resume-command) as an example. The flow
 contains a bug in the step `b`. When you run it, the failed run will look like this on
 the Argo Workflows UI:
 
 ![](/assets/argo-ui-fail.png)
 
 Notice the execution ID of `branchflow-r8qcn`. When running on Argo Workflows, Metaflow
 uses the Argo Workflows _workflow execution_ name (prefixed with `argo-`) as the run id.
 
 The graph visualization shows that step `b` failed, as expected. First, you should
 inspect the logs of the failed step to get an idea of why it failed. You can access
 Kubernetes step logs in the Argo Workflows UI by selecting the failed task and clicking
 on the logs button. 
 
 Next, we want to reproduce the above error locally. We do this by resuming the specific
 Argo Workflows run that failed:
 
 ```bash
 python debug.py resume --origin-run-id argo-branchflow-r8qcn
 ```
 
 This will reuse the results of the `start` and `a` step from the Argo Workflows run. It
 will try to rerun the step `b` locally, which fails with the same error as it does in
 production.
 
 You can fix the error locally as above. In the case of this simple flow, you can run the
 whole flow locally to confirm that the fix works. After validating the results, you
 would deploy a new version to production with `argo-workflows create`.
 
 However, this might not be a feasible approach for complex production flow. For
 instance, the flow might process large amounts of data that can not be handled in your
 local instance. We have better approaches for staging flows for production:",H2,https://docs.metaflow.org/production/scheduling-metaflow-flows/scheduling-with-argo-workflows#reproducing-failed-production-runs,False,1624.0,268.0
1426,Staging flows for production,"The easiest approach to test a demanding flow is to run it with Kubernetes. This works
 even with resume:
 
 ```bash
 python debug.py resume --origin-run-id argo-branchflow-r8qcn --with kubernetes
 ```
 
 This will resume your flow and run every step on Kubernetes. When you are ready to test
 a fixed flow end-to-end, just run it as follows:
 
 ```bash
 python debug.py run --with kubernetes
 ```
 
 Alternatively, you can change the name of the flow temporarily, e.g. from DebugFlow to
 DebugFlowStaging. Then you can run `argo-workflows create` with the new name, which will
 create a separate staging flow on Argo Workflows. You can also use the
 [`@project`](/production/coordinating-larger-metaflow-projects.md#the-project-decorator)
 decorator.
 
 You can test the staging flow freely without interfering with the production flow. Once
 the staging flow runs successfully, you can confidently deploy a new version to
 production.",H3,https://docs.metaflow.org/production/scheduling-metaflow-flows/scheduling-with-argo-workflows#staging-flows-for-production,False,936.0,136.0
1427,FlowSpec - Constructing flows,"Metaflow [flows are defined](/metaflow/basics) by inhering from the `FlowSpec` class:
 ```python
 from metaflow import FlowSpec, step
 
 class MyFlow(FlowSpec):
 
     @step
     def start(self):
         self.next(self.end)
        
     @step
     def end(self):
         pass
 
 if __name__ == '__main__':
     MyFlow()
 ```
 
 This class has no other uses. It can't be instantiated directly.
 
 `FlowSpec` exposes a few methods and attributes that you can use to construct a flow, which are listed below. You can add more functionality in your flows through [step-level decorators](step-decorators) and [flow-level decorators](flow-decorators).
 
 You can parametrize flows through the [`Parameter`](#parameters) object that are defined as class variables inside a flow. You can also include files as parameters through the [`IncludeFile`](#includefile) object.
 
 To query and manipulate the currently executing run inside your flow, see the [`current`](current) object. To access results produced by a flow, see the [Client API](client).",H1,https://docs.metaflow.org/api/flowspec#flowspec-constructing-flows,False,1043.0,181.0
1428,Defining a workflow,"Annotate methods that are a part of your Metaflow workflow with [the `@step` decorator](/api/step-decorators/step). Use `FlowSpec.next` to define transitions between steps:
 
 
 <DocSection type=""method"" name=""FlowSpec.next"" module=""metaflow"" show_import=""False"" heading_level=""4"" link=""https://github.com/Netflix/metaflow/tree/master/metaflow/flowspec.py#L502"">
 <SigArgSection>
 <SigArg name=""*dsts, foreach=None"" />
 </SigArgSection>
 <Description summary=""Indicates the next step to execute after this step has completed."" extended_summary=""This statement should appear as the last statement of each step, except\nthe end step.\n\nThere are several valid formats to specify the next step:\n\n- Straight-line connection: `self.next(self.next_step)` where `next_step` is a method in\n  the current class decorated with the `@step` decorator.\n\n- Static fan-out connection: `self.next(self.step1, self.step2, ...)` where `stepX` are\n  methods in the current class decorated with the `@step` decorator.\n\n- Foreach branch:\n  ```\n  self.next(self.foreach_step, foreach='foreach_iterator')\n  ```\n  In this situation, `foreach_step` is a method in the current class decorated with the\n  `@step` decorator and `foreach_iterator` is a variable name in the current class that\n  evaluates to an iterator. A task will be launched for each value in the iterator and\n  each task will execute the code specified by the step `foreach_step`."" />
 <ParamSection name=""Parameters"">
 	<Parameter name=""dsts"" type=""Method"" desc=""One or more methods annotated with `@step`."" />
 </ParamSection>
 <ParamSection name=""Raises"">
 	<Parameter type=""InvalidNextException"" desc=""Raised if the format of the arguments does not match one of the ones given above."" />
 </ParamSection>
 </DocSection>",H2,https://docs.metaflow.org/api/flowspec#defining-a-workflow,False,1781.0,214.0
1429,Working with foreaches,"Use the operations below, `FlowSpec.input`, `FlowSpec.index`, and `FlowSpec.foreach_stack` to query the status of the currently executing foreach branch. Use `FlowSpec.merge_artifacts()` to handle incoming artifacts in a join step.
 
 
 <DocSection type=""property"" name=""FlowSpec.input"" module=""metaflow.flowspec"" show_import=""False"" heading_level=""4"" link=""https://github.com/Netflix/metaflow/tree/master/"">
 
 <Description summary=""The value of the foreach artifact in this foreach branch.\n\nIn a foreach step, multiple instances of this step (tasks) will be executed,\none for each element in the foreach. This property returns the element passed\nto the current task. If this is not a foreach step, this returns None.\n\nIf you need to know the values of the parent tasks in a nested foreach, use\n`FlowSpec.foreach_stack`.\n"" />
 <ParamSection name=""Returns"">
 <Parameter type=""object, optional"" desc=""Input passed to the foreach task."" />
 </ParamSection>
 </DocSection>
 
 
 
 <DocSection type=""property"" name=""FlowSpec.index"" module=""metaflow.flowspec"" show_import=""False"" heading_level=""4"" link=""https://github.com/Netflix/metaflow/tree/master/"">
 
 <Description summary=""The index of this foreach branch.\n\nIn a foreach step, multiple instances of this step (tasks) will be executed,\none for each element in the foreach. This property returns the zero based index\nof the current task. If this is not a foreach step, this returns None.\n\nIf you need to know the indices of the parent tasks in a nested foreach, use\n`FlowSpec.foreach_stack`.\n"" />
 <ParamSection name=""Returns"">
 <Parameter type=""int, optional"" desc=""Index of the task in a foreach step."" />
 </ParamSection>
 </DocSection>
 
 
 
 <DocSection type=""method"" name=""FlowSpec.foreach_stack"" module=""metaflow"" show_import=""False"" heading_level=""4"" link=""https://github.com/Netflix/metaflow/tree/master/metaflow/flowspec.py#L276"">
 <SigArgSection>
 <SigArg name=""self"" />
 </SigArgSection>
 <Description summary=""Returns the current stack of foreach indexes and values for the current step."" extended_summary=""Use this information to understand what data is being processed in the current\nforeach branch. For example, considering the following code:\n```\n@step\ndef root(self):\n    self.split_1 = ['a', 'b', 'c']\n    self.next(self.nest_1, foreach='split_1')\n\n@step\ndef nest_1(self):\n    self.split_2 = ['d', 'e', 'f', 'g']\n    self.next(self.nest_2, foreach='split_2'):\n\n@step\ndef nest_2(self):\n    foo = self.foreach_stack()\n```\n\n`foo` will take the following values in the various tasks for nest_2:\n```\n    [(0, 3, 'a'), (0, 4, 'd')]\n    [(0, 3, 'a'), (1, 4, 'e')]\n    ...\n    [(0, 3, 'a'), (3, 4, 'g')]\n    [(1, 3, 'b'), (0, 4, 'd')]\n    ...\n```\nwhere each tuple corresponds to:\n\n- The index of the task for that level of the loop.\n- The number of splits for that level of the loop.\n- The value for that level of the loop.\n\nNote that the last tuple returned in a task corresponds to:\n\n- 1st element: value returned by `self.index`.\n- 3rd element: value returned by `self.input`."" />
 <ParamSection name=""Returns"">
 	<Parameter type=""List[Tuple[int, int, object]]"" desc=""An array describing the current stack of foreach steps."" />
 </ParamSection>
 </DocSection>
 
 
 
 <DocSection type=""method"" name=""FlowSpec.merge_artifacts"" module=""metaflow"" show_import=""False"" heading_level=""4"" link=""https://github.com/Netflix/metaflow/tree/master/metaflow/flowspec.py#L360"">
 <SigArgSection>
 <SigArg name=""self"" /><SigArg name=""inputs"" type=""Inputs"" /><SigArg name=""exclude"" type=""Optional"" default=""None"" /><SigArg name=""include"" type=""Optional"" default=""None"" />
 </SigArgSection>
 <Description summary=""Helper function for merging artifacts in a join step."" extended_summary=""This function takes all the artifacts coming from the branches of a\njoin point and assigns them to self in the calling step. Only artifacts\nnot set in the current step are considered. If, for a given artifact, different\nvalues are present on the incoming edges, an error will be thrown and the artifacts\nthat conflict will be reported.\n\nAs a few examples, in the simple graph: A splitting into B and C and joining in D:\n```\nA:\n  self.x = 5\n  self.y = 6\nB:\n  self.b_var = 1\n  self.x = from_b\nC:\n  self.x = from_c\n\nD:\n  merge_artifacts(inputs)\n```\nIn D, the following artifacts are set:\n  - `y` (value: 6), `b_var` (value: 1)\n  - if `from_b` and `from_c` are the same, `x` will be accessible and have value `from_b`\n  - if `from_b` and `from_c` are different, an error will be thrown. To prevent this error,\n    you need to manually set `self.x` in D to a merged value (for example the max) prior to\n    calling `merge_artifacts`."" />
 <ParamSection name=""Parameters"">
 	<Parameter name=""inputs"" type=""Inputs"" desc=""Incoming steps to the join point."" />
 	<Parameter name=""exclude"" type=""List[str], optional"" desc=""If specified, do not consider merging artifacts with a name in `exclude`.\nCannot specify if `include` is also specified."" />
 	<Parameter name=""include"" type=""List[str], optional"" desc=""If specified, only merge artifacts specified. Cannot specify if `exclude` is\nalso specified."" />
 </ParamSection>
 <ParamSection name=""Raises"">
 	<Parameter type=""MetaflowException"" desc=""This exception is thrown if this is not called in a join step."" />
 	<Parameter type=""UnhandledInMergeArtifactsException"" desc=""This exception is thrown in case of unresolved conflicts."" />
 	<Parameter type=""MissingInMergeArtifactsException"" desc=""This exception is thrown in case an artifact specified in `include` cannot\nbe found."" />
 </ParamSection>
 </DocSection>",H2,https://docs.metaflow.org/api/flowspec#working-with-foreaches,False,5667.0,724.0
1430,Parameters,"The `Parameter` class is used to define parameters for a flow.
 
 The `Parameter` objects must be defined as class variables inside a flow. The parameter values are available as read-only artifacts in all steps of the flow. For instructions, see [How to define parameters for flows](/metaflow/basics#how-to-define-parameters-for-flows).
 
 
 <DocSection type=""class"" name=""Parameter"" module=""metaflow"" show_import=""False"" heading_level=""3"" link=""https://github.com/Netflix/metaflow/tree/master/metaflow/parameters.py#L217"">
 <SigArgSection>
 <SigArg name=""name"" type=""str"" /><SigArg name=""default"" type=""Union"" default=""None"" /><SigArg name=""type"" type=""Union"" default=""None"" /><SigArg name=""help"" type=""Optional"" default=""None"" /><SigArg name=""required"" type=""bool"" default=""False"" /><SigArg name=""show_default"" type=""bool"" default=""True"" /><SigArg name=""**kwargs: Dict[str, Any]"" type=""Dict"" />
 </SigArgSection>
 <Description summary=""Defines a parameter for a flow."" extended_summary=""Parameters must be instantiated as class variables in flow classes, e.g.\n```\nclass MyFlow(FlowSpec):\n    param = Parameter('myparam')\n```\nin this case, the parameter is specified on the command line as\n```\npython myflow.py run --myparam=5\n```\nand its value is accessible through a read-only artifact like this:\n```\nprint(self.param == 5)\n```\nNote that the user-visible parameter name, `myparam` above, can be\ndifferent from the artifact name, `param` above.\n\nThe parameter value is converted to a Python type based on the `type`\nargument or to match the type of `default`, if it is set."" />
 <ParamSection name=""Parameters"">
 	<Parameter name=""name"" type=""str"" desc=""User-visible parameter name."" />
 	<Parameter name=""default"" type=""str or float or int or bool or `JSONType` or a function."" desc=""Default value for the parameter. Use a special `JSONType` class to\nindicate that the value must be a valid JSON object. A function\nimplies that the parameter corresponds to a *deploy-time parameter*.\nThe type of the default value is used as the parameter `type`."" />
 	<Parameter name=""type"" type=""Type, default: None"" desc=""If `default` is not specified, define the parameter type. Specify\none of `str`, `float`, `int`, `bool`, or `JSONType`. If None, defaults\nto the type of `default` or `str` if none specified."" />
 	<Parameter name=""help"" type=""str, optional"" desc=""Help text to show in `run --help`."" />
 	<Parameter name=""required"" type=""bool, default: False"" desc=""Require that the user specified a value for the parameter.\n`required=True` implies that the `default` is not used."" />
 	<Parameter name=""show_default"" type=""bool, default: True"" desc=""If True, show the default value in the help text."" />
 </ParamSection>
 </DocSection>",H2,https://docs.metaflow.org/api/flowspec#parameters,False,2753.0,330.0
1431,Deploy-time parameters,"It is possible to define the `default` value programmatically before a run or a deployment is executed through a user-defined function. For more information, see [documentation for Deploy Time Parameters](/production/scheduling-metaflow-flows/scheduling-with-aws-step-functions#deploy-time-parameters).
 
 For instance, the following deploy-time parameter, `time`, uses the current time as its default value:
 ```python
 def time_now(context):
     return int(time.time())
 
 class MyFlow(FlowSpec):
     myparam = Parameter(""time"", type=int, default=time_now)
 ```
 Note that if the function returns a non-string value, you must specify the parameter `type` when using deploy-time parameters, as the type of `default` can't be inferred automatically.
 
 The function called gets a parameter `context` that contains attributes about the current parameter which you can use to customize the value returned:
 
 
 <DocSection type=""class"" name=""ParameterContext"" module=""metaflow"" show_import=""False"" heading_level=""3"" link=""https://github.com/Netflix/metaflow/tree/master/"">
 <SigArgSection>
 
 </SigArgSection>
 <Description summary=""Information about the parameter being evaluated."" />
 <ParamSection name=""Attributes"">
 	<Parameter name=""flow_name"" type=""str"" desc=""Flow name"" />
 	<Parameter name=""user_name"" type=""str"" desc=""User name"" />
 	<Parameter name=""parameter_name"" type=""str"" desc=""Parameter name"" />
 </ParamSection>
 </DocSection>",H3,https://docs.metaflow.org/api/flowspec#deploy-time-parameters,False,1444.0,162.0
1432,IncludeFile,"The `IncludeFile` object is a special `Parameter` that reads its value from a local file. For an example, see [Data in Local Files](/scaling/data#data-in-local-files).
 
 
 <DocSection type=""class"" name=""IncludeFile"" module=""metaflow"" show_import=""False"" heading_level=""3"" link=""https://github.com/Netflix/metaflow/tree/master/metaflow/includefile.py#L237"">
 <SigArgSection>
 <SigArg name=""name, **kwargs"" />
 </SigArgSection>
 <Description summary=""Includes a local file as a parameter for the flow."" extended_summary=""`IncludeFile` behaves like `Parameter` except that it reads its value from a file instead of\nthe command line. The user provides a path to a file on the command line. The file contents\nare saved as a read-only artifact which is available in all steps of the flow."" />
 <ParamSection name=""Parameters"">
 	<Parameter name=""name"" type=""str"" desc=""User-visible parameter name."" />
 	<Parameter name=""default"" type=""str or a function"" desc=""Default path to a local file. A function\nimplies that the parameter corresponds to a *deploy-time parameter*."" />
 	<Parameter name=""is_text"" type=""bool, default: True"" desc=""Convert the file contents to a string using the provided `encoding`.\nIf False, the artifact is stored in `bytes`."" />
 	<Parameter name=""encoding"" type=""str, optional, default: 'utf-8'"" desc=""Use this encoding to decode the file contexts if `is_text=True`."" />
 	<Parameter name=""required"" type=""bool, default: False"" desc=""Require that the user specified a value for the parameter.\n`required=True` implies that the `default` is not used."" />
 	<Parameter name=""help"" type=""str, optional"" desc=""Help text to show in `run --help`."" />
 	<Parameter name=""show_default"" type=""bool, default: True"" desc=""If True, show the default value in the help text."" />
 </ParamSection>
 </DocSection>",H2,https://docs.metaflow.org/api/flowspec#includefile,False,1821.0,222.0
1433,S3 - Accessing data in S3 quickly,"The `S3` client is a wrapper over the standard AWS Python library, `boto`. It contains enhancements that are relevant for data-intensive applications:
 
  - Supports accessing large amounts of data quickly through parallel operations (functions with the `_many` suffix). You can download up to 20Gbps on a large EC2 instance.
  - Improved error handling.
  - Supports versioned data through `S3(run=self)` and `S3(run=Run)`.
  - User-friendly API with minimal boilerplate.
  - Convenient API for advanced featured such as range requests (downloading partial files) and object headers.
  
 For instructions how to use the class, see [Loading and Storing Data](/scaling/data).",H1,https://docs.metaflow.org/api/S3#s3-accessing-data-in-s3-quickly,False,674.0,103.0
1434,The `S3` client,"<DocSection type=""class"" name=""S3"" module=""metaflow"" show_import=""True"" heading_level=""3"" link=""https://github.com/Netflix/metaflow/tree/master/metaflow/plugins/datatools/s3/s3.py#L423"">
 <SigArgSection>
 <SigArg name=""tmproot='.', bucket=None, prefix=None, run=None, s3root=None"" />
 </SigArgSection>
 <Description summary=""The Metaflow S3 client."" extended_summary=""This object manages the connection to S3 and a temporary diretory that is used\nto download objects. Note that in most cases when the data fits in memory, no local\ndisk IO is needed as operations are cached by the operating system, which makes\noperations fast as long as there is enough memory available.\n\nThe easiest way is to use this object as a context manager:\n```\nwith S3() as s3:\n    data = [obj.blob for obj in s3.get_many(urls)]\nprint(data)\n```\nThe context manager takes care of creating and deleting a temporary directory\nautomatically. Without a context manager, you must call `.close()` to delete\nthe directory explicitly:\n```\ns3 = S3()\ndata = [obj.blob for obj in s3.get_many(urls)]\ns3.close()\n```\nYou can customize the location of the temporary directory with `tmproot`. It\ndefaults to the current working directory.\n\nTo make it easier to deal with object locations, the client can be initialized\nwith an S3 path prefix. There are three ways to handle locations:\n\n1. Use a `metaflow.Run` object or `self`, e.g. `S3(run=self)` which\n   initializes the prefix with the global `DATATOOLS_S3ROOT` path, combined\n   with the current run ID. This mode makes it easy to version data based\n   on the run ID consistently. You can use the `bucket` and `prefix` to\n   override parts of `DATATOOLS_S3ROOT`.\n\n2. Specify an S3 prefix explicitly with `s3root`,\n   e.g. `S3(s3root='s3://mybucket/some/path')`.\n\n3. Specify nothing, i.e. `S3()`, in which case all operations require\n   a full S3 url prefixed with `s3://`."" />
 <ParamSection name=""Parameters"">
 	<Parameter name=""tmproot"" type=""str, default: '.'"" desc=""Where to store the temporary directory."" />
 	<Parameter name=""bucket"" type=""str, optional"" desc=""Override the bucket from `DATATOOLS_S3ROOT` when `run` is specified."" />
 	<Parameter name=""prefix"" type=""str, optional"" desc=""Override the path from `DATATOOLS_S3ROOT` when `run` is specified."" />
 	<Parameter name=""run"" type=""FlowSpec or Run, optional"" desc=""Derive path prefix from the current or a past run ID, e.g. S3(run=self)."" />
 	<Parameter name=""s3root"" type=""str, optional"" desc=""If `run` is not specified, use this as the S3 prefix."" />
 </ParamSection>
 </DocSection>
 
 
 
 <DocSection type=""method"" name=""S3.close"" module=""metaflow"" show_import=""False"" heading_level=""4"" link=""https://github.com/Netflix/metaflow/tree/master/metaflow/plugins/datatools/s3/s3.py#L549"">
 <SigArgSection>
 <SigArg name=""self"" />
 </SigArgSection>
 <Description summary=""Delete all temporary files downloaded in this context."" />
 
 </DocSection>",H2,https://docs.metaflow.org/api/S3#the-s3-client,False,2957.0,362.0
1435,Downloading data,"<DocSection type=""method"" name=""S3.get"" module=""metaflow"" show_import=""False"" heading_level=""4"" link=""https://github.com/Netflix/metaflow/tree/master/metaflow/plugins/datatools/s3/s3.py#L822"">
 <SigArgSection>
 <SigArg name=""self"" /><SigArg name=""key"" type=""Union"" default=""None"" /><SigArg name=""return_missing"" type=""bool"" default=""False"" /><SigArg name=""return_info"" type=""bool"" default=""True"" />
 </SigArgSection>
 <Description summary=""Get a single object from S3."" />
 <ParamSection name=""Parameters"">
 	<Parameter name=""key"" type=""str or `S3GetObject`, optional"" desc=""Object to download. It can be an S3 url, a path suffix, or\nan `S3GetObject` that defines a range of data to download. If None, or\nnot provided, gets the S3 root."" />
 	<Parameter name=""return_missing"" type=""bool, default: False"" desc=""If set to True, do not raise an exception for a missing key but\nreturn it as an `S3Object` with `.exists == False`."" />
 	<Parameter name=""return_info"" type=""bool, default: True"" desc=""If set to True, fetch the content-type and user metadata associated\nwith the object at no extra cost, included for symmetry with `get_many`"" />
 </ParamSection>
 <ParamSection name=""Returns"">
 	<Parameter type=""`S3Object`"" desc=""An S3Object corresponding to the object requested."" />
 </ParamSection>
 </DocSection>
 
 
 
 <DocSection type=""method"" name=""S3.get_many"" module=""metaflow"" show_import=""False"" heading_level=""4"" link=""https://github.com/Netflix/metaflow/tree/master/metaflow/plugins/datatools/s3/s3.py#L915"">
 <SigArgSection>
 <SigArg name=""self"" /><SigArg name=""keys"" type=""Iterable"" /><SigArg name=""return_missing"" type=""bool"" default=""False"" /><SigArg name=""return_info"" type=""bool"" default=""True"" />
 </SigArgSection>
 <Description summary=""Get many objects from S3 in parallel."" />
 <ParamSection name=""Parameters"">
 	<Parameter name=""keys"" type=""Iterable[str or `S3GetObject`]"" desc=""Objects to download. Each object can be an S3 url, a path suffix, or\nan `S3GetObject` that defines a range of data to download."" />
 	<Parameter name=""return_missing"" type=""bool, default: False"" desc=""If set to True, do not raise an exception for a missing key but\nreturn it as an `S3Object` with `.exists == False`."" />
 	<Parameter name=""return_info"" type=""bool, default: True"" desc=""If set to True, fetch the content-type and user metadata associated\nwith the object at no extra cost, included for symmetry with `get_many`."" />
 </ParamSection>
 <ParamSection name=""Returns"">
 	<Parameter type=""List[`S3Object`]"" desc=""S3Objects corresponding to the objects requested."" />
 </ParamSection>
 </DocSection>
 
 
 
 <DocSection type=""method"" name=""S3.get_recursive"" module=""metaflow"" show_import=""False"" heading_level=""4"" link=""https://github.com/Netflix/metaflow/tree/master/metaflow/plugins/datatools/s3/s3.py#L988"">
 <SigArgSection>
 <SigArg name=""self"" /><SigArg name=""keys"" type=""Iterable"" /><SigArg name=""return_info"" type=""bool"" default=""False"" />
 </SigArgSection>
 <Description summary=""Get many objects from S3 recursively in parallel."" />
 <ParamSection name=""Parameters"">
 	<Parameter name=""keys"" type=""Iterable[str]"" desc=""Prefixes to download recursively. Each prefix can be an S3 url or a path suffix\nwhich define the root prefix under which all objects are downloaded."" />
 	<Parameter name=""return_info"" type=""bool, default: False"" desc=""If set to True, fetch the content-type and user metadata associated\nwith the object."" />
 </ParamSection>
 <ParamSection name=""Returns"">
 	<Parameter type=""List[`S3Object`]"" desc=""S3Objects stored under the given prefixes."" />
 </ParamSection>
 </DocSection>
 
 
 
 <DocSection type=""method"" name=""S3.get_all"" module=""metaflow"" show_import=""False"" heading_level=""4"" link=""https://github.com/Netflix/metaflow/tree/master/metaflow/plugins/datatools/s3/s3.py#L1042"">
 <SigArgSection>
 <SigArg name=""self"" /><SigArg name=""return_info"" type=""bool"" default=""False"" />
 </SigArgSection>
 <Description summary=""Get all objects under the prefix set in the `S3` constructor."" extended_summary=""This method requires that the `S3` object is initialized either with `run` or\n`s3root`."" />
 <ParamSection name=""Parameters"">
 	<Parameter name=""return_info"" type=""bool, default: False"" desc=""If set to True, fetch the content-type and user metadata associated\nwith the object."" />
 </ParamSection>
 <ParamSection name=""Returns"">
 	<Parameter type=""Iterable[`S3Object`]"" desc=""S3Objects stored under the main prefix."" />
 </ParamSection>
 </DocSection>",H2,https://docs.metaflow.org/api/S3#downloading-data,False,4497.0,454.0
1436,Listing objects,"<DocSection type=""method"" name=""S3.list_paths"" module=""metaflow"" show_import=""False"" heading_level=""4"" link=""https://github.com/Netflix/metaflow/tree/master/metaflow/plugins/datatools/s3/s3.py#L614"">
 <SigArgSection>
 <SigArg name=""self"" /><SigArg name=""keys"" type=""Optional"" default=""None"" />
 </SigArgSection>
 <Description summary=""List the next level of paths in S3."" extended_summary=""If multiple keys are specified, listings are done in parallel. The returned\nS3Objects have `.exists == False` if the path refers to a prefix, not an\nexisting S3 object.\n\nFor instance, if the directory hierarchy is\n```\na/0.txt\na/b/1.txt\na/c/2.txt\na/d/e/3.txt\nf/4.txt\n```\nThe `list_paths(['a', 'f'])` call returns\n```\na/0.txt (exists == True)\na/b/ (exists == False)\na/c/ (exists == False)\na/d/ (exists == False)\nf/4.txt (exists == True)\n```"" />
 <ParamSection name=""Parameters"">
 	<Parameter name=""keys"" type=""Iterable[str], optional"" desc=""List of paths."" />
 </ParamSection>
 <ParamSection name=""Returns"">
 	<Parameter type=""List[`S3Object`]"" desc=""S3Objects under the given paths, including prefixes (directories) that\ndo not correspond to leaf objects."" />
 </ParamSection>
 </DocSection>
 
 
 
 <DocSection type=""method"" name=""S3.list_recursive"" module=""metaflow"" show_import=""False"" heading_level=""4"" link=""https://github.com/Netflix/metaflow/tree/master/metaflow/plugins/datatools/s3/s3.py#L664"">
 <SigArgSection>
 <SigArg name=""self"" /><SigArg name=""keys"" type=""Optional"" default=""None"" />
 </SigArgSection>
 <Description summary=""List all objects recursively under the given prefixes."" extended_summary=""If multiple keys are specified, listings are done in parallel. All objects\nreturned have `.exists == True` as this call always returns leaf objects.\n\nFor instance, if the directory hierarchy is\n```\na/0.txt\na/b/1.txt\na/c/2.txt\na/d/e/3.txt\nf/4.txt\n```\nThe `list_paths(['a', 'f'])` call returns\n```\na/0.txt (exists == True)\na/b/1.txt (exists == True)\na/c/2.txt (exists == True)\na/d/e/3.txt (exists == True)\nf/4.txt (exists == True)\n```"" />
 <ParamSection name=""Parameters"">
 	<Parameter name=""keys"" type=""Iterable[str], optional"" desc=""List of paths."" />
 </ParamSection>
 <ParamSection name=""Returns"">
 	<Parameter type=""List[`S3Object`]"" desc=""S3Objects under the given paths."" />
 </ParamSection>
 </DocSection>",H2,https://docs.metaflow.org/api/S3#listing-objects,False,2350.0,210.0
1437,Uploading data,"<DocSection type=""method"" name=""S3.put"" module=""metaflow"" show_import=""False"" heading_level=""4"" link=""https://github.com/Netflix/metaflow/tree/master/metaflow/plugins/datatools/s3/s3.py#L1068"">
 <SigArgSection>
 <SigArg name=""self"" /><SigArg name=""key"" type=""Union"" /><SigArg name=""obj"" type=""Union"" /><SigArg name=""overwrite"" type=""bool"" default=""True"" /><SigArg name=""content_type"" type=""Optional"" default=""None"" /><SigArg name=""metadata"" type=""Optional"" default=""None"" />
 </SigArgSection>
 <Description summary=""Upload a single object to S3."" />
 <ParamSection name=""Parameters"">
 	<Parameter name=""key"" type=""str or `S3PutObject`"" desc=""Object path. It can be an S3 url or a path suffix."" />
 	<Parameter name=""obj"" type=""bytes or str"" desc=""An object to store in S3. Strings are converted to UTF-8 encoding."" />
 	<Parameter name=""overwrite"" type=""bool, default: True"" desc=""Overwrite the object if it exists. If set to False, the operation\nsucceeds without uploading anything if the key already exists."" />
 	<Parameter name=""content_type"" type=""str, optional"" desc=""Optional MIME type for the object."" />
 	<Parameter name=""metadata"" type=""Dict, optional"" desc=""A JSON-encodable dictionary of additional headers to be stored\nas metadata with the object."" />
 </ParamSection>
 <ParamSection name=""Returns"">
 	<Parameter type=""str"" desc=""URL of the object stored."" />
 </ParamSection>
 </DocSection>
 
 
 
 <DocSection type=""method"" name=""S3.put_many"" module=""metaflow"" show_import=""False"" heading_level=""4"" link=""https://github.com/Netflix/metaflow/tree/master/metaflow/plugins/datatools/s3/s3.py#L1156"">
 <SigArgSection>
 <SigArg name=""self"" /><SigArg name=""key_objs"" type=""List"" /><SigArg name=""overwrite"" type=""bool"" default=""True"" />
 </SigArgSection>
 <Description summary=""Upload many objects to S3."" extended_summary=""Each object to be uploaded can be specified in two ways:\n\n1. As a `(key, obj)` tuple where `key` is a string specifying\n   the path and `obj` is a string or a bytes object.\n\n2. As a `S3PutObject` which contains additional metadata to be\n   stored with the object."" />
 <ParamSection name=""Parameters"">
 	<Parameter name=""key_objs"" type=""List[(str, str) or `S3PutObject`]"" desc=""List of key-object pairs to upload."" />
 	<Parameter name=""overwrite"" type=""bool, default : True"" desc=""Overwrite the object if it exists. If set to False, the operation\nsucceeds without uploading anything if the key already exists."" />
 </ParamSection>
 <ParamSection name=""Returns"">
 	<Parameter type=""List[(str, str)]"" desc=""List of `(key, url)` pairs corresponding to the objects uploaded."" />
 </ParamSection>
 </DocSection>
 
 
 
 <DocSection type=""method"" name=""S3.put_files"" module=""metaflow"" show_import=""False"" heading_level=""4"" link=""https://github.com/Netflix/metaflow/tree/master/metaflow/plugins/datatools/s3/s3.py#L1228"">
 <SigArgSection>
 <SigArg name=""self"" /><SigArg name=""key_paths"" type=""List"" /><SigArg name=""overwrite"" type=""bool"" default=""True"" />
 </SigArgSection>
 <Description summary=""Upload many local files to S3."" extended_summary=""Each file to be uploaded can be specified in two ways:\n\n1. As a `(key, path)` tuple where `key` is a string specifying\n   the S3 path and `path` is the path to a local file.\n\n2. As a `S3PutObject` which contains additional metadata to be\n   stored with the file."" />
 <ParamSection name=""Parameters"">
 	<Parameter name=""key_paths"" type=""List[(str, str) or `S3PutObject`]"" desc=""List of files to upload."" />
 	<Parameter name=""overwrite"" type=""bool, default: True"" desc=""Overwrite the object if it exists. If set to False, the operation\nsucceeds without uploading anything if the key already exists."" />
 </ParamSection>
 <ParamSection name=""Returns"">
 	<Parameter type=""List[(str, str)]"" desc=""List of `(key, url)` pairs corresponding to the files uploaded."" />
 </ParamSection>
 </DocSection>",H2,https://docs.metaflow.org/api/S3#uploading-data,False,3881.0,424.0
1438,Querying metadata,"<DocSection type=""method"" name=""S3.info"" module=""metaflow"" show_import=""False"" heading_level=""4"" link=""https://github.com/Netflix/metaflow/tree/master/metaflow/plugins/datatools/s3/s3.py#L710"">
 <SigArgSection>
 <SigArg name=""self"" /><SigArg name=""key"" type=""Optional"" default=""None"" /><SigArg name=""return_missing"" type=""bool"" default=""False"" />
 </SigArgSection>
 <Description summary=""Get metadata about a single object in S3."" extended_summary=""This call makes a single `HEAD` request to S3 which can be\nmuch faster than downloading all data with `get`."" />
 <ParamSection name=""Parameters"">
 	<Parameter name=""key"" type=""str, optional"" desc=""Object to query. It can be an S3 url or a path suffix."" />
 	<Parameter name=""return_missing"" type=""bool, default: False"" desc=""If set to True, do not raise an exception for a missing key but\nreturn it as an `S3Object` with `.exists == False`."" />
 </ParamSection>
 <ParamSection name=""Returns"">
 	<Parameter type=""`S3Object`"" desc=""An S3Object corresponding to the object requested. The object\nwill have `.downloaded == False`."" />
 </ParamSection>
 </DocSection>
 
 
 
 <DocSection type=""method"" name=""S3.info_many"" module=""metaflow"" show_import=""False"" heading_level=""4"" link=""https://github.com/Netflix/metaflow/tree/master/metaflow/plugins/datatools/s3/s3.py#L764"">
 <SigArgSection>
 <SigArg name=""self"" /><SigArg name=""keys"" type=""Iterable"" /><SigArg name=""return_missing"" type=""bool"" default=""False"" />
 </SigArgSection>
 <Description summary=""Get metadata about many objects in S3 in parallel."" extended_summary=""This call makes a single `HEAD` request to S3 which can be\nmuch faster than downloading all data with `get`."" />
 <ParamSection name=""Parameters"">
 	<Parameter name=""keys"" type=""Iterable[str]"" desc=""Objects to query. Each key can be an S3 url or a path suffix."" />
 	<Parameter name=""return_missing"" type=""bool, default: False"" desc=""If set to True, do not raise an exception for a missing key but\nreturn it as an `S3Object` with `.exists == False`."" />
 </ParamSection>
 <ParamSection name=""Returns"">
 	<Parameter type=""List[`S3Object`]"" desc=""A list of `S3Object`s corresponding to the paths requested. The\nobjects will have `.downloaded == False`."" />
 </ParamSection>
 </DocSection>",H2,https://docs.metaflow.org/api/S3#querying-metadata,False,2260.0,241.0
1439,Handling results with `S3Object`,"Most operations above return `S3Object`s that encapsulate information about S3 paths and objects.
 
 Note that the data itself is not kept in these objects but it is stored in a temporary directory which is accessible through the properties of this object.
 
 
 <DocSection type=""class"" name=""S3Object"" module=""metaflow"" show_import=""False"" heading_level=""3"" link=""https://github.com/Netflix/metaflow/tree/master/metaflow/plugins/datatools/s3/s3.py#L126"">
 <SigArgSection>
 <SigArg name="""" />
 </SigArgSection>
 <Description summary=""This object represents a path or an object in S3,\nwith an optional local copy."" extended_summary=""`S3Object`s are not instantiated directly, but they are returned\nby many methods of the `S3` client."" />
 
 </DocSection>
 
 
 
 <DocSection type=""property"" name=""S3Object.exists"" module=""metaflow.plugins.datatools.s3.s3"" show_import=""False"" heading_level=""4"" link=""https://github.com/Netflix/metaflow/tree/master/"">
 
 <Description summary=""Does this key correspond to an object in S3?\n"" />
 <ParamSection name=""Returns"">
 <Parameter type=""bool"" desc=""True if this object points at an existing object (file) in S3."" />
 </ParamSection>
 </DocSection>
 
 
 
 <DocSection type=""property"" name=""S3Object.downloaded"" module=""metaflow.plugins.datatools.s3.s3"" show_import=""False"" heading_level=""4"" link=""https://github.com/Netflix/metaflow/tree/master/"">
 
 <Description summary=""Has this object been downloaded?\n\nIf True, the contents can be accessed through `path`, `blob`,\nand `text` properties.\n"" />
 <ParamSection name=""Returns"">
 <Parameter type=""bool"" desc=""True if the contents of this object have been downloaded."" />
 </ParamSection>
 </DocSection>
 
 
 
 <DocSection type=""property"" name=""S3Object.url"" module=""metaflow.plugins.datatools.s3.s3"" show_import=""False"" heading_level=""4"" link=""https://github.com/Netflix/metaflow/tree/master/"">
 
 <Description summary=""S3 location of the object\n"" />
 <ParamSection name=""Returns"">
 <Parameter type=""str"" desc=""The S3 location of this object."" />
 </ParamSection>
 </DocSection>
 
 
 
 <DocSection type=""property"" name=""S3Object.prefix"" module=""metaflow.plugins.datatools.s3.s3"" show_import=""False"" heading_level=""4"" link=""https://github.com/Netflix/metaflow/tree/master/"">
 
 <Description summary=""Prefix requested that matches this object.\n"" />
 <ParamSection name=""Returns"">
 <Parameter type=""str"" desc=""Requested prefix"" />
 </ParamSection>
 </DocSection>
 
 
 
 <DocSection type=""property"" name=""S3Object.key"" module=""metaflow.plugins.datatools.s3.s3"" show_import=""False"" heading_level=""4"" link=""https://github.com/Netflix/metaflow/tree/master/"">
 
 <Description summary=""Key corresponds to the key given to the get call that produced\nthis object.\n\nThis may be a full S3 URL or a suffix based on what\nwas requested.\n"" />
 <ParamSection name=""Returns"">
 <Parameter type=""str"" desc=""Key requested."" />
 </ParamSection>
 </DocSection>
 
 
 
 <DocSection type=""property"" name=""S3Object.path"" module=""metaflow.plugins.datatools.s3.s3"" show_import=""False"" heading_level=""4"" link=""https://github.com/Netflix/metaflow/tree/master/"">
 
 <Description summary=""Path to a local temporary file corresponding to the object downloaded.\n\nThis file gets deleted automatically when a S3 scope exits.\nReturns None if this S3Object has not been downloaded.\n"" />
 <ParamSection name=""Returns"">
 <Parameter type=""str"" desc=""Local path, if the object has been downloaded."" />
 </ParamSection>
 </DocSection>
 
 
 
 <DocSection type=""property"" name=""S3Object.blob"" module=""metaflow.plugins.datatools.s3.s3"" show_import=""False"" heading_level=""4"" link=""https://github.com/Netflix/metaflow/tree/master/"">
 
 <Description summary=""Contents of the object as a byte string or None if the\nobject hasn't been downloaded.\n"" />
 <ParamSection name=""Returns"">
 <Parameter type=""bytes"" desc=""Contents of the object as bytes."" />
 </ParamSection>
 </DocSection>
 
 
 
 <DocSection type=""property"" name=""S3Object.text"" module=""metaflow.plugins.datatools.s3.s3"" show_import=""False"" heading_level=""4"" link=""https://github.com/Netflix/metaflow/tree/master/"">
 
 <Description summary=""Contents of the object as a string or None if the\nobject hasn't been downloaded.\n\nThe object is assumed to contain UTF-8 encoded data.\n"" />
 <ParamSection name=""Returns"">
 <Parameter type=""str"" desc=""Contents of the object as text."" />
 </ParamSection>
 </DocSection>
 
 
 
 <DocSection type=""property"" name=""S3Object.size"" module=""metaflow.plugins.datatools.s3.s3"" show_import=""False"" heading_level=""4"" link=""https://github.com/Netflix/metaflow/tree/master/"">
 
 <Description summary=""Size of the object in bytes.\n\nReturns None if the key does not correspond to an object in S3.\n"" />
 <ParamSection name=""Returns"">
 <Parameter type=""int"" desc=""Size of the object in bytes, if the object exists."" />
 </ParamSection>
 </DocSection>
 
 
 
 <DocSection type=""property"" name=""S3Object.has_info"" module=""metaflow.plugins.datatools.s3.s3"" show_import=""False"" heading_level=""4"" link=""https://github.com/Netflix/metaflow/tree/master/"">
 
 <Description summary=""Returns true if this `S3Object` contains the content-type MIME header or\nuser-defined metadata.\n\nIf False, this means that `content_type`, `metadata`, `range_info` and\n`last_modified` will return None.\n"" />
 <ParamSection name=""Returns"">
 <Parameter type=""bool"" desc=""True if additional metadata is available."" />
 </ParamSection>
 </DocSection>
 
 
 
 <DocSection type=""property"" name=""S3Object.metadata"" module=""metaflow.plugins.datatools.s3.s3"" show_import=""False"" heading_level=""4"" link=""https://github.com/Netflix/metaflow/tree/master/"">
 
 <Description summary=""Returns a dictionary of user-defined metadata, or None if no metadata\nis defined.\n"" />
 <ParamSection name=""Returns"">
 <Parameter type=""Dict"" desc=""User-defined metadata."" />
 </ParamSection>
 </DocSection>
 
 
 
 <DocSection type=""property"" name=""S3Object.content_type"" module=""metaflow.plugins.datatools.s3.s3"" show_import=""False"" heading_level=""4"" link=""https://github.com/Netflix/metaflow/tree/master/"">
 
 <Description summary=""Returns the content-type of the S3 object or None if it is not defined.\n"" />
 <ParamSection name=""Returns"">
 <Parameter type=""str"" desc=""Content type or None if the content type is undefined."" />
 </ParamSection>
 </DocSection>
 
 
 
 <DocSection type=""property"" name=""S3Object.range_info"" module=""metaflow.plugins.datatools.s3.s3"" show_import=""False"" heading_level=""4"" link=""https://github.com/Netflix/metaflow/tree/master/"">
 
 <Description summary=""If the object corresponds to a partially downloaded object, returns\ninformation of what was downloaded.\n\nThe returned object has the following fields:\n- `total_size`: Size of the object in S3.\n- `request_offset`: The starting offset.\n- `request_length`: The number of bytes downloaded.\n"" />
 </DocSection>
 
 
 
 <DocSection type=""property"" name=""S3Object.last_modified"" module=""metaflow.plugins.datatools.s3.s3"" show_import=""False"" heading_level=""4"" link=""https://github.com/Netflix/metaflow/tree/master/"">
 
 <Description summary=""Returns the last modified unix timestamp of the object.\n"" />
 <ParamSection name=""Returns"">
 <Parameter type=""int"" desc=""Unix timestamp corresponding to the last modified time."" />
 </ParamSection>
 </DocSection>",H2,https://docs.metaflow.org/api/S3#handling-results-with-s3object,False,7327.0,691.0
1440,Helper Objects,"These objects are simple containers that are used to pass information to `get_many`, `put_many`, and `put_files`. You may use your own objects instead of them, as long as they provide the same set of attributes.
 
 
 <DocSection type=""class"" name=""S3GetObject"" module=""metaflow.datatools.s3"" show_import=""True"" heading_level=""3"" link=""https://github.com/Netflix/metaflow/tree/master/"">
 <SigArgSection>
 <SigArg name=""key"" default=""None"" /><SigArg name=""offset"" default=""None"" /><SigArg name=""length"" default=""None"" />
 </SigArgSection>
 <Description summary=""Represents a chunk of an S3 object. A range query is performed to download only a subset of data,\n`object[key][offset:offset + length]`, from S3."" />
 <ParamSection name=""Attributes"">
 	<Parameter name=""key"" type=""str"" desc=""Key identifying the object. Works the same way as any `key` passed to `get` or `get_many`."" />
 	<Parameter name=""offset"" type=""int"" desc=""A byte offset in the file."" />
 	<Parameter name=""length"" type=""int"" desc=""The number of bytes to download."" />
 </ParamSection>
 </DocSection>
 
 
 
 <DocSection type=""class"" name=""S3PutObject"" module=""metaflow.datatools.s3"" show_import=""True"" heading_level=""3"" link=""https://github.com/Netflix/metaflow/tree/master/"">
 <SigArgSection>
 <SigArg name=""key"" default=""None"" /><SigArg name=""value"" default=""None"" /><SigArg name=""path"" default=""None"" /><SigArg name=""content_type"" default=""None"" /><SigArg name=""metadata"" default=""None"" />
 </SigArgSection>
 <Description summary=""Defines an object with metadata to be uplaoded with `put_many` or `put_files`."" />
 <ParamSection name=""Attributes"">
 	<Parameter name=""key"" type=""str"" desc=""Key identifying the object. Works the same way as `key` passed to `put` or `put_many`."" />
 	<Parameter name=""value"" type=""str or bytes"" desc=""Object to upload. Works the same way as `obj` passed `to `put` or `put_many`."" />
 	<Parameter name=""path"" type=""str"" desc=""Path to a local file. Works the same way as `path` passed to `put_files`."" />
 	<Parameter name=""content_type"" type=""str"" desc=""Optional MIME type for the file."" />
 	<Parameter name=""metadata"" type=""Dict"" desc=""A JSON-encodable dictionary of additional headers to be stored\nas metadata with the file."" />
 </ParamSection>
 </DocSection>",H2,https://docs.metaflow.org/api/S3#helper-objects,False,2265.0,255.0
1441,ArgoEvent - Create events to trigger flows,"This class is mainly used **outside** Metaflow flows to create events that trigger Metaflow flows. For instance,
 you can use this class inside a non-Metaflow ETL pipeline or a microservice to trigger a Metaflow execution.
 
 Note that this class assumes that you have an appropriate Metaflow configuration available. Make sure to copy the config file in any environment where this class is being used.
 
 Read more in [Triggering Flows Based on External Events](/production/event-triggering/external-events).
 
 
 
 <DocSection type=""class"" name=""ArgoEvent"" module=""metaflow.integrations"" show_import=""True"" heading_level=""3"" link=""https://github.com/Netflix/metaflow/tree/master/metaflow/plugins/argo/argo_events.py#L17"">
 <SigArgSection>
 <SigArg name=""name, url=None, payload={}"" />
 </SigArgSection>
 <Description summary=""ArgoEvent is a small event, a message, that can be published to Argo Workflows. The\nevent will eventually start all flows which have been previously deployed with `@trigger`\nto wait for this particular named event."" />
 <ParamSection name=""Parameters"">
 	<Parameter name=""name"" type=""str,"" desc=""Name of the event"" />
 	<Parameter name=""url"" type=""str, optional"" desc=""Override the event endpoint from `ARGO_EVENTS_WEBHOOK_URL`."" />
 	<Parameter name=""payload"" type=""Dict, optional"" desc=""A set of key-value pairs delivered in this event. Used to set parameters of triggered flows."" />
 </ParamSection>
 </DocSection>
 
 
 
 <DocSection type=""method"" name=""ArgoEvent.add_to_payload"" module=""metaflow.integrations"" show_import=""False"" heading_level=""4"" link=""https://github.com/Netflix/metaflow/tree/master/metaflow/plugins/argo/argo_events.py#L42"">
 <SigArgSection>
 <SigArg name=""self"" /><SigArg name=""key"" /><SigArg name=""value"" />
 </SigArgSection>
 <Description summary=""Add a key-value pair in the payload. This is typically used to set parameters\nof triggered flows. Often, `key` is the parameter name you want to set to\n`value`. Overrides any existing value of `key`."" />
 <ParamSection name=""Parameters"">
 	<Parameter name=""key"" type=""str"" desc=""Key"" />
 	<Parameter name=""value"" type=""str"" desc=""Value"" />
 </ParamSection>
 </DocSection>
 
 
 
 <DocSection type=""method"" name=""ArgoEvent.publish"" module=""metaflow.integrations"" show_import=""False"" heading_level=""4"" link=""https://github.com/Netflix/metaflow/tree/master/metaflow/plugins/argo/argo_events.py#L78"">
 <SigArgSection>
 <SigArg name=""self"" /><SigArg name=""payload"" default=""None"" /><SigArg name=""force"" default=""True"" /><SigArg name=""ignore_errors"" default=""True"" />
 </SigArgSection>
 <Description summary=""Publishes an event."" extended_summary=""Note that the function returns immediately after the event has been sent. It\ndoes not wait for flows to start, nor it guarantees that any flows will start."" />
 <ParamSection name=""Parameters"">
 	<Parameter name=""payload"" type=""dict"" desc=""Additional key-value pairs to add to the payload."" />
 	<Parameter name=""ignore_errors"" type=""bool, default: True"" desc=""If True, events are created on a best effort basis - errors are silently ignored."" />
 </ParamSection>
 </DocSection>
 
 
 
 <DocSection type=""method"" name=""ArgoEvent.safe_publish"" module=""metaflow.integrations"" show_import=""False"" heading_level=""4"" link=""https://github.com/Netflix/metaflow/tree/master/metaflow/plugins/argo/argo_events.py#L59"">
 <SigArgSection>
 <SigArg name=""self"" /><SigArg name=""payload"" default=""None"" /><SigArg name=""ignore_errors"" default=""True"" />
 </SigArgSection>
 <Description summary=""Publishes an event when called inside a deployed workflow. Outside a deployed workflow\nthis function does nothing."" extended_summary=""Use this function inside flows to create events safely. As this function is a no-op\nfor local runs, you can safely call it during local development without causing unintended\nside-effects. It takes effect only when deployed on Argo Workflows."" />
 <ParamSection name=""Parameters"">
 	<Parameter name=""payload"" type=""dict"" desc=""Additional key-value pairs to add to the payload."" />
 	<Parameter name=""ignore_errors"" type=""bool, default: True"" desc=""If True, events are created on a best effort basis - errors are silently ignored."" />
 </ParamSection>
 </DocSection>",H1,https://docs.metaflow.org/api/argoevent#argoevent-create-events-to-trigger-flows,False,4227.0,445.0
1442,Current - Operating a run,"The `current` object is used to inspect and manipulate the currently executing run. It is only available during flow execution, i.e. inside a `FlowSpec` class and functions called from its steps. You can access the object simply by importing it: `from metaflow import current`.
 
 The attributes available in `current` depend on the decorators assigned to the flow and the step where `current` is used. Attributes that are always available are listed under *Common Attributes* below. Decorator-specific attributes are listed under the decorator name.",H1,https://docs.metaflow.org/api/current#current-operating-a-run,False,550.0,84.0
1443,Common Attributes,"These attributes are always available in the `current` object.
 
 
 <DocSection type=""property"" name=""current.is_running_flow"" module=""__main__"" show_import=""False"" heading_level=""4"">
 
 <Description summary=""Returns True if called inside a running Flow, False otherwise.\n\nYou can use this property e.g. inside a library to choose the desired\nbehavior depending on the execution context.\n"" />
 <ParamSection name=""Returns"">
 <Parameter type=""bool"" desc=""True if called inside a run, False otherwise."" />
 </ParamSection>
 </DocSection>
 
 
 
 <DocSection type=""property"" name=""current.flow_name"" module=""__main__"" show_import=""False"" heading_level=""4"">
 
 <Description summary=""The name of the currently executing flow.\n"" />
 <ParamSection name=""Returns"">
 <Parameter type=""str, optional"" desc=""Flow name."" />
 </ParamSection>
 </DocSection>
 
 
 
 <DocSection type=""property"" name=""current.run_id"" module=""__main__"" show_import=""False"" heading_level=""4"">
 
 <Description summary=""The run ID of the currently executing run.\n"" />
 <ParamSection name=""Returns"">
 <Parameter type=""str, optional"" desc=""Run ID."" />
 </ParamSection>
 </DocSection>
 
 
 
 <DocSection type=""property"" name=""current.step_name"" module=""__main__"" show_import=""False"" heading_level=""4"">
 
 <Description summary=""The name of the currently executing step.\n"" />
 <ParamSection name=""Returns"">
 <Parameter type=""str, optional"" desc=""Step name."" />
 </ParamSection>
 </DocSection>
 
 
 
 <DocSection type=""property"" name=""current.task_id"" module=""__main__"" show_import=""False"" heading_level=""4"">
 
 <Description summary=""The task ID of the currently executing task.\n"" />
 <ParamSection name=""Returns"">
 <Parameter type=""str, optional"" desc=""Task ID."" />
 </ParamSection>
 </DocSection>
 
 
 
 <DocSection type=""property"" name=""current.retry_count"" module=""__main__"" show_import=""False"" heading_level=""4"">
 
 <Description summary=""The index of the task execution attempt.\n\nThis property returns 0 for the first attempt to execute the task.\nIf the @retry decorator is used and the first attempt fails, this\nproperty returns the number of times the task was attempted prior\nto the current attempt.\n"" />
 <ParamSection name=""Returns"">
 <Parameter type=""int"" desc=""The retry count."" />
 </ParamSection>
 </DocSection>
 
 
 
 <DocSection type=""property"" name=""current.origin_run_id"" module=""__main__"" show_import=""False"" heading_level=""4"">
 
 <Description summary=""The run ID of the original run this run was resumed from.\n\nThis property returns None for ordinary runs. If the run\nwas started by the resume command, the property returns\nthe ID of the original run.\n\nYou can use this property to detect if the run is resumed\nor not.\n"" />
 <ParamSection name=""Returns"">
 <Parameter type=""str, optional"" desc=""Run ID of the original run."" />
 </ParamSection>
 </DocSection>
 
 
 
 <DocSection type=""property"" name=""current.pathspec"" module=""__main__"" show_import=""False"" heading_level=""4"">
 
 <Description summary=""Pathspec of the current task, i.e. a unique\nidentifier of the current task. The returned\nstring follows this format:\n```\n{flow_name}/{run_id}/{step_name}/{task_id}\n```\n\nThis is a shorthand to `current.task.pathspec`.\n"" />
 <ParamSection name=""Returns"">
 <Parameter type=""str, optional"" desc=""Pathspec."" />
 </ParamSection>
 </DocSection>
 
 
 
 <DocSection type=""property"" name=""current.namespace"" module=""__main__"" show_import=""False"" heading_level=""4"">
 
 <Description summary=""The current namespace.\n"" />
 <ParamSection name=""Returns"">
 <Parameter type=""str"" desc=""Namespace."" />
 </ParamSection>
 </DocSection>
 
 
 
 <DocSection type=""property"" name=""current.username"" module=""__main__"" show_import=""False"" heading_level=""4"">
 
 <Description summary=""The name of the user who started the run, if available.\n"" />
 <ParamSection name=""Returns"">
 <Parameter type=""str, optional"" desc=""User name."" />
 </ParamSection>
 </DocSection>
 
 
 
 <DocSection type=""property"" name=""current.tempdir"" module=""__main__"" show_import=""False"" heading_level=""4"">
 
 <Description summary=""Currently configured temporary directory.\n"" />
 <ParamSection name=""Returns"">
 <Parameter type=""str, optional"" desc=""Temporary director."" />
 </ParamSection>
 </DocSection>",H2,https://docs.metaflow.org/api/current#common-attributes,False,4253.0,441.0
1444,Decorator-specific attributes,These attributes are only available when the decorator is present.,H2,https://docs.metaflow.org/api/current#decorator-specific-attributes,False,66.0,10.0
1445,@project,"[The @project decorator](/production/coordinating-larger-metaflow-projects) exposes attributes related to the current deployment.
 
 
 <DocSection type=""property"" name=""current.project_name"" module=""__main__"" show_import=""False"" heading_level=""4"">
 
 <Description summary=""The name of the project assigned to this flow,\ni.e. `X` in `@project(name=X)`.\n"" />
 <ParamSection name=""Returns"">
 <Parameter type=""str"" desc=""Project name."" />
 </ParamSection>
 </DocSection>
 
 
 
 <DocSection type=""property"" name=""current.project_flow_name"" module=""__main__"" show_import=""False"" heading_level=""4"">
 
 <Description summary=""The flow name prefixed with the current project\nand branch. This name identifies the deployment\non a production scheduler.\n"" />
 <ParamSection name=""Returns"">
 <Parameter type=""str"" desc=""Flow name prefixed with project information."" />
 </ParamSection>
 </DocSection>
 
 
 
 <DocSection type=""property"" name=""current.branch_name"" module=""__main__"" show_import=""False"" heading_level=""4"">
 
 <Description summary=""The current branch, i.e. `X` in\n`--branch=X` set during deployment.\n"" />
 <ParamSection name=""Returns"">
 <Parameter type=""str"" desc=""Branch name."" />
 </ParamSection>
 </DocSection>
 
 
 
 <DocSection type=""property"" name=""current.is_user_branch"" module=""__main__"" show_import=""False"" heading_level=""4"">
 
 <Description summary=""True if the flow is deployed without a\nspecific `--branch` or a `--production` flag.\n"" />
 <ParamSection name=""Returns"">
 <Parameter type=""bool"" desc=""True if the deployment does not correspond to a specific branch."" />
 </ParamSection>
 </DocSection>
 
 
 
 <DocSection type=""property"" name=""current.is_production"" module=""__main__"" show_import=""False"" heading_level=""4"">
 
 <Description summary=""True if the flow is deployed with the `--production`\nflag.\n"" />
 <ParamSection name=""Returns"">
 <Parameter type=""bool"" desc=""True if the flow is deployed in `--production`."" />
 </ParamSection>
 </DocSection>",H3,https://docs.metaflow.org/api/current#project,False,1976.0,193.0
1446,@card,"[The @card decorator](/metaflow/visualizing-results) exposes functions in `current` that allow you to customize
 the contents of cards using [card components](/api/cards#Card-components). For an overview of card-related APIs, see [the API reference for cards](/api/cards).
 
 
 <DocSection type=""method"" name=""current.card.__getitem__"" module=""metaflow"" show_import=""False"" heading_level=""4"" link=""https://github.com/Netflix/metaflow/tree/master/__main__.py#L28"">
 <SigArgSection>
 <SigArg name=""self"" />
 </SigArgSection>
 <Description summary=""Choose a specific card for manipulation."" extended_summary=""When multiple @card decorators are present, you can add an\n`ID` to distinguish between them, `@card(id=ID)`. This allows you\nto add components to a specific card like this:\n```\ncurrent.card[ID].append(component)\n```"" />
 <ParamSection name=""Parameters"">
 	<Parameter name=""key"" type=""str"" desc=""Card ID."" />
 </ParamSection>
 <ParamSection name=""Returns"">
 	<Parameter type=""CardComponentCollector"" desc=""An object with `append` and `extend` calls which allow you to\nadd components to the chosen card."" />
 </ParamSection>
 </DocSection>
 
 
 
 <DocSection type=""method"" name=""current.card.__setitem__"" module=""metaflow"" show_import=""False"" heading_level=""4"" link=""https://github.com/Netflix/metaflow/tree/master/__main__.py#L30"">
 <SigArgSection>
 <SigArg name=""self"" />
 </SigArgSection>
 <Description summary=""Specify components of the chosen card."" extended_summary=""Instead of adding components to a card individually with `current.card[ID].append(component)`,\nuse this method to assign a list of components to a card, replacing the existing components:\n```\ncurrent.card[ID] = [FirstComponent, SecondComponent]\n```"" />
 <ParamSection name=""Parameters"">
 	<Parameter name=""key: str"" desc=""Card ID."" />
 	<Parameter name=""value: List[CardComponent]"" desc=""List of card components to assign to this card."" />
 </ParamSection>
 </DocSection>
 
 
 
 <DocSection type=""method"" name=""current.card.append"" module=""metaflow"" show_import=""False"" heading_level=""4"" link=""https://github.com/Netflix/metaflow/tree/master/__main__.py#L32"">
 <SigArgSection>
 <SigArg name=""self"" />
 </SigArgSection>
 <Description summary=""Appends a component to the current card."" />
 <ParamSection name=""Parameters"">
 	<Parameter name=""component"" type=""CardComponent"" desc=""Card component to add to this card."" />
 </ParamSection>
 </DocSection>
 
 
 
 <DocSection type=""method"" name=""current.card.extend"" module=""metaflow"" show_import=""False"" heading_level=""4"" link=""https://github.com/Netflix/metaflow/tree/master/__main__.py#L34"">
 <SigArgSection>
 <SigArg name=""self"" />
 </SigArgSection>
 <Description summary=""Appends many components to the current card."" />
 <ParamSection name=""Parameters"">
 	<Parameter name=""component"" type=""Iterator[CardComponent]"" desc=""Card components to add to this card."" />
 </ParamSection>
 </DocSection>",H3,https://docs.metaflow.org/api/current#card,False,2929.0,263.0
1447,@trigger and @trigger_on_finish,"You can inspect event(s) that triggered [an event-triggered](/production/event-triggering) run through `current.trigger` which returns a [`MetaflowTrigger`](/api/client#metaflowtrigger) object, if the run was
 triggered by an event.
 
 
 <DocSection type=""property"" name=""current.trigger"" module=""__main__"" show_import=""False"" heading_level=""4"">
 
 <Description summary=""Returns `MetaflowTrigger` if the current run is triggered by an event.\n"" />
 <ParamSection name=""Returns"">
 <Parameter type=""MetaflowTrigger"" desc=""`MetaflowTrigger` if triggered by a run."" />
 </ParamSection>
 </DocSection>",H3,https://docs.metaflow.org/api/current#trigger-and-trigger-on-finish,False,596.0,59.0
1448,Cards - Visualizing results,"Metaflow Cards allows you to produce [human readable reports in workflows](/metaflow/visualizing-results). Use the following APIs to enable, customize, and access cards:
 
  - **Enable cards** by adding [the `@card` decorator](/api/step-decorators/card) in any step.
  - **Specify card contents** with [card components](#card-components).
  - **Populate card components** with [the `current` object](/api/current#card).
  - **Retrive cards** with [the `get_cards` method](#retrieving-cards) or on the command line with the `card` commands.
  - **Create fully custom, shareable cards** with custom [`MetaflowCard` classes](#defining-a-custom-card).",H1,https://docs.metaflow.org/api/cards#cards-visualizing-results,False,647.0,79.0
1449,Retrieving cards,"To retrieve a card after a run has finished, use the `get_cards` function e.g. in a notebook or the `card get` command on the CLI.
 
 Since a task can contain multiple cards `get_cards` returns a container object, `CardContainer`, which holds `Card` objects corresponding to individual cards. Notably both `CardContainer` and `Card` objects contain a function that allow them to visualize cards in the notebook output cell automatically, so a single `get_cards` call can be used to show all cards of a step in a notebook.
 
 
 <DocSection type=""function"" name=""get_cards"" module=""metaflow.cards"" show_import=""True"" heading_level=""3"" link=""https://github.com/Netflix/metaflow/tree/master/metaflow/plugins/cards/card_client.py#L207"">
 <SigArgSection>
 <SigArg name=""task"" type=""Union"" /><SigArg name=""id"" type=""Optional"" default=""None"" /><SigArg name=""type"" type=""Optional"" default=""None"" /><SigArg name=""follow_resumed"" type=""bool"" default=""True"" />
 </SigArgSection>
 <Description summary=""Get cards related to a `Task`."" extended_summary=""Note that `get_cards` resolves the cards contained by the task, but it doesn't actually\nretrieve them from the datastore. Actual card contents are retrieved lazily either when\nthe card is rendered in a notebook to when you call `Card.get`. This means that\n`get_cards` is a fast call even when individual cards contain a lot of data."" />
 <ParamSection name=""Parameters"">
 	<Parameter name=""task"" type=""str or `Task`"" desc=""A `Task` object or pathspec `{flow_name}/{run_id}/{step_name}/{task_id}` that\nuniquely identifies a task."" />
 	<Parameter name=""id"" type=""str, optional"" desc=""The ID of card to retrieve if multiple cards are present."" />
 	<Parameter name=""type"" type=""str, optional"" desc=""The type of card to retrieve if multiple cards are present."" />
 	<Parameter name=""follow_resumed"" type=""bool, default: True"" desc=""If the task has been resumed, then setting this flag will resolve the card for\nthe origin task."" />
 </ParamSection>
 <ParamSection name=""Returns"">
 	<Parameter type=""CardContainer"" desc=""A list-like object that holds `Card` objects."" />
 </ParamSection>
 </DocSection>
 
 
 
 <DocSection type=""class"" name=""CardContainer"" module=""metaflow.cards"" show_import=""False"" heading_level=""3"" link=""https://github.com/Netflix/metaflow/tree/master/metaflow/plugins/cards/card_client.py#L128"">
 <SigArgSection>
 <SigArg name="""" />
 </SigArgSection>
 <Description summary=""`CardContainer` is an immutable list-like object, returned by `get_cards`,\nwhich contains individual `Card`s."" extended_summary=""Notably, `CardContainer` contains a special\n`_repr_html_` function which renders cards automatically in an output\ncell of a notebook.\n\nThe following operations are supported:\n```\ncards = get_cards(MyTask)\n\n# retrieve by index\nfirst_card = cards[0]\n\n# check length\nif len(cards) > 1:\n    print('many cards present!')\n\n# iteration\nlist_of_cards = list(cards)\n```"" />
 
 </DocSection>
 
 
 
 <DocSection type=""class"" name=""Card"" module=""metaflow.cards"" show_import=""False"" heading_level=""3"" link=""https://github.com/Netflix/metaflow/tree/master/metaflow/plugins/cards/card_client.py#L22"">
 <SigArgSection>
 <SigArg name="""" />
 </SigArgSection>
 <Description summary=""`Card` represents an individual Metaflow Card, a single HTML file, produced by\nthe card `@card` decorator. `Card`s are contained by `CardContainer`, returned by\n`get_cards`."" extended_summary=""Note that the contents of the card, an HTML file, is retrieved lazily when you call\n`Card.get` for the first time or when the card is rendered in a notebook."" />
 <ParamSection name=""Attributes"">
 	<Parameter name=""id"" desc=""The ID of the card, if specified with `@card(id=ID)`."" />
 	<Parameter name=""path"" desc=""The path of the card in the datastore which uniquely\nidentifies the card.\n\nReturns\n-------\nstr\n    Path to the card"" />
 </ParamSection>
 </DocSection>
 
 
 
 <DocSection type=""method"" name=""Card.get"" module=""metaflow.cards"" show_import=""False"" heading_level=""4"" link=""https://github.com/Netflix/metaflow/tree/master/metaflow/plugins/cards/card_client.py#L60"">
 <SigArgSection>
 <SigArg name=""self"" />
 </SigArgSection>
 <Description summary=""Retrieves the HTML contents of the card from the\nMetaflow datastore."" />
 <ParamSection name=""Returns"">
 	<Parameter type=""str"" desc=""HTML contents of the card."" />
 </ParamSection>
 </DocSection>
 
 
 
 <DocSection type=""method"" name=""Card.view"" module=""metaflow.cards"" show_import=""False"" heading_level=""4"" link=""https://github.com/Netflix/metaflow/tree/master/metaflow/plugins/cards/card_client.py#L98"">
 <SigArgSection>
 <SigArg name=""self"" />
 </SigArgSection>
 <Description summary=""Opens the card in a local web browser."" extended_summary=""This call uses Python's built-in [`webbrowser`](https://docs.python.org/3/library/webbrowser.html)\nmodule to open the card."" />
 
 </DocSection>",H2,https://docs.metaflow.org/api/cards#retrieving-cards,False,4899.0,516.0
1450,Card components,"You can customize the contents of a card easily using *card components*, a set of visual elements included in Metaflow which are documented below. See [Easy Custom Reports with Card Components](/metaflow/visualizing-results/easy-custom-reports-with-card-components) for instructions.
 
 The components are added to cards in `@step` methods (or functions called from steps), using [the `current.card` object](/api/current#card).",H2,https://docs.metaflow.org/api/cards#card-components,False,427.0,52.0
1451,Markdown,"<DocSection type=""class"" name=""Markdown"" module=""metaflow.cards"" show_import=""True"" heading_level=""3"" link=""https://github.com/Netflix/metaflow/tree/master/metaflow/plugins/cards/card_modules/components.py#L420"">
 <SigArgSection>
 <SigArg name=""text"" default=""None"" />
 </SigArgSection>
 <Description summary=""A block of text formatted in Markdown."" extended_summary=""Example:\n```\ncurrent.card.append(\n    Markdown(&#34;# This is a header appended from `@step` code&#34;)\n)\n```"" />
 <ParamSection name=""Parameters"">
 	<Parameter name=""text"" type=""str"" desc=""Text formatted in Markdown."" />
 </ParamSection>
 </DocSection>",H3,https://docs.metaflow.org/api/cards#markdown,False,626.0,47.0
1452,Image,"<DocSection type=""class"" name=""Image"" module=""metaflow.cards"" show_import=""True"" heading_level=""3"" link=""https://github.com/Netflix/metaflow/tree/master/metaflow/plugins/cards/card_modules/components.py#L165"">
 <SigArgSection>
 <SigArg name=""src"" default=""None"" /><SigArg name=""label"" default=""None"" />
 </SigArgSection>
 <Description summary=""An image."" extended_summary=""`Images can be created directly from PNG/JPG/GIF `bytes`, `PIL.Image`s,\nor Matplotlib figures. Note that the image data is embedded in the card,\nso no external files are required to show the image.\n\nExample: Create an `Image` from bytes:\n```\ncurrent.card.append(\n    Image(\n        requests.get(&#34;https://www.gif-vif.com/hacker-cat.gif&#34;).content,\n        &#34;Image From Bytes&#34;\n    )\n)\n```\n\nExample: Create an `Image` from a Matplotlib figure\n```\nimport pandas as pd\nimport numpy as np\ncurrent.card.append(\n    Image.from_matplotlib(\n        pandas.DataFrame(\n            np.random.randint(0, 100, size=(15, 4)),\n            columns=list(&#34;ABCD&#34;),\n        ).plot()\n    )\n)\n```\n\nExample: Create an `Image` from a [PIL](https://pillow.readthedocs.io/) Image\n```\nfrom PIL import Image as PILImage\ncurrent.card.append(\n    Image.from_pil_image(\n        PILImage.fromarray(np.random.randn(1024, 768), &#34;RGB&#34;),\n        &#34;From PIL Image&#34;\n    )\n)\n```"" />
 <ParamSection name=""Parameters"">
 	<Parameter name=""src"" type=""bytes"" desc=""The image data in `bytes`."" />
 	<Parameter name=""label"" type=""str"" desc=""Optional label for the image."" />
 </ParamSection>
 </DocSection>
 
 
 
 <DocSection type=""method"" name=""Image.from_matplotlib"" module=""metaflow.cards"" show_import=""False"" heading_level=""4"" link=""https://github.com/Netflix/metaflow/tree/master/metaflow/plugins/cards/card_modules/components.py#L314"">
 <SigArgSection>
 <SigArg name=""plot"" /><SigArg name=""label"" type=""Optional"" default=""None"" />
 </SigArgSection>
 <Description summary=""Create an `Image` from a Matplotlib plot."" />
 <ParamSection name=""Parameters"">
 	<Parameter name=""plot"" type=""matplotlib.figure.Figure or matplotlib.axes.Axes or matplotlib.axes._subplots.AxesSubplot"" desc=""a PIL axes (plot) object."" />
 	<Parameter name=""label"" type=""str, optional"" desc=""Optional label for the image."" />
 </ParamSection>
 </DocSection>
 
 
 
 <DocSection type=""method"" name=""Image.from_pil_image"" module=""metaflow.cards"" show_import=""False"" heading_level=""4"" link=""https://github.com/Netflix/metaflow/tree/master/metaflow/plugins/cards/card_modules/components.py#L266"">
 <SigArgSection>
 <SigArg name=""pilimage"" /><SigArg name=""label"" type=""Optional"" default=""None"" />
 </SigArgSection>
 <Description summary=""Create an `Image` from a PIL image."" />
 <ParamSection name=""Parameters"">
 	<Parameter name=""pilimage"" type=""PIL.Image"" desc=""a PIL image object."" />
 	<Parameter name=""label"" type=""str, optional"" desc=""Optional label for the image."" />
 </ParamSection>
 </DocSection>",H3,https://docs.metaflow.org/api/cards#image,False,2976.0,312.0
1453,Artifact,"<DocSection type=""class"" name=""Artifact"" module=""metaflow.cards"" show_import=""True"" heading_level=""3"" link=""https://github.com/Netflix/metaflow/tree/master/metaflow/plugins/cards/card_modules/components.py#L20"">
 <SigArgSection>
 <SigArg name=""artifact"" type=""Any"" /><SigArg name=""name"" type=""Optional"" default=""None"" /><SigArg name=""compressed"" type=""bool"" default=""True"" />
 </SigArgSection>
 <Description summary=""A pretty-printed version of any Python object."" extended_summary=""Large objects are truncated using Python's built-in [`reprlib`](https://docs.python.org/3/library/reprlib.html).\n\nExample:\n```\nfrom datetime import datetime\ncurrent.card.append(Artifact({'now': datetime.utcnow()}))\n```"" />
 <ParamSection name=""Parameters"">
 	<Parameter name=""artifact"" type=""object"" desc=""Any Python object."" />
 	<Parameter name=""name"" type=""str, optional"" desc=""Optional label for the object."" />
 	<Parameter name=""compressed"" type=""bool, default: True"" desc=""Use a truncated representation."" />
 </ParamSection>
 </DocSection>",H3,https://docs.metaflow.org/api/cards#artifact,False,1036.0,73.0
1454,Table,"<DocSection type=""class"" name=""Table"" module=""metaflow.cards"" show_import=""True"" heading_level=""3"" link=""https://github.com/Netflix/metaflow/tree/master/metaflow/plugins/cards/card_modules/components.py#L58"">
 <SigArgSection>
 <SigArg name=""data"" type=""Optional"" default=""None"" /><SigArg name=""headers"" type=""Optional"" default=""None"" />
 </SigArgSection>
 <Description summary=""A table."" extended_summary=""The contents of the table can be text or numerical data, a Pandas dataframe,\nor other card components: `Artifact`, `Image`, `Markdown` objects.\n\nExample: Text and artifacts\n```\nfrom metaflow.cards import Table, Artifact\ncurrent.card.append(\n    Table([\n        ['first row', Artifact({'a': 2})],\n        ['second row', Artifact(3)]\n    ])\n)\n```\n\nExample: Table from a Pandas dataframe\n```\nfrom metaflow.cards import Table\nimport pandas as pd\nimport numpy as np\ncurrent.card.append(\n    Table.from_dataframe(\n        pd.DataFrame(\n            np.random.randint(0, 100, size=(15, 4)),\n            columns=list(&#34;ABCD&#34;)\n        )\n    )\n)\n```"" />
 <ParamSection name=""Parameters"">
 	<Parameter name=""data"" type=""List[List[str or MetaflowCardComponent]], optional"" desc=""List (rows) of lists (columns). Each item can be a string or a `MetaflowCardComponent`."" />
 	<Parameter name=""headers"" type=""List[str], optional"" desc=""Optional header row for the table."" />
 </ParamSection>
 </DocSection>
 
 
 
 <DocSection type=""method"" name=""Table.from_dataframe"" module=""metaflow.cards"" show_import=""False"" heading_level=""4"" link=""https://github.com/Netflix/metaflow/tree/master/metaflow/plugins/cards/card_modules/components.py#L114"">
 <SigArgSection>
 <SigArg name=""dataframe"" default=""None"" /><SigArg name=""truncate"" type=""bool"" default=""True"" />
 </SigArgSection>
 <Description summary=""Create a `Table` based on a Pandas dataframe."" />
 <ParamSection name=""Parameters"">
 	<Parameter name=""dataframe"" type=""Optional[pandas.DataFrame]"" desc=""Pandas dataframe."" />
 	<Parameter name=""truncate"" type=""bool, default: True"" desc=""Truncate large dataframe instead of showing all rows (default: True)."" />
 </ParamSection>
 </DocSection>",H3,https://docs.metaflow.org/api/cards#table,False,2162.0,236.0
1455,Defining a custom card,"You can define custom cards types (`T` in `@card(type=T)`) by creating a Python package that includes a class that derives from `MetaflowCard`, documented below.
 
 Find detailed instructions, a starter template, and an example of a simple custom card at [https://github.com/outerbounds/metaflow-card-html]( https://github.com/outerbounds/metaflow-card-html).
 
 
 <DocSection type=""class"" name=""MetaflowCard"" module=""metaflow.cards"" show_import=""True"" heading_level=""3"" link=""https://github.com/Netflix/metaflow/tree/master/metaflow/plugins/cards/card_modules/card.py#L7"">
 <SigArgSection>
 <SigArg name=""options"" />
 </SigArgSection>
 <Description summary=""Metaflow cards derive from this base class."" extended_summary=""Subclasses of this class are called *card types*. The desired card\ntype `T` is defined in the `@card` decorator as `@card(type=T)`.\n\nAfter a task with `@card(type=T, options=S)` finishes executing, Metaflow instantiates\na subclass `C` of `MetaflowCard` that has its `type` attribute set to `T`, i.e. `C.type=T`.\nThe constructor is given the options dictionary `S` that contains arbitrary\nJSON-encodable data that is passed to the instance, parametrizing the card. The subclass\nmay override the constructor to capture and process the options.\n\nThe subclass needs to implement a `render(task)` method that produces the card\ncontents in HTML, given the finished task that is represented by a `Task` object."" />
 <ParamSection name=""Parameters"">
 	<Parameter name=""options"" type=""Dict"" desc=""JSON-encodable dictionary containing user-definable options for the class."" />
 </ParamSection>
 <ParamSection name=""Attributes"">
 	<Parameter name=""type"" type=""str"" desc=""Card type string. Note that this should be a globally unique name, similar to a\nPython package name, to avoid name clashes between different custom cards."" />
 </ParamSection>
 </DocSection>
 
 
 
 <DocSection type=""method"" name=""MetaflowCard.render"" module=""metaflow.cards"" show_import=""False"" heading_level=""4"" link=""https://github.com/Netflix/metaflow/tree/master/metaflow/plugins/cards/card_modules/card.py#L52"">
 <SigArgSection>
 <SigArg name=""self"" /><SigArg name=""task"" />
 </SigArgSection>
 <Description summary=""Produce custom card contents in HTML."" extended_summary=""Subclasses override this method to customize the card contents."" />
 <ParamSection name=""Parameters"">
 	<Parameter name=""task"" type=""Task"" desc=""A `Task` object that allows you to access data from the finished task and tasks\npreceding it."" />
 </ParamSection>
 <ParamSection name=""Returns"">
 	<Parameter type=""str"" desc=""Card contents as an HTML string."" />
 </ParamSection>
 </DocSection>",H2,https://docs.metaflow.org/api/cards#defining-a-custom-card,False,2661.0,281.0
1456,Client API - Accessing past results,"Use these objects to [access data from past runs and to manipulate tags](/metaflow/client). Objects in this module are organized as a hierarchy:
 
 ![Object hierarchy](/assets/hierarchy.png)",H1,https://docs.metaflow.org/api/client#client-api-accessing-past-results,False,190.0,25.0
1457,Instantiating Objects,"You can instantiate a specific object at any level of the hierarchy by providing a corresponding `pathspec`, e.g. from Metaflow logs.
 
  - `Metaflow()`
  - `Flow('HelloFlow')`
  - `Run('HelloFlow/2')`
  - `Step('HelloFlow/2/start')`
  - `Task('HelloFlow/2/start/1')`
  - `DataArtifact('HelloFlow/2/start/1/name')`",H3,https://docs.metaflow.org/api/client#instantiating-objects,False,314.0,40.0
1458,Listing objects,"Each object is a container (*an iterable*) that can be used to iterate over objects that are below it in the hierarchy. For instance, iterating over a `list(Flow(...))` yields a list of `Run`s, and `list(Run(...))` yields a list of `Step`s.",H3,https://docs.metaflow.org/api/client#listing-objects,False,240.0,40.0
1459,Accessing children,"Since each object is a container, you can access its children through the square-bracket notation, as if each object was a dictionary. For instance, you can access the object `Task('HelloFlow/2/start/1')` as follows:
 ```python
 Flow('HelloFlow')['2']['start']['1']
 ```
 You can also test if the object has a certain child:
 ```python
 if '2' in Flow('HelloFlow'):
     print('Run found')
 ```",H3,https://docs.metaflow.org/api/client#accessing-children,False,394.0,58.0
1460,Common attributes,"All objects at the `Run` level and below have the following attributes:
 
  - `tags` (set) - tags associated with the run this object belongs to (user and system tags).
  - `user_tags` (set) - user tags associated with the run this object belongs to.
  - `system_tags` (set) - system tags associated with the run this object belongs to.
  - `created_at` (datetime) - Date and time this object was created.
  - `parent` (Metaflow object) - Parent of this object (e.g. `Run(...).parent` is a `Flow`).
  - `pathspec` (string) - Pathspec of this object (e.g. `HelloFlow/2` for a `Run`).
  - `path_components` (list) - Components of the pathspec.
  - `origin_pathspec` (string) - If the object was produced via [resume](/metaflow/debugging#how-to-use-the-resume-command), pathspec of the original object this object was cloned from.",H3,https://docs.metaflow.org/api/client#common-attributes,False,827.0,133.0
1461,Object visibility,"Note that only objects in [the current namespace](/scaling/tagging) can be instantiated. See [Namespace functions](#namespace) to see how to switch between namespaces. 
 
 This module accesses all objects through the current metadata provider - either Metaflow Service or local metadata. See [Metadata functions](#metadata) for utilities related to metadata provider.",H3,https://docs.metaflow.org/api/client#object-visibility,False,367.0,49.0
1462,Object Hierarchy,,H2,https://docs.metaflow.org/api/client#object-hierarchy,False,0.0,1.0
1463,Metaflow,"<DocSection type=""class"" name=""Metaflow"" module=""metaflow"" show_import=""False"" heading_level=""3"" link=""https://github.com/Netflix/metaflow/tree/master/metaflow/client/core.py#L2081"">
 <SigArgSection>
 
 </SigArgSection>
 <Description summary=""Entry point to all objects in the Metaflow universe."" extended_summary=""This object can be used to list all the flows present either through the explicit property\nor by iterating over this object."" />
 <ParamSection name=""Attributes"">
 	<Parameter name=""flows"" type=""List[Flow]"" desc=""Returns the list of all `Flow` objects known to this metadata provider. Note that only\nflows present in the current namespace will be returned. A `Flow` is present in a namespace\nif it has at least one run in the namespace."" />
 </ParamSection>
 </DocSection>",H3,https://docs.metaflow.org/api/client#metaflow,False,790.0,89.0
1464,Flow,"<DocSection type=""class"" name=""Flow"" module=""metaflow"" show_import=""False"" heading_level=""3"" link=""https://github.com/Netflix/metaflow/tree/master/metaflow/client/core.py#L2004"">
 <SigArgSection>
 <SigArg name=""pathspec"" />
 </SigArgSection>
 <Description summary=""A Flow represents all existing flows with a certain name, in other words,\nclasses derived from `FlowSpec`. A container of `Run` objects."" />
 <ParamSection name=""Attributes"">
 	<Parameter name=""latest_run"" type=""Run"" desc=""Latest `Run` (in progress or completed, successfully or not) of this flow."" />
 	<Parameter name=""latest_successful_run"" type=""Run"" desc=""Latest successfully completed `Run` of this flow."" />
 </ParamSection>
 </DocSection>
 
 
 
 <DocSection type=""method"" name=""Flow.runs"" module=""metaflow"" show_import=""False"" heading_level=""4"" link=""https://github.com/Netflix/metaflow/tree/master/metaflow/client/core.py#L2054"">
 <SigArgSection>
 <SigArg name=""self"" /><SigArg name=""*tags: str"" type=""str"" />
 </SigArgSection>
 <Description summary=""Returns an iterator over all `Run`s of this flow."" extended_summary=""An optional filter is available that allows you to filter on tags.\nIf multiple tags are specified, only runs that have all the\nspecified tags are returned."" />
 <ParamSection name=""Parameters"">
 	<Parameter name=""tags"" type=""str"" desc=""Tags to match."" />
 </ParamSection>
 <ParamSection name=""Returns"">
 	<Parameter type=""Iterable[Run]"" desc=""Iterator over `Run` objects in this flow."" />
 </ParamSection>
 </DocSection>",H3,https://docs.metaflow.org/api/client#flow,False,1517.0,145.0
1465,Run,"<DocSection type=""class"" name=""Run"" module=""metaflow"" show_import=""False"" heading_level=""3"" link=""https://github.com/Netflix/metaflow/tree/master/metaflow/client/core.py#L1696"">
 <SigArgSection>
 <SigArg name=""pathspec"" />
 </SigArgSection>
 <Description summary=""A `Run` represents an execution of a `Flow`. It is a container of `Step`s."" />
 <ParamSection name=""Attributes"">
 	<Parameter name=""data"" type=""MetaflowData"" desc=""a shortcut to run['end'].task.data, i.e. data produced by this run."" />
 	<Parameter name=""successful"" type=""bool"" desc=""True if the run completed successfully."" />
 	<Parameter name=""finished"" type=""bool"" desc=""True if the run completed."" />
 	<Parameter name=""finished_at"" type=""datetime"" desc=""Time this run finished."" />
 	<Parameter name=""code"" type=""MetaflowCode"" desc=""Code package for this run (if present). See `MetaflowCode`."" />
 	<Parameter name=""trigger"" type=""MetaflowTrigger"" desc=""Information about event(s) that triggered this run (if present). See `MetaflowTrigger`."" />
 	<Parameter name=""end_task"" type=""Task"" desc=""`Task` for the end step (if it is present already)."" />
 </ParamSection>
 </DocSection>
 
 
 
 <DocSection type=""method"" name=""Run.add_tag"" module=""metaflow"" show_import=""False"" heading_level=""4"" link=""https://github.com/Netflix/metaflow/tree/master/metaflow/client/core.py#L1863"">
 <SigArgSection>
 <SigArg name=""self"" /><SigArg name=""tag"" type=""str"" />
 </SigArgSection>
 <Description summary=""Add a tag to this `Run`."" extended_summary=""Note that if the tag is already a system tag, it is not added as a user tag,\nand no error is thrown."" />
 <ParamSection name=""Parameters"">
 	<Parameter name=""tag"" type=""str"" desc=""Tag to add."" />
 </ParamSection>
 </DocSection>
 
 
 
 <DocSection type=""method"" name=""Run.add_tags"" module=""metaflow"" show_import=""False"" heading_level=""4"" link=""https://github.com/Netflix/metaflow/tree/master/metaflow/client/core.py#L1885"">
 <SigArgSection>
 <SigArg name=""self"" /><SigArg name=""tags"" type=""Iterable"" />
 </SigArgSection>
 <Description summary=""Add one or more tags to this `Run`."" extended_summary=""Note that if any tag is already a system tag, it is not added as a user tag\nand no error is thrown."" />
 <ParamSection name=""Parameters"">
 	<Parameter name=""tags"" type=""Iterable[str]"" desc=""Tags to add."" />
 </ParamSection>
 </DocSection>
 
 
 
 <DocSection type=""method"" name=""Run.remove_tag"" module=""metaflow"" show_import=""False"" heading_level=""4"" link=""https://github.com/Netflix/metaflow/tree/master/metaflow/client/core.py#L1899"">
 <SigArgSection>
 <SigArg name=""self"" /><SigArg name=""tag"" type=""str"" />
 </SigArgSection>
 <Description summary=""Remove one tag from this `Run`."" extended_summary=""Removing a system tag is an error. Removing a non-existent\nuser tag is a no-op."" />
 <ParamSection name=""Parameters"">
 	<Parameter name=""tag"" type=""str"" desc=""Tag to remove."" />
 </ParamSection>
 </DocSection>
 
 
 
 <DocSection type=""method"" name=""Run.remove_tags"" module=""metaflow"" show_import=""False"" heading_level=""4"" link=""https://github.com/Netflix/metaflow/tree/master/metaflow/client/core.py#L1921"">
 <SigArgSection>
 <SigArg name=""self"" /><SigArg name=""tags"" type=""Iterable"" />
 </SigArgSection>
 <Description summary=""Remove one or more tags to this `Run`."" extended_summary=""Removing a system tag will result in an error. Removing a non-existent\nuser tag is a no-op."" />
 <ParamSection name=""Parameters"">
 	<Parameter name=""tags"" type=""Iterable[str]"" desc=""Tags to remove."" />
 </ParamSection>
 </DocSection>
 
 
 
 <DocSection type=""method"" name=""Run.replace_tag"" module=""metaflow"" show_import=""False"" heading_level=""4"" link=""https://github.com/Netflix/metaflow/tree/master/metaflow/client/core.py#L1935"">
 <SigArgSection>
 <SigArg name=""self"" /><SigArg name=""tag_to_remove"" type=""str"" /><SigArg name=""tag_to_add"" type=""str"" />
 </SigArgSection>
 <Description summary=""Remove a tag and add a tag atomically. Removal is done first.\nThe rules for `Run.add_tag` and `Run.remove_tag` also apply here."" />
 <ParamSection name=""Parameters"">
 	<Parameter name=""tag_to_remove"" type=""str"" desc=""Tag to remove."" />
 	<Parameter name=""tag_to_add"" type=""str"" desc=""Tag to add."" />
 </ParamSection>
 </DocSection>
 
 
 
 <DocSection type=""method"" name=""Run.replace_tags"" module=""metaflow"" show_import=""False"" heading_level=""4"" link=""https://github.com/Netflix/metaflow/tree/master/metaflow/client/core.py#L1959"">
 <SigArgSection>
 <SigArg name=""self"" /><SigArg name=""tags_to_remove"" type=""Iterable"" /><SigArg name=""tags_to_add"" type=""Iterable"" />
 </SigArgSection>
 <Description summary=""Remove and add tags atomically; the removal is done first.\nThe rules for `Run.add_tag` and `Run.remove_tag` also apply here."" />
 <ParamSection name=""Parameters"">
 	<Parameter name=""tags_to_remove"" type=""Iterable[str]"" desc=""Tags to remove."" />
 	<Parameter name=""tags_to_add"" type=""Iterable[str]"" desc=""Tags to add."" />
 </ParamSection>
 </DocSection>",H3,https://docs.metaflow.org/api/client#run,False,4948.0,461.0
1466,Step,"<DocSection type=""class"" name=""Step"" module=""metaflow"" show_import=""False"" heading_level=""3"" link=""https://github.com/Netflix/metaflow/tree/master/metaflow/client/core.py#L1535"">
 <SigArgSection>
 <SigArg name=""pathspec"" />
 </SigArgSection>
 <Description summary=""A `Step` represents a user-defined step, that is, a method annotated with the `@step` decorator."" extended_summary=""It contains `Task` objects associated with the step, that is, all executions of the\n`Step`. The step may contain multiple `Task`s in the case of a foreach step."" />
 <ParamSection name=""Attributes"">
 	<Parameter name=""task"" type=""Task"" desc=""The first `Task` object in this step. This is a shortcut for retrieving the only\ntask contained in a non-foreach step."" />
 	<Parameter name=""finished_at"" type=""datetime"" desc=""Time when the latest `Task` of this step finished. Note that in the case of foreaches,\nthis time may change during execution of the step."" />
 	<Parameter name=""environment_info"" type=""Dict[str, Any]"" desc=""Information about the execution environment."" />
 </ParamSection>
 </DocSection>",H3,https://docs.metaflow.org/api/client#step,False,1090.0,122.0
1467,Task,"<DocSection type=""class"" name=""Task"" module=""metaflow"" show_import=""False"" heading_level=""3"" link=""https://github.com/Netflix/metaflow/tree/master/metaflow/client/core.py#L984"">
 <SigArgSection>
 <SigArg name=""pathspec, attempt=None"" />
 </SigArgSection>
 <Description summary=""A `Task` represents an execution of a `Step`."" extended_summary=""It contains all `DataArtifact` objects produced by the task as\nwell as metadata related to execution.\n\nNote that the `@retry` decorator may cause multiple attempts of\nthe task to be present. Usually you want the latest attempt, which\nis what instantiating a `Task` object returns by default. If\nyou need to e.g. retrieve logs from a failed attempt, you can\nexplicitly get information about a specific attempt by using the\nfollowing syntax when creating a task:\n\n`Task('flow/run/step/task', attempt=<attempt>)`\n\nwhere `attempt=0` corresponds to the first attempt etc."" />
 <ParamSection name=""Attributes"">
 	<Parameter name=""metadata"" type=""List[Metadata]"" desc=""List of all metadata events associated with the task."" />
 	<Parameter name=""metadata_dict"" type=""Dict[str, str]"" desc=""A condensed version of `metadata`: A dictionary where keys\nare names of metadata events and values the latest corresponding event."" />
 	<Parameter name=""data"" type=""MetaflowData"" desc=""Container of all data artifacts produced by this task. Note that this\ncall downloads all data locally, so it can be slower than accessing\nartifacts individually. See `MetaflowData` for more information."" />
 	<Parameter name=""artifacts"" type=""MetaflowArtifacts"" desc=""Container of `DataArtifact` objects produced by this task."" />
 	<Parameter name=""successful"" type=""bool"" desc=""True if the task completed successfully."" />
 	<Parameter name=""finished"" type=""bool"" desc=""True if the task completed."" />
 	<Parameter name=""exception"" type=""object"" desc=""Exception raised by this task if there was one."" />
 	<Parameter name=""finished_at"" type=""datetime"" desc=""Time this task finished."" />
 	<Parameter name=""runtime_name"" type=""str"" desc=""Runtime this task was executed on."" />
 	<Parameter name=""stdout"" type=""str"" desc=""Standard output for the task execution."" />
 	<Parameter name=""stderr"" type=""str"" desc=""Standard error output for the task execution."" />
 	<Parameter name=""code"" type=""MetaflowCode"" desc=""Code package for this task (if present). See `MetaflowCode`."" />
 	<Parameter name=""environment_info"" type=""Dict[str, str]"" desc=""Information about the execution environment."" />
 </ParamSection>
 </DocSection>
 
 
 
 <DocSection type=""method"" name=""Task.loglines"" module=""metaflow"" show_import=""False"" heading_level=""4"" link=""https://github.com/Netflix/metaflow/tree/master/metaflow/client/core.py#L1440"">
 <SigArgSection>
 <SigArg name=""self"" /><SigArg name=""stream"" type=""str"" /><SigArg name=""as_unicode"" type=""bool"" default=""True"" /><SigArg name=""meta_dict"" type=""Dict"" default=""None"" />
 </SigArgSection>
 <Description summary=""Return an iterator over (utc_timestamp, logline) tuples."" />
 <ParamSection name=""Parameters"">
 	<Parameter name=""stream"" type=""str"" desc=""Either 'stdout' or 'stderr'."" />
 	<Parameter name=""as_unicode"" type=""bool, default: True"" desc=""If as_unicode=False, each logline is returned as a byte object. Otherwise,\nit is returned as a (unicode) string."" />
 </ParamSection>
 <ParamSection name=""Returns"">
 	<Parameter type=""Iterable[(datetime, str)]"" desc=""Iterator over timestamp, logline pairs."" />
 </ParamSection>
 </DocSection>",H3,https://docs.metaflow.org/api/client#task,False,3499.0,362.0
1468,DataArtifact,"<DocSection type=""class"" name=""DataArtifact"" module=""metaflow"" show_import=""False"" heading_level=""3"" link=""https://github.com/Netflix/metaflow/tree/master/metaflow/client/core.py#L851"">
 <SigArgSection>
 <SigArg name=""pathspec"" />
 </SigArgSection>
 <Description summary=""A single data artifact and associated metadata. Note that this object does\nnot contain other objects as it is the leaf object in the hierarchy."" />
 <ParamSection name=""Attributes"">
 	<Parameter name=""data"" type=""object"" desc=""The data contained in this artifact, that is, the object produced during\nexecution of this run."" />
 	<Parameter name=""sha"" type=""string"" desc=""A unique ID of this artifact."" />
 	<Parameter name=""finished_at"" type=""datetime"" desc=""Corresponds roughly to the `Task.finished_at` time of the parent `Task`.\nAn alias for `DataArtifact.created_at`."" />
 </ParamSection>
 </DocSection>",H3,https://docs.metaflow.org/api/client#dataartifact,False,882.0,88.0
1469,Helper Objects,,H2,https://docs.metaflow.org/api/client#helper-objects,False,0.0,1.0
1470,MetaflowData,"<DocSection type=""class"" name=""MetaflowData"" module=""metaflow"" show_import=""False"" heading_level=""3"" link=""https://github.com/Netflix/metaflow/tree/master/metaflow/client/core.py#L706"">
 <SigArgSection>
 <SigArg name="""" />
 </SigArgSection>
 <Description summary=""Container of data artifacts produced by a `Task`. This object is\ninstantiated through `Task.data`."" extended_summary=""`MetaflowData` allows results to be retrieved by their name\nthrough a convenient dot notation:\n\n```python\nTask(...).data.my_object\n```\n\nYou can also test the existence of an object\n\n```python\nif 'my_object' in Task(...).data:\n    print('my_object found')\n```\n\nNote that this container relies on the local cache to load all data\nartifacts. If your `Task` contains a lot of data, a more efficient\napproach is to load artifacts individually like so\n\n```\nTask(...)['my_object'].data\n```"" />
 
 </DocSection>",H3,https://docs.metaflow.org/api/client#metaflowdata,False,906.0,88.0
1471,MetaflowCode,"<DocSection type=""class"" name=""MetaflowCode"" module=""metaflow"" show_import=""False"" heading_level=""3"" link=""https://github.com/Netflix/metaflow/tree/master/metaflow/client/core.py#L750"">
 <SigArgSection>
 <SigArg name="""" />
 </SigArgSection>
 <Description summary=""Snapshot of the code used to execute this `Run`. Instantiate the object through\n`Run(...).code` (if any step is executed remotely) or `Task(...).code` for an\nindividual task. The code package is the same for all steps of a `Run`."" extended_summary=""`MetaflowCode` includes a package of the user-defined `FlowSpec` class and supporting\nfiles, as well as a snapshot of the Metaflow library itself.\n\nCurrently, `MetaflowCode` objects are stored only for `Run`s that have at least one `Step`\nexecuting outside the user's local environment.\n\nThe `TarFile` for the `Run` is given by `Run(...).code.tarball`"" />
 <ParamSection name=""Attributes"">
 	<Parameter name=""path"" type=""str"" desc=""Location (in the datastore provider) of the code package."" />
 	<Parameter name=""info"" type=""Dict[str, str]"" desc=""Dictionary of information related to this code-package."" />
 	<Parameter name=""flowspec"" type=""str"" desc=""Source code of the file containing the `FlowSpec` in this code package."" />
 	<Parameter name=""tarball"" type=""TarFile"" desc=""Python standard library `tarfile.TarFile` archive containing all the code."" />
 </ParamSection>
 </DocSection>",H3,https://docs.metaflow.org/api/client#metaflowcode,False,1409.0,155.0
1472,MetaflowTrigger,"`MetaflowTrigger` is returned by `Run.trigger` if the `Run` was [triggered by an event](/production/event-triggering). It is also returned by [`current.trigger`](/api/current) when called from an event-triggered flow.
 
 
 <DocSection type=""property"" name=""Trigger.event"" module=""metaflow.events"" show_import=""False"" heading_level=""4"" link=""https://github.com/Netflix/metaflow/tree/master/"">
 <SigArgSection>
 <SigArg name="""" />
 </SigArgSection>
 <Description summary=""The `MetaflowEvent` object corresponding to the triggering event.\n\nIf multiple events triggered the run, this property is the latest event.\n"" />
 <ParamSection name=""Returns"">
 <Parameter type=""MetaflowEvent, optional"" desc=""The latest event that triggered the run, if applicable."" />
 </ParamSection>
 </DocSection>
 
 
 
 <DocSection type=""property"" name=""Trigger.events"" module=""metaflow.events"" show_import=""False"" heading_level=""4"" link=""https://github.com/Netflix/metaflow/tree/master/"">
 <SigArgSection>
 <SigArg name="""" />
 </SigArgSection>
 <Description summary=""The list of `MetaflowEvent` objects correspondings to all the triggering events.\n"" />
 <ParamSection name=""Returns"">
 <Parameter type=""List[MetaflowEvent], optional"" desc=""List of all events that triggered the run"" />
 </ParamSection>
 </DocSection>
 
 
 
 <DocSection type=""property"" name=""Trigger.run"" module=""metaflow.events"" show_import=""False"" heading_level=""4"" link=""https://github.com/Netflix/metaflow/tree/master/"">
 <SigArgSection>
 <SigArg name="""" />
 </SigArgSection>
 <Description summary=""The corresponding `Run` object if the triggering event is a Metaflow run.\n\nIn case multiple runs triggered the run, this property is the latest run.\nReturns `None` if none of the triggering events are a `Run`.\n"" />
 <ParamSection name=""Returns"">
 <Parameter type=""Run, optional"" desc=""Latest Run that triggered this run, if applicable."" />
 </ParamSection>
 </DocSection>
 
 
 
 <DocSection type=""property"" name=""Trigger.runs"" module=""metaflow.events"" show_import=""False"" heading_level=""4"" link=""https://github.com/Netflix/metaflow/tree/master/"">
 <SigArgSection>
 <SigArg name="""" />
 </SigArgSection>
 <Description summary=""The list of `Run` objects in the triggering events.\nReturns `None` if none of the triggering events are `Run` objects.\n"" />
 <ParamSection name=""Returns"">
 <Parameter type=""List[Run], optional"" desc=""List of runs that triggered this run, if applicable."" />
 </ParamSection>
 </DocSection>
 
 
 
 <DocSection type=""method"" name=""Trigger.__getitem__"" module=""metaflow"" show_import=""False"" heading_level=""4"" link=""https://github.com/Netflix/metaflow/tree/master/metaflow/events.py#L139"">
 <SigArgSection>
 <SigArg name="""" />
 </SigArgSection>
 <Description summary=""If triggering events are runs, `key` corresponds to the flow name of the triggering run.\nOtherwise, `key` corresponds to the event name and a `MetaflowEvent` object is returned."" />
 <ParamSection name=""Returns"">
 	<Parameter type=""Run or MetaflowEvent"" desc=""`Run` object if triggered by a run. Otherwise returns a `MetaflowEvent`."" />
 </ParamSection>
 </DocSection>",H3,https://docs.metaflow.org/api/client#metaflowtrigger,False,3111.0,305.0
1473,MetaflowEvent,"`MetaflowEvent` is returned by `MetaflowTrigger` (see above) for [event-triggered](/production/event-triggering) runs.
 
 
 <DocSection type=""class"" name=""MetaflowEvent"" module=""metaflow"" show_import=""False"" heading_level=""3"" link=""https://github.com/Netflix/metaflow/tree/master/"">
 <SigArgSection>
 <SigArg name="""" />
 </SigArgSection>
 <Description summary=""Container of metadata that identifies the event that triggered\nthe `Run` under consideration."" />
 <ParamSection name=""Attributes"">
 	<Parameter name=""name"" type=""str"" desc=""name of the event."" />
 	<Parameter name=""id"" type=""str"" desc=""unique identifier for the event."" />
 	<Parameter name=""timestamp"" type=""datetime"" desc=""timestamp recording creation time for the event."" />
 	<Parameter name=""type"" type=""str"" desc=""type for the event - one of `event` or `run`"" />
 </ParamSection>
 </DocSection>",H3,https://docs.metaflow.org/api/client#metaflowevent,False,863.0,84.0
1474,Namespace functions,"<DocSection type=""function"" name=""namespace"" module=""metaflow"" show_import=""True"" heading_level=""3"" link=""https://github.com/Netflix/metaflow/tree/master/metaflow/client/core.py#L150"">
 <SigArgSection>
 <SigArg name=""ns"" type=""Optional"" />
 </SigArgSection>
 <Description summary=""Switch namespace to the one provided."" extended_summary=""This call has a global effect. No objects outside this namespace\nwill be accessible. To access all objects regardless of namespaces,\npass None to this call."" />
 <ParamSection name=""Parameters"">
 	<Parameter name=""ns"" type=""str, optional"" desc=""Namespace to switch to or None to ignore namespaces."" />
 </ParamSection>
 <ParamSection name=""Returns"">
 	<Parameter type=""str, optional"" desc=""Namespace set (result of get_namespace())."" />
 </ParamSection>
 </DocSection>
 
 
 
 <DocSection type=""function"" name=""get_namespace"" module=""metaflow"" show_import=""False"" heading_level=""3"" link=""https://github.com/Netflix/metaflow/tree/master/metaflow/client/core.py#L173"">
 <SigArgSection>
 
 </SigArgSection>
 <Description summary=""Return the current namespace that is currently being used to filter objects."" extended_summary=""The namespace is a tag associated with all objects in Metaflow."" />
 <ParamSection name=""Returns"">
 	<Parameter type=""str, optional"" desc=""The current namespace used to filter objects."" />
 </ParamSection>
 </DocSection>
 
 
 
 <DocSection type=""function"" name=""default_namespace"" module=""metaflow"" show_import=""False"" heading_level=""3"" link=""https://github.com/Netflix/metaflow/tree/master/metaflow/client/core.py#L191"">
 <SigArgSection>
 
 </SigArgSection>
 <Description summary=""Resets the namespace used to filter objects to the default one, i.e. the one that was\nused prior to any `namespace` calls."" />
 <ParamSection name=""Returns"">
 	<Parameter type=""str"" desc=""The result of get_namespace() after the namespace has been reset."" />
 </ParamSection>
 </DocSection>",H2,https://docs.metaflow.org/api/client#namespace-functions,False,1934.0,181.0
1475,Metadata functions,"<DocSection type=""function"" name=""metadata"" module=""metaflow"" show_import=""False"" heading_level=""3"" link=""https://github.com/Netflix/metaflow/tree/master/metaflow/client/core.py#L48"">
 <SigArgSection>
 <SigArg name=""ms"" type=""str"" />
 </SigArgSection>
 <Description summary=""Switch Metadata provider."" extended_summary=""This call has a global effect. Selecting the local metadata will,\nfor example, not allow access to information stored in remote\nmetadata providers.\n\nNote that you don't typically have to call this function directly. Usually\nthe metadata provider is set through the Metaflow configuration file. If you\nneed to switch between multiple providers, you can use the `METAFLOW_PROFILE`\nenvironment variable to switch between configurations."" />
 <ParamSection name=""Parameters"">
 	<Parameter name=""ms"" type=""str"" desc=""Can be a path (selects local metadata), a URL starting with http (selects\nthe service metadata) or an explicit specification <metadata_type>@<info>; as an\nexample, you can specify local@<path> or service@<url>."" />
 </ParamSection>
 <ParamSection name=""Returns"">
 	<Parameter type=""str"" desc=""The description of the metadata selected (equivalent to the result of\nget_metadata())."" />
 </ParamSection>
 </DocSection>
 
 
 
 <DocSection type=""function"" name=""get_metadata"" module=""metaflow"" show_import=""False"" heading_level=""3"" link=""https://github.com/Netflix/metaflow/tree/master/metaflow/client/core.py#L100"">
 <SigArgSection>
 
 </SigArgSection>
 <Description summary=""Returns the current Metadata provider."" extended_summary=""If this is not set explicitly using `metadata`, the default value is\ndetermined through the Metaflow configuration. You can use this call to\ncheck that your configuration is set up properly.\n\nIf multiple configuration profiles are present, this call returns the one\nselected through the `METAFLOW_PROFILE` environment variable."" />
 <ParamSection name=""Returns"">
 	<Parameter type=""str"" desc=""Information about the Metadata provider currently selected. This information typically\nreturns provider specific information (like URL for remote providers or local paths for\nlocal providers)."" />
 </ParamSection>
 </DocSection>
 
 
 
 <DocSection type=""function"" name=""default_metadata"" module=""metaflow"" show_import=""False"" heading_level=""3"" link=""https://github.com/Netflix/metaflow/tree/master/metaflow/client/core.py#L123"">
 <SigArgSection>
 
 </SigArgSection>
 <Description summary=""Resets the Metadata provider to the default value, that is, to the value\nthat was used prior to any `metadata` calls."" />
 <ParamSection name=""Returns"">
 	<Parameter type=""str"" desc=""The result of get_metadata() after resetting the provider."" />
 </ParamSection>
 </DocSection>",H2,https://docs.metaflow.org/api/client#metadata-functions,False,2739.0,273.0
1476,@kubernetes,"The `@kubernetes` decorator sends a step for execution on a [Kubernetes](https://kubernetes.io) cluster. For more information, see [Executing Tasks Remotely](/scaling/remote-tasks/introduction).
 
 For options related to `tmpfs`, see [Using `metaflow.S3` for in-memory processing](/scaling/data#using-metaflows3-for-in-memory-processing).
 
 
 
 <DocSection type=""decorator"" name=""kubernetes"" module=""metaflow"" show_import=""True"" heading_level=""3"" link=""https://github.com/Netflix/metaflow/tree/master/metaflow/plugins/kubernetes/kubernetes_decorator.py#L38"">
 <SigArgSection>
 <SigArg name=""..."" />
 </SigArgSection>
 <Description summary=""Specifies that this step should execute on Kubernetes."" />
 <ParamSection name=""Parameters"">
 	<Parameter name=""cpu"" type=""int, default: 1"" desc=""Number of CPUs required for this step. If `@resources` is\nalso present, the maximum value from all decorators is used."" />
 	<Parameter name=""memory"" type=""int, default: 4096"" desc=""Memory size (in MB) required for this step. If\n`@resources` is also present, the maximum value from all decorators is\nused."" />
 	<Parameter name=""disk"" type=""int, default: 10240"" desc=""Disk size (in MB) required for this step. If\n`@resources` is also present, the maximum value from all decorators is\nused."" />
 	<Parameter name=""image"" type=""str, optional"" desc=""Docker image to use when launching on Kubernetes. If not specified, and\nMETAFLOW_KUBERNETES_CONTAINER_IMAGE is specified, that image is used. If\nnot, a default Docker image mapping to the current version of Python is used."" />
 	<Parameter name=""service_account"" type=""str, default: METAFLOW_KUBERNETES_SERVICE_ACCOUNT"" desc=""Kubernetes service account to use when launching pod in Kubernetes."" />
 	<Parameter name=""namespace"" type=""str, default: METAFLOW_KUBERNETES_NAMESPACE"" desc=""Kubernetes namespace to use when launching pod in Kubernetes."" />
 	<Parameter name=""secrets"" type=""List[str], optional"" desc=""Kubernetes secrets to use when launching pod in Kubernetes. These\nsecrets are in addition to the ones defined in `METAFLOW_KUBERNETES_SECRETS`\nin Metaflow configuration."" />
 	<Parameter name=""tolerations"" type=""List[str], default: METAFLOW_KUBERNETES_TOLERATIONS"" desc=""Kubernetes tolerations to use when launching pod in Kubernetes."" />
 	<Parameter name=""use_tmpfs: bool, default: False"" desc=""This enables an explicit tmpfs mount for this step."" />
 	<Parameter name=""tmpfs_tempdir: bool, default: True"" desc=""sets METAFLOW_TEMPDIR to tmpfs_path if set for this step."" />
 	<Parameter name=""tmpfs_size: int, optional"" desc=""The value for the size (in MiB) of the tmpfs mount for this step.\nThis parameter maps to the `--tmpfs` option in Docker. Defaults to 50% of the\nmemory allocated for this step."" />
 	<Parameter name=""tmpfs_path: string, optional"" desc=""Path to tmpfs mount for this step. Defaults to /metaflow_temp."" />
 	<Parameter name=""persistent_volume_claims: Dict[str, str], optional"" desc=""A map (dictionary) of persistent volumes to be mounted to the pod for this step. The map is from persistent\nvolumes to the path to which the volume is to be mounted, e.g., `{'pvc-name': '/path/to/mount/on'}`."" />
 </ParamSection>
 </DocSection>",H1,https://docs.metaflow.org/api/step-decorators/kubernetes#kubernetes,False,3209.0,365.0
1477,@resources,"The `@resources` decorator specifies resource requirements for a step.
 
 Note that `@resources` takes effect only when combined with another decorator like `@batch` or `@kubernetes` which specifies the compute layer. For more information, see [Executing Tasks Remotely](/scaling/remote-tasks/introduction).
 
 
 
 <DocSection type=""decorator"" name=""resources"" module=""metaflow"" show_import=""True"" heading_level=""3"" link=""https://github.com/Netflix/metaflow/tree/master/metaflow/plugins/resources_decorator.py#L4"">
 <SigArgSection>
 <SigArg name=""..."" />
 </SigArgSection>
 <Description summary=""Specifies the resources needed when executing this step."" extended_summary=""Use `@resources` to specify the resource requirements\nindependently of the specific compute layer (`@batch`, `@kubernetes`).\n\nYou can choose the compute layer on the command line by executing e.g.\n```\npython myflow.py run --with batch\n```\nor\n```\npython myflow.py run --with kubernetes\n```\nwhich executes the flow on the desired system using the\nrequirements specified in `@resources`."" />
 <ParamSection name=""Parameters"">
 	<Parameter name=""cpu"" type=""int, default: 1"" desc=""Number of CPUs required for this step."" />
 	<Parameter name=""gpu"" type=""int, default: 0"" desc=""Number of GPUs required for this step."" />
 	<Parameter name=""memory"" type=""int, default: 4096"" desc=""Memory size (in MB) required for this step."" />
 	<Parameter name=""shared_memory"" type=""int, optional"" desc=""The value for the size (in MiB) of the /dev/shm volume for this step.\nThis parameter maps to the `--shm-size` option in Docker."" />
 </ParamSection>
 </DocSection>",H1,https://docs.metaflow.org/api/step-decorators/resources#resources,False,1631.0,179.0
1478,@environment,"The `@environment` decorator specifies environment variables for a step.
 
 Note that `@environment` is mainly useful for setting special environment variables that need to be present when a container launched by `@batch` or `@kubernetes` starts, which happens before any user code is executed. Variables that are needed after the user code has started can be set as usual in Python:
 ```
 import os
 os.environ['SOME_VAL'] = ""some value""
 ```
 
 To set credentials and other secret as environment variables, [see the `@secrets` decorator](/api/step-decorators/secrets).
 
 
 
 <DocSection type=""decorator"" name=""environment"" module=""metaflow"" show_import=""True"" heading_level=""3"" link=""https://github.com/Netflix/metaflow/tree/master/metaflow/plugins/environment_decorator.py#L4"">
 <SigArgSection>
 <SigArg name=""..."" />
 </SigArgSection>
 <Description summary=""Specifies environment variables to be set prior to the execution of a step."" />
 <ParamSection name=""Parameters"">
 	<Parameter name=""vars"" type=""Dict[str, str], default: {}"" desc=""Dictionary of environment variables to set."" />
 </ParamSection>
 </DocSection>",H1,https://docs.metaflow.org/api/step-decorators/environment#environment,False,1122.0,129.0
1479,@step,"The `@step` decorator converts a method to a step of a flow.
 
 Use `@step` to construct Metaflow workflows. For more information, see [Basics of Metaflow](/metaflow/basics).
 
 
 
 <DocSection type=""function"" name=""step"" module=""metaflow"" show_import=""True"" heading_level=""3"" link=""https://github.com/Netflix/metaflow/tree/master/metaflow/decorators.py#L552"">
 <SigArgSection>
 <SigArg name="""" />
 </SigArgSection>
 <Description summary=""The step decorator. Makes a method a step in the workflow."" />
 
 </DocSection>",H1,https://docs.metaflow.org/api/step-decorators/step#step,False,518.0,56.0
1480,@card,"Creates a report card after the step completes. For more information, see [Visualizing Results](/metaflow/visualizing-results) and [the main API docs for cards](/api/cards).
 
 
 
 <DocSection type=""decorator"" name=""card"" module=""metaflow"" show_import=""True"" heading_level=""3"" link=""https://github.com/Netflix/metaflow/tree/master/metaflow/plugins/cards/card_decorator.py#L27"">
 <SigArgSection>
 <SigArg name=""..."" />
 </SigArgSection>
 <Description summary=""Creates a human-readable report, a Metaflow Card, after this step completes."" extended_summary=""Note that you may add multiple `@card` decorators in a step with different parameters."" />
 <ParamSection name=""Parameters"">
 	<Parameter name=""type"" type=""str, default: 'default'"" desc=""Card type."" />
 	<Parameter name=""id"" type=""str, optional, default: None"" desc=""If multiple cards are present, use this id to identify this card."" />
 	<Parameter name=""options"" type=""Dict[str, Any], default: {}"" desc=""Options passed to the card. The contents depend on the card type."" />
 	<Parameter name=""timeout"" type=""int, default: 45"" desc=""Interrupt reporting if it takes more than this many seconds."" />
 </ParamSection>
 </DocSection>",H1,https://docs.metaflow.org/api/step-decorators/card#card,False,1185.0,129.0
1481,@retry,"The `@retry` decorator specifies how many times the task(s) corresponding to a step should be retried before failing the flow.
 
 For more information, see [Dealing with Failures](/scaling/failures).
 
 
 
 <DocSection type=""decorator"" name=""retry"" module=""metaflow"" show_import=""True"" heading_level=""3"" link=""https://github.com/Netflix/metaflow/tree/master/metaflow/plugins/retry_decorator.py#L6"">
 <SigArgSection>
 <SigArg name=""..."" />
 </SigArgSection>
 <Description summary=""Specifies the number of times the task corresponding\nto a step needs to be retried."" extended_summary=""This decorator is useful for handling transient errors, such as networking issues.\nIf your task contains operations that can't be retried safely, e.g. database updates,\nit is advisable to annotate it with `@retry(times=0)`.\n\nThis can be used in conjunction with the `@catch` decorator. The `@catch`\ndecorator will execute a no-op task after all retries have been exhausted,\nensuring that the flow execution can continue."" />
 <ParamSection name=""Parameters"">
 	<Parameter name=""times"" type=""int, default: 3"" desc=""Number of times to retry this task."" />
 	<Parameter name=""minutes_between_retries"" type=""int, default: 2"" desc=""Number of minutes between retries."" />
 </ParamSection>
 </DocSection>",H1,https://docs.metaflow.org/api/step-decorators/retry#retry,False,1287.0,146.0
1482,@secrets,"The `@secrets` decorator allows you to access secrets, such as database credentials, securely from a secrets manager. For more details, see [Accessing Secrets](/scaling/secrets).
 
 
 
 <DocSection type=""decorator"" name=""secrets"" module=""metaflow"" show_import=""True"" heading_level=""3"" link=""https://github.com/Netflix/metaflow/tree/master/metaflow/plugins/secrets/secrets_decorator.py#L182"">
 <SigArgSection>
 <SigArg name=""..."" />
 </SigArgSection>
 <Description summary=""Specifies secrets to be retrieved and injected as environment variables prior to\nthe execution of a step."" />
 <ParamSection name=""Parameters"">
 	<Parameter name=""sources"" type=""List[Union[str, Dict[str, Any]]], default: []"" desc=""List of secret specs, defining how the secrets are to be retrieved"" />
 </ParamSection>
 </DocSection>",H1,https://docs.metaflow.org/api/step-decorators/secrets#secrets,False,807.0,80.0
1483,@catch,"The `@catch` decorator ensures that a step will succeed, despite any errors in the user code or transient platform issues.
 
 It is best used in conjunction with the `@retry` decorator: After all retries have been exhausted, `@catch` executes a no-op task that allows the run to continue. For more information, see [Dealing with Failures](/scaling/failures).
 
 
 
 <DocSection type=""decorator"" name=""catch"" module=""metaflow"" show_import=""True"" heading_level=""3"" link=""https://github.com/Netflix/metaflow/tree/master/metaflow/plugins/catch_decorator.py#L22"">
 <SigArgSection>
 <SigArg name=""..."" />
 </SigArgSection>
 <Description summary=""Specifies that the step will success under all circumstances."" extended_summary=""The decorator will create an optional artifact, specified by `var`, which\ncontains the exception raised. You can use it to detect the presence\nof errors, indicating that all happy-path artifacts produced by the step\nare missing."" />
 <ParamSection name=""Parameters"">
 	<Parameter name=""var"" type=""str, optional"" desc=""Name of the artifact in which to store the caught exception.\nIf not specified, the exception is not stored."" />
 	<Parameter name=""print_exception"" type=""bool, default: True"" desc=""Determines whether or not the exception is printed to\nstdout when caught."" />
 </ParamSection>
 </DocSection>",H1,https://docs.metaflow.org/api/step-decorators/catch#catch,False,1334.0,158.0
1484,@timeout,"The `@timeout` decorator specifies for how long a step should execute before it is interrupted.
 
 You can use it to handle tasks that may get stuck, or to set a time-budget for a step. For more information, see [Dealing with Failures](/scaling/failures).
 
 
 
 <DocSection type=""decorator"" name=""timeout"" module=""metaflow"" show_import=""True"" heading_level=""3"" link=""https://github.com/Netflix/metaflow/tree/master/metaflow/plugins/timeout_decorator.py#L13"">
 <SigArgSection>
 <SigArg name=""..."" />
 </SigArgSection>
 <Description summary=""Specifies a timeout for your step."" extended_summary=""This decorator is useful if this step may hang indefinitely.\n\nThis can be used in conjunction with the `@retry` decorator as well as the `@catch` decorator.\nA timeout is considered to be an exception thrown by the step. It will cause the step to be\nretried if needed and the exception will be caught by the `@catch` decorator, if present.\n\nNote that all the values specified in parameters are added together so if you specify\n60 seconds and 1 hour, the decorator will have an effective timeout of 1 hour and 1 minute."" />
 <ParamSection name=""Parameters"">
 	<Parameter name=""seconds"" type=""int, default: 0"" desc=""Number of seconds to wait prior to timing out."" />
 	<Parameter name=""minutes"" type=""int, default: 0"" desc=""Number of minutes to wait prior to timing out."" />
 	<Parameter name=""hours"" type=""int, default: 0"" desc=""Number of hours to wait prior to timing out."" />
 </ParamSection>
 </DocSection>",H1,https://docs.metaflow.org/api/step-decorators/timeout#timeout,False,1509.0,202.0
1485,@conda,"The `@conda` decorator specifies what libraries should be made available for a step.
 
 The libraries are installed from [Conda repositories](https://anaconda.org/). For more information, see [Managing External Libraries](/scaling/dependencies).
 
 
 
 <DocSection type=""decorator"" name=""conda"" module=""metaflow"" show_import=""True"" heading_level=""3"" link=""https://github.com/Netflix/metaflow/tree/master/metaflow/plugins/conda/conda_step_decorator.py#L40"">
 <SigArgSection>
 <SigArg name=""..."" />
 </SigArgSection>
 <Description summary=""Specifies the Conda environment for the step."" extended_summary=""Information in this decorator will augment any\nattributes set in the `@conda_base` flow-level decorator. Hence,\nyou can use `@conda_base` to set common libraries required by all\nsteps and use `@conda` to specify step-specific additions."" />
 <ParamSection name=""Parameters"">
 	<Parameter name=""libraries"" type=""Dict[str, str], default: {}"" desc=""Libraries to use for this step. The key is the name of the package\nand the value is the version to use."" />
 	<Parameter name=""python"" type=""str, optional"" desc=""Version of Python to use, e.g. '3.7.4'. A default value of None means to\nuse the current Python version."" />
 	<Parameter name=""disabled"" type=""bool, default: False"" desc=""If set to True, disables Conda."" />
 </ParamSection>
 </DocSection>",H1,https://docs.metaflow.org/api/step-decorators/conda#conda,False,1355.0,150.0
1486,@batch,"The `@batch` decorator sends a step for execution on the [AWS Batch](https://aws.amazon.com/batch/) compute layer. For more information, see [Executing Tasks Remotely](/scaling/remote-tasks/introduction).
 
 Note that while `@batch` doesn't allow mounting arbitrary disk volumes on the fly, you can create in-memory filesystems easily with `tmpfs` options. For more details, see [using `metaflow.S3` for in-memory processing](/scaling/data#using-metaflows3-for-in-memory-processing).
 
 
 <DocSection type=""decorator"" name=""batch"" module=""metaflow"" show_import=""True"" heading_level=""3"" link=""https://github.com/Netflix/metaflow/tree/master/metaflow/plugins/aws/batch/batch_decorator.py#L34"">
 <SigArgSection>
 <SigArg name=""..."" />
 </SigArgSection>
 <Description summary=""Specifies that this step should execute on [AWS Batch](https://aws.amazon.com/batch/)."" />
 <ParamSection name=""Parameters"">
 	<Parameter name=""cpu"" type=""int, default: 1"" desc=""Number of CPUs required for this step. If `@resources` is\nalso present, the maximum value from all decorators is used."" />
 	<Parameter name=""gpu"" type=""int, default: 0"" desc=""Number of GPUs required for this step. If `@resources` is\nalso present, the maximum value from all decorators is used."" />
 	<Parameter name=""memory"" type=""int, default: 4096"" desc=""Memory size (in MB) required for this step. If\n`@resources` is also present, the maximum value from all decorators is\nused."" />
 	<Parameter name=""image"" type=""str, optional"" desc=""Docker image to use when launching on AWS Batch. If not specified, and\nMETAFLOW_BATCH_CONTAINER_IMAGE is specified, that image is used. If\nnot, a default Docker image mapping to the current version of Python is used."" />
 	<Parameter name=""queue"" type=""str, default: METAFLOW_BATCH_JOB_QUEUE"" desc=""AWS Batch Job Queue to submit the job to."" />
 	<Parameter name=""iam_role"" type=""str, default: METAFLOW_ECS_S3_ACCESS_IAM_ROLE"" desc=""AWS IAM role that AWS Batch container uses to access AWS cloud resources."" />
 	<Parameter name=""execution_role"" type=""str, default: METAFLOW_ECS_FARGATE_EXECUTION_ROLE"" desc=""AWS IAM role that AWS Batch can use [to trigger AWS Fargate tasks]\n(https://docs.aws.amazon.com/batch/latest/userguide/execution-IAM-role.html)."" />
 	<Parameter name=""shared_memory"" type=""int, optional"" desc=""The value for the size (in MiB) of the /dev/shm volume for this step.\nThis parameter maps to the `--shm-size` option in Docker."" />
 	<Parameter name=""max_swap"" type=""int, optional"" desc=""The total amount of swap memory (in MiB) a container can use for this\nstep. This parameter is translated to the `--memory-swap` option in\nDocker where the value is the sum of the container memory plus the\n`max_swap` value."" />
 	<Parameter name=""swappiness"" type=""int, optional"" desc=""This allows you to tune memory swappiness behavior for this step.\nA swappiness value of 0 causes swapping not to happen unless absolutely\nnecessary. A swappiness value of 100 causes pages to be swapped very\naggressively. Accepted values are whole numbers between 0 and 100."" />
 	<Parameter name=""use_tmpfs: bool, default: False"" desc=""This enables an explicit tmpfs mount for this step."" />
 	<Parameter name=""tmpfs_tempdir: bool, default: True"" desc=""sets METAFLOW_TEMPDIR to tmpfs_path if set for this step."" />
 	<Parameter name=""tmpfs_size: int, optional"" desc=""The value for the size (in MiB) of the tmpfs mount for this step.\nThis parameter maps to the `--tmpfs` option in Docker. Defaults to 50% of the\nmemory allocated for this step."" />
 	<Parameter name=""tmpfs_path: string, optional"" desc=""Path to tmpfs mount for this step. Defaults to /metaflow_temp."" />
 	<Parameter name=""inferentia"" type=""int, default: 0"" desc=""Number of Inferentia chips required for this step."" />
 </ParamSection>
 </DocSection>",H1,https://docs.metaflow.org/api/step-decorators/batch#batch,False,3813.0,458.0
1487,@trigger_on_finish,"Use the `@trigger_on_finish` decorator to trigger a flow [deployed on Argo Workflows](/production/scheduling-metaflow-flows/scheduling-with-argo-workflows) when another flow finishes.
 
 Read more in [Triggering Flows Based on Other Flows](/production/event-triggering/flow-events).
 
 
 
 <DocSection type=""decorator"" name=""trigger_on_finish"" module=""metaflow"" show_import=""True"" heading_level=""3"" link=""https://github.com/Netflix/metaflow/tree/master/metaflow/plugins/events_decorator.py#L142"">
 <SigArgSection>
 <SigArg name=""..."" />
 </SigArgSection>
 <Description summary=""Specifies the flow(s) that this flow depends on."" extended_summary=""```\n@trigger_on_finish(flow='FooFlow')\n```\nor\n```\n@trigger_on_finish(flows=['FooFlow', 'BarFlow'])\n```\nThis decorator respects the @project decorator and triggers the flow\nwhen upstream runs within the same namespace complete successfully\n\nAdditionally, you can specify project aware upstream flow dependencies\nby specifying the fully qualified project_flow_name.\n```\n@trigger_on_finish(flow='my_project.branch.my_branch.FooFlow')\n```\nor\n```\n@trigger_on_finish(flows=['my_project.branch.my_branch.FooFlow', 'BarFlow'])\n```"" />
 <ParamSection name=""Parameters"">
 	<Parameter name=""flow"" type=""str, optional"" desc=""Upstream flow dependency for this flow."" />
 	<Parameter name=""flows"" type=""List[str], optional"" desc=""Upstream flow dependencies for this flow."" />
 	<Parameter name=""options"" type=""dict, optional"" desc=""Backend-specific configuration for tuning eventing behavior."" />
 </ParamSection>
 </DocSection>",H1,https://docs.metaflow.org/api/flow-decorators/trigger_on_finish#trigger-on-finish,False,1578.0,121.0
1488,@trigger,"Use the `@trigger` decorator to trigger a flow [deployed on Argo Workflows](/production/scheduling-metaflow-flows/scheduling-with-argo-workflows) based on an external event.
 
 Read more in [Triggering Flows Based on External Events](/production/event-triggering/external-events).
 
 
 
 <DocSection type=""decorator"" name=""trigger"" module=""metaflow"" show_import=""True"" heading_level=""3"" link=""https://github.com/Netflix/metaflow/tree/master/metaflow/plugins/events_decorator.py#L14"">
 <SigArgSection>
 <SigArg name=""..."" />
 </SigArgSection>
 <Description summary=""Specifies the event(s) that this flow depends on."" extended_summary=""```\n@trigger(event='foo')\n```\nor\n```\n@trigger(events=['foo', 'bar'])\n```\n\nAdditionally, you can specify the parameter mappings\nto map event payload to Metaflow parameters for the flow.\n```\n@trigger(event={'name':'foo', 'parameters':{'my_param': 'event_field'})\n```\nor\n```\n@trigger(events=[{'name':'foo', 'parameters':{'my_param_1': 'event_field_1'},\n                 {'name':'bar', 'parameters':{'my_param_2': 'event_field_2'}])\n```"" />
 <ParamSection name=""Parameters"">
 	<Parameter name=""event"" type=""Union[str, dict], optional"" desc=""Event dependency for this flow."" />
 	<Parameter name=""events"" type=""List[Union[str, dict]], optional"" desc=""Events dependency for this flow."" />
 	<Parameter name=""options"" type=""dict, optional"" desc=""Backend-specific configuration for tuning eventing behavior."" />
 </ParamSection>
 </DocSection>",H1,https://docs.metaflow.org/api/flow-decorators/trigger#trigger,False,1486.0,129.0
1489,@project,"The `@project` decorator makes it convenient to create isolated [Metaflow namespaces](/scaling/tagging) and
 corresponding [proudction deployments](/production/scheduling-metaflow-flows/introduction). When
 multiple flows use the same project name, they can fetch data across them safely using [the Client API](/metaflow/client), without interference from other users running the same flows.
 
 By itself, `@project` doesn't change the behavior besides [exposing new attributes in the `current` object](/api/current#project). It alters the flow names when they are deployed in production, allowing multiple parallel deployments.
 
 For more information, see [Coordinating Larger Metaflow Projects](/production/coordinating-larger-metaflow-projects).
 
 
 
 <DocSection type=""decorator"" name=""project"" module=""metaflow"" show_import=""True"" heading_level=""3"" link=""https://github.com/Netflix/metaflow/tree/master/metaflow/plugins/project_decorator.py#L15"">
 <SigArgSection>
 <SigArg name=""..."" />
 </SigArgSection>
 <Description summary=""Specifies what flows belong to the same project."" extended_summary=""A project-specific namespace is created for all flows that\nuse the same `@project(name)`."" />
 <ParamSection name=""Parameters"">
 	<Parameter name=""name"" type=""str"" desc=""Project name. Make sure that the name is unique amongst all\nprojects that use the same production scheduler. The name may\ncontain only lowercase alphanumeric characters and underscores."" />
 </ParamSection>
 </DocSection>",H1,https://docs.metaflow.org/api/flow-decorators/project#project,False,1497.0,154.0
1490,@schedule,"The `@schedule` decorator specifies the times when the flow should be run when deployed to an external orchestrator like AWS Step Functions or Argo Workflows.
 
 For more information, see [Scheduling Metaflow Flows](/production/scheduling-metaflow-flows/introduction).
 
 
 
 <DocSection type=""decorator"" name=""schedule"" module=""metaflow"" show_import=""True"" heading_level=""3"" link=""https://github.com/Netflix/metaflow/tree/master/metaflow/plugins/aws/step_functions/schedule_decorator.py#L5"">
 <SigArgSection>
 <SigArg name=""..."" />
 </SigArgSection>
 <Description summary=""Specifies the times when the flow should be run when running on a\nproduction scheduler."" />
 <ParamSection name=""Parameters"">
 	<Parameter name=""hourly"" type=""bool, default: False"" desc=""Run the workflow hourly."" />
 	<Parameter name=""daily"" type=""bool, default: True"" desc=""Run the workflow daily."" />
 	<Parameter name=""weekly"" type=""bool, default: False"" desc=""Run the workflow weekly."" />
 	<Parameter name=""cron"" type=""str, optional"" desc=""Run the workflow at [a custom Cron schedule](https://docs.aws.amazon.com/eventbridge/latest/userguide/scheduled-events.html#cron-expressions)\nspecified by this expression."" />
 	<Parameter name=""timezone"" type=""str, optional"" desc=""Timezone on which the schedule runs (default: None). Currently supported only for Argo workflows,\nwhich accepts timezones in [IANA format](https://nodatime.org/TimeZones)."" />
 </ParamSection>
 </DocSection>",H1,https://docs.metaflow.org/api/flow-decorators/schedule#schedule,False,1461.0,138.0
1491,@conda_base,"The `@conda_base` decorator specifies what libraries should be made available for all steps of a flow.
 
 The libraries are installed from [Conda repositories](https://anaconda.org/). For more information, see [Managing External Libraries](/scaling/dependencies).
 
 
 
 <DocSection type=""decorator"" name=""conda_base"" module=""metaflow"" show_import=""True"" heading_level=""3"" link=""https://github.com/Netflix/metaflow/tree/master/metaflow/plugins/conda/conda_flow_decorator.py#L7"">
 <SigArgSection>
 <SigArg name=""..."" />
 </SigArgSection>
 <Description summary=""Specifies the Conda environment for all steps of the flow."" extended_summary=""Use `@conda_base` to set common libraries required by all\nsteps and use `@conda` to specify step-specific additions."" />
 <ParamSection name=""Parameters"">
 	<Parameter name=""libraries"" type=""Dict[str, str], default: {}"" desc=""Libraries to use for this flow. The key is the name of the package\nand the value is the version to use."" />
 	<Parameter name=""python"" type=""str, optional"" desc=""Version of Python to use, e.g. '3.7.4'. A default value of None means\nto use the current Python version."" />
 	<Parameter name=""disabled"" type=""bool, default: False"" desc=""If set to True, disables Conda."" />
 </ParamSection>
 </DocSection>",H1,https://docs.metaflow.org/api/flow-decorators/conda_base#conda-base,False,1268.0,141.0
1492,Creating Flows,"This document introduces the basic concepts of Metaflow. If you are eager to try out
 Metaflow in practice, you can start with the [tutorial](../getting-started/tutorials/).
 After the tutorial, you can return to this document to learn more about how Metaflow
 works.",H1,https://docs.metaflow.org/metaflow/basics#creating-flows,False,267.0,40.0
1493,The Structure of Metaflow Code,"Metaflow follows [the dataflow
 paradigm](https://en.wikipedia.org/wiki/Dataflow_programming) which models a program as
 a directed graph of operations. This is a natural paradigm for expressing data
 processing pipelines, machine learning in particular.
 
 We call the graph of operations **a flow**. You define the operations, called **steps**,
 which are nodes of the graph and contain transitions to the next steps, which serve as
 edges.
 
 Metaflow sets some constraints on the structure of the graph. For starters, every flow
 needs a step called `start` and a step called `end`. An execution of the flow, which we
 call **a run**, starts at `start`. The run is successful if the final `end` step
 finishes successfully.
 
 What happens between `start` and `end` is up to you. You can construct the graph in
 between using an arbitrary combination of the following three types of transitions
 supported by Metaflow:",H2,https://docs.metaflow.org/metaflow/basics#the-structure-of-metaflow-code,False,922.0,142.0
1494,Linear,"The most basic type of transition is **a linear** transition. It moves from one step to
 another one.
 
 Here is a graph with two linear transitions:
 
 ![](/assets/graph_linear.png)
 
 The corresponding Metaflow script looks like this:
 
 ```python
 from metaflow import FlowSpec, step
 
 class LinearFlow(FlowSpec):
 
     @step
     def start(self):
         self.my_var = 'hello world'
         self.next(self.a)
 
     @step
     def a(self):
         print('the data artifact is: %s' % self.my_var)
         self.next(self.end)
 
     @step
     def end(self):
         print('the data artifact is still: %s' % self.my_var)
 
 if __name__ == '__main__':
     LinearFlow()
 ```
 
 Save this snippet to a file, `linear.py`. You can execute Metaflow flows on the command
 line like any other Python scripts. Try this:
 
 ```bash
 python linear.py run
 ```
 
 Whenever you see a flow like this in the documentation, just save it in a file and
 execute it like above.",H3,https://docs.metaflow.org/metaflow/basics#linear,False,968.0,207.0
1495,Artifacts,"Besides executing the steps `start`, `a`, and `end` in order, this flow creates **a data
 artifact** called `my_var`. In Metaflow, data artifacts are created simply by assigning
 values to instance variables like `my_var`.
 
 Artifacts are a core concept of Metaflow. They have a number of uses:
 
  - They allow you to manage the data flow through the flow without having to load and
    store data manually.
 
  - All artifacts are persisted so that they can be analyzed later using the [Client
    API](/metaflow/client), visualized with [Cards](/metaflow/visualizing-results), and
    even used across flows.
 
  - Artifacts works consistently across environments, so you can run some steps locally
    and [some steps in the cloud](/scaling/introduction) without having to worry about
    transferring data explicitly.
 
  - Having access to past artifacts greatly helps [debugging](/metaflow/debugging), since
  you can eyeball data before failures and even [resume past
  executions](/metaflow/debugging#how-to-use-the-resume-command) after fixing bugs.
 
 Data artifacts are available in all steps after they have been created, so they behave
 as any normal instance variables. An exception to this rule are branches, as explained
 below.",H3,https://docs.metaflow.org/metaflow/basics#artifacts,False,1246.0,198.0
1496,Branch,"You can express parallel steps with **a branch**. In the figure below, `start`
 transitions to two parallel steps, `a` and `b`. Any number of parallel steps are
 allowed. A benefit of a branch like this is performance: Metaflow can execute `a` and
 `b` over multiple CPU cores or over multiple instances in the cloud.
 
 ![](/assets/graph_branch.png)
 
 ```python
 from metaflow import FlowSpec, step
 
 class BranchFlow(FlowSpec):
 
     @step
     def start(self):
         self.next(self.a, self.b)
 
     @step
     def a(self):
         self.x = 1
         self.next(self.join)
 
     @step
     def b(self):
         self.x = 2
         self.next(self.join)
 
     @step
     def join(self, inputs):
         print('a is %s' % inputs.a.x)
         print('b is %s' % inputs.b.x)
         print('total is %d' % sum(input.x for input in inputs))
         self.next(self.end)
 
     @step
     def end(self):
         pass
 
 if __name__ == '__main__':
     BranchFlow()
 ```
 
 Every branch must be joined. The join step does not need to be called `join` as above
 but it must take an extra argument, like `inputs` above.
 
 In the example above, the value of `x` above is ambiguous: `a` sets it to `1` and `b` to
 `2`. To disambiguate the branches, the join step can refer to a specific step in the
 branch, like `inputs.a.x` above. For convenience, you can also iterate over all steps in
 the branch using `inputs`, as done in the last print statement in the above `join` step.
 For more details, see the section about [data flow through the
 graph](basics#data-flow-through-the-graph).
 
 Note that you can nest branches arbitrarily, that is, you can branch inside a branch.
 Just remember to join all the branches that you create.",H3,https://docs.metaflow.org/metaflow/basics#branch,False,1737.0,381.0
1497,Foreach,"Static branches like above are useful when you know the branches at the development
 time. Alternatively, you may want to branch based on data dynamically. This is the use
 case for **a foreach** branch.
 
 Foreach works similarly like the branch above but instead of creating named step
 methods, many parallel copies of steps inside a foreach loop are executed.
 
 A foreach loop can iterate over any list like `titles` below.
 
 ![](/assets/graph_foreach.png)
 
 ```python
 from metaflow import FlowSpec, step
 
 class ForeachFlow(FlowSpec):
 
     @step
     def start(self):
         self.titles = ['Stranger Things',
                        'House of Cards',
                        'Narcos']
         self.next(self.a, foreach='titles')
 
     @step
     def a(self):
         self.title = '%s processed' % self.input
         self.next(self.join)
 
     @step
     def join(self, inputs):
         self.results = [input.title for input in inputs]
         self.next(self.end)
 
     @step
     def end(self):
         print('\n'.join(self.results))
 
 if __name__ == '__main__':
     ForeachFlow()
 ```
 
 The foreach loop is initialized by specifying a keyword argument `foreach` in
 `self.next()`. The `foreach` argument takes a string that is the name of a list stored
 in an instance variable, like `titles` above.
 
 Steps inside a foreach loop create separate **tasks** to process each item of the list.
 Here, Metaflow creates three parallel tasks for the step `a` to process the three items
 of the `titles` list in parallel. You can access the specific item assigned to a task
 with an instance variable called `input`.
 
 Foreach loops must be joined like static branches. Note that tasks inside a foreach loop
 are not named, so you can only iterate over them with `inputs`. If you want, you can
 assign a value to an instance variable in a foreach step which helps you to identify the
 task.
 
 You can nest foreaches and combine them with branches and linear steps arbitrarily.",H3,https://docs.metaflow.org/metaflow/basics#foreach,False,1998.0,425.0
1498,What should be a step?,"There is not a single right way of structuring code as a graph of steps but here are
 some best practices that you can follow.
 
 Metaflow treats steps as indivisible units of execution. That is, a step either succeeds
 or fails as a whole. After the step has finished successfully, Metaflow persists all
 instance variables that were created in the step code, so the step does not have to be
 executed again even if a subsequent step fails. In other words, you can inspect data
 artifacts that were present when the step finished, but you can not inspect data that
 were manipulated within a step.
 
 This makes a step [a
 checkpoint](https://en.wikipedia.org/wiki/Application_checkpointing). The more granular
 your steps are, the more control you have over inspecting results and resuming failed
 runs.
 
 A downside of making steps too granular is that checkpointing adds some overhead. It
 would not make sense to execute each line of code as a separate step. Keep your steps
 small but not too small. A good rule of thumb is that a single step should not take more
 than an hour to run, preferably much less than that.
 
 Another important consideration is the readability of your code. Try running
 
 ```bash
 python myflow.py show
 ```
 
 which prints out the steps of your flow. Does the overview give you a good idea of your
 code? If the steps are too broad, it might make sense to split them up just to make the
 overall flow more descriptive.",H2,https://docs.metaflow.org/metaflow/basics#what-should-be-a-step,False,1455.0,252.0
1499,How to define parameters for flows?,"Here is an example of a flow that defines a parameter, `alpha`:
 
 ```python
 from metaflow import FlowSpec, Parameter, step
 
 class ParameterFlow(FlowSpec):
     alpha = Parameter('alpha',
                       help='Learning rate',
                       default=0.01)
 
     @step
     def start(self):
         print('alpha is %f' % self.alpha)
         self.next(self.end)
 
     @step
     def end(self):
         print('alpha is still %f' % self.alpha)
 
 if __name__ == '__main__':
     ParameterFlow()
 ```
 
 Parameters are defined by assigning a `Parameter` object to a class variable. Parameter
 variables are automatically available in all steps, like `alpha` above.
 
 You can set the parameter values on the command line as follows:
 
 ```bash
 python parameter_flow.py run --alpha 0.6
 ```
 
 You can see available parameters with:
 
 ```bash
 python parameter_flow.py run --help
 ```
 
 Parameters are typed based on the type of their default value. If there is no meaningful
 default for a parameter, you can define it as follows:
 
 ```python
 num_components = Parameter('num_components',
                            help='Number of components',
                            required=True,
                            type=int)
 ```
 
 Now the flow can not be run without setting `--num_components` to an integer value.
 
 See the API reference for [the `Parameter` class](/api/flowspec#parameters) for more
 information.",H2,https://docs.metaflow.org/metaflow/basics#how-to-define-parameters-for-flows,False,1441.0,353.0
1500,Advanced parameters,"In the example above, `Parameters` took simple scalar values, such as integers or
 floating point values. To support more complex values for `Parameter`, Metaflow allows
 you to specify the value as JSON. This feature comes in handy if your `Parameter` is a
 list of values, a mapping, or a more complex data structure.
 
 This example allows the user to define a GDP by country mapping as a `Parameter`:
 
 ```python
 from metaflow import FlowSpec, Parameter, step, JSONType
 
 class JSONParameterFlow(FlowSpec):
     gdp = Parameter('gdp',
                     help='Country-GDP Mapping',
                     type=JSONType,
                     default='{""US"": 1939}')
 
     country = Parameter('country',
                         help='Choose a country',
                         default='US')
 
     @step
     def start(self):
         print('The GDP of %s is $%dB' % (self.country, self.gdp[self.country]))
         self.next(self.end)
 
     @step
     def end(self):
         pass
 
 if __name__ == '__main__':
     JSONParameterFlow()
 ```
 
 Execute the code as follows:
 
 ```bash
 python parameter_flow.py run --gdp '{""US"": 1}'
 ```
 
 Parameters can also be used to include local files. See the section on
 [IncludeFile](/scaling/data#data-in-local-files) for more information.",H3,https://docs.metaflow.org/metaflow/basics#advanced-parameters,False,1292.0,316.0
1501,Data flow through the graph,"As previously mentioned, for [linear](basics#linear) steps, data artifacts are
 propagated and any linear step can access data artifacts created by previous steps using
 instance variables. In this case, Metaflow can easily determine the value of each
 artifact by simply taking the value of that artifact at the end of the previous step.
 
 In a join step, however, the value of artifacts can potentially be set to different
 values on the incoming branches; the value of the artifact is said to be ambiguous.
 
 To make it easier to implement a join step after foreach or branch, Metaflow provides a
 utility function, `merge_artifacts`, to aid in propagating unambiguous values.
 
 ```python
 from metaflow import FlowSpec, step
 
 class MergeArtifactsFlow(FlowSpec):
 
     @step
     def start(self):
         self.pass_down = 'a'
         self.next(self.a, self.b)
 
     @step
     def a(self):
         self.common = 5
         self.x = 1
         self.y = 3
         self.from_a = 6
         self.next(self.join)
 
     @step
     def b(self):
         self.common = 5
         self.x = 2
         self.y = 4
         self.next(self.join)
 
     @step
     def join(self, inputs):
         self.x = inputs.a.x
         self.merge_artifacts(inputs, exclude=['y'])
         print('x is %s' % self.x)
         print('pass_down is %s' % self.pass_down)
         print('common is %d' % self.common)
         print('from_a is %d' % self.from_a)
         self.next(self.c)
 
     @step
     def c(self):
         self.next(self.d, self.e)
 
     @step
     def d(self):
         self.conflicting = 7
         self.next(self.join2)
 
     @step
     def e(self):
         self.conflicting = 8
         self.next(self.join2)
 
     @step
     def join2(self, inputs):
         self.merge_artifacts(inputs, include=['pass_down', 'common'])
         print('Only pass_down and common exist here')
         self.next(self.end)
 
     @step
     def end(self):
         pass
 
 if __name__ == '__main__':
     MergeArtifactsFlow()
 ```
 
 In the example above, the `merge_artifacts` function behaves as follows:",H2,https://docs.metaflow.org/metaflow/basics#data-flow-through-the-graph,False,2106.0,542.0
1502,Developing with Metaflow,"Every project is a journey. Each stage of the journey presents different challenges and
 requirements. Luckily Metaflow allows you to start experimenting easily without having
 to worry about all details upfront. Rest assured that when your project grows in scale
 and impact, Metaflow helps your code grow accordingly.
 
 A typical project goes through three major stages that are illustrated below. This
 section focuses on the first stage - local development. If you are curious about the
 next two stages, you can take a peek at [Scalable Flows](/scaling/introduction) and
 [Production Deployments](/production/introduction).
 
 ![](/assets/intro-cartoon-1.svg)",H1,https://docs.metaflow.org/metaflow/introduction#developing-with-metaflow,False,665.0,93.0
1503,Developing on a Personal Workstation,"As a data scientist or engineer, your main productivity tool is your personal
 workstation, such as a laptop or a cloud workstation. A great thing about a personal
 computer is that they allow you to iterate quickly and experiment freely knowing that
 you are not accidentally interfering with a colleague's work.
 
 Metaflow treats local development as the first class concern. You can develop and test
 Metaflow code locally like any other Python project or a notebook. Here's what often
 happens in the early phases of a project:
 
 1. Many data scientists are familiar with notebooks that shine at open-ended exploration
    and quick sketching of solutions. When developing with Metaflow, it is totally ok
    (although not required) to use notebooks for analysis. Use the [Metaflow Client
    API](/metaflow/client) to access and organize results of Metaflow runs in a notebook.
 
 2. Once you have a rough idea for the first version of your project, it is useful to
 structure it as a workflow, or *flows* in Metaflow parlance. Metaflow makes this easy:
 You can copy-paste the best parts of a notebook as steps of a Metaflow flow. For details
 why this is a good idea and how to create flows in practice, see [Creating
 Flows](/metaflow/basics).
 
 3. Don't consider flows just as static configuration. They are living and dynamic
 entities that you should be able to execute locally and improve gradually (this is where
 [`resume` comes in handy!](/metaflow/debugging#how-to-use-the-resume-command)). The
 workflow becomes the backbone of your application - in particular helping with [data
 flow through artifacts](/metaflow/basics#artifacts) - which enables much of the
 functionality in the next phases of the project.",H2,https://docs.metaflow.org/metaflow/introduction#developing-on-a-personal-workstation,False,1730.0,278.0
1504,What You Will Learn,"Let's go ahead and learn how to create and test Metaflow flows. This stage covers four
 core topics:
 
  1. [Creating flows](/metaflow/basics)
  2. [Inspecting results of flows](/metaflow/client)
  3. [Visualizing results](/metaflow/visualizing-results)
  4. [Debugging flows](/metaflow/debugging)
 
 These topics work locally on your workstation without any additional infrastructure, so
 it is easy to get started.",H2,https://docs.metaflow.org/metaflow/introduction#what-you-will-learn,False,416.0,56.0
1505,Debugging Flows,"Metaflow wants to make debugging failed flows as painless as possible.
 
 Debugging issues during development is a normal part of the development process. You
 should be able to develop and debug your Metaflow scripts similar to how you develop any
 Python scripts locally.
 
 Debugging a failure can either happen **after** a failed execution or **during**
 execution. In the first case, Metaflow provides two mechanisms:",H1,https://docs.metaflow.org/metaflow/debugging#debugging-flows,False,422.0,66.0
1506,How to debug failed flows,"The process of debugging failed flows is similar both for development-time and
 production-time issues:
 
 1. Identify the step that failed. The failed step is reported as the last line of the
    error report where it is easy to spot.
 2. Identify the run id of the failed run. On the console output, each line is prefixed
    with an identifier like `2/start/21426`. Here, `2` is the run id, `start` is the step
    name, and `21426` is the task id.
 3. Reproduce the failed run with `resume` as [described
    below](debugging#how-to-use-the-resume-command). Confirm that the error message you
    get locally matches to the original error message.
 4. Identify the failed logic inside the failed step. You can do this by adding `print`
    statements in the step until `resume` reveals enough information. Alternatively, you
    can reproduce the faulty logic in a notebook using input data artifacts for the step,
    as described below in the section about
    [notebooks](debugging#inspecting-data-with-a-notebook).
 5. Confirm that the fix works with `resume`. Return to 4 if the error has not been
    fixed.
 6. When the step works locally, rerun the whole flow from `start` to `end` and confirm
    that the fix works as intended.",H2,https://docs.metaflow.org/metaflow/debugging#how-to-debug-failed-flows,False,1241.0,226.0
1507,How to use the `resume` command,"The `resume` command allows you to resume execution of a past run at a failed step.
 Resuming makes it easy to quickly reproduce the failure and iterate on the step code
 until a fix has been found.
 
 Here is how it works. First, save the snippet below :
 
 ```python
 from metaflow import FlowSpec, step
 
 class DebugFlow(FlowSpec):
 
     @step
     def start(self):
         self.next(self.a, self.b)
 
     @step
     def a(self):
         self.x = 1
         self.next(self.join)
 
     @step
     def b(self):
         self.x = int('2fail')
         self.next(self.join)
 
     @step
     def join(self, inputs):
         print('a is %s' % inputs.a.x)
         print('b is %s' % inputs.b.x)
         print('total is %d' % sum(input.x for input in inputs))
         self.next(self.end)
 
     @step
     def end(self):
         pass
 
 if __name__ == '__main__':
     DebugFlow()
 ```
 
 Run the script with:
 
 ```bash
 python debug.py run
 ```
 
 The run should fail. The output should look like:
 
 ```python
 ...
 2018-01-27 22:59:40.313 [3/b/21638 (pid 13720)] File ""debug.py"", line 17, in b
 2018-01-27 22:59:40.313 [3/b/21638 (pid 13720)] self.x = int('2fail')
 2018-01-27 22:59:40.314 [3/b/21638 (pid 13720)] ValueError: invalid literal for int() with base 10: '2fail'
 2018-01-27 22:59:40.314 [3/b/21638 (pid 13720)]
 2018-01-27 22:59:40.361 [3/a/21637 (pid 13719)] Task finished successfully.
 2018-01-27 22:59:40.362 [3/b/21638 (pid 13720)] Task failed.
 2018-01-27 22:59:40.362 Workflow failed.
     Step failure:
     Step b (task-id 21638) failed.
 ```
 
 This shows that the step `b` of the run `3` failed. In your case, the run id could be
 different.
 
 The `resume` command runs the flow similar to `run`. However, in contrast to `run`
 resuming reuses results of every successful step instead of actually running them.
 
 Try it with
 
 ```bash
 python debug.py resume
 ```
 
 Metaflow remembers the run number of the last local run, which in this case is `3`, so
 you should see `resume` reusing results of the run above. Since we have not changed
 anything yet, you should see the above error again but with an incremented run number.
 
 You can also resume a specific run using the CLI option `--origin-run-id` if you don't
 like the default value selected by Metaflow. To get the same behavior as above, you can
 also do:
 
 ```bash
 python debug.py resume --origin-run-id 3
 ```
 
 If you'd like programmatic access to the `--origin-run-id` selected for the `resume`
 (either implicitly selected by Metaflow as last `run` invocation, or explicitly declared
 by the user via the CLI), you can use the `current` singleton. Read more
 [here](/scaling/tagging#accessing-current-ids-in-a-flow).
 
 Next, fix the error by replacing `int('2fail')` in `debug.py` with `int('2')`. Try again
 after the fix. This time, you should see the flow completing successfully.
 
 Resuming uses the flow and step names to decide what results can be reused. This means
 that the results of previously successful steps will get reused even if you change their
 step code. You can add new steps and alter code of failed steps safely with `resume`",H3,https://docs.metaflow.org/metaflow/debugging#how-to-use-the-resume-command,False,3154.0,601.0
1508,Resuming from an arbitrary step,"By default, `resume` resumes from the step that failed, like `b` above. Sometimes fixing
 the failed step requires re-execution of some steps that precede it.
 
 You can choose the step to resume from by specifying the step name on the command line:
 
 ```bash
 python debug.py resume start
 ```
 
 This would resume execution from the step `start`. If you specify a step that comes
 after the step that failed, execution resumes from the failed step - you can't skip over
 steps.",H4,https://docs.metaflow.org/metaflow/debugging#resuming-from-an-arbitrary-step,False,480.0,83.0
1509,Resume and parameters,"If your flow has [`Parameters`](basics#how-to-define-parameters-for-flows), you can't
 change their values when resuming. Changing parameter values could change the results of
 any steps, including those that `resume` skips over, which could result to unexpected
 behavior in subsequent steps.
 
 The `resume` command reuses the parameter values that you set with `run` originally.",H4,https://docs.metaflow.org/metaflow/debugging#resume-and-parameters,False,381.0,51.0
1510,Reproducing production issues locally,"The `resume` command can come in handy when debugging failed production runs too. This
 works exactly the same way as described above: Just specify a production run ID as the
 `--origin-run-id`. Crucially, the resumed producation run executes in your own
 namespace, so it doesn't affect other production runs directly, making it safe to debug,
 test, and iterate on issues locally.
 
 Here's a high-level recipe:
 
  1. You deploy a flow to [a production workflow orchestrator](/production/introduction)
     supported by Metaflow.
  2. A production run fails. Note its run ID, `R`.
  3. To debug the issue, you resume the failed run locally with `resume --origin-run-id
     R`.
  4. You can repeat (3) until the issue has been fixed.
  5. Once the issue has been fixed, you deploy the fixed version to production and
     restart the production run.
 
 To apply the above recipe on your orchestrator of choice, see the following sections:
 
  - [Resuming with Argo
    Workflows](/production/scheduling-metaflow-flows/scheduling-with-argo-workflows#reproducing-failed-production-runs)
  - [Resuming with AWS Step
    Functions](/production/scheduling-metaflow-flows/scheduling-with-aws-step-functions#reproducing-failed-production-runs)
  - [Resuming with Apache
    Airflow](/production/scheduling-metaflow-flows/scheduling-with-airflow#reproducing-failed-production-runs)",H3,https://docs.metaflow.org/metaflow/debugging#reproducing-production-issues-locally,False,1376.0,195.0
1511,Inspecting data with a notebook,"The above example demonstrates a trivial error. In the real life, errors can be much
 trickier to debug. In the case of machine learning, a flow may fail because of an
 unexpected distribution of input data, although nothing is wrong with the code per se.
 
 Being able to inspect data produced by every step is a powerful feature of Metaflow
 which can help in situations like this.
 
 This clip (no audio) demonstrates inspecting values in a flow:
 
 <div style={{position: ""relative"", width: ""100%"", height: 500}}>
 <iframe src=""https://cdn.iframe.ly/3Ffh7OX"" style={{top: 0, left: 0, width: ""100%"", 
 height: ""100%"", position: ""absolute"", border: 0}} allowfullscreen="""" scrolling=""no""
 allow=""accelerometer *; clipboard-write *; encrypted-media *; gyroscope *; picture-in-picture *;""></iframe>
 </div>
 
 In the above clip, you will see:
 
 1. In the flow from the [tutorials](../getting-started/tutorials/) ([Episode
    1](../getting-started/tutorials/season-1-the-local-experience/episode01)), the
    `genre_movies` step calculates an artifact `movies`. We are going to demonstrate how
    this artifact can be inspected after the flow has executed;
 2. In a Jupyter notebook, you can list all the flows and select the latest run of the
    Episode 1 flow;
 3. Further, you can select the `genre_movies` step from this flow and inspect its value.
    As you can see, the value computed at that step is fully available via the [Client
    API](client) and this works for any completed step even steps that completed
    successfully in a failed run.
 
 For more details about the notebook API, see the [Client API](client).",H2,https://docs.metaflow.org/metaflow/debugging#inspecting-data-with-a-notebook,False,1630.0,258.0
1512,Debugging your Flow code using an IDE,"If anything fails in your code, Metaflow prints out the normal Python stack trace
 showing the line of code that caused the error. Typically, this error message provides
 enough information so that you can fix the code using your favorite editor.
 
 Alternatively, you can use a built-in debugger available in many modern IDEs. Since
 Metaflow uses subprocesses to launch steps, the IDE may need some additional
 configuration to handle this properly. We detail the configuration for two popular IDEs
 here. Other IDEs may also work similarly - let us know and we can add information about
 your favorite tool.",H2,https://docs.metaflow.org/metaflow/debugging#debugging-your-flow-code-using-an-ide,False,610.0,100.0
1513,Debugging with PyCharm,"The following steps will allow you to debug your Flow within PyCharm:
 
 1. In the ""Run"" menu, select ""Edit Configurations...""
 2. Create a new configuration with the following items:
    1. Set the ""Script path"" field to point to the absolute path of your Flow script
    2. Set the ""Parameters"" field to ""run""
    3. Set the ""Working directory"" field to the directory containing your Flow script
 3. You can now set your breakpoints as usual in your Flow code and select ""Debug"" from
    the ""Run"" menu.
 
 Note that since Metaflow may launch multiple steps in parallel, you may actually hit
 multiple breakpoints at the same time; you will be able to switch between those
 breakpoints using the dropdown menu (it will say ""MainThread""). You can also restrict
 Metaflow to only execute one step at a time by adding ""--max-workers 1"" to the
 ""Parameters"" field.",H3,https://docs.metaflow.org/metaflow/debugging#debugging-with-pycharm,False,862.0,157.0
1514,Debugging with VSCode,"You can enable debugging of a Flow in VSCode by adjusting your project's configuration
 in `.vscode/launch.json`.
 
 Here is a recording of the end-to-end setup process:
 
 <ReactPlayer controls url=""https://www.youtube.com/watch?v=xWGxDeojqeM"" />
 
 The configuration file as illustrated in the recording is provided below. Make sure you
 are extra careful to update the json structure appropriately if you already have
 existing settings.
 
 ```javascript
 {
     ""version"": ""0.2.0"",
     ""configurations"": [
         {
             ""name"": ""Metaflow Debug"",
             ""type"": ""python"",
             ""request"": ""launch"",
             ""program"": ""${file}"",
             ""args"": [
                 ""run""
             ],
             ""env"": {
                 ""USERNAME"": ""hamel""
             },
             ""subProcess"": true,
             ""console"": ""integratedTerminal""
         }
     ]
 }
 ```
 
 You can now set breakpoints and then select ""Start Debugging"" from the ""Debug"" menu or
 command pallete as illustrated in the recording. Note that since Metaflow may launch
 multiple steps in parallel, you may actually hit multiple breakpoints at the same time;
 you will be able to switch between those breakpoints by selecting the proper function
 stack in the ""Call Stack"" window. You can also restrict Metaflow to only execute one
 step at a time by adding the values ""--max-workers"" and ""1"" to the ""args"" array in the
 configuration.",H3,https://docs.metaflow.org/metaflow/debugging#debugging-with-vscode,False,1443.0,365.0
1515,Combining debugging with resume,"You can naturally combine the techniques described in this section with the ""resume""
 command described previously. Instead of passing ""run"" as the program argument, simply
 pass ""resume"".",H3,https://docs.metaflow.org/metaflow/debugging#combining-debugging-with-resume,False,188.0,27.0
1516,Compatibility with Conda decorator,"The above instructions work even if you use [`@conda`
 decorators](/scaling/dependencies#managing-dependencies-with-conda-decorator) in your
 code; you need, however, to ensure that the `conda` binary is available in your `PATH`.
 The easiest way to do this is to set the `PATH` environment variable to properly include
 the path to the `conda` binary if it is in a non-standard location. In VSCode, you can
 simply add this value in the env section of launch.json and in PyCharm, the UI allows
 you to set environment variables.",H3,https://docs.metaflow.org/metaflow/debugging#compatibility-with-conda-decorator,False,529.0,81.0
1517,Inspecting Flows and Results,"Metaflow provides a client API that is used to inspect results of past runs. It is
 particularly well suited to being used in notebooks.
 
 This document provides an overview of the client API. See the complete API in [the
 Client API reference page](/api/client).",H1,https://docs.metaflow.org/metaflow/client#inspecting-flows-and-results,False,264.0,44.0
1518,Object hierarchy,"Note that all operations in the Client API are filtered by the current namespace, as
 explained in [Organizing Results](/scaling/tagging.md). If you do not get the results
 you expect, make sure you are in the correct namespace. The Client API consults the
 metadata service to gather results, so make sure that the client is properly configured
 to use the correct [metadata provider](/metaflow/client.md#metadata-provider).
 
 ![Object hierarchy](/assets/hierarchy.png)
 
 You can import any of the objects shown above directly from the metaflow package as
 follows (for example):
 
 ```python
 from metaflow import Run
 ```
 
 The root object, `Metaflow`, can be instantiated simply with
 
 ```python
 from metaflow import Metaflow
 mf = Metaflow()
 ```
 
 This is the entry point to all other objects. For instance, you can list all flows that
 have been run in the past with:
 
 ```python
 from metaflow import Metaflow
 print(Metaflow().flows)
 ```",H2,https://docs.metaflow.org/metaflow/client#object-hierarchy,False,954.0,143.0
1519,Navigating the object hierarchy,"Every object listed above follows a consistent interface. All the operations below are
 available in all objects, not just the ones demonstrated.",H2,https://docs.metaflow.org/metaflow/client#navigating-the-object-hierarchy,False,145.0,22.0
1520,Listing children,"You can list child objects of any parent object simply by iterating over the parent:
 
 ```python
 from metaflow import Flow
 flow = Flow('HelloFlow')
 runs = list(flow)
 ```
 
 Expectedly, this works too:
 
 ```python
 from metaflow import Flow
 flow = Flow('HelloFlow')
 for run in flow:
     print(run)
 ```",H3,https://docs.metaflow.org/metaflow/client#listing-children,False,310.0,52.0
1521,Accessing a specific child,"You can access a specific child with square brackets, similar to a key lookup in a
 dictionary. Note that keys are always strings (even if they are numerical IDs):
 
 ```python
 from metaflow import Flow
 flow = Flow('HelloFlow')
 run = flow['2']
 ```",H3,https://docs.metaflow.org/metaflow/client#accessing-a-specific-child,False,251.0,42.0
1522,Accessing a specific object by its address,"Besides navigating from the root downwards, you can instantiate every object directly
 with its fully qualified name, called `pathspec`. Note that also this operation is
 subject to the current namespace, as explained in [Organizing
 Results](/scaling/tagging.md); in short, you will not be able to access a Flow that is
 not the current namespace; the error message returned will make it clear whether an
 object exists and is not in the namespace or does not exist at all.
 
 You can instantiate, for example, a particular flow by its name:
 
 ```python
 from metaflow import Flow
 flow = Flow('HelloFlow')
 ```
 
 You can instantiate a particular run of a flow by its run id:
 
 ```python
 from metaflow import Run
 run = Run('HelloFlow/2')
 ```
 
 And every step in a run by its name:
 
 ```python
 from metaflow import Step
 step = Step('HelloFlow/2/start')
 ```",H3,https://docs.metaflow.org/metaflow/client#accessing-a-specific-object-by-its-address,False,867.0,142.0
1523,Accessing data,"One of the most typical use cases of the client API is to access data artifacts produced
 by runs. Each data artifact is represented by a `DataArtifact` object whose parent is a
 `Task`.
 
 `DataArtifact` is a container object for the actual value. Besides the value,
 `DataArtifact` includes metadata about the artifact, such as its time of creation.
 
 Often you are only interested in the value of an artifact. For this typical use case we
 provide a convenience property `.data` in the `Task` object. The `.data` property
 returns a container which has all artifacts produced by the task as attributes.
 
 For instance, this the shortest way to access a value produced by a step in a run:
 
 ```python
 from metaflow import Step
 print(Step('DebugFlow/2/a').task.data.x)
 ```
 
 Here, we print the value of `self.x` in the step `a` of the run `2` of the flow
 `DebugFlow`.",H3,https://docs.metaflow.org/metaflow/client#accessing-data,False,876.0,147.0
1524,"Adding, removing, and replacing tags","*New in Metaflow 2.7.1: You need to upgrade your Metaflow library and the metadata
 service to benefit from this feature.*
 
 Every run has [a set of tags](/scaling/tagging.md#tagging) attached, that is,
 user-defined annotations. You can add and remove tags as follows:
 
 ```python
 from metaflow import Run
 run = Run('HelloFlow/2')
 run.add_tag('one_tag') # add one tag
 run.add_tags(['another_tag', 'yet_another', 'one_tag']) # add many tags
 print(run.user_tags)
 ```
 
 This will print `one_tag`, `another_tag`, `yet_another`. Note that `one_tag` is added
 twice but since tags are a set, duplicates are ignored.
 
 Removing works symmetrically:
 ```python
 from metaflow import Run
 run = Run('HelloFlow/2')
 run.remove_tag('one_tag') # remove one tag
 run.remove_tags(['another_tag', 'yet_another']) # remove many tags
 ```
 
 You can also replace tags with other tags:
 
 ```python
 from metaflow import Run
 run = Run('HelloFlow/2')
 run.replace_tag('one_tag', 'better_tag')
 run.replace_tags(['yet_another', 'another_tag'], ['better_tag'])
 ```
 
 The replace calls first removes the tags specified as the first argument and then adds
 the tag(s) in the second argument. Crucially, this is guaranteed to be an *atomic
 operation*: If another party lists the tags while replace is running, they won't see a
 partial state between remove and adds.
 
 Note you can perform these operations also on the command line using the `tag` command,
 for instance:
 ```
 python helloflow.py tag add --run-id 2 one_tag
 ```",H3,https://docs.metaflow.org/metaflow/client#adding-removing-and-replacing-tags,False,1521.0,212.0
1525,System tags,"In addition to user-defined tags, Metaflow assigns a handful of *system tags* to runs
 automatically. These tags can be used for filtering and organizing runs, but they can
 not be removed or replaced with other tags.
 
 You can see the set of system tags assigned to a run like this:
 ```python
 from metaflow import Run
 print(Run('HelloFlow/2').system_tags)
 ```
 
 Or the union of system tags and user-defined tags like this:
 ```python
 from metaflow import Run
 print(Run('HelloFlow/2').tags)
 ```",H4,https://docs.metaflow.org/metaflow/client#system-tags,False,503.0,77.0
1526,Common properties,Every object has the following properties available:,H3,https://docs.metaflow.org/metaflow/client#common-properties,False,52.0,7.0
1527,Properties related to flows,"To access an iterator over runs and filter by tags, use the `runs()` method. See
 [Tagging](/scaling/tagging.md#tagging) for more detail.
 
 `Flow` has two additional properties related to determining the latest run for the flow.
 Note that any `Run` returned will be in the current namespace.",H3,https://docs.metaflow.org/metaflow/client#properties-related-to-flows,False,293.0,45.0
1528,Properties related to runs,"To access an iterator over the steps of a run and filter by tags, use the `steps()`
 method. See [Tagging](/scaling/tagging.md#tagging) for more detail.
 
 `Run` also has a few additional properties to make it easy to access commonly used
 information:",H3,https://docs.metaflow.org/metaflow/client#properties-related-to-runs,False,252.0,40.0
1529,Properties related to steps,"A `Step` typically has a single `Task`. A Step will have multiple `Task` objects as its
 children if it is a `foreach` step; each `Task` will correspond to a single execution of
 the `Step`.
 
 To access an iterator over the tasks of a step and filter by tags, use the `tasks()`
 method. See [Tagging](/scaling/tagging.md#tagging) for more detail.
 
 `Step` has a few additional properties as well:",H3,https://docs.metaflow.org/metaflow/client#properties-related-to-steps,False,398.0,67.0
1530,Properties related to tasks,"Since a `Task` is the actual unit of execution in Metaflow, these objects contain the
 richest set of properties:",H3,https://docs.metaflow.org/metaflow/client#properties-related-to-tasks,False,113.0,19.0
1531,Metadata provider,"The Client API relies on a metadata service to gather results appropriately. Metaflow
 supports a local mode (`.metaflow` directory on your filesystem) and a [remote
 mode](https://github.com/Netflix/metaflow-service).
 
 ```python
 from metaflow import get_metadata, metadata",H2,https://docs.metaflow.org/metaflow/client#metadata-provider,False,276.0,33.0
1532,Effortless Task Inspection with Default Cards,"Metaflow comes with a built-in _Default Card_ that shows all artifacts produced by a
 task. Let’s create a simple flow to test it.
 
 ```python
 from metaflow import FlowSpec, Parameter, step, card
 from datetime import datetime
 
 class DefaultCardFlow(FlowSpec):
 
     alpha = Parameter('alpha', default=0.5)
 
     @card
     @step
     def start(self):
         self.example_dict = {'first_key': list(range(10)),
                              'second_key': {'one', 'two'}}
         self.timestamp = datetime.utcnow()
         self.next(self.end)
 
     @step
     def end(self):
         pass
 
 if __name__ == ""__main__"":
     DefaultCardFlow()
 ```
 
 The only new feature introduced in this flow is the `@card` decorator which attaches a
 card to the `start` step. Since no arguments are given to the decorator, the _Default
 Card_ is used. Save the example in `defaultcard.py` and execute the flow as usual:
 
 `python defaultcard.py run`
 
 After the run has finished, you can open a generated card on the command line:
 
 `python defaultcard.py card view start`
 
 The command will open the card in your local web browser. It will look like this:
 
 ![](/assets/card-docs-defaultcard.png)
 
 The _Default Card_ shows basic metadata about the task, parameters given for the flow,
 artifacts accessible in the task, as well as a visualization of the flow DAG. You can
 use this information to quickly observe and verify results of a task without making any
 changes in the code.",H1,https://docs.metaflow.org/metaflow/visualizing-results/effortless-task-inspection-with-default-cards#effortless-task-inspection-with-default-cards,False,1487.0,301.0
1533,**Visualizing Artifacts with the Default Card**,"As shown in the screenshot above, the artifacts table shows all Metaflow artifacts
 related to the task. Large artifacts are truncated for display - you can access the
 originals using [the Client API](../client).
 
 If an artifact contains an image or a dataframe, the artifact is visualized in a
 separate section in addition to its string representation. Take a look at the following
 example which contains an artifact, `self.image` storing an animated GIF and another
 artifact, a Pandas `dataframe`:
 
 ```python
 from metaflow import FlowSpec, Parameter, step
 import requests, pandas, string
 
 URL = ""https://upload.wikimedia.org/wikipedia/commons/4/45/Blue_Marble_rotating.gif""
 
 class FancyDefaultCardFlow(FlowSpec):
 
     image_url = Parameter('image_url', default=URL)
 
     @step
     def start(self):
         self.image = requests.get(self.image_url,
                                   headers={'user-agent': 'metaflow-example'}).content
         self.dataframe = pandas.DataFrame({'lowercase': list(string.ascii_lowercase),
                                            'uppercase': list(string.ascii_uppercase)})
         self.next(self.end)
 
     @step
     def end(self):
         pass
 
 if __name__ == ""__main__"":
     FancyDefaultCardFlow()
 ```
 
 To demonstrate how cards can be attached to runs on the fly, this example doesn’t
 include the `@card` decorator in the code. Save the code to `fancydefaultcard.py` and
 run it as follows:
 
 `python fancydefaultcard.py run --with card`
 
 Note that the example expects that you have the `requests` and `pandas` libraries
 installed. The `--with card` option attaches a `@card` decorator to every step without
 changes in the code. You can execute any existing flow `--with card` to inspect its
 results visually.
 
 You can open the card as before:
 
 `python fancydefaultcard.py card view start`
 
 You will see additional sections in the card which visualize `dataframe` as a table and
 show the image stored in the `image` artifact.
 
 ![](/assets/card-docs-fancydefaultcard.png)
 
 Thanks to this feature, you can use any plotting library such as
 [Matplotlib](https://matplotlib.org) to create arbitrary visualizations in a Metaflow
 task, which are then shown in the _Default Card_ automatically without you having to
 write a line of additional code. You can use this feature during development to quickly
 debug flows.",H2,https://docs.metaflow.org/metaflow/visualizing-results/effortless-task-inspection-with-default-cards#visualizing-artifacts-with-the-default-card,False,2401.0,439.0
1534,Cards Are Stored And Versioned Automatically,"A major benefit of `@card` is that reports produced by it are versioned and stored in
 the Metaflow datastore automatically, alongside their parent task. This way, you or your
 colleagues can easily access any historical card, e.g. a model scorecard associated with
 a particular version of the model.
 
 You can access any historical card on the command line by using a run ID of a past run.
 For instance
 
 `python fancydefaultcard.py card view 1638257165470922/start`
 
 In the case of [foreach](../basics#foreach), a single step can produce multiple tasks
 and cards. You can view an individual card by giving a full task ID (aka _pathspec_)
 corresponding to a task:
 
 `python fancydefaultcard.py card view 1638257165470922/start/1`
 
 You can see all available cards in the latest run with the “`card list`” command:
 
 `python fancydefaultcard.py card list`
 
 It is possible to produce multiple separate cards from a single task by adding multiple
 `@card` decorators in a step, which are all shown by “`card list`”. To make it easier to
 identify specific cards, you can also assign them a unique ID, as described in [Multiple
 Cards In a Step](easy-custom-reports-with-card-components#multiple-cards-in-a-step).",H2,https://docs.metaflow.org/metaflow/visualizing-results/effortless-task-inspection-with-default-cards#cards-are-stored-and-versioned-automatically,False,1223.0,188.0
1535,Accessing Cards via an API,"Besides the command line interface, you can access and view cards programmatically
 through an API. This is particularly convenient, if you want to access cards in a
 Jupyter notebook.
 
 Given a Task ID (a _pathspec_), or [a Task object from the Client API](../client), the
 `get_cards` function lists all cards of the task. You can try this in a notebook cell.
 Replace the Task ID with an actual ID from a previous run:
 
 ```python
 from metaflow.cards import get_cards
 get_cards('CountryModelFlow/1641937201798104/train_country_model/2')
 ```
 
 This will show the card in the output cell:
 
 ![](/assets/card-docs-notebook.png)
 
 The `get_cards` function works well in conjunction with [the Client API](../client). For
 instance, you can use the Client API to search for a task with a specific artifact and
 view its card:
 
 ```python
 run = Run('CountryModelFlow/1641937201798104')
 [brazil] = [task for task in run['train_country_model']
             if task['country'].data == 'Brazil']
 get_cards(brazil)
 ```
 
 ![](/assets/card-docs-clientapi.png)",H2,https://docs.metaflow.org/metaflow/visualizing-results/effortless-task-inspection-with-default-cards#accessing-cards-via-an-api,False,1062.0,158.0
1536,Sharing Cards,"Since cards are self-contained HTML files, they can be easily shared and viewed by
 anyone without having to install additional software. To share a card, first save the
 desired card to a file:
 
 `python fancydefaultcard.py card get start mycard.html`
 
 Use the “`card get`” command to save the HTML without opening it in a browser. You can
 attach the resulting card file, here `mycard.html`, say, in an email or a Slack message.
 If you want to share reports automatically e.g. via email, you can use the `get_cards`
 API discussed above to obtain the HTML programmatically.
 
 Some recipients may prefer a PDF file over HTML. In this case, you can simply choose
 “Print” in your browser followed by “Save as PDF”.",H2,https://docs.metaflow.org/metaflow/visualizing-results/effortless-task-inspection-with-default-cards#sharing-cards,False,719.0,122.0
1537,Easy Custom Reports with Card Components,"_Default Cards_ are useful during development when you need to quickly inspect artifacts
 produced by a task or visualize the overall structure of the flow. As your project
 progresses, you may want to create a custom card that highlights information specific to
 your project.
 
 The easiest way to create a custom card is to use built-in components: _Images_,
 _Tables_, _Artifacts_, and _Markdown_ text. You can construct a report with these
 components in Python without having to worry about HTML or styling in CSS. Rest assured
 that if components ever show their limits, you have an option to customize reports even
 further using [_Card Templates_](advanced-shareable-cards-with-card-templates).
 
 Let’s start with a simple example:
 
 ```python
 from metaflow import FlowSpec, step, card, Parameter, current
 from metaflow.cards import Markdown
 
 class GuessCardFlow(FlowSpec):
 
     number = Parameter('number', default=3)
 
     @card(type='blank')
     @step
     def start(self):
         current.card.append(Markdown(""# Guess my number""))
         if self.number > 5:
             current.card.append(Markdown(""My number is **smaller** ⬇️""))
         elif self.number < 5:
             current.card.append(Markdown(""My number is **larger** ⬆️""))
         else:
             current.card.append(Markdown(""## Correct! 🎉""))
         self.next(self.end)
 
     @step
     def end(self):
         pass
 
 if __name__ == ""__main__"":
     GuessCardFlow()
 ```
 
 Notice how in the `@card` decorator we specify `type=’blank’.` Instead of the _Default
 Card_, we want an empty card with no content by default. The `blank` card provides a
 nice empty canvas for custom components.
 
 The `current.card.append` call adds a component in the card. Each component occupies a
 row in the card, so you don’t have to worry about the layout. If you run
 `GuessCardFlow`, you will see a card like below. The exact content depends on the value
 of the number parameter.
 
 ![](/assets/card-docs-guess.png)
 
 Currently, the following components are provided:",H1,https://docs.metaflow.org/metaflow/visualizing-results/easy-custom-reports-with-card-components#easy-custom-reports-with-card-components,False,2055.0,384.0
1538,Showing Plots,"A data scientist may care more about showing data visualizations rather than photos of
 cats. Technically there isn’t a huge difference: You can use any existing visualization
 library in Python to produce plots, save the resulting image in a file or an in-memory
 object, and provide the contents of the file (bytes) to the `Image` component.
 
 For convenience, the `Image` component provides a utility method,
 `Image.from_matplotlib`, that extracts bytes from a [Matplotlib](https://matplotlib.org)
 figure automatically. Here’s an example that uses the [@conda
 decorator](/scaling/dependencies) to make sure that Matplotlib is available. If you have
 Matplotlib and Numpy already installed in your environment, you can run the example
 without `@conda_base`.
 
 ```python
 from metaflow import FlowSpec, step, current, card, conda_base
 from metaflow.cards import Image
 
 @conda_base(python='3.8.1',
             libraries={'numpy':'1.20.3', 'matplotlib':'3.4.2'})
 class PlotDemoFlow(FlowSpec):
 
     @card(type='blank')
     @step
     def start(self):
         import matplotlib.pyplot as plt
         import numpy
         fig = plt.figure()
         x = numpy.random.normal(0, 0.1, 100000)
         y = numpy.random.normal(0, 0.1, 100000)
         plt.scatter(x, y, s=0.1, color=(0.2, 0.2, 1.0, 0.2))
         current.card.append(Image.from_matplotlib(fig))
         self.next(self.end)
 
     @step
     def end(self):
         pass
 
 if __name__ == '__main__':
     PlotDemoFlow()
 ```
 
 The resulting card will look like this:
 
 ![](/assets/card-docs-plot.png)
 
 Note that you can click the image in the card to see a larger version of it.",H2,https://docs.metaflow.org/metaflow/visualizing-results/easy-custom-reports-with-card-components#showing-plots,False,1659.0,309.0
1539,Multiple Cards In a Step,"You may want to produce multiple separate cards in a step. Maybe one card shows
 high-level business metrics that are suitable for wide distribution, while another shows
 technical details for debugging purposes.
 
 When multiple cards are present, calling `current.card.append` is ambiguous: As such, it
 doesn’t know which of the many cards the component should be added to. Metaflow will
 show a warning if you try to do this, but it won’t crash the flow - nothing card-related
 should ever cause the flow to crash.
 
 Use the id keyword argument in the `@card` decorator to uniquely identify each card.
 Then, you can refer to a specific card with the `current.card[card_id].append` notation.
 Here’s an example:
 
 ```python
 from metaflow import FlowSpec, step, current, card
 from metaflow.cards import Markdown
 
 class ManyCardsFlow(FlowSpec):
 
     @card(type='blank', id='first')
     @card(type='blank', id='second')
     @step
     def start(self):
         current.card['first'].append(
             Markdown('# I am the first card'))
         current.card['second'].append(
             Markdown('# I am the second card'))
         self.next(self.end)
 
     @step
     def end(self):
         pass
 
 if __name__ == '__main__':
     ManyCardsFlow()
 ```
 
 When a task has multiple cards, the “`card view`” command will list all cards that are
 viewable for the task. You must specify which exact card you want to view:",H2,https://docs.metaflow.org/metaflow/visualizing-results/easy-custom-reports-with-card-components#multiple-cards-in-a-step,False,1436.0,281.0
1540,Comparing Data Across Runs,"In many cases, you may want to produce a single card that characterizes the results of
 the whole flow. A natural way to do this is to assign a card to the `end` step that has
 access to all results produced by a run.
 
 Besides accessing all results of a single run, you may want to access results across
 multiple runs and produce a card that compares the latest data to past results. Thanks
 to the fact that Metaflow persists and versions all results, this can be done easily:
 Just use [the Client API](../client) to access past results.
 
 The following example demonstrates how you can create a card that accesses all data
 produced by a flow at the `end` step, as well as compares results across historical
 runs.
 
 ```python
 from metaflow import FlowSpec, step, current, card, conda_base, Flow, Parameter
 from metaflow.cards import Image, Table, Artifact
 from itertools import islice
 
 @conda_base(python='3.8.1',
             libraries={'numpy':'1.20.3', 'matplotlib':'3.4.2'})
 class CompareRunsFlow(FlowSpec):
 
     alpha = Parameter('alpha', default=0.1)
 
     @step
     def start(self):
         import numpy as np
         self.x = np.linspace(-1, 2, 100)
         self.y = self.alpha * np.exp(self.x)
         self.next(self.end)
 
     @card(type='blank')
     @step
     def end(self):
         self.compare_runs()
 
     def compare_runs(self):
         import matplotlib.pyplot as plt
         rows = []
         fig = plt.figure()
         for run in islice(Flow('CompareRunsFlow'), 3):
             data = run['start'].task.data
             rows.append(list(map(Artifact, (run.id,
                                             run.created_at,
                                             data.alpha))))
             plt.plot(data.x, data.y, label=run.id)
         plt.legend()
         current.card.append(Table(rows,\
             headers=['Run ID', 'Created', 'Alpha']))
         current.card.append(Image.from_matplotlib(fig))
 
 if __name__ == '__main__':
     CompareRunsFlow()
 ```
 
 To see the comparison in action, run the flow at least three times with varying values
 of the `–alpha` parameter. Note the following features of the flow:",H2,https://docs.metaflow.org/metaflow/visualizing-results/easy-custom-reports-with-card-components#comparing-data-across-runs,False,2176.0,534.0
1541,"Advanced, Shareable Cards with Card Templates","The built-in [_Card Components_](easy-custom-reports-with-card-components) allow you to
 create a visual report with a few lines of Python code. This is by far the easiest way
 to output visualizations using Metaflow’s default visual style and layout.
 
 This section describes a more advanced concept of _Card Templates_ which are more
 flexible than [_Default Cards_](effortless-task-inspection-with-default-cards) and Card
 Components, but they require more upfront effort to create. However, using an existing
 Card Template is very easy, as shown below. They are a good match for use cases such as",H1,https://docs.metaflow.org/metaflow/visualizing-results/advanced-shareable-cards-with-card-templates#advanced-shareable-cards-with-card-templates,False,602.0,86.0
1542,Using a Card Template,"A _Card Template_ is a normal Python package, hosted in a Git repository of its own,
 optionally published to a private or public package repository. By convention, public
 Card Templates have a `metaflow-card` prefix, so you can easily [find public card
 templates on PyPi](https://pypi.org/search/?q=metaflow-card-&o=).
 
 Let’s test a public template,
 [metaflow-card-html](https://github.com/outerbounds/metaflow-card-html), which simply
 converts HTML stored in an artifact to a card. First, install the template using `pip`:
 
 `pip install metaflow-card-html`
 
 Now we can use the card in any flow by adding a decorator, `@card(type=’html’)`. The
 type attribute refers to the template name. Let’s test it:
 
 ```python
 from metaflow import FlowSpec, step, card
 
 class HtmlCardFlow(FlowSpec):
 
     @card(type='html')
     @step
     def start(self):
         self.html = """"""
         <html>
           <body style='color: blue'>
             Hello World!
           </body>
         </html>
         """"""
         self.next(self.end)
 
     @step
     def end(self):
         pass
 
 if __name__ == ""__main__"":
     HtmlCardFlow()
 ```
 
 Note that this a just a simple example what a custom template can do. Other custom
 templates don't require writing HTML by hand. Save the flow in `htmlcardflow.py`. Then,
 you can run it
 
 `python htmlcardflow.py run`
 
 and view the card
 
 `python htmlcardflow.py card view start`
 
 You should see a blank page with a blue “Hello World!” text.
 
 ![](</assets/card-docs-html_(2).png>)
 
 A particularly useful feature of card templates is that they work in any compute
 environment, even when [executing tasks remotely](/scaling/remote-tasks/introduction).
 For instance, if you have AWS Batch set up, you can run the flow as follows:
 
 `python htmlcardflow.py run --with batch`
 
 The card will get produced without you having to worry about installing anything on the
 remote instances! You can [deploy flows to
 production](../../production/scheduling-metaflow-flows/introduction/) with custom
 templates too:
 
 `python htmlcardflow.py step-functions create`
 
 Now, every time a production run executes, cards will get produced exactly as during
 prototyping. Behind the scenes, Metaflow takes care of packaging any card templates
 whenever you execute code remotely.",H2,https://docs.metaflow.org/metaflow/visualizing-results/advanced-shareable-cards-with-card-templates#using-a-card-template,False,2329.0,415.0
1543,Developing a Card Template,"If you want to develop a card template of your own, it is useful to have a mental model
 of how cards work under the hood:
 
 ![](</assets/card-docs-template_(1).png>)
 
 The blue box is a Metaflow task executing a step from the user’s flow. It is decorated
 with a `@card` decorator that has a `type` attribute referring to your custom template,
 e.g. `mycard`. The task executes before the card template. After the task has finished,
 a new subprocess is started that executes a card template. This ensures that even if the
 template fails for any reason, it won’t crash the task.
 
 The card template is given the Task ID of the task that the card corresponds to. Using
 this Task ID, the template can use [the Client API](../client) to query any information
 about the task, its parent run, and any past runs. Using this information, the template
 needs to output a single stand-alone HTML file - the actual card. Note that the HTML
 file can’t depend on any other local files. In particular, you must include any images
 as [Data URIs](https://css-tricks.com/data-uris/) in the file itself.
 
 The template itself is a Python class, derived from _MetaflowCard_, which needs to
 implement one method, _render_, which is given [a Task object from the Client
 API](../client) - see [the `MetaflowCard` API
 reference](/api/cards#defining-a-custom-card) for details.
 
 This is the complete implementation of the `@card(type='html')` which we used above:
 
 ```python
 from metaflow.cards import MetaflowCard
 
 class HTMLCard(MetaflowCard):
 
     type = 'html'
 
     def __init__(self, options={""artifact"":""html""}, **kwargs):
         self._attr_nm = options.get(""artifact"", ""html"")
 
     def render(self, task):
         if self._attr_nm in task:
             return str(task[self._attr_nm].data)
 
 CARDS = [HTMLCard]
 ```
 
 The example above used the default `self.html` artifact to pass HTML code to the
 template. You can choose another artifact by specifying an artifact name in the
 _options_ dictionary that is passed to the template: `@card(type='html',
 options={'artifact': 'other_html')`.
 
 The _render_ method needs to return a self-contained HTML as a string. This template has
 it easy, since all it has to do is to return the user-defined artifact. Other templates
 can do much more complex processing to produce a suitable HTML page.
 
 To implement and publish a template of your own, take a look at the
 [metaflow-card-html](https://github.com/outerbounds/metaflow-card-html/) repository
 which shows how to structure the package, as well as step-by-step instructions on how to
 create one of your own. If you create a Card Template that other people might benefit
 from, let our [Slack community](http://slack.outerbounds.co) know about it!",H2,https://docs.metaflow.org/metaflow/visualizing-results/advanced-shareable-cards-with-card-templates#developing-a-card-template,False,2767.0,450.0
1544,Managing Dependencies in Card Templates.,"Card templates may rely on 3rd party libraries for their functionality, say, to produce
 advanced visualizations. To make sure the card can be rendered in remote environments
 that might not have all dependencies already installed, Metaflow takes care of packaging
 any files included directly in the template itself. However, it can’t handle 3rd party
 dependencies automatically. Hence, to make sure your template works without friction,
 you need to pay attention to its dependencies.
 
 Here are recommended strategies for handling 3rd party library dependencies in card
 templates:
 
 1. You can rely on Javascript libraries to move functionality to the frontend side. For
    instance, instead of producing visualizations in Python, you can produce them in
    Javascript. Take a look at
    [metaflow-card-uplot-timeseries](https://github.com/outerbounds/metaflow-card-uplot-timeseries)
    template to see how to use a Javascript library in your template.
 2. You can include small Python libraries in the template package itself, aka _vendor_
    them.
 
 If these approaches don’t work, you can instruct users to include the dependencies of
 the template in their [@conda libraries](/scaling/dependencies). For templates shared
 privately, you may also rely on dependencies included in a Docker image shared by all
 users and `@batch` executions.",H4,https://docs.metaflow.org/metaflow/visualizing-results/advanced-shareable-cards-with-card-templates#managing-dependencies-in-card-templates,False,1356.0,206.0
1545,limitations under the License.,,H2,https://huggingface.co/docs/accelerate/README#limitations-under-the-license,False,0.0,1.0
1546,Generating the documentation,"To generate the documentation, you first have to build it. Several packages are necessary to build the doc, 
 you can install them with the following command, at the root of the code repository:
 
 ```bash
 pip install -e "".[docs]""
 ```
 
 Then you need to install our special tool that builds the documentation:
 
 ```bash
 pip install git+https://github.com/huggingface/doc-builder
 ```
 
 **NOTE**
 
 You only need to generate the documentation to inspect it locally (if you're planning changes and want to
 check how they look before committing for instance). You don't have to commit the built documentation.",H1,https://huggingface.co/docs/accelerate/README#generating-the-documentation,False,613.0,97.0
1547,Building the documentation,"Once you have setup the `doc-builder` and additional packages, you can generate the documentation by 
 typing the following command:
 
 ```bash
 doc-builder build accelerate docs/source/ --build_dir ~/tmp/test-build
 ```
 
 You can adapt the `--build_dir` to set any temporary folder that you prefer. This command will create it and generate
 the MDX files that will be rendered as the documentation on the main website. You can inspect them in your favorite
 Markdown editor.",H2,https://huggingface.co/docs/accelerate/README#building-the-documentation,False,476.0,73.0
1548,Previewing the documentation,"To preview the docs, first install the `watchdog` module with:
 
 ```bash
 pip install watchdog
 ```
 
 Then run the following command:
 
 ```bash
 doc-builder preview {package_name} {path_to_docs}
 ```
 
 For example:
 
 ```bash
 doc-builder preview accelerate docs/source/
 ```
 
 The docs will be viewable at [http://localhost:3000](http://localhost:3000). You can also preview the docs once you have opened a PR. You will see a bot add a comment to a link where the documentation with your changes lives.
 
 **NOTE**
 
 The `preview` command only works with existing doc files. When you add a completely new file, you need to update `_toctree.yml` & restart `preview` command (`ctrl-c` to stop it & call `doc-builder preview ...` again).",H2,https://huggingface.co/docs/accelerate/README#previewing-the-documentation,False,741.0,115.0
1549,Adding a new element to the navigation bar,"Accepted files are Markdown (.md).
 
 Create a file with its extension and put it in the source directory. You can then link it to the toc-tree by putting
 the filename without the extension in the [`_toctree.yml`](https://github.com/huggingface/accelerate/blob/main/docs/source/_toctree.yml) file.",H2,https://huggingface.co/docs/accelerate/README#adding-a-new-element-to-the-navigation-bar,False,298.0,38.0
1550,Renaming section headers and moving sections,"It helps to keep the old links working when renaming the section header and/or moving sections from one document to another. This is because the old links are likely to be used in Issues, Forums, and Social media and it'd make for a much more superior user experience if users reading those months later could still easily navigate to the originally intended information.
 
 Therefore, we simply keep a little map of moved sections at the end of the document where the original section was. The key is to preserve the original anchor.
 
 So if you renamed a section from: ""Section A"" to ""Section B"", then you can add at the end of the file:
 
 ```
 Sections that were moved:
 
 [ <a href=""#section-b"">Section A</a><a id=""section-a""></a> ]
 ```
 and of course, if you moved it to another file, then:
 
 ```
 Sections that were moved:
 
 [ <a href=""../new-file#section-b"">Section A</a><a id=""section-a""></a> ]
 ```
 
 Use the relative style to link to the new file so that the versioned docs continue to work.",H2,https://huggingface.co/docs/accelerate/README#renaming-section-headers-and-moving-sections,False,1007.0,174.0
1551,Writing Documentation - Specification,"The `huggingface/accelerate` documentation follows the
 [Google documentation](https://sphinxcontrib-napoleon.readthedocs.io/en/latest/example_google.html) style for docstrings,
 although we can write them directly in Markdown.",H2,https://huggingface.co/docs/accelerate/README#writing-documentation-specification,False,227.0,18.0
1552,Adding a new tutorial,Adding a new tutorial or section is done in two steps:,H3,https://huggingface.co/docs/accelerate/README#adding-a-new-tutorial,False,54.0,11.0
1553,Writing source documentation,"Values that should be put in `code` should either be surrounded by backticks: \`like so\`. Note that argument names
 and objects like True, None, or any strings should usually be put in `code`.
 
 When mentioning a class, function, or method, it is recommended to use our syntax for internal links so that our tool
 adds a link to its documentation with this syntax: \[\`XXXClass\`\] or \[\`function\`\]. This requires the class or 
 function to be in the main package.
 
 If you want to create a link to some internal class or function, you need to
 provide its path. For instance: \[\`utils.gather\`\]. This will be converted into a link with
 `utils.gather` in the description. To get rid of the path and only keep the name of the object you are
 linking to in the description, add a ~: \[\`~utils.gather\`\] will generate a link with `gather` in the description.
 
 The same works for methods so you can either use \[\`XXXClass.method\`\] or \[~\`XXXClass.method\`\].",H3,https://huggingface.co/docs/accelerate/README#writing-source-documentation,False,971.0,163.0
1554,Defining arguments in a method,"Arguments should be defined with the `Args:` (or `Arguments:` or `Parameters:`) prefix, followed by a line return and
 an indentation. The argument should be followed by its type, with its shape if it is a tensor, a colon, and its
 description:
 
 ```
     Args:
         n_layers (`int`): The number of layers of the model.
 ```
 
 If the description is too long to fit in one line (more than 119 characters in total), another indentation is necessary 
 before writing the description after the argument.
 
 Finally, to maintain uniformity if any *one* description is too long to fit on one line, the 
 rest of the parameters should follow suit and have an indention before their description.
 
 Here's an example showcasing everything so far:
 
 ```
     Args:
         gradient_accumulation_steps (`int`, *optional*, default to 1):
             The number of steps that should pass before gradients are accumulated. A number > 1 should be combined with `Accelerator.accumulate`.
         cpu (`bool`, *optional*):
             Whether or not to force the script to execute on CPU. Will ignore GPU available if set to `True` and force the execution on one process only.
 ```
 
 For optional arguments or arguments with defaults we follow the following syntax: imagine we have a function with the
 following signature:
 
 ```
 def my_function(x: str = None, a: float = 1):
 ```
 
 then its documentation should look like this:
 
 ```
     Args:
         x (`str`, *optional*):
             This argument controls ... and has a description longer than 119 chars.
         a (`float`, *optional*, defaults to 1):
             This argument is used to ... and has a description longer than 119 chars.
 ```
 
 Note that we always omit the ""defaults to \`None\`"" when None is the default for any argument. Also note that even
 if the first line describing your argument type and its default gets long, you can't break it on several lines. You can
 however write as many lines as you want in the indented description (see the example above with `input_ids`).",H4,https://huggingface.co/docs/accelerate/README#defining-arguments-in-a-method,False,2053.0,428.0
1555,Writing a multi-line code block,"Multi-line code blocks can be useful for displaying examples. They are done between two lines of three backticks as usual in Markdown:
 
 
 ````
 ```python",H4,https://huggingface.co/docs/accelerate/README#writing-a-multi-line-code-block,False,155.0,26.0
1556,first line of code,,H1,https://huggingface.co/docs/accelerate/README#first-line-of-code,False,0.0,1.0
1557,second line,,H1,https://huggingface.co/docs/accelerate/README#second-line,False,0.0,1.0
1558,etc,"```
 ````",H1,https://huggingface.co/docs/accelerate/README#etc,False,9.0,2.0
1559,Writing a return block,"The return block should be introduced with the `Returns:` prefix, followed by a line return and an indentation.
 The first line should be the type of the return, followed by a line return. No need to indent further for the elements
 building the return.
 
 Here's an example of a single value return:
 
 ```
     Returns:
         `List[int]`: A list of integers in the range [0, 1] --- 1 for a special token, 0 for a sequence token.
 ```
 
 Here's an example of a tuple return, comprising several objects:
 
 ```
     Returns:
         `tuple(torch.FloatTensor)` comprising various elements depending on the configuration ([`BertConfig`]) and inputs:
         - ** loss** (*optional*, returned when `masked_lm_labels` is provided) `torch.FloatTensor` of shape `(1,)` --
           Total loss is the sum of the masked language modeling loss and the next sequence prediction (classification) loss.
         - **prediction_scores** (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`) --
           Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).
 ```",H4,https://huggingface.co/docs/accelerate/README#writing-a-return-block,False,1129.0,219.0
1560,Styling the docstring,,H2,https://huggingface.co/docs/accelerate/README#styling-the-docstring,False,0.0,1.0
1561,Writing documentation examples,"The syntax for Example docstrings can look as follows:
 
 ```
     Example:
 
     ```python
     >>> import time
     >>> from accelerate import Accelerator
     >>> accelerator = Accelerator()
     >>> if accelerator.is_main_process:
     ...     time.sleep(2)
     >>> else:
     ...     print(""I'm waiting for the main process to finish its sleep..."")
     >>> accelerator.wait_for_everyone()
     >>> # Should print on every process at the same time
     >>> print(""Everyone is here"")
     ```
 ```
 
 The docstring should give a minimal, clear example of how the respective function 
 is to be used in inference and also include the expected (ideally sensible)
 output.
 Often, readers will try out the example before even going through the function 
 or class definitions. Therefore, it is of utmost importance that the example 
 works as expected.",H2,https://huggingface.co/docs/accelerate/README#writing-documentation-examples,False,855.0,182.0
1562,rendered properly in your Markdown viewer.,,H2,https://huggingface.co/docs/accelerate/source/quicktour#rendered-properly-in-your-markdown-viewer,False,0.0,1.0
1563,Quick tour,Let's have a look at the 🤗 Accelerate main features and traps to avoid.,H1,https://huggingface.co/docs/accelerate/source/quicktour#quick-tour,False,71.0,14.0
1564,Main use,"To use 🤗 Accelerate in your own script, you have to change four things:
 
 1. Import the [`Accelerator`] main class and instantiate one in an `accelerator` object:
 
 ```python
 from accelerate import Accelerator
 
 accelerator = Accelerator()
 ```
 
 This should happen as early as possible in your training script as it will initialize everything necessary for
 distributed training. You don't need to indicate the kind of environment you are in (just one machine with a GPU, one
 machines with several GPUs, several machines with multiple GPUs or a TPU), the library will detect this automatically.
 
 2. Remove the call `.to(device)` or `.cuda()` for your model and input data. The `accelerator` object
 will handle this for you and place all those objects on the right device for you. If you know what you're doing, you
 can leave those `.to(device)` calls but you should use the device provided by the `accelerator` object:
 `accelerator.device`.
 
 To fully deactivate the automatic device placement, pass along `device_placement=False` when initializing your
 [`Accelerator`].
 
 <Tip warning={true}>
 
     If you place your objects manually on the proper device, be careful to create your optimizer after putting your
     model on `accelerator.device` or your training will fail on TPU.
 
 </Tip>
 
 3. Pass all objects relevant to training (optimizer, model, training dataloader, learning rate scheduler) to the
 [`~Accelerator.prepare`] method. This will make sure everything is ready for training.
 
 ```python
 model, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(
     model, optimizer, train_dataloader, lr_scheduler
 )
 ```
 
 In particular, your training dataloader will be sharded across all GPUs/TPU cores available so that each one sees a
 different portion of the training dataset. Also, the random states of all processes will be synchronized at the
 beginning of each iteration through your dataloader, to make sure the data is shuffled the same way (if you decided to
 use `shuffle=True` or any kind of random sampler).
 
 <Tip>
 
     The actual batch size for your training will be the number of devices used multiplied by the batch size you set in
     your script: for instance training on 4 GPUs with a batch size of 16 set when creating the training dataloader will
     train at an actual batch size of 64.
 
 </Tip>
 
 Alternatively, you can use the option `split_batches=True` when creating and initializing your
 [`Accelerator`], in which case the batch size will always stay the same, whether you run your
 script on 1, 2, 4, or 64 GPUs.
 
 You should execute this instruction as soon as all objects for training are created, before starting your actual
 training loop.
 
 <Tip warning={true}>
 
     You should only pass the learning rate scheduler to [`~Accelerator.prepare`] when the scheduler needs to be stepped
     at each optimizer step.
 
 </Tip>
 
 <Tip warning={true}>
 
     Your training dataloader may change length when going through this method: if you run on X GPUs, it will have its
     length divided by X (since your actual batch size will be multiplied by X), unless you set
     `split_batches=True`.
 
 </Tip>
 
 Any instruction using your training dataloader length (for instance if you want to log the number of total training
 steps) should go after the call to [`~Accelerator.prepare`].
 
 You can perfectly send your dataloader to [`~Accelerator.prepare`] on its own, but it's best to send the
 model and optimizer to [`~Accelerator.prepare`] together.
 
 You may or may not want to send your validation dataloader to [`~Accelerator.prepare`], depending on
 whether you want to run distributed evaluation or not (see below).
 
 4. Replace the line `loss.backward()` by `accelerator.backward(loss)`.
 
 And you're all set! With all these changes, your script will run on your local machine as well as on multiple GPUs or a
 TPU! You can either use your favorite tool to launch the distributed training, or you can use the 🤗 Accelerate
 launcher.",H2,https://huggingface.co/docs/accelerate/source/quicktour#main-use,False,4028.0,676.0
1565,Distributed evaluation,"You can perform regular evaluation in your training script, if you leave your validation dataloader out of the
 [`~Accelerator.prepare`] method. In this case, you will need to put the input data on the
 `accelerator.device` manually.
 
 To perform distributed evaluation, send along your validation dataloader to the [`~Accelerator.prepare`]
 method:
 
 ```python
 validation_dataloader = accelerator.prepare(validation_dataloader)
 ```
 
 As for your training dataloader, it will mean that (should you run your script on multiple devices) each device will
 only see part of the evaluation data. This means you will need to group your predictions together. This is very easy to
 do with the [`~Accelerator.gather_for_metrics`] method.
 
 ```python
 for inputs, targets in validation_dataloader:
     predictions = model(inputs)
     # Gather all predictions and targets
     all_predictions, all_targets = accelerator.gather_for_metrics((predictions, targets))
     # Example of use with a *Datasets.Metric*
     metric.add_batch(all_predictions, all_targets)
 ```
 
 <Tip warning={true}>
 
     Similar to the training dataloader, passing your validation dataloader through
     [`~Accelerator.prepare`] may change it: if you run on X GPUs, it will have its length divided by X
     (since your actual batch size will be multiplied by X), unless you set `split_batches=True`.
 
 </Tip>
 
 Any instruction using your training dataloader length (for instance if you need the number of total training steps
 to create a learning rate scheduler) should go after the call to [`~Accelerator.prepare`]. 
 
 Some data at the end of the dataset may be duplicated so the batch can be divided equally among all workers. As a result, metrics
 should be calculated through the [`~Accelerator.gather_for_metrics`] method to automatically remove the duplicated data while gathering.
 
 <Tip>
 
     If for some reason you don't wish to have this automatically done, [`~Accelerator.gather`] can be used instead to gather 
     the data across all processes and this can manually be done instead.
 
 </Tip>
 
 
 <Tip warning={true}>
 
     The [`~Accelerator.gather`] and [`~Accelerator.gather_for_metrics`] methods require the tensors to be all the same size on each process. If
     you have tensors of different sizes on each process (for instance when dynamically padding to the maximum length in
     a batch), you should use the [`~Accelerator.pad_across_processes`] method to pad you tensor to the
     biggest size across processes.
 
 </Tip>",H2,https://huggingface.co/docs/accelerate/source/quicktour#distributed-evaluation,False,2534.0,411.0
1566,Launching your distributed script,"You can use the regular commands to launch your distributed training (like `torch.distributed.run` for
 PyTorch), they are fully compatible with 🤗 Accelerate.
 
 🤗 Accelerate also provides a CLI tool that unifies all launchers, so you only have to remember one command. To use it,
 just run:
 
 ```bash
 accelerate config
 ```
 
 on your machine and reply to the questions asked. This will save a *default_config.yaml* file in your cache folder for
 🤗 Accelerate. That cache folder is (with decreasing order of priority):",H2,https://huggingface.co/docs/accelerate/source/quicktour#launching-your-distributed-script,False,521.0,84.0
1567,Launching training from a notebook,"In Accelerate 0.3.0, a new [`notebook_launcher`] has been introduced to help you launch your training
 function from a notebook. This launcher supports launching a training with TPUs on Colab or Kaggle, as well as training
 on several GPUs (if the machine on which you are running your notebook has them).
 
 Just define a function responsible for your whole training and/or evaluation in a cell of the notebook, then execute a
 cell with the following code:
 
 ```python
 from accelerate import notebook_launcher
 
 notebook_launcher(training_function)
 ```
 
 <Tip warning={true}>
 
     Your [`Accelerator`] object should only be defined inside the training function. This is because the
     initialization should be done inside the launcher only.
 
 </Tip>
 
 Check out the [Notebook Launcher tutorial](basic_tutorials/notebook) for more information about training on TPUs.",H2,https://huggingface.co/docs/accelerate/source/quicktour#launching-training-from-a-notebook,False,878.0,136.0
1568,Training on TPU,"If you want to launch your script on TPUs, there are a few caveats you should be aware of. Behind the scenes, the TPUs
 will create a graph of all the operations happening in your training step (forward pass, backward pass and optimizer
 step). This is why your first step of training will always be very long as building and compiling this graph for
 optimizations takes some time.
 
 The good news is that this compilation will be cached so the second step and all the following will be much faster. The
 bad news is that it only applies if all of your steps do exactly the same operations, which implies:",H2,https://huggingface.co/docs/accelerate/source/quicktour#training-on-tpu,False,607.0,111.0
1569,Other caveats,We list here all smaller issues you could have in your script conversion and how to resolve them.,H2,https://huggingface.co/docs/accelerate/source/quicktour#other-caveats,False,97.0,18.0
1570,Execute a statement only on one processes,"Some of your instructions only need to run for one process on a given server: for instance a data download or a log
 statement. To do this, wrap the statement in a test like this:
 
 ```python docstyle-ignore
 if accelerator.is_local_main_process:
     # Is executed once per server
 ```
 
 Another example is progress bars: to avoid having multiple progress bars in your output, you should only display one on
 the local main process:
 
 ```python
 from tqdm.auto import tqdm
 
 progress_bar = tqdm(range(args.max_train_steps), disable=not accelerator.is_local_main_process)
 ```
 
 The *local* means per machine: if you are running your training on two servers with several GPUs, the instruction will
 be executed once on each of those servers. If you need to execute something only once for all processes (and not per
 machine) for instance, uploading the final model to the 🤗 model hub, wrap it in a test like this:
 
 ```python docstyle-ignore
 if accelerator.is_main_process:
     # Is executed once only
 ```
 
 For printing statements you only want executed once per machine, you can just replace the `print` function by
 `accelerator.print`.",H3,https://huggingface.co/docs/accelerate/source/quicktour#execute-a-statement-only-on-one-processes,False,1150.0,186.0
1571,Defer execution,"When you run your usual script, instructions are executed in order. Using 🤗 Accelerate to deploy your script on several
 GPUs at the same time introduces a complication: while each process executes all instructions in order, some may be
 faster than others.
 
 You might need to wait for all processes to have reached a certain point before executing a given instruction. For
 instance, you shouldn't save a model before being sure every process is done with training. To do this, just write the
 following line in your code:
 
 ```
 accelerator.wait_for_everyone()
 ```
 
 This instruction will block all the processes that arrive first until all the other processes have reached that
 point (if you run your script on just one GPU or CPU, this won't do anything).",H3,https://huggingface.co/docs/accelerate/source/quicktour#defer-execution,False,765.0,128.0
1572,Saving/loading a model,"Saving the model you trained might need a bit of adjustment: first you should wait for all processes to reach that
 point in the script as shown above, and then, you should unwrap your model before saving it. This is because when going
 through the [`~Accelerator.prepare`] method, your model may have been placed inside a bigger model,
 which deals with the distributed training. This in turn means that saving your model state dictionary without taking
 any precaution will take that potential extra layer into account, and you will end up with weights you can't load back
 in your base model. The [`~Accelerator.save_model`] method will help you to achieve that. It will unwrap your model and save
 the model state dictionnary.
 
 Here is an example:
 ```
 accelerator.wait_for_everyone()
 accelerator.save_model(model, save_directory)
 ```
 The [`~Accelerator.save_model`] method can also save a model into sharded checkpoints or with safetensors format.
 Here is an example: 
 
 ```python
 accelerator.wait_for_everyone()
 accelerator.save_model(model, save_directory, max_shard_size=""1GB"", safe_serialization=True)
 ```
 
 If your script contains logic to load a checkpoint, we also recommend you load your weights in the unwrapped model
 (this is only useful if you use the load function after making your model go through
 [`~Accelerator.prepare`]). Here is an example:
 
 ```python
 unwrapped_model = accelerator.unwrap_model(model)
 path_to_checkpoint = os.path.join(save_directory,""pytorch_model.bin"")
 unwrapped_model.load_state_dict(torch.load(path_to_checkpoint))
 ```
 
 Note that since all the model parameters are references to tensors, this will load your weights inside `model`.
 
 If you want to load a sharded checkpoint or a checkpoint with safetensors format into the model with a specific `device`, we recommend you to load it with [`~utils.load_checkpoint_in_model`] function. Here's an example:
 
 ```python
 load_checkpoint_in_model(unwrapped_model, save_directory, device_map={"""":device})
 ```",H3,https://huggingface.co/docs/accelerate/source/quicktour#saving-loading-a-model,False,2021.0,269.0
1573,Saving/loading entire states,"When training your model, you may want to save the current state of the model, optimizer, random generators, and potentially LR schedulers to be restored in the _same script_.
 You can use [`~Accelerator.save_state`] and [`~Accelerator.load_state`] respectively to do so.
 
 To further customize where and how states saved through [`~Accelerator.save_state`] the [`~utils.ProjectConfiguration`] class can be used. For example 
 if `automatic_checkpoint_naming` is enabled each saved checkpoint will be located then at `Accelerator.project_dir/checkpoints/checkpoint_{checkpoint_number}`.
 
 If you have registered any other stateful items to be stored through [`~Accelerator.register_for_checkpointing`] they will also be saved and/or loaded.
 
 <Tip>
 
     Every object passed to [`~Accelerator.register_for_checkpointing`] must have a `load_state_dict` and `state_dict` function to be stored
 
 </Tip>",H2,https://huggingface.co/docs/accelerate/source/quicktour#saving-loading-entire-states,False,904.0,117.0
1574,Gradient clipping,"If you are using gradient clipping in your script, you should replace the calls to
 `torch.nn.utils.clip_grad_norm_` or `torch.nn.utils.clip_grad_value_` with [`~Accelerator.clip_grad_norm_`]
 and [`~Accelerator.clip_grad_value_`] respectively.",H3,https://huggingface.co/docs/accelerate/source/quicktour#gradient-clipping,False,244.0,23.0
1575,Mixed Precision training,"If you are running your training in Mixed Precision with 🤗 Accelerate, you will get the best result with your loss being
 computed inside your model (like in Transformer models for instance). Every computation outside of the model will be
 executed in full precision (which is generally what you want for loss computation, especially if it involves a
 softmax). However you might want to put your loss computation inside the [`~Accelerator.autocast`] context manager:
 
 ```
 with accelerator.autocast():
     loss = complex_loss_function(outputs, target):
 ```
 
 Another caveat with Mixed Precision training is that the gradient will skip a few updates at the beginning and
 sometimes during training: because of the dynamic loss scaling strategy, there are points during training where the
 gradients have overflown, and the loss scaling factor is reduced to avoid this happening again at the next step.
 
 This means that you may update your learning rate scheduler when there was no update, which is fine in general, but may
 have an impact when you have very little training data, or if the first learning rate values of your scheduler are very
 important. In this case, you can skip the learning rate scheduler updates when the optimizer step was not done like
 this:
 
 ```
 if not accelerator.optimizer_step_was_skipped:
     lr_scheduler.step()
 ```",H3,https://huggingface.co/docs/accelerate/source/quicktour#mixed-precision-training,False,1359.0,219.0
1576,Gradient Accumulation,"To perform gradient accumulation use [`~Accelerator.accumulate`] and specify a `gradient_accumulation_steps`. 
 This will also automatically ensure the gradients are synced or unsynced when on multi-device training, check if the step should
 actually be performed, and auto-scale the loss:
 
 ```python
 accelerator = Accelerator(gradient_accumulation_steps=2)
 model, optimizer, training_dataloader = accelerator.prepare(model, optimizer, training_dataloader)
 
 for input, label in training_dataloader:
     with accelerator.accumulate(model):
         predictions = model(input)
         loss = loss_function(predictions, label)
         accelerator.backward(loss)
         optimizer.step()
         scheduler.step()
         optimizer.zero_grad()
 ```",H3,https://huggingface.co/docs/accelerate/source/quicktour#gradient-accumulation,False,755.0,122.0
1577,DeepSpeed,"DeepSpeed support is experimental, so the underlying API will evolve in the near future and may have some slight
 breaking changes. In particular, 🤗 Accelerate does not support DeepSpeed config you have written yourself yet, this
 will be added in a next version.
 
 <Tip warning={true}>
 
     The [`notebook_launcher`] does not support the DeepSpeed integration yet.
 
 </Tip>",H3,https://huggingface.co/docs/accelerate/source/quicktour#deepspeed,False,378.0,62.0
1578,Internal mechanism,"Internally, the library works by first analyzing the environment in which the script is launched to determine which
 kind of distributed setup is used, how many different processes there are and which one the current script is in. All
 that information is stored in the [`~AcceleratorState`].
 
 This class is initialized the first time you instantiate an [`~Accelerator`] as well as performing any
 specific initialization your distributed setup needs. Its state is then uniquely shared through all instances of
 [`~state.AcceleratorState`].
 
 Then, when calling [`~Accelerator.prepare`], the library:",H2,https://huggingface.co/docs/accelerate/source/quicktour#internal-mechanism,False,603.0,87.0
1579,rendered properly in your Markdown viewer.,,H2,https://huggingface.co/docs/accelerate/source/index#rendered-properly-in-your-markdown-viewer,False,0.0,1.0
1580,Accelerate,"🤗 Accelerate is a library that enables the same PyTorch code to be run across any distributed configuration by adding just four lines of code! In short, training and inference at scale made simple, efficient and adaptable.
 
 ```diff
 + from accelerate import Accelerator
 + accelerator = Accelerator()
 
 + model, optimizer, training_dataloader, scheduler = accelerator.prepare(
 +     model, optimizer, training_dataloader, scheduler
 + )
 
   for batch in training_dataloader:
       optimizer.zero_grad()
       inputs, targets = batch
       inputs = inputs.to(device)
       targets = targets.to(device)
       outputs = model(inputs)
       loss = loss_function(outputs, targets)
 +     accelerator.backward(loss)
       optimizer.step()
       scheduler.step()
 ```
 
 Built on `torch_xla` and `torch.distributed`, 🤗 Accelerate takes care of the heavy lifting, so you don't have to write any custom code to adapt to these platforms.
 Convert existing codebases to utilize [DeepSpeed](usage_guides/deepspeed), perform [fully sharded data parallelism](usage_guides/fsdp), and have automatic support for mixed-precision training! 
 
 <Tip> 
 
   To get a better idea of this process, make sure to check out the [Tutorials](basic_tutorials/overview)! 
 
 </Tip>
 
 
 This code can then be launched on any system through Accelerate's CLI interface:
 ```bash
 accelerate launch {my_script.py}
 ```
 
 <div class=""mt-10"">
   <div class=""w-full flex flex-col space-y-4 md:space-y-0 md:grid md:grid-cols-2 md:gap-y-4 md:gap-x-5"">
     <a class=""!no-underline border dark:border-gray-700 p-5 rounded-lg shadow hover:shadow-lg"" href=""./basic_tutorials/overview""
       ><div class=""w-full text-center bg-gradient-to-br from-blue-400 to-blue-500 rounded-lg py-1.5 font-semibold mb-5 text-white text-lg leading-relaxed"">Tutorials</div>
       <p class=""text-gray-700"">Learn the basics and become familiar with using 🤗 Accelerate. Start here if you are using 🤗 Accelerate for the first time!</p>
     </a>
     <a class=""!no-underline border dark:border-gray-700 p-5 rounded-lg shadow hover:shadow-lg"" href=""./usage_guides/explore""
       ><div class=""w-full text-center bg-gradient-to-br from-indigo-400 to-indigo-500 rounded-lg py-1.5 font-semibold mb-5 text-white text-lg leading-relaxed"">How-to guides</div>
       <p class=""text-gray-700"">Practical guides to help you achieve a specific goal. Take a look at these guides to learn how to use 🤗 Accelerate to solve real-world problems.</p>
     </a>
     <a class=""!no-underline border dark:border-gray-700 p-5 rounded-lg shadow hover:shadow-lg"" href=""./concept_guides/gradient_synchronization""
       ><div class=""w-full text-center bg-gradient-to-br from-pink-400 to-pink-500 rounded-lg py-1.5 font-semibold mb-5 text-white text-lg leading-relaxed"">Conceptual guides</div>
       <p class=""text-gray-700"">High-level explanations for building a better understanding of important topics such as avoiding subtle nuances and pitfalls in distributed training and DeepSpeed.</p>
    </a>
     <a class=""!no-underline border dark:border-gray-700 p-5 rounded-lg shadow hover:shadow-lg"" href=""./package_reference/accelerator""
       ><div class=""w-full text-center bg-gradient-to-br from-purple-400 to-purple-500 rounded-lg py-1.5 font-semibold mb-5 text-white text-lg leading-relaxed"">Reference</div>
       <p class=""text-gray-700"">Technical descriptions of how 🤗 Accelerate classes and methods work.</p>
     </a>
   </div>
 </div>",H1,https://huggingface.co/docs/accelerate/source/index#accelerate,False,3474.0,516.0
1581,rendered properly in your Markdown viewer.,,H2,https://huggingface.co/docs/accelerate/source/basic_tutorials/overview#rendered-properly-in-your-markdown-viewer,False,0.0,1.0
1582,Overview,"Welcome to the 🤗 Accelerate tutorials! These introductory guides will help catch you up to speed on working with 🤗 Accelerate.
 You'll learn how to modify your code to have it work with the API seamlessly, how to launch your script properly,
 and more!
 
 These tutorials assume some basic knowledge of Python and familiarity with the PyTorch framework.
 
 If you have any questions about 🤗 Accelerate, feel free to join and ask the community on our [forum](https://discuss.huggingface.co/c/accelerate/18).",H1,https://huggingface.co/docs/accelerate/source/basic_tutorials/overview#overview,False,506.0,79.0
1583,rendered properly in your Markdown viewer.,,H2,https://huggingface.co/docs/accelerate/source/basic_tutorials/launch#rendered-properly-in-your-markdown-viewer,False,0.0,1.0
1584,Launching your 🤗 Accelerate scripts,"In the previous tutorial, you were introduced to how to modify your current training script to use 🤗 Accelerate.
 The final version of that code is shown below:
 
 ```python
 from accelerate import Accelerator
 
 accelerator = Accelerator()
 
 model, optimizer, training_dataloader, scheduler = accelerator.prepare(
     model, optimizer, training_dataloader, scheduler
 )
 
 for batch in training_dataloader:
     optimizer.zero_grad()
     inputs, targets = batch
     outputs = model(inputs)
     loss = loss_function(outputs, targets)
     accelerator.backward(loss)
     optimizer.step()
     scheduler.step()
 ```
 
 But how do you run this code and have it utilize the special hardware available to it?
 
 First, you should rewrite the above code into a function, and make it callable as a script. For example:
 
 ```diff
   from accelerate import Accelerator
   
 + def main():
       accelerator = Accelerator()
 
       model, optimizer, training_dataloader, scheduler = accelerator.prepare(
           model, optimizer, training_dataloader, scheduler
       )
 
       for batch in training_dataloader:
           optimizer.zero_grad()
           inputs, targets = batch
           outputs = model(inputs)
           loss = loss_function(outputs, targets)
           accelerator.backward(loss)
           optimizer.step()
           scheduler.step()
 
 + if __name__ == ""__main__"":
 +     main()
 ```
 
 Next, you need to launch it with `accelerate launch`. 
 
 <Tip warning={true}>
 
   It's recommended you run `accelerate config` before using `accelerate launch` to configure your environment to your liking. 
   Otherwise 🤗 Accelerate will use very basic defaults depending on your system setup.
 
 </Tip>",H1,https://huggingface.co/docs/accelerate/source/basic_tutorials/launch#launching-your-accelerate-scripts,False,1720.0,359.0
1585,Using accelerate launch,"🤗 Accelerate has a special CLI command to help you launch your code in your system through `accelerate launch`.
 This command wraps around all of the different commands needed to launch your script on various platforms, without you having to remember what each of them is.
 
 <Tip>
 
   If you are familiar with launching scripts in PyTorch yourself such as with `torchrun`, you can still do this. It is not required to use `accelerate launch`.
 
 </Tip>
 
 You can launch your script quickly by using:
 
 ```bash
 accelerate launch {script_name.py} --arg1 --arg2 ...
 ```
 
 Just put `accelerate launch` at the start of your command, and pass in additional arguments and parameters to your script afterward like normal!
 
 Since this runs the various torch spawn methods, all of the expected environment variables can be modified here as well.
 For example, here is how to use `accelerate launch` with a single GPU:
 
 ```bash
 CUDA_VISIBLE_DEVICES=""0"" accelerate launch {script_name.py} --arg1 --arg2 ...
 ```
 
 You can also use `accelerate launch` without performing `accelerate config` first, but you may need to manually pass in the right configuration parameters.
 In this case, 🤗 Accelerate will make some hyperparameter decisions for you, e.g., if GPUs are available, it will use all of them by default without the mixed precision.
 Here is how you would use all GPUs and train with mixed precision disabled:
 
 ```bash
 accelerate launch --multi_gpu {script_name.py} {--arg1} {--arg2} ...
 ```
 
 Or by specifying a number of GPUs to use:
 
 ```bash
 accelerate launch --num_processes=2 {script_name.py} {--arg1} {--arg2} ...
 ```
 
 To get more specific you should pass in the needed parameters yourself. For instance, here is how you 
 would also launch that same script on two GPUs using mixed precision while avoiding all of the warnings: 
 
 ```bash
 accelerate launch --multi_gpu --mixed_precision=fp16 --num_processes=2 {script_name.py} {--arg1} {--arg2} ...
 ```
 
 For a complete list of parameters you can pass in, run:
 
 ```bash
 accelerate launch -h
 ```
 
 <Tip>
 
   Even if you are not using 🤗 Accelerate in your code, you can still use the launcher for starting your scripts!
 
 </Tip>
 
 For a visualization of this difference, that earlier `accelerate launch` on multi-gpu would look something like so with `torchrun`:
 
 ```bash
 MIXED_PRECISION=""fp16"" torchrun --nproc_per_node=2 --num_machines=1 {script_name.py} {--arg1} {--arg2} ...
 ```
 
 You can also launch your script utilizing the launch CLI as a python module itself, enabling the ability to pass in other python-specific
 launching behaviors. To do so, use `accelerate.commands.launch` instead of `accelerate launch`:
 
 ```bash
 python -m accelerate.commands.launch --num_processes=2 {script_name.py} {--arg1} {--arg2}
 ```
 
 If you want to execute the script with any other python flags, you can pass them in as well similar to `-m`, such as 
 the below example enabling unbuffered stdout and stderr:
 
 ```bash
 python -u -m accelerate.commands.launch --num_processes=2 {script_name.py} {--arg1} {--arg2}
 ```",H2,https://huggingface.co/docs/accelerate/source/basic_tutorials/launch#using-accelerate-launch,False,3105.0,481.0
1586,Why you should always use `accelerate config`,"Why is it useful to the point you should **always** run `accelerate config`? 
 
 Remember that earlier call to `accelerate launch` as well as `torchrun`?
 Post configuration, to run that script with the needed parts you just need to use `accelerate launch` outright, without passing anything else in:
 
 ```bash
 accelerate launch {script_name.py} {--arg1} {--arg2} ...
 ```",H2,https://huggingface.co/docs/accelerate/source/basic_tutorials/launch#why-you-should-always-use-accelerate-config,False,374.0,58.0
1587,Custom Configurations,"As briefly mentioned earlier, `accelerate launch` should be mostly used through combining set configurations 
 made with the `accelerate config` command. These configs are saved to a `default_config.yaml` file in your cache folder for 🤗 Accelerate. 
 This cache folder is located at (with decreasing order of priority):",H2,https://huggingface.co/docs/accelerate/source/basic_tutorials/launch#custom-configurations,False,319.0,48.0
1588,rendered properly in your Markdown viewer.,,H2,https://huggingface.co/docs/accelerate/source/basic_tutorials/install#rendered-properly-in-your-markdown-viewer,False,0.0,1.0
1589,Installation and Configuration,"Before you start, you will need to setup your environment, install the appropriate packages, and configure 🤗 Accelerate. 🤗 Accelerate is tested on **Python 3.8+**.",H1,https://huggingface.co/docs/accelerate/source/basic_tutorials/install#installation-and-configuration,False,163.0,25.0
1590,Installing 🤗 Accelerate,"🤗 Accelerate is available on pypi and conda, as well as on GitHub. Details to install from each are below:",H2,https://huggingface.co/docs/accelerate/source/basic_tutorials/install#installing-accelerate,False,106.0,20.0
1591,pip,"To install 🤗 Accelerate from pypi, perform:
 
 ```bash
 pip install accelerate
 ```",H3,https://huggingface.co/docs/accelerate/source/basic_tutorials/install#pip,False,83.0,13.0
1592,conda,"🤗 Accelerate can also be installed with conda with:
 
 ```bash
 conda install -c conda-forge accelerate
 ```",H3,https://huggingface.co/docs/accelerate/source/basic_tutorials/install#conda,False,108.0,17.0
1593,Source,"New features are added every day that haven't been released yet. To try them out yourself, install
 from the GitHub repository:
 
 ```bash
 pip install git+https://github.com/huggingface/accelerate
 ```
 
 If you're working on contributing to the library or wish to play with the source code and see live 
 results as you run the code, an editable version can be installed from a locally-cloned version of the 
 repository:
 
 ```bash
 git clone https://github.com/huggingface/accelerate
 cd accelerate
 pip install -e .
 ```",H3,https://huggingface.co/docs/accelerate/source/basic_tutorials/install#source,False,525.0,80.0
1594,Configuring 🤗 Accelerate,"After installing, you need to configure 🤗 Accelerate for how the current system is setup for training. 
 To do so run the following and answer the questions prompted to you:
 
 ```bash
 accelerate config
 ```
 
 To write a barebones configuration that doesn't include options such as DeepSpeed configuration or running on TPUs, you can quickly run:
 
 ```bash
 python -c ""from accelerate.utils import write_basic_config; write_basic_config(mixed_precision='fp16')""
 ```
 🤗 Accelerate will automatically utilize the maximum number of GPUs available and set the mixed precision mode.
 
 To check that your configuration looks fine, run:
 
 ```bash
 accelerate env
 ```
 
 An example output is shown below, which describes two GPUs on a single machine with no mixed precision being used:",H2,https://huggingface.co/docs/accelerate/source/basic_tutorials/install#configuring-accelerate,False,784.0,120.0
1595,rendered properly in your Markdown viewer.,,H2,https://huggingface.co/docs/accelerate/source/basic_tutorials/migration#rendered-properly-in-your-markdown-viewer,False,0.0,1.0
1596,Migrating your code to 🤗 Accelerate,"This tutorial will detail how to easily convert existing PyTorch code to use 🤗 Accelerate!
 You'll see that by just changing a few lines of code, 🤗 Accelerate can perform its magic and get you on 
 your way toward running your code on distributed systems with ease!",H1,https://huggingface.co/docs/accelerate/source/basic_tutorials/migration#migrating-your-code-to-accelerate,False,265.0,48.0
1597,The base training loop,"To begin, write out a very basic PyTorch training loop. 
 
 <Tip>
 
     We are under the presumption that `training_dataloader`, `model`, `optimizer`, `scheduler`, and `loss_function` have been defined beforehand.
 
 </Tip>
 
 ```python
 device = ""cuda""
 model.to(device)
 
 for batch in training_dataloader:
     optimizer.zero_grad()
     inputs, targets = batch
     inputs = inputs.to(device)
     targets = targets.to(device)
     outputs = model(inputs)
     loss = loss_function(outputs, targets)
     loss.backward()
     optimizer.step()
     scheduler.step()
 ```",H2,https://huggingface.co/docs/accelerate/source/basic_tutorials/migration#the-base-training-loop,False,574.0,105.0
1598,Add in 🤗 Accelerate,"To start using 🤗 Accelerate, first import and create an [`Accelerator`] instance:
 ```python
 from accelerate import Accelerator
 
 accelerator = Accelerator()
 ```
 [`Accelerator`] is the main force behind utilizing all the possible options for distributed training!",H2,https://huggingface.co/docs/accelerate/source/basic_tutorials/migration#add-in-accelerate,False,267.0,36.0
1599,Setting the right device,"The [`Accelerator`] class knows the right device to move any PyTorch object to at any time, so you should
 change the definition of `device` to come from [`Accelerator`]:",H3,https://huggingface.co/docs/accelerate/source/basic_tutorials/migration#setting-the-right-device,False,170.0,28.0
1600,Preparing your objects,"Next, you need to pass all of the important objects related to training into [`~Accelerator.prepare`]. 🤗 Accelerate will
 make sure everything is setup in the current environment for you to start training:
 
 ```
 model, optimizer, training_dataloader, scheduler = accelerator.prepare(
     model, optimizer, training_dataloader, scheduler
 )
 ```
 These objects are returned in the same order they were sent in. By default when using `device_placement=True`, all of the objects that can be sent to the right device will be.
 If you need to work with data that isn't passed to [~Accelerator.prepare] but should be on the active device, you should pass in the `device` you made earlier. 
 
 <Tip warning={true}>
 
     Accelerate will only prepare objects that inherit from their respective PyTorch classes (such as `torch.optim.Optimizer`).
 
 </Tip>",H3,https://huggingface.co/docs/accelerate/source/basic_tutorials/migration#preparing-your-objects,False,850.0,135.0
1601,Modifying the training loop,"Finally, three lines of code need to be changed in the training loop. 🤗 Accelerate's DataLoader classes will automatically handle the device placement by default,
 and [`~Accelerator.backward`] should be used for performing the backward pass:",H3,https://huggingface.co/docs/accelerate/source/basic_tutorials/migration#modifying-the-training-loop,False,242.0,35.0
1602,The finished code,"Below is the final version of the converted code: 
 
 ```python
 from accelerate import Accelerator
 
 accelerator = Accelerator()
 
 model, optimizer, training_dataloader, scheduler = accelerator.prepare(
     model, optimizer, training_dataloader, scheduler
 )
 
 for batch in training_dataloader:
     optimizer.zero_grad()
     inputs, targets = batch
     outputs = model(inputs)
     loss = loss_function(outputs, targets)
     accelerator.backward(loss)
     optimizer.step()
     scheduler.step()
 ```",H2,https://huggingface.co/docs/accelerate/source/basic_tutorials/migration#the-finished-code,False,509.0,85.0
1603,More Resources,"To check out more ways on how to migrate to 🤗 Accelerate, check out our [interactive migration tutorial](https://huggingface.co/docs/accelerate/usage_guides/explore) which showcases other items that need to be watched for when using Accelerate and how to do so quickly.",H2,https://huggingface.co/docs/accelerate/source/basic_tutorials/migration#more-resources,False,269.0,37.0
1604,rendered properly in your Markdown viewer.,,H2,https://huggingface.co/docs/accelerate/source/basic_tutorials/notebook#rendered-properly-in-your-markdown-viewer,False,0.0,1.0
1605,Launching Multi-GPU Training from a Jupyter Environment,"This tutorial teaches you how to fine tune a computer vision model with 🤗 Accelerate from a Jupyter Notebook on a distributed system.
 You will also learn how to setup a few requirements needed for ensuring your environment is configured properly, your data has been prepared properly, and finally how to launch training.
 
 <Tip>
 
     This tutorial is also available as a Jupyter Notebook [here](https://github.com/huggingface/notebooks/blob/main/examples/accelerate_examples/simple_cv_example.ipynb)
 
 </Tip>",H1,https://huggingface.co/docs/accelerate/source/basic_tutorials/notebook#launching-multi-gpu-training-from-a-jupyter-environment,False,513.0,72.0
1606,Configuring the Environment,"Before any training can be performed, a 🤗 Accelerate config file must exist in the system. Usually this can be done by running the following in a terminal and answering the prompts:
 
 ```bash
 accelerate config
 ```
 
 However, if general defaults are fine and you are *not* running on a TPU, 🤗Accelerate has a utility to quickly write your GPU configuration into a config file via [`utils.write_basic_config`].
 
 The following code will restart Jupyter after writing the configuration, as CUDA code was called to perform this. 
 
 <Tip warning={true}>
 
     CUDA can't be initialized more than once on a multi-GPU system. It's fine to debug in the notebook and have calls to CUDA, but in order to finally train a full cleanup and restart will need to be performed.
     
 </Tip>
 
 ```python
 import os
 from accelerate.utils import write_basic_config
 
 write_basic_config()  # Write a config file
 os._exit(00)  # Restart the notebook
 ```",H2,https://huggingface.co/docs/accelerate/source/basic_tutorials/notebook#configuring-the-environment,False,945.0,164.0
1607,Preparing the Dataset and Model,"Next you should prepare your dataset. As mentioned at earlier, great care should be taken when preparing the `DataLoaders` and model to make sure that **nothing** is put on *any* GPU. 
 
 If you do, it is recommended to put that specific code into a function and call that from within the notebook launcher interface, which will be shown later. 
 
 Make sure the dataset is downloaded based on the directions [here](https://github.com/huggingface/accelerate/tree/main/examples#simple-vision-example)
 
 ```python
 import os, re, torch, PIL
 import numpy as np
 
 from torch.optim.lr_scheduler import OneCycleLR
 from torch.utils.data import DataLoader, Dataset
 from torchvision.transforms import Compose, RandomResizedCrop, Resize, ToTensor
 
 from accelerate import Accelerator
 from accelerate.utils import set_seed
 from timm import create_model
 ```
 
 First you need to create a function to extract the class name based on a filename:
 
 ```python
 import os
 
 data_dir = ""../../images""
 fnames = os.listdir(data_dir)
 fname = fnames[0]
 print(fname)
 ```
 
 ```python out
 beagle_32.jpg
 ```
 
 In the case here, the label is `beagle`. Using regex you can extract the label from the filename:
 
 ```python
 import re
 
 
 def extract_label(fname):
     stem = fname.split(os.path.sep)[-1]
     return re.search(r""^(.*)_\d+\.jpg$"", stem).groups()[0]
 ```
 
 ```python
 extract_label(fname)
 ```
 
 And you can see it properly returned the right name for our file:
 
 ```python out
 ""beagle""
 ```
 
 Next a `Dataset` class should be made to handle grabbing the image and the label:
 
 ```python
 class PetsDataset(Dataset):
     def __init__(self, file_names, image_transform=None, label_to_id=None):
         self.file_names = file_names
         self.image_transform = image_transform
         self.label_to_id = label_to_id
 
     def __len__(self):
         return len(self.file_names)
 
     def __getitem__(self, idx):
         fname = self.file_names[idx]
         raw_image = PIL.Image.open(fname)
         image = raw_image.convert(""RGB"")
         if self.image_transform is not None:
             image = self.image_transform(image)
         label = extract_label(fname)
         if self.label_to_id is not None:
             label = self.label_to_id[label]
         return {""image"": image, ""label"": label}
 ```
 
 Now to build the dataset. Outside the training function you can find and declare all the filenames and labels and use them as references inside the 
 launched function:
 
 ```python
 fnames = [os.path.join(""../../images"", fname) for fname in fnames if fname.endswith("".jpg"")]
 ```
 
 Next gather all the labels:
 
 ```python
 all_labels = [extract_label(fname) for fname in fnames]
 id_to_label = list(set(all_labels))
 id_to_label.sort()
 label_to_id = {lbl: i for i, lbl in enumerate(id_to_label)}
 ```
 
 Next, you should make a `get_dataloaders` function that will return your built dataloaders for you. As mentioned earlier, if data is automatically 
 sent to the GPU or a TPU device when building your `DataLoaders`, they must be built using this method. 
 
 ```python
 def get_dataloaders(batch_size: int = 64):
     ""Builds a set of dataloaders with a batch_size""
     random_perm = np.random.permutation(len(fnames))
     cut = int(0.8 * len(fnames))
     train_split = random_perm[:cut]
     eval_split = random_perm[cut:]
 
     # For training a simple RandomResizedCrop will be used
     train_tfm = Compose([RandomResizedCrop((224, 224), scale=(0.5, 1.0)), ToTensor()])
     train_dataset = PetsDataset([fnames[i] for i in train_split], image_transform=train_tfm, label_to_id=label_to_id)
 
     # For evaluation a deterministic Resize will be used
     eval_tfm = Compose([Resize((224, 224)), ToTensor()])
     eval_dataset = PetsDataset([fnames[i] for i in eval_split], image_transform=eval_tfm, label_to_id=label_to_id)
 
     # Instantiate dataloaders
     train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size, num_workers=4)
     eval_dataloader = DataLoader(eval_dataset, shuffle=False, batch_size=batch_size * 2, num_workers=4)
     return train_dataloader, eval_dataloader
 ```
 
 Finally, you should import the scheduler to be used later:
 
 ```python
 from torch.optim.lr_scheduler import CosineAnnealingLR
 ```",H2,https://huggingface.co/docs/accelerate/source/basic_tutorials/notebook#preparing-the-dataset-and-model,False,4293.0,715.0
1608,Writing the Training Function,"Now you can build the training loop. [`notebook_launcher`] works by passing in a function to call that will be ran across the distributed system.
 
 Here is a basic training loop for the animal classification problem:
 
 <Tip>
 
     The code has been split up to allow for explainations on each section. A full version that can be copy and pasted will be available at the end
 
 </Tip>
 
 
 ```python
 def training_loop(mixed_precision=""fp16"", seed: int = 42, batch_size: int = 64):
     set_seed(seed)
     accelerator = Accelerator(mixed_precision=mixed_precision)
 ```
 
 First you should set the seed and create an [`Accelerator`] object as early in the training loop as possible.
 
 <Tip warning={true}>
 
     If training on the TPU, your training loop should take in the model as a parameter and it should be instantiated 
     outside of the training loop function. See the [TPU best practices](../concept_guides/training_tpu) 
     to learn why
 
 </Tip>
 
 Next you should build your dataloaders and create your model:
 
 ```python
     train_dataloader, eval_dataloader = get_dataloaders(batch_size)
     model = create_model(""resnet50d"", pretrained=True, num_classes=len(label_to_id))
 ```
 
 <Tip>
 
     You build the model here so that the seed also controls the new weight initialization
 
 </Tip>
 
 As you are performing transfer learning in this example, the encoder of the model starts out frozen so the head of the model can be 
 trained only initially:
 
 ```python
     for param in model.parameters():
         param.requires_grad = False
     for param in model.get_classifier().parameters():
         param.requires_grad = True
 ```
 
 Normalizing the batches of images will make training a little faster:
 
 ```python
     mean = torch.tensor(model.default_cfg[""mean""])[None, :, None, None]
     std = torch.tensor(model.default_cfg[""std""])[None, :, None, None]
 ```
 
 To make these constants available on the active device, you should set it to the Accelerator's device:
 
 ```python
     mean = mean.to(accelerator.device)
     std = std.to(accelerator.device)
 ```
 
 Next instantiate the rest of the PyTorch classes used for training:
 
 ```python
     optimizer = torch.optim.Adam(params=model.parameters(), lr=3e-2 / 25)
     lr_scheduler = OneCycleLR(optimizer=optimizer, max_lr=3e-2, epochs=5, steps_per_epoch=len(train_dataloader))
 ```
 
 Before passing everything to [`~Accelerator.prepare`].
 
 <Tip>
 
     There is no specific order to remember, you just need to unpack the objects in the same order you gave them to the prepare method.
 
 </Tip>
 
 ```python
     model, optimizer, train_dataloader, eval_dataloader, lr_scheduler = accelerator.prepare(
         model, optimizer, train_dataloader, eval_dataloader, lr_scheduler
     )
 ```
 
 Now train the model:
 
 ```python
     for epoch in range(5):
         model.train()
         for batch in train_dataloader:
             inputs = (batch[""image""] - mean) / std
             outputs = model(inputs)
             loss = torch.nn.functional.cross_entropy(outputs, batch[""label""])
             accelerator.backward(loss)
             optimizer.step()
             lr_scheduler.step()
             optimizer.zero_grad()
 ```
 
 The evaluation loop will look slightly different compared to the training loop. The number of elements passed as well as the overall 
 total accuracy of each batch will be added to two constants:
 
 ```python
         model.eval()
         accurate = 0
         num_elems = 0
 ```
 
 Next you have the rest of your standard PyTorch loop:
 
 ```python
         for batch in eval_dataloader:
             inputs = (batch[""image""] - mean) / std
             with torch.no_grad():
                 outputs = model(inputs)
             predictions = outputs.argmax(dim=-1)
 ```
 
 Before finally the last major difference. 
 
 When performing distributed evaluation, the predictions and labels need to be passed through 
 [`~Accelerator.gather`] so that all of the data is available on the current device and a properly calculated metric can be achieved:
 
 ```python
             accurate_preds = accelerator.gather(predictions) == accelerator.gather(batch[""label""])
             num_elems += accurate_preds.shape[0]
             accurate += accurate_preds.long().sum()
 ```
 
 Now you just need to calculate the actual metric for this problem, and you can print it on the main process using [`~Accelerator.print`]:
 
 ```python
         eval_metric = accurate.item() / num_elems
         accelerator.print(f""epoch {epoch}: {100 * eval_metric:.2f}"")
 ```
 
 A full version of this training loop is available below:
 
 ```python
 def training_loop(mixed_precision=""fp16"", seed: int = 42, batch_size: int = 64):
     set_seed(seed)
     # Initialize accelerator
     accelerator = Accelerator(mixed_precision=mixed_precision)
     # Build dataloaders
     train_dataloader, eval_dataloader = get_dataloaders(batch_size)
 
     # Instantiate the model (you build the model here so that the seed also controls new weight initaliziations)
     model = create_model(""resnet50d"", pretrained=True, num_classes=len(label_to_id))
 
     # Freeze the base model
     for param in model.parameters():
         param.requires_grad = False
     for param in model.get_classifier().parameters():
         param.requires_grad = True
 
     # You can normalize the batches of images to be a bit faster
     mean = torch.tensor(model.default_cfg[""mean""])[None, :, None, None]
     std = torch.tensor(model.default_cfg[""std""])[None, :, None, None]
 
     # To make these constants available on the active device, set it to the accelerator device
     mean = mean.to(accelerator.device)
     std = std.to(accelerator.device)
 
     # Intantiate the optimizer
     optimizer = torch.optim.Adam(params=model.parameters(), lr=3e-2 / 25)
 
     # Instantiate the learning rate scheduler
     lr_scheduler = OneCycleLR(optimizer=optimizer, max_lr=3e-2, epochs=5, steps_per_epoch=len(train_dataloader))
 
     # Prepare everything
     # There is no specific order to remember, you just need to unpack the objects in the same order you gave them to the
     # prepare method.
     model, optimizer, train_dataloader, eval_dataloader, lr_scheduler = accelerator.prepare(
         model, optimizer, train_dataloader, eval_dataloader, lr_scheduler
     )
 
     # Now you train the model
     for epoch in range(5):
         model.train()
         for batch in train_dataloader:
             inputs = (batch[""image""] - mean) / std
             outputs = model(inputs)
             loss = torch.nn.functional.cross_entropy(outputs, batch[""label""])
             accelerator.backward(loss)
             optimizer.step()
             lr_scheduler.step()
             optimizer.zero_grad()
 
         model.eval()
         accurate = 0
         num_elems = 0
         for batch in eval_dataloader:
             inputs = (batch[""image""] - mean) / std
             with torch.no_grad():
                 outputs = model(inputs)
             predictions = outputs.argmax(dim=-1)
             accurate_preds = accelerator.gather(predictions) == accelerator.gather(batch[""label""])
             num_elems += accurate_preds.shape[0]
             accurate += accurate_preds.long().sum()
 
         eval_metric = accurate.item() / num_elems
         # Use accelerator.print to print only on the main process.
         accelerator.print(f""epoch {epoch}: {100 * eval_metric:.2f}"")
 ```",H2,https://huggingface.co/docs/accelerate/source/basic_tutorials/notebook#writing-the-training-function,False,7462.0,1593.0
1609,Using the notebook_launcher,"All that's left is to use the [`notebook_launcher`].
 
 You pass in the function, the arguments (as a tuple), and the number of processes to train on. (See the [documentation](../package_reference/launchers) for more information)
 
 ```python
 from accelerate import notebook_launcher
 ```
 
 ```python
 args = (""fp16"", 42, 64)
 notebook_launcher(training_loop, args, num_processes=2)
 ```
 
 In the case of running on the TPU, it would look like so:
 
 ```python
 model = create_model(""resnet50d"", pretrained=True, num_classes=len(label_to_id))
 
 args = (model, ""fp16"", 42, 64)
 notebook_launcher(training_loop, args, num_processes=8)
 ```
 
 As it's running it will print the progress as well as state how many devices you ran on. This tutorial was ran with two GPUs:
 
 ```python out
 Launching training on 2 GPUs.
 epoch 0: 88.12
 epoch 1: 91.73
 epoch 2: 92.58
 epoch 3: 93.90
 epoch 4: 94.71
 ```
 
 And that's it!",H2,https://huggingface.co/docs/accelerate/source/basic_tutorials/notebook#using-the-notebook-launcher,False,921.0,137.0
1610,Conclusion,This notebook showed how to perform distributed training from inside of a Jupyter Notebook. Some key notes to remember:,H2,https://huggingface.co/docs/accelerate/source/basic_tutorials/notebook#conclusion,False,119.0,19.0
1611,rendered properly in your Markdown viewer.,,H2,https://huggingface.co/docs/accelerate/source/package_reference/cli#rendered-properly-in-your-markdown-viewer,False,0.0,1.0
1612,The Command Line,Below is a list of all the available commands 🤗 Accelerate with their parameters,H1,https://huggingface.co/docs/accelerate/source/package_reference/cli#the-command-line,False,80.0,14.0
1613,accelerate config,"**Command**:
 
 `accelerate config` or `accelerate-config`
 
 Launches a series of prompts to create and save a `default_config.yml` configuration file for your training system. Should 
 always be ran first on your machine.
 
 **Usage**: 
 
 ```bash
 accelerate config [arguments]
 ```
 
 **Optional Arguments**:
 * `--config_file CONFIG_FILE` (`str`) -- The path to use to store the config file. Will default to a file named default_config.yaml in the cache location, which is the content
                         of the environment `HF_HOME` suffixed with 'accelerate', or if you don't have such an environment variable, your cache directory
                         (`~/.cache` or the content of `XDG_CACHE_HOME`) suffixed with `huggingface`.
 * `-h`, `--help` (`bool`) -- Show a help message and exit",H2,https://huggingface.co/docs/accelerate/source/package_reference/cli#accelerate-config,False,804.0,161.0
1614,accelerate config default,"**Command**:
 
 `accelerate config default` or `accelerate-config default`
 
 Create a default config file for Accelerate with only a few flags set.
 
 **Usage**: 
 
 ```bash
 accelerate config default [arguments]
 ```
 
 **Optional Arguments**:
 * `--config_file CONFIG_FILE` (`str`) -- The path to use to store the config file. Will default to a file named default_config.yaml in the cache location, which is the content
                         of the environment `HF_HOME` suffixed with 'accelerate', or if you don't have such an environment variable, your cache directory
                         (`~/.cache` or the content of `XDG_CACHE_HOME`) suffixed with `huggingface`.
 
 * `-h`, `--help` (`bool`) -- Show a help message and exit
 * `--mixed_precision {no,fp16,bf16}` (`str`) -- Whether or not to use mixed precision training. Choose between FP16 and BF16 (bfloat16) training. BF16 training is only supported on Nvidia Ampere GPUs and PyTorch 1.10 or later.",H2,https://huggingface.co/docs/accelerate/source/package_reference/cli#accelerate-config-default,False,967.0,186.0
1615,accelerate config update,"**Command**:
 
 `accelerate config update` or `accelerate-config update`
 
 Update an existing config file with the latest defaults while maintaining the old configuration.
 
 **Usage**: 
 
 ```bash
 accelerate config update [arguments]
 ```
 
 **Optional Arguments**:
 * `--config_file CONFIG_FILE` (`str`) -- The path to the config file to update. Will default to a file named default_config.yaml in the cache location, which is the content
                         of the environment `HF_HOME` suffixed with 'accelerate', or if you don't have such an environment variable, your cache directory
                         (`~/.cache` or the content of `XDG_CACHE_HOME`) suffixed with `huggingface`.
 
 * `-h`, `--help` (`bool`) -- Show a help message and exit",H2,https://huggingface.co/docs/accelerate/source/package_reference/cli#accelerate-config-update,False,759.0,152.0
1616,accelerate env,"**Command**:
 
 `accelerate env` or `accelerate-env` or `python -m accelerate.commands.env`
 
 Lists the contents of the passed 🤗 Accelerate configuration file. Should always be used when opening an issue on the [GitHub repository](https://github.com/huggingface/accelerate).
 
 **Usage**:
 
 ```bash
 accelerate env [arguments]
 ```
 
 **Optional Arguments**:
 * `--config_file CONFIG_FILE` (`str`) -- The path to use to store the config file. Will default to a file named default_config.yaml in the cache location, which is the content
                         of the environment `HF_HOME` suffixed with 'accelerate', or if you don't have such an environment variable, your cache directory
                         (`~/.cache` or the content of `XDG_CACHE_HOME`) suffixed with `huggingface`.
 * `-h`, `--help` (`bool`) -- Show a help message and exit",H2,https://huggingface.co/docs/accelerate/source/package_reference/cli#accelerate-env,False,852.0,160.0
1617,accelerate launch,"**Command**:
 
 `accelerate launch` or `accelerate-launch` or `python -m accelerate.commands.launch`
 
 Launches a specified script on a distributed system with the right parameters.
 
 **Usage**: 
 
 ```bash
 accelerate launch [arguments] {training_script} --{training_script-argument-1} --{training_script-argument-2} ...
 ```
 
 **Positional Arguments**:",H2,https://huggingface.co/docs/accelerate/source/package_reference/cli#accelerate-launch,False,357.0,39.0
1618,accelerate tpu-config,"`accelerate tpu-config`
 
 **Usage**:
 
 ```bash
 accelerate tpu-config [arguments]
 ```
 
 **Optional Arguments**:
 * `-h`, `--help` (`bool`) -- Show a help message and exit
 
 **Config Arguments**:
 
 Arguments that can be configured through `accelerate config`.
 
 * `--config_file` (`str`) -- Path to the config file to use for accelerate.
 * `--tpu_name` (`str`) -- The name of the TPU to use. If not specified, will use the TPU specified in the config file.
 * `--tpu_zone` (`str`) -- The zone of the TPU to use. If not specified, will use the zone specified in the config file.
 
 **TPU Arguments**:
 
 Arguments for options ran inside the TPU.
 
 * `--command_file` (`str`) -- The path to the file containing the commands to run on the pod on startup.
 * `--command` (`str`) -- A command to run on the pod. Can be passed multiple times.
 * `--install_accelerate` (`bool`) -- Whether to install accelerate on the pod. Defaults to False.
 * `--accelerate_version` (`str`) -- The version of accelerate to install on the pod. If not specified, will use the latest pypi version. Specify 'dev' to install from GitHub.
 * `--debug` (`bool`) -- If set, will print the command that would be run instead of running it.",H2,https://huggingface.co/docs/accelerate/source/package_reference/cli#accelerate-tpu-config,False,1216.0,203.0
1619,accelerate test,"`accelerate test` or `accelerate-test`
 
 Runs `accelerate/test_utils/test_script.py` to verify that 🤗 Accelerate has been properly configured on your system and runs. 
 
 **Usage**: 
 
 ```bash
 accelerate test [arguments]
 ```
 
 **Optional Arguments**:
 * `--config_file CONFIG_FILE` (`str`) -- The path to use to store the config file. Will default to a file named default_config.yaml in the cache location, which is the content
                         of the environment `HF_HOME` suffixed with 'accelerate', or if you don't have such an environment variable, your cache directory
                         (`~/.cache` or the content of `XDG_CACHE_HOME`) suffixed with `huggingface`.
 * `-h`, `--help` (`bool`) -- Show a help message and exit",H2,https://huggingface.co/docs/accelerate/source/package_reference/cli#accelerate-test,False,747.0,150.0
1620,rendered properly in your Markdown viewer.,,H2,https://huggingface.co/docs/accelerate/source/package_reference/megatron_lm#rendered-properly-in-your-markdown-viewer,False,0.0,1.0
1621,Utilities for Megatron-LM,"[[autodoc]] utils.MegatronLMPlugin
 
 [[autodoc]] utils.MegatronLMDummyScheduler
 
 [[autodoc]] utils.MegatronLMDummyDataLoader
 
 [[autodoc]] utils.AbstractTrainStep
 
 [[autodoc]] utils.GPTTrainStep
 
 [[autodoc]] utils.BertTrainStep
 
 [[autodoc]] utils.T5TrainStep
 
 [[autodoc]] utils.avg_losses_across_data_parallel_group",H1,https://huggingface.co/docs/accelerate/source/package_reference/megatron_lm#utilities-for-megatron-lm,False,327.0,23.0
1622,rendered properly in your Markdown viewer.,,H2,https://huggingface.co/docs/accelerate/source/package_reference/kwargs#rendered-properly-in-your-markdown-viewer,False,0.0,1.0
1623,Kwargs Handlers,"The following objects can be passed to the main [`Accelerator`] to customize how some PyTorch objects
 related to distributed training or mixed precision are created.",H1,https://huggingface.co/docs/accelerate/source/package_reference/kwargs#kwargs-handlers,False,166.0,25.0
1624,AutocastKwargs,[[autodoc]] AutocastKwargs,H2,https://huggingface.co/docs/accelerate/source/package_reference/kwargs#autocastkwargs,False,26.0,2.0
1625,DistributedDataParallelKwargs,[[autodoc]] DistributedDataParallelKwargs,H2,https://huggingface.co/docs/accelerate/source/package_reference/kwargs#distributeddataparallelkwargs,False,41.0,2.0
1626,FP8RecipeKwargs,[[autodoc]] utils.FP8RecipeKwargs,H2,https://huggingface.co/docs/accelerate/source/package_reference/kwargs#fp8recipekwargs,False,33.0,2.0
1627,GradScalerKwargs,[[autodoc]] GradScalerKwargs,H2,https://huggingface.co/docs/accelerate/source/package_reference/kwargs#gradscalerkwargs,False,28.0,2.0
1628,InitProcessGroupKwargs,[[autodoc]] InitProcessGroupKwargs,H2,https://huggingface.co/docs/accelerate/source/package_reference/kwargs#initprocessgroupkwargs,False,34.0,2.0
1629,rendered properly in your Markdown viewer.,,H2,https://huggingface.co/docs/accelerate/source/package_reference/state#rendered-properly-in-your-markdown-viewer,False,0.0,1.0
1630,Stateful Classes,"Below are variations of a [singleton class](https://en.wikipedia.org/wiki/Singleton_pattern) in the sense that all
 instances share the same state, which is initialized on the first instantiation.
 
 These classes are immutable and store information about certain configurations or 
 states.
 
 [[autodoc]] state.PartialState
 
 [[autodoc]] state.AcceleratorState
 
 [[autodoc]] state.GradientState",H1,https://huggingface.co/docs/accelerate/source/package_reference/state#stateful-classes,False,398.0,47.0
1631,rendered properly in your Markdown viewer.,,H2,https://huggingface.co/docs/accelerate/source/package_reference/accelerator#rendered-properly-in-your-markdown-viewer,False,0.0,1.0
1632,Accelerator,"The [`Accelerator`] is the main class provided by 🤗 Accelerate. 
 It serves at the main entry point for the API.",H1,https://huggingface.co/docs/accelerate/source/package_reference/accelerator#accelerator,False,112.0,21.0
1633,Quick adaptation of your code,"To quickly adapt your script to work on any kind of setup with 🤗 Accelerate just:
 
 1. Initialize an [`Accelerator`] object (that we will call `accelerator` throughout this page) as early as possible in your script.
 2. Pass your dataloader(s), model(s), optimizer(s), and scheduler(s) to the [`~Accelerator.prepare`] method.
 3. Remove all the `.cuda()` or `.to(device)` from your code and let the `accelerator` handle the device placement for you. 
 
 <Tip>
 
     Step three is optional, but considered a best practice.
 
 </Tip>
 
 4. Replace `loss.backward()` in your code with `accelerator.backward(loss)`
 5. Gather your predictions and labels before storing them or using them for metric computation using [`~Accelerator.gather`]
 
 <Tip warning={true}>
 
     Step five is mandatory when using distributed evaluation
     
 </Tip>
 
 In most cases this is all that is needed. The next section lists a few more advanced use cases and nice features
 you should search for and replace by the corresponding methods of your `accelerator`:",H2,https://huggingface.co/docs/accelerate/source/package_reference/accelerator#quick-adaptation-of-your-code,False,1043.0,172.0
1634,Advanced recommendations,,H2,https://huggingface.co/docs/accelerate/source/package_reference/accelerator#advanced-recommendations,False,0.0,1.0
1635,Printing,`print` statements should be replaced by [`~Accelerator.print`] to be printed once per process:,H3,https://huggingface.co/docs/accelerate/source/package_reference/accelerator#printing,False,95.0,13.0
1636,Executing processes,,H3,https://huggingface.co/docs/accelerate/source/package_reference/accelerator#executing-processes,False,0.0,1.0
1637,Once on a single server,"For statements that should be executed once per server, use [`~Accelerator.is_local_main_process`]:
 
 ```python
 if accelerator.is_local_main_process:
     do_thing_once_per_server()
 ```
 
 A function can be wrapped using the [`~Accelerator.on_local_main_process`] function to achieve the same 
 behavior on a function's execution:
 
 ```python
 @accelerator.on_local_main_process
 def do_my_thing():
     ""Something done once per server""
     do_thing_once_per_server()
 ```",H4,https://huggingface.co/docs/accelerate/source/package_reference/accelerator#once-on-a-single-server,False,477.0,61.0
1638,Only ever once across all servers,"For statements that should only ever be executed once, use [`~Accelerator.is_main_process`]:
 
 ```python
 if accelerator.is_main_process:
     do_thing_once()
 ```
 
 A function can be wrapped using the [`~Accelerator.on_main_process`] function to achieve the same 
 behavior on a function's execution:
 
 ```python
 @accelerator.on_main_process
 def do_my_thing():
     ""Something done once per server""
     do_thing_once()
 ```",H4,https://huggingface.co/docs/accelerate/source/package_reference/accelerator#only-ever-once-across-all-servers,False,430.0,61.0
1639,On specific processes,"If a function should be ran on a specific overall or local process index, there are similar decorators 
 to achieve this:
 
 ```python
 @accelerator.on_local_process(local_process_idx=0)
 def do_my_thing():
     ""Something done on process index 0 on each server""
     do_thing_on_index_zero_on_each_server()
 ```
 
 ```python
 @accelerator.on_process(process_index=0)
 def do_my_thing():
     ""Something done on process index 0""
     do_thing_on_index_zero()
 ```",H4,https://huggingface.co/docs/accelerate/source/package_reference/accelerator#on-specific-processes,False,463.0,67.0
1640,Synchronicity control,Use [`~Accelerator.wait_for_everyone`] to make sure all processes join that point before continuing. (Useful before a model save for instance).,H3,https://huggingface.co/docs/accelerate/source/package_reference/accelerator#synchronicity-control,False,143.0,19.0
1641,Saving and loading,"```python
 model = MyModel()
 model = accelerator.prepare(model)
 ```
 
 Use [`~Accelerator.save_model`] instead of `torch.save` to save a model. It will remove all model wrappers added during the distributed process, get the state_dict of the model and save it.",H3,https://huggingface.co/docs/accelerate/source/package_reference/accelerator#saving-and-loading,False,262.0,38.0
1642,🤗 Transformers models,"If you are using models from the [🤗 Transformers](https://huggingface.co/docs/transformers/) library, you can use the `.save_pretrained()` method.
 
 ```python
 from transformers import AutoModel
 
 model = AutoModel.from_pretrained(""bert-base-cased"")
 model = accelerator.prepare(model)",H4,https://huggingface.co/docs/accelerate/source/package_reference/accelerator#transformers-models,False,287.0,29.0
1643,Operations,Use [`~Accelerator.clip_grad_norm_`] instead of ``torch.nn.utils.clip_grad_norm_`` and [`~Accelerator.clip_grad_value_`] instead of ``torch.nn.utils.clip_grad_value``,H3,https://huggingface.co/docs/accelerate/source/package_reference/accelerator#operations,False,166.0,10.0
1644,Gradient Accumulation,"To perform gradient accumulation use [`~Accelerator.accumulate`] and specify a gradient_accumulation_steps. 
 This will also automatically ensure the gradients are synced or unsynced when on 
 multi-device training, check if the step should actually be performed, and auto-scale the loss:",H3,https://huggingface.co/docs/accelerate/source/package_reference/accelerator#gradient-accumulation,False,288.0,39.0
1645,GradientAccumulationPlugin,"[[autodoc]] utils.GradientAccumulationPlugin
 
 
 Instead of passing `gradient_accumulation_steps` you can instantiate a GradientAccumulationPlugin and pass it to the [`Accelerator`]'s `__init__`
 as `gradient_accumulation_plugin`. You can only pass either one of `gradient_accumulation_plugin` or `gradient_accumulation_steps` passing both will raise an error.
 ```diff
 from accelerate.utils import GradientAccumulationPlugin",H4,https://huggingface.co/docs/accelerate/source/package_reference/accelerator#gradientaccumulationplugin,False,427.0,43.0
1646,Overall API documentation:,[[autodoc]] Accelerator,H2,https://huggingface.co/docs/accelerate/source/package_reference/accelerator#overall-api-documentation,False,23.0,2.0
1647,rendered properly in your Markdown viewer.,,H2,https://huggingface.co/docs/accelerate/source/package_reference/tracking#rendered-properly-in-your-markdown-viewer,False,0.0,1.0
1648,Experiment Tracking,,H1,https://huggingface.co/docs/accelerate/source/package_reference/tracking#experiment-tracking,False,0.0,1.0
1649,The Base Tracker Class,[[autodoc]] tracking.GeneralTracker,H2,https://huggingface.co/docs/accelerate/source/package_reference/tracking#the-base-tracker-class,False,35.0,2.0
1650,Integrated Trackers,"[[autodoc]] tracking.TensorBoardTracker
     - __init__
 [[autodoc]] tracking.WandBTracker
     - __init__
 [[autodoc]] tracking.CometMLTracker
     - __init__
 [[autodoc]] tracking.AimTracker
     - __init__
 [[autodoc]] tracking.MLflowTracker
     - __init__",H2,https://huggingface.co/docs/accelerate/source/package_reference/tracking#integrated-trackers,False,260.0,40.0
1651,rendered properly in your Markdown viewer.,,H2,https://huggingface.co/docs/accelerate/source/package_reference/big_modeling#rendered-properly-in-your-markdown-viewer,False,0.0,1.0
1652,Working with large models,,H1,https://huggingface.co/docs/accelerate/source/package_reference/big_modeling#working-with-large-models,False,0.0,1.0
1653,Dispatching and Offloading Models,"[[autodoc]] big_modeling.init_empty_weights
 [[autodoc]] big_modeling.cpu_offload
 [[autodoc]] big_modeling.disk_offload
 [[autodoc]] big_modeling.dispatch_model
 [[autodoc]] big_modeling.load_checkpoint_and_dispatch
 [[autodoc]] big_modeling.load_checkpoint_in_model
 [[autodoc]] utils.infer_auto_device_map",H2,https://huggingface.co/docs/accelerate/source/package_reference/big_modeling#dispatching-and-offloading-models,False,308.0,14.0
1654,Model Hooks,,H2,https://huggingface.co/docs/accelerate/source/package_reference/big_modeling#model-hooks,False,0.0,1.0
1655,Hook Classes,"[[autodoc]] hooks.ModelHook
 [[autodoc]] hooks.AlignDevicesHook
 [[autodoc]] hooks.SequentialHook",H3,https://huggingface.co/docs/accelerate/source/package_reference/big_modeling#hook-classes,False,97.0,6.0
1656,Adding Hooks,"[[autodoc]] hooks.add_hook_to_module
 [[autodoc]] hooks.attach_execution_device_hook
 [[autodoc]] hooks.attach_align_device_hook
 [[autodoc]] hooks.attach_align_device_hook_on_blocks",H3,https://huggingface.co/docs/accelerate/source/package_reference/big_modeling#adding-hooks,False,182.0,8.0
1657,Removing Hooks,"[[autodoc]] hooks.remove_hook_from_module
 [[autodoc]] hooks.remove_hook_from_submodules",H3,https://huggingface.co/docs/accelerate/source/package_reference/big_modeling#removing-hooks,False,88.0,4.0
1658,rendered properly in your Markdown viewer.,,H2,https://huggingface.co/docs/accelerate/source/package_reference/utilities#rendered-properly-in-your-markdown-viewer,False,0.0,1.0
1659,Helpful Utilities,"Below are a variety of utility functions that 🤗 Accelerate provides, broken down by use-case.",H1,https://huggingface.co/docs/accelerate/source/package_reference/utilities#helpful-utilities,False,93.0,15.0
1660,Constants,"Constants used throughout 🤗 Accelerate for reference
 
 The following are constants used when utilizing [`Accelerator.save_state`]
 
 `utils.MODEL_NAME`: `""pytorch_model""`
 `utils.OPTIMIZER_NAME`: `""optimizer""`
 `utils.RNG_STATE_NAME`: `""random_states""`
 `utils.SCALER_NAME`: `""scaler.pt`
 `utils.SCHEDULER_NAME`: `""scheduler`
 
 The following are constants used when utilizing [`Accelerator.save_model`]
 
 `utils.WEIGHTS_NAME`: `""pytorch_model.bin""`
 `utils.SAFE_WEIGHTS_NAME`: `""model.safetensors""`
 `utils.WEIGHTS_INDEX_NAME`: `""pytorch_model.bin.index.json""`
 `utils.SAFE_WEIGHTS_INDEX_NAME`: `""model.safetensors.index.json""`",H2,https://huggingface.co/docs/accelerate/source/package_reference/utilities#constants,False,630.0,45.0
1661,Data Classes,"These are basic dataclasses used throughout 🤗 Accelerate and they can be passed in as parameters.
 
 [[autodoc]] utils.DistributedType
 
 [[autodoc]] utils.DynamoBackend
 
 [[autodoc]] utils.LoggerType
 
 [[autodoc]] utils.PrecisionType
 
 [[autodoc]] utils.ProjectConfiguration",H2,https://huggingface.co/docs/accelerate/source/package_reference/utilities#data-classes,False,278.0,31.0
1662,Plugins,"These are plugins that can be passed to the [`Accelerator`] object. While they are defined elsewhere in the documentation, 
 for convience all of them are available to see here:
 
 [[autodoc]] utils.DeepSpeedPlugin
 
 [[autodoc]] utils.FullyShardedDataParallelPlugin
 
 [[autodoc]] utils.GradientAccumulationPlugin
 
 [[autodoc]] utils.MegatronLMPlugin
 
 [[autodoc]] utils.TorchDynamoPlugin",H2,https://huggingface.co/docs/accelerate/source/package_reference/utilities#plugins,False,391.0,45.0
1663,Data Manipulation and Operations,"These include data operations that mimic the same `torch` ops but can be used on distributed processes.
 
 [[autodoc]] utils.broadcast
 
 [[autodoc]] utils.concatenate
 
 [[autodoc]] utils.gather
 
 [[autodoc]] utils.pad_across_processes
 
 [[autodoc]] utils.reduce
 
 [[autodoc]] utils.send_to_device",H2,https://huggingface.co/docs/accelerate/source/package_reference/utilities#data-manipulation-and-operations,False,301.0,35.0
1664,Environment Checks,"These functionalities check the state of the current working environment including information about the operating system itself, what it can support, and if particular dependencies are installed. 
 
 [[autodoc]] utils.is_bf16_available
 
 [[autodoc]] utils.is_ipex_available
 
 [[autodoc]] utils.is_mps_available
 
 [[autodoc]] utils.is_npu_available
 
 [[autodoc]] utils.is_torch_version
 
 [[autodoc]] utils.is_tpu_available
 
 [[autodoc]] utils.is_xpu_available",H2,https://huggingface.co/docs/accelerate/source/package_reference/utilities#environment-checks,False,465.0,49.0
1665,Environment Manipulation,"[[autodoc]] utils.patch_environment
 
 [[autodoc]] utils.clear_environment
 
 [[autodoc]] utils.write_basic_config
 
 When setting up 🤗 Accelerate for the first time, rather than running `accelerate config` [~utils.write_basic_config] can be used as an alternative for quick configuration.",H2,https://huggingface.co/docs/accelerate/source/package_reference/utilities#environment-manipulation,False,289.0,33.0
1666,Memory,"[[autodoc]] utils.get_max_memory
 
 [[autodoc]] utils.find_executable_batch_size",H2,https://huggingface.co/docs/accelerate/source/package_reference/utilities#memory,False,80.0,5.0
1667,Modeling,"These utilities relate to interacting with PyTorch models
 
 [[autodoc]] utils.extract_model_from_parallel
 
 [[autodoc]] utils.get_max_layer_size
 
 [[autodoc]] utils.offload_state_dict",H2,https://huggingface.co/docs/accelerate/source/package_reference/utilities#modeling,False,186.0,17.0
1668,Parallel,"These include general utilities that should be used when working in parallel.
 
 [[autodoc]] utils.extract_model_from_parallel
 
 [[autodoc]] utils.save
 
 [[autodoc]] utils.wait_for_everyone",H2,https://huggingface.co/docs/accelerate/source/package_reference/utilities#parallel,False,191.0,21.0
1669,Random,"These utilities relate to setting and synchronizing of all the random states.
 
 [[autodoc]] utils.set_seed
 
 [[autodoc]] utils.synchronize_rng_state
 
 [[autodoc]] utils.synchronize_rng_states",H2,https://huggingface.co/docs/accelerate/source/package_reference/utilities#random,False,194.0,21.0
1670,PyTorch XLA,"These include utilities that are useful while using PyTorch with XLA.
 
 [[autodoc]] utils.install_xla",H2,https://huggingface.co/docs/accelerate/source/package_reference/utilities#pytorch-xla,False,102.0,14.0
1671,Loading model weights,"These include utilities that are useful to load checkpoints.
 
 [[autodoc]] utils.load_checkpoint_in_model",H2,https://huggingface.co/docs/accelerate/source/package_reference/utilities#loading-model-weights,False,106.0,12.0
1672,Quantization,"These include utilities that are useful to quantize model.
 
 [[autodoc]] utils.load_and_quantize_model
 
 [[autodoc]] utils.BnbQuantizationConfig",H2,https://huggingface.co/docs/accelerate/source/package_reference/utilities#quantization,False,146.0,15.0
1673,rendered properly in your Markdown viewer.,,H2,https://huggingface.co/docs/accelerate/source/package_reference/deepspeed#rendered-properly-in-your-markdown-viewer,False,0.0,1.0
1674,Utilities for DeepSpeed,"[[autodoc]] utils.DeepSpeedPlugin
 
 [[autodoc]] utils.DummyOptim
 
 [[autodoc]] utils.DummyScheduler
 
 [[autodoc]] utils.DeepSpeedEngineWrapper
 
 [[autodoc]] utils.DeepSpeedOptimizerWrapper
 
 [[autodoc]] utils.DeepSpeedSchedulerWrapper",H1,https://huggingface.co/docs/accelerate/source/package_reference/deepspeed#utilities-for-deepspeed,False,239.0,17.0
1675,rendered properly in your Markdown viewer.,,H2,https://huggingface.co/docs/accelerate/source/package_reference/logging#rendered-properly-in-your-markdown-viewer,False,0.0,1.0
1676,Logging with Accelerate,"Accelerate has its own logging utility to handle logging while in a distributed system.
 To utilize this replace cases of `logging` with `accelerate.logging`:",H1,https://huggingface.co/docs/accelerate/source/package_reference/logging#logging-with-accelerate,False,158.0,23.0
1677,Setting the log level,"The log level can be set with the `ACCELERATE_LOG_LEVEL` environment variable or by passing 
 `log_level` to `get_logger`:
 ```python
 from accelerate.logging import get_logger
 
 logger = get_logger(__name__, log_level=""INFO"")
 ```
 
 [[autodoc]] logging.get_logger",H2,https://huggingface.co/docs/accelerate/source/package_reference/logging#setting-the-log-level,False,266.0,32.0
1678,rendered properly in your Markdown viewer.,,H2,https://huggingface.co/docs/accelerate/source/package_reference/fsdp#rendered-properly-in-your-markdown-viewer,False,0.0,1.0
1679,Utilities for Fully Sharded Data Parallelism,[[autodoc]] utils.FullyShardedDataParallelPlugin,H1,https://huggingface.co/docs/accelerate/source/package_reference/fsdp#utilities-for-fully-sharded-data-parallelism,False,48.0,2.0
1680,rendered properly in your Markdown viewer.,,H2,https://huggingface.co/docs/accelerate/source/package_reference/torch_wrappers#rendered-properly-in-your-markdown-viewer,False,0.0,1.0
1681,"Wrapper classes for torch Dataloaders, Optimizers, and Schedulers","The internal classes Accelerate uses to prepare objects for distributed training
 when calling [`~Accelerator.prepare`].",H1,https://huggingface.co/docs/accelerate/source/package_reference/torch_wrappers#wrapper-classes-for-torch-dataloaders-optimizers-and-schedulers,False,120.0,14.0
1682,Datasets and DataLoaders,"[[autodoc]] data_loader.prepare_data_loader
 [[autodoc]] data_loader.skip_first_batches
 
 [[autodoc]] data_loader.BatchSamplerShard
 [[autodoc]] data_loader.IterableDatasetShard
 [[autodoc]] data_loader.DataLoaderShard
 [[autodoc]] data_loader.DataLoaderDispatcher",H2,https://huggingface.co/docs/accelerate/source/package_reference/torch_wrappers#datasets-and-dataloaders,False,265.0,13.0
1683,Optimizers,[[autodoc]] optimizer.AcceleratedOptimizer,H2,https://huggingface.co/docs/accelerate/source/package_reference/torch_wrappers#optimizers,False,42.0,2.0
1684,Schedulers,[[autodoc]] scheduler.AcceleratedScheduler,H2,https://huggingface.co/docs/accelerate/source/package_reference/torch_wrappers#schedulers,False,42.0,2.0
1685,rendered properly in your Markdown viewer.,,H2,https://huggingface.co/docs/accelerate/source/package_reference/launchers#rendered-properly-in-your-markdown-viewer,False,0.0,1.0
1686,Launchers,"Functions for launching training on distributed processes.
 
 
 [[autodoc]] accelerate.notebook_launcher
 [[autodoc]] accelerate.debug_launcher",H1,https://huggingface.co/docs/accelerate/source/package_reference/launchers#launchers,False,143.0,13.0
1687,rendered properly in your Markdown viewer.,,H2,https://huggingface.co/docs/accelerate/source/usage_guides/debug#rendered-properly-in-your-markdown-viewer,False,0.0,1.0
1688,Debugging Distributed Operations,"When running scripts in a distributed fashion, often functions such as [`Accelerator.gather`] and [`Accelerator.reduce`] (and others) are neccessary to grab tensors across devices and perform certain operations on them. However, if the tensors which are being grabbed are not the proper shapes then this will result in your code hanging forever. The only sign that exists of this truly happening is hitting a timeout exception from `torch.distributed`, but this can get quite costly as usually the timeout is 10 minutes.
 
 Accelerate now has a `debug` mode which adds a neglible amount of time to each operation, but allows it to verify that the inputs you are bringing in can *actually* perform the operation you want **without** hitting this timeout problem!",H1,https://huggingface.co/docs/accelerate/source/usage_guides/debug#debugging-distributed-operations,False,761.0,121.0
1689,Visualizing the problem,"To have a tangible example of this issue, let's take the following setup (on 2 GPUs):
 
 ```python
 from accelerate import PartialState
 
 state = PartialState()
 if state.process_index == 0:
     tensor = torch.tensor([[0.0, 1, 2, 3, 4]]).to(state.device)
 else:
     tensor = torch.tensor([[[0.0, 1, 2, 3, 4], [5, 6, 7, 8, 9]]]).to(state.device)
 
 broadcast_tensor = broadcast(tensor)
 print(broadcast_tensor)
 ```
 
 We've created a single tensor on each device, with two radically different shapes. With this setup if we want to perform an operation such as [`utils.broadcast`], we would forever hit a timeout because `torch.distributed` requires that these operations have the **exact same shape** across all processes for it to work.
 
 If you run this yourself, you will find that `broadcast_tensor` can be printed on the main process, but its results won't quite be right, and then it will just hang never printing it on any of the other processes:
 
 ```
 >>> tensor([[0, 1, 2, 3, 4]], device='cuda:0')
 ```",H2,https://huggingface.co/docs/accelerate/source/usage_guides/debug#visualizing-the-problem,False,1017.0,165.0
1690,The solution,"By enabling Accelerate's operational debug mode, Accelerate will properly find and catch errors such as this and provide a very clear traceback immediatly: 
 
 ```
 Traceback (most recent call last):
   File ""/home/zach_mueller_huggingface_co/test.py"", line 18, in <module>
     main()
   File ""/home/zach_mueller_huggingface_co/test.py"", line 15, in main
         main()broadcast_tensor = broadcast(tensor)
   File ""/home/zach_mueller_huggingface_co/accelerate/src/accelerate/utils/operations.py"", line 303, in wrapper
     broadcast_tensor = broadcast(tensor)
 accelerate.utils.operations.DistributedOperationException: Cannot apply desired operation due to shape mismatches. All shapes across devices must be valid.
 
 Operation: `accelerate.utils.operations.broadcast`
 Input shapes:
   - Process 0: [1, 5]
   - Process 1: [1, 2, 5]
 ```
 
 This explains that the shapes across our devices were *not* the same, and that we should ensure that they match properly to be compatible. Typically this means that there is either an extra dimension, or certain dimensions are incompatible with the operation.
 
 To enable this please do one of the following:
 
 Enable it through the questionarre during `accelerate config` (recommended)
 
 From the CLI: 
 
 ```
 accelerate launch --debug {my_script.py} --arg1 --arg2
 ```
 
 As an environmental variable (which avoids the need for `accelerate launch`):
 
 ```
 ACCELERATE_DEBUG_MODE=""1"" accelerate launch {my_script.py} --arg1 --arg2
 ```
 
 Manually changing the `config.yaml` file:
 
 ```diff
  compute_environment: LOCAL_MACHINE
 +debug: true
 ```",H2,https://huggingface.co/docs/accelerate/source/usage_guides/debug#the-solution,False,1598.0,227.0
1691,rendered properly in your Markdown viewer.,,H2,https://huggingface.co/docs/accelerate/source/usage_guides/sagemaker#rendered-properly-in-your-markdown-viewer,False,0.0,1.0
1692,Amazon SageMaker,"Hugging Face and Amazon introduced new [Hugging Face Deep Learning Containers (DLCs)](https://github.com/aws/deep-learning-containers/blob/master/available_images.md#huggingface-training-containers) to
 make it easier than ever to train Hugging Face Transformer models in [Amazon SageMaker](https://aws.amazon.com/sagemaker/).",H1,https://huggingface.co/docs/accelerate/source/usage_guides/sagemaker#amazon-sagemaker,False,326.0,27.0
1693,Getting Started,,H2,https://huggingface.co/docs/accelerate/source/usage_guides/sagemaker#getting-started,False,0.0,1.0
1694,Setup & Installation,"Before you can run your 🤗 Accelerate scripts on Amazon SageMaker you need to sign up for an AWS account. If you do not
 have an AWS account yet learn more [here](https://docs.aws.amazon.com/sagemaker/latest/dg/gs-set-up.html).
 
 After you have your AWS Account you need to install the `sagemaker` sdk for 🤗 Accelerate with:
 
 ```bash
 pip install ""accelerate[sagemaker]"" --upgrade
 ```
 
 🤗 Accelerate currently uses the 🤗 DLCs, with `transformers`, `datasets` and `tokenizers` pre-installed. 🤗
 Accelerate is not in the DLC yet (will soon be added!) so to use it within Amazon SageMaker you need to create a
 `requirements.txt` in the same directory where your training script is located and add it as dependency:
 
 ```
 accelerate
 ```
 
 You should also add any other dependencies you have to this `requirements.txt`.",H3,https://huggingface.co/docs/accelerate/source/usage_guides/sagemaker#setup-installation,False,823.0,128.0
1695,Configure 🤗 Accelerate,"You can configure the launch configuration for Amazon SageMaker the same as you do for non SageMaker training jobs with
 the 🤗 Accelerate CLI:
 
 ```bash
 accelerate config",H3,https://huggingface.co/docs/accelerate/source/usage_guides/sagemaker#configure-accelerate,False,172.0,28.0
1696,Prepare a 🤗 Accelerate fine-tuning script,"The training script is very similar to a training script you might run outside of SageMaker, but to save your model
 after training you need to specify either `/opt/ml/model` or use `os.environ[""SM_MODEL_DIR""]` as your save
 directory. After training, artifacts in this directory are uploaded to S3:",H3,https://huggingface.co/docs/accelerate/source/usage_guides/sagemaker#prepare-a-accelerate-fine-tuning-script,False,299.0,46.0
1697,Launch Training,"You can launch your training with 🤗 Accelerate CLI with:
 
 ```
 accelerate launch path_to_script.py --args_to_the_script
 ```
 
 This will launch your training script using your configuration. The only thing you have to do is provide all the
 arguments needed by your training script as named arguments.
 
 **Examples**
 
 <Tip>
 
     If you run one of the example scripts, don't forget to add `accelerator.save('/opt/ml/model')` to it.
 
 </Tip>
 
 ```bash
 accelerate launch ./examples/sagemaker_example.py
 ```
 
 Outputs:
 
 ```
 Configuring Amazon SageMaker environment
 Converting Arguments to Hyperparameters
 Creating Estimator
 2021-04-08 11:56:50 Starting - Starting the training job...
 2021-04-08 11:57:13 Starting - Launching requested ML instancesProfilerReport-1617883008: InProgress
 .........
 2021-04-08 11:58:54 Starting - Preparing the instances for training.........
 2021-04-08 12:00:24 Downloading - Downloading input data
 2021-04-08 12:00:24 Training - Downloading the training image..................
 2021-04-08 12:03:39 Training - Training image download completed. Training in progress..
 ........
 epoch 0: {'accuracy': 0.7598039215686274, 'f1': 0.8178438661710037}
 epoch 1: {'accuracy': 0.8357843137254902, 'f1': 0.882249560632689}
 epoch 2: {'accuracy': 0.8406862745098039, 'f1': 0.8869565217391304}
 ........
 2021-04-08 12:05:40 Uploading - Uploading generated training model
 2021-04-08 12:05:40 Completed - Training job completed
 Training seconds: 331
 Billable seconds: 331
 You can find your model data at: s3://your-bucket/accelerate-sagemaker-1-2021-04-08-11-56-47-108/output/model.tar.gz
 ```",H3,https://huggingface.co/docs/accelerate/source/usage_guides/sagemaker#launch-training,False,1637.0,196.0
1698,Advanced Features,,H2,https://huggingface.co/docs/accelerate/source/usage_guides/sagemaker#advanced-features,False,0.0,1.0
1699,Distributed Training: Data Parallelism,"Set up the accelerate config by running `accelerate config` and answer the SageMaker questions and set it up.
 To use SageMaker DDP, select it when asked 
 `What is the distributed mode? ([0] No distributed training, [1] data parallelism):`.
 Example config below:
 ```yaml
 base_job_name: accelerate-sagemaker-1
 compute_environment: AMAZON_SAGEMAKER
 distributed_type: DATA_PARALLEL
 ec2_instance_type: ml.p3.16xlarge
 iam_role_name: xxxxx
 image_uri: null
 mixed_precision: fp16
 num_machines: 1
 profile: xxxxx
 py_version: py38
 pytorch_version: 1.10.2
 region: us-east-1
 transformers_version: 4.17.0
 use_cpu: false
 ```",H3,https://huggingface.co/docs/accelerate/source/usage_guides/sagemaker#distributed-training-data-parallelism,False,627.0,72.0
1700,Distributed Training: Model Parallelism,"*currently in development, will be supported soon.*",H3,https://huggingface.co/docs/accelerate/source/usage_guides/sagemaker#distributed-training-model-parallelism,False,51.0,7.0
1701,Python packages and dependencies,"🤗 Accelerate currently uses the 🤗 DLCs, with `transformers`, `datasets` and `tokenizers` pre-installed. If you
 want to use different/other Python packages you can do this by adding them to the `requirements.txt`. These packages
 will be installed before your training script is started.",H3,https://huggingface.co/docs/accelerate/source/usage_guides/sagemaker#python-packages-and-dependencies,False,287.0,42.0
1702,Local Training: SageMaker Local mode,"The local mode in the SageMaker SDK allows you to run your training script locally inside the HuggingFace DLC (Deep Learning container) 
 or using your custom container image. This is useful for debugging and testing your training script inside the final container environment.
 Local mode uses Docker compose (*Note: Docker Compose V2 is not supported yet*). The SDK will handle the authentication against ECR
 to pull the DLC to your local environment. You can emulate CPU (single and multi-instance) and GPU (single instance) SageMaker training jobs.
 
 To use local mode, you need to set your `ec2_instance_type` to `local`.
 
 ```yaml
 ec2_instance_type: local
 ```",H3,https://huggingface.co/docs/accelerate/source/usage_guides/sagemaker#local-training-sagemaker-local-mode,False,670.0,105.0
1703,Advanced configuration,"The configuration allows you to override parameters for the [Estimator](https://sagemaker.readthedocs.io/en/stable/api/training/estimators.html).
 These settings have to be applied in the config file and are not part of `accelerate config`. You can control many additional aspects of the training job, e.g. use Spot instances, enable network isolation and many more.
 
 ```yaml
 additional_args:
   # enable network isolation to restrict internet access for containers
   enable_network_isolation: True
 ```
 
 You can find all available configuration [here](https://sagemaker.readthedocs.io/en/stable/api/training/estimators.html).",H3,https://huggingface.co/docs/accelerate/source/usage_guides/sagemaker#advanced-configuration,False,632.0,75.0
1704,Use Spot Instances,"You can use Spot Instances e.g. using (see [Advanced configuration](#advanced-configuration)):
 ```yaml
 additional_args:
   use_spot_instances: True
   max_wait: 86400
 ```
 
 *Note: Spot Instances are subject to be terminated and training to be continued from a checkpoint. This is not handled in 🤗 Accelerate out of the box. Contact us if you would like this feature.*",H3,https://huggingface.co/docs/accelerate/source/usage_guides/sagemaker#use-spot-instances,False,371.0,57.0
1705,Remote scripts: Use scripts located on Github,*undecided if feature is needed. Contact us if you would like this feature.*,H3,https://huggingface.co/docs/accelerate/source/usage_guides/sagemaker#remote-scripts-use-scripts-located-on-github,False,76.0,13.0
1706,rendered properly in your Markdown viewer.,,H2,https://huggingface.co/docs/accelerate/source/usage_guides/explore#rendered-properly-in-your-markdown-viewer,False,0.0,1.0
1707,Learning how to incorporate 🤗 Accelerate features quickly!,"Please use the interactive tool below to help you get started with learning about a particular 
 feature of 🤗 Accelerate and how to utilize it! It will provide you with a code diff, an explaination
 towards what is going on, as well as provide you with some useful links to explore more within
 the documentation!
 
 Most code examples start from the following python code before integrating 🤗 Accelerate in some way:
 
 ```python
 for batch in dataloader:
     optimizer.zero_grad()
     inputs, targets = batch
     inputs = inputs.to(device)
     targets = targets.to(device)
     outputs = model(inputs)
     loss = loss_function(outputs, targets)
     loss.backward()
     optimizer.step()
     scheduler.step()
 ```
 
 <div class=""block dark:hidden"">
 	<iframe 
         src=""https://muellerzr-accelerate-examples.hf.space?__theme=light""
         width=""850""
         height=""1600""
     ></iframe>
 </div>
 <div class=""hidden dark:block"">
     <iframe 
         src=""https://muellerzr-accelerate-examples.hf.space?__theme=dark""
         width=""850""
         height=""1600""
     ></iframe>
 </div>",H1,https://huggingface.co/docs/accelerate/source/usage_guides/explore#learning-how-to-incorporate-accelerate-features-quickly,False,1101.0,218.0
1708,rendered properly in your Markdown viewer.,,H2,https://huggingface.co/docs/accelerate/source/usage_guides/distributed_inference#rendered-properly-in-your-markdown-viewer,False,0.0,1.0
1709,Distributed Inference with 🤗 Accelerate,"Distributed inference is a common use case, especially with natural language processing (NLP) models. Users often want to
 send a number of different prompts, each to a different GPU, and then get the results back. This also has other cases
 outside of just NLP, however for this tutorial we will focus on just this idea of each GPU receiving a different prompt,
 and then returning the results.",H1,https://huggingface.co/docs/accelerate/source/usage_guides/distributed_inference#distributed-inference-with-accelerate,False,395.0,67.0
1710,The Problem,"Normally when doing this, users send the model to a specific device to load it from the CPU, and then move each prompt to a different device. 
 
 A basic pipeline using the `diffusers` library might look something like so:
 
 ```python
 import torch
 import torch.distributed as dist
 from diffusers import DiffusionPipeline
 
 pipe = DiffusionPipeline.from_pretrained(""runwayml/stable-diffusion-v1-5"", torch_dtype=torch.float16)
 ```
 Followed then by performing inference based on the specific prompt:
 
 ```python
 def run_inference(rank, world_size):
     dist.init_process_group(""nccl"", rank=rank, world_size=world_size)
     pipe.to(rank)
 
     if torch.distributed.get_rank() == 0:
         prompt = ""a dog""
     elif torch.distributed.get_rank() == 1:
         prompt = ""a cat""
 
     result = pipe(prompt).images[0]
     result.save(f""result_{rank}.png"")
 ```
 One will notice how we have to check the rank to know what prompt to send, which can be a bit tedious.
 
 A user might then also think that with 🤗 Accelerate, using the `Accelerator` to prepare a dataloader for such a task might also be 
 a simple way to manage this. (To learn more, check out the relvent section in the [Quick Tour](../quicktour#distributed-evaluation))
 
 Can it manage it? Yes. Does it add unneeded extra code however: also yes.",H2,https://huggingface.co/docs/accelerate/source/usage_guides/distributed_inference#the-problem,False,1319.0,222.0
1711,The Solution,"With 🤗 Accelerate, we can simplify this process by using the [`Accelerator.split_between_processes`] context manager (which also exists in `PartialState` and `AcceleratorState`). 
 This function will automatically split whatever data you pass to it (be it a prompt, a set of tensors, a dictionary of the prior data, etc.) across all the processes (with a potential
 to be padded) for you to use right away.
 
 Let's rewrite the above example using this context manager:
 
 ```python
 from accelerate import PartialState  # Can also be Accelerator or AcceleratorState
 from diffusers import DiffusionPipeline
 
 pipe = DiffusionPipeline.from_pretrained(""runwayml/stable-diffusion-v1-5"", torch_dtype=torch.float16)
 distributed_state = PartialState()
 pipe.to(distributed_state.device)",H2,https://huggingface.co/docs/accelerate/source/usage_guides/distributed_inference#the-solution,False,783.0,101.0
1712,rendered properly in your Markdown viewer.,,H2,https://huggingface.co/docs/accelerate/source/usage_guides/megatron_lm#rendered-properly-in-your-markdown-viewer,False,0.0,1.0
1713,Megatron-LM,"[Megatron-LM](https://github.com/NVIDIA/Megatron-LM) enables training large transformer language models at scale.
 It provides efficient tensor, pipeline and sequence based model parallelism for pre-training transformer based
 Language Models such as [GPT](https://arxiv.org/abs/2005.14165) (Decoder Only), [BERT](https://arxiv.org/pdf/1810.04805.pdf) (Encoder Only) and [T5](https://arxiv.org/abs/1910.10683) (Encoder-Decoder).
 For detailed information and how things work behind the scene please refer the github [repo](https://github.com/NVIDIA/Megatron-LM).",H1,https://huggingface.co/docs/accelerate/source/usage_guides/megatron_lm#megatron-lm,False,562.0,51.0
1714,What is integrated?,"Accelerate integrates following feature of Megatron-LM to enable large scale pre-training/finetuning
 of BERT (Encoder), GPT (Decoder) or T5 models (Encoder and Decoder):
 
 a. **Tensor Parallelism (TP)**: Reduces memory footprint without much additional communication on intra-node ranks.
 Each tensor is split into multiple chunks with each shard residing on separate GPU. At each step, the same mini-batch of data is processed
 independently and in parallel by each shard followed by syncing across all GPUs (`all-reduce` operation). 
 In a simple transformer layer, this leads to 2 `all-reduces` in the forward path and 2 in the backward path.
 For more details, please refer research paper [Megatron-LM: Training Multi-Billion Parameter Language Models Using
 Model Parallelism](https://arxiv.org/pdf/1909.08053.pdf) and 
 this section of 🤗 blogpost [The Technology Behind BLOOM Training](https://huggingface.co/blog/bloom-megatron-deepspeed#tensor-parallelism).
 
 
 b. **Pipeline Parallelism (PP)**: Reduces memory footprint and enables large scale training via inter-node parallelization. 
 Reduces the bubble of naive PP via PipeDream-Flush schedule/1F1B schedule and Interleaved 1F1B schedule. 
 Layers are distributed uniformly across PP stages. For example, if a model has `24` layers and we have `4` GPUs for
 pipeline parallelism, each GPU will have `6` layers (24/4). For more details on schedules to reduce the idle time of PP,
 please refer to the research paper [Efficient Large-Scale Language Model Training on GPU Clusters
 Using Megatron-LM](https://arxiv.org/pdf/2104.04473.pdf) and 
 this section of 🤗 blogpost [The Technology Behind BLOOM Training](https://huggingface.co/blog/bloom-megatron-deepspeed#pipeline-parallelism).
 
 c. **Sequence Parallelism (SP)**: Reduces memory footprint without any additional communication. Only applicable when using TP.
 It reduces activation memory required as it prevents the same copies to be on the tensor parallel ranks 
 post `all-reduce` by replacing then with `reduce-scatter` and `no-op` operation would be replaced by `all-gather`. 
 As `all-reduce = reduce-scatter + all-gather`, this saves a ton of activation memory at no added communication cost. 
 To put it simply, it shards the outputs of each transformer layer along sequence dimension, e.g., 
 if the sequence length is `1024` and the TP size is `4`, each GPU will have `256` tokens (1024/4) for each sample. 
 This increases the batch size that can be supported for training. For more details, please refer to the research paper
 [Reducing Activation Recomputation in Large Transformer Models](https://arxiv.org/pdf/2205.05198.pdf). 
 
 d. **Data Parallelism (DP)** via Distributed Optimizer: Reduces the memory footprint by sharding optimizer states and gradients across DP ranks
 (versus the traditional method of replicating the optimizer state across data parallel ranks). 
 For example, when using Adam optimizer with mixed-precision training, each parameter accounts for 12 bytes of memory.
 This gets distributed equally across the GPUs, i.e., each parameter would account for 3 bytes (12/4) if we have 4 GPUs.
 For more details, please refer the research paper [ZeRO: Memory Optimizations Toward Training Trillion
 Parameter Models](https://arxiv.org/pdf/1910.02054.pdf) and following section of 🤗 blog 
 [The Technology Behind BLOOM Training](https://huggingface.co/blog/bloom-megatron-deepspeed#zero-data-parallelism).
 
 e. **Selective Activation Recomputation**: Reduces the memory footprint of activations significantly via smart activation checkpointing.
 It doesn't store activations occupying large memory while being fast to recompute thereby achieving great tradeoff between memory and recomputation.
 For example, for GPT-3, this leads to 70% reduction in required memory for activations at the expense of
 only 2.7% FLOPs overhead for recomputation of activations. For more details, please refer to the research paper 
 [Reducing Activation Recomputation in Large Transformer Models](https://arxiv.org/pdf/2205.05198.pdf).
 
 f. **Fused Kernels**: Fused Softmax, Mixed Precision Fused Layer Norm and  Fused gradient accumulation to weight gradient computation of linear layer.
 PyTorch JIT compiled Fused GeLU and Fused Bias+Dropout+Residual addition.
 
 g. **Support for Indexed datasets**: Efficient binary format of datasets for large scale training. Support for the `mmap`, `cached` index file and the `lazy` loader format.
 
 h. **Checkpoint reshaping and interoperability**: Utility for reshaping Megatron-LM checkpoints of variable 
 tensor and pipeline parallel sizes to the beloved 🤗 Transformers sharded checkpoints as it has great support with plethora of tools
 such as 🤗 Accelerate Big Model Inference, Megatron-DeepSpeed Inference etc. 
 Support is also available for converting 🤗 Transformers sharded checkpoints to Megatron-LM checkpoint of variable tensor and pipeline parallel sizes
 for large scale training.",H2,https://huggingface.co/docs/accelerate/source/usage_guides/megatron_lm#what-is-integrated,False,4977.0,676.0
1715,Pre-Requisites,"You will need to install the latest pytorch, cuda, nccl, and NVIDIA [APEX](https://github.com/NVIDIA/apex#quick-start) releases and the nltk library.
 See [documentation](https://github.com/NVIDIA/Megatron-LM#setup) for more details. 
 Another way to setup the environment is to pull an NVIDIA PyTorch Container that comes with all the required installations from NGC.
 
 Below is a step-by-step method to set up the conda environment:
 
 1. Create a virtual environment
 ```
 conda create --name ml
 ```
 
 2. Assuming that the machine has CUDA 11.3 installed, installing the corresponding PyTorch GPU Version
 ```
 conda install pytorch torchvision torchaudio cudatoolkit=11.3 -c pytorch
 ```
 
 3. Install Nvidia APEX
 ```
 git clone https://github.com/NVIDIA/apex
 cd apex
 pip install -v --disable-pip-version-check --no-cache-dir --global-option=""--cpp_ext"" --global-option=""--cuda_ext"" ./
 cd ..
 ```
 
 4. Installing Megatron-LM
 
 ```
 pip install git+https://github.com/huggingface/Megatron-LM.git
 ```",H2,https://huggingface.co/docs/accelerate/source/usage_guides/megatron_lm#pre-requisites,False,1012.0,128.0
1716,Accelerate Megatron-LM Plugin,"Important features are directly supported via the `accelerate config` command. 
 An example of thr corresponding questions for using Megatron-LM features is shown below:
 
 ```bash
 :~$ accelerate config --config_file ""megatron_gpt_config.yaml""
 In which compute environment are you running? ([0] This machine, [1] AWS (Amazon SageMaker)): 0
 Which type of machine are you using? ([0] No distributed training, [1] multi-CPU, [2] multi-GPU, [3] TPU): 2
 How many different machines will you use (use more than 1 for multi-node training)? [1]: 
 Do you want to use DeepSpeed? [yes/NO]: 
 Do you want to use FullyShardedDataParallel? [yes/NO]: 
 Do you want to use Megatron-LM ? [yes/NO]: yes
 What is the Tensor Parallelism degree/size? [1]:2
 Do you want to enable Sequence Parallelism? [YES/no]: 
 What is the Pipeline Parallelism degree/size? [1]:2
 What is the number of micro-batches? [1]:2
 Do you want to enable selective activation recomputation? [YES/no]: 
 Do you want to use distributed optimizer which shards optimizer state and gradients across data pralellel ranks? [YES/no]: 
 What is the gradient clipping value based on global L2 Norm (0 to disable)? [1.0]: 
 How many GPU(s) should be used for distributed training? [1]:4
 Do you wish to use FP16 or BF16 (mixed precision)? [NO/fp16/bf16]: bf16
 ```
 
 The resulting config is shown below:
 
 ```
 ~$ cat megatron_gpt_config.yaml 
 compute_environment: LOCAL_MACHINE
 deepspeed_config: {}
 distributed_type: MEGATRON_LM
 downcast_bf16: 'no'
 fsdp_config: {}
 machine_rank: 0
 main_process_ip: null
 main_process_port: null
 main_training_function: main
 megatron_lm_config:
   megatron_lm_gradient_clipping: 1.0
   megatron_lm_num_micro_batches: 2
   megatron_lm_pp_degree: 2
   megatron_lm_recompute_activations: true
   megatron_lm_sequence_parallelism: true
   megatron_lm_tp_degree: 2
   megatron_lm_use_distributed_optimizer: true
 mixed_precision: bf16
 num_machines: 1
 num_processes: 4
 rdzv_backend: static
 same_network: true
 use_cpu: false
 ```
 
 We will take the example of GPT pre-training. The minimal changes required to the official `run_clm_no_trainer.py` 
 to use Megatron-LM are as follows:
 
 1. As Megatron-LM uses its own implementation of Optimizer, the corresponding scheduler compatible with it needs to be used.
 As such, support for only the Megatron-LM's scheduler is present. User will need to create `accelerate.utils.MegatronLMDummyScheduler`.
 Example is given below:
 
 ```python
 from accelerate.utils import MegatronLMDummyScheduler
 
 if accelerator.distributed_type == DistributedType.MEGATRON_LM:
     lr_scheduler = MegatronLMDummyScheduler(
         optimizer=optimizer,
         total_num_steps=args.max_train_steps,
         warmup_num_steps=args.num_warmup_steps,
     )
 else:
     lr_scheduler = get_scheduler(
         name=args.lr_scheduler_type,
         optimizer=optimizer,
         num_warmup_steps=args.num_warmup_steps * args.gradient_accumulation_steps,
         num_training_steps=args.max_train_steps * args.gradient_accumulation_steps,
     )
 ```
 
 2. Getting the details of the total batch size now needs to be cognization of tensor and pipeline parallel sizes.
 Example of getting the effective total batch size is shown below:
 
 ```python
 if accelerator.distributed_type == DistributedType.MEGATRON_LM:
     total_batch_size = accelerator.state.megatron_lm_plugin.global_batch_size
 else:
     total_batch_size = args.per_device_train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps
 ```
 
 3. When using Megatron-LM, the losses are already averaged across the data parallel group
 
 ```python
 if accelerator.distributed_type == DistributedType.MEGATRON_LM:
     losses.append(loss)
 else:
     losses.append(accelerator.gather_for_metrics(loss.repeat(args.per_device_eval_batch_size)))
 
 if accelerator.distributed_type == DistributedType.MEGATRON_LM:
     losses = torch.tensor(losses)
 else:
     losses = torch.cat(losses)
 ```
 
 4. For Megatron-LM, we need to save the model using `accelerator.save_state`
 
 ```python
 if accelerator.distributed_type == DistributedType.MEGATRON_LM:
     accelerator.save_state(args.output_dir)
 else:
     unwrapped_model = accelerator.unwrap_model(model)
     unwrapped_model.save_pretrained(
         args.output_dir, is_main_process=accelerator.is_main_process, save_function=accelerator.save
     )
 ```
 
 That's it! We are good to go 🚀. Please find the example script in the examples folder at the path `accelerate/examples/by_feature/megatron_lm_gpt_pretraining.py`.
 Let's run it for `gpt-large` model architecture using 4 A100-80GB GPUs.
 
 ```bash
 accelerate launch --config_file megatron_gpt_config.yaml \",H2,https://huggingface.co/docs/accelerate/source/usage_guides/megatron_lm#accelerate-megatron-lm-plugin,False,4718.0,648.0
1717,Advanced features to leverage writing custom train step and Megatron-LM Indexed Datasets,"For leveraging more features, please go through below details.
 
 1. Below is an example of changes required to customize the Train Step while using Megatron-LM. 
 You will implement the `accelerate.utils.AbstractTrainStep` or inherit from their corresponding children 
 `accelerate.utils.GPTTrainStep`, `accelerate.utils.BertTrainStep` or `accelerate.utils.T5TrainStep`.
 
 ```python
 from accelerate.utils import MegatronLMDummyScheduler, GPTTrainStep, avg_losses_across_data_parallel_group",H2,https://huggingface.co/docs/accelerate/source/usage_guides/megatron_lm#advanced-features-to-leverage-writing-custom-train-step-and-megatron-lm-indexed-datasets,False,492.0,51.0
1718,Utility for Checkpoint reshaping and interoperability,"1. The scripts for these are present in 🤗 Transformers library under respective models. 
 Currently, it is available for GPT model [checkpoint_reshaping_and_interoperability.py](https://github.com/huggingface/transformers/blob/main/src/transformers/models/megatron_gpt2/checkpoint_reshaping_and_interoperability.py)
 
 2. Below is an example of conversion of checkpoint from Megatron-LM to universal 🤗 Transformers sharded checkpoint.
 ```bash",H2,https://huggingface.co/docs/accelerate/source/usage_guides/megatron_lm#utility-for-checkpoint-reshaping-and-interoperability,False,443.0,42.0
1719,Megatron-LM GPT models support returning logits and `megatron_generate` function for text generation,"1. Returning logits require setting `require_logits=True` in MegatronLMPlugin as shown below. 
 These would be available on the in the last stage of pipeline.
 ```python
 megatron_lm_plugin = MegatronLMPlugin(return_logits=True)
 ```
 
 2. `megatron_generate` method for Megatron-LM GPT model: This will use Tensor and Pipeline Parallelism to complete 
 generations for a batch of inputs when using greedy with/without top_k/top_p sampling and for individual prompt inputs when using beam search decoding. 
 Only a subset of features of transformers generate is supported. This will help in using large models via tensor and pipeline parallelism 
 for generation (already does key-value caching and uses fused kernels by default).
 This requires data parallel size to be 1, sequence parallelism and activation checkpointing to be disabled.
 It also requires specifying path to tokenizer's vocab file and merges file. 
 Below example shows how to configure and use `megatron_generate` method for Megatron-LM GPT model.
 ```python",H2,https://huggingface.co/docs/accelerate/source/usage_guides/megatron_lm#megatron-lm-gpt-models-support-returning-logits-and-megatron-generate-function-for-text-generation,False,1028.0,149.0
1720,Support for ROPE and ALiBi Positional embeddings and Multi-Query Attention,"1. For ROPE/ALiBi attention, pass `position_embedding_type` with `(""absolute"" | ""rotary"" | ""alibi"")` to `MegatronLMPlugin` as shown below.
 ```python
 other_megatron_args = {""position_embedding_type"": ""alibi""}
 megatron_lm_plugin = MegatronLMPlugin(other_megatron_args=other_megatron_args)
 ```
 
 2. For Multi-Query Attention, pass `attention_head_type` with `(""multihead"" | ""multiquery"")` to `MegatronLMPlugin` as shown below.
 ```python
 other_megatron_args = {""attention_head_type"": ""multiquery""}
 megatron_lm_plugin = MegatronLMPlugin(other_megatron_args=other_megatron_args)
 ```",H2,https://huggingface.co/docs/accelerate/source/usage_guides/megatron_lm#support-for-rope-and-alibi-positional-embeddings-and-multi-query-attention,False,585.0,51.0
1721,Caveats,"1. Supports Transformers GPT2, Megatron-BERT and T5 models.
 This covers Decoder only, Encode only and Encoder-Decoder model classes.
 
 2. Only loss is returned from model forward pass as 
 there is quite complex interplay of pipeline, tensor and data parallelsim behind the scenes.
 The `model(**batch_data)` call return loss(es) averaged across the data parallel ranks.
 This is fine for most cases wherein pre-training jobs are run using Megatron-LM features and
 you can easily compute the `perplexity` using the loss. 
 For GPT model, returning logits in addition to loss(es) is supported. 
 These logits aren't gathered across data prallel ranks. Use `accelerator.utils.gather_across_data_parallel_groups`
 to gather logits across data parallel ranks. These logits along with labels can be used for computing various 
 performance metrics. 
 
 3. The main process is the last rank as the losses/logits are available in the last stage of pipeline.
 `accelerator.is_main_process` and `accelerator.is_local_main_process` return `True` for last rank when using 
 Megatron-LM integration.
 
 4. In `accelerator.prepare` call, a Megatron-LM model corresponding to a given Transformers model is created
 with random weights. Please use `accelerator.load_state` to load the Megatron-LM checkpoint with matching TP, PP and DP partitions.
 
 5. Currently, checkpoint reshaping and interoperability support is only available for GPT. 
 Soon it will be extended to BERT and T5.
 
 6. `gradient_accumulation_steps` needs to be 1. When using Megatron-LM, micro batches in pipeline parallelism 
 setting is synonymous with gradient accumulation. 
 
 7. When using Megatron-LM, use `accelerator.save_state` and `accelerator.load_state` for saving and loading checkpoints.
 
 8. Below are the mapping from Megatron-LM model architectures to the the equivalent 🤗 transformers model architectures.
 Only these 🤗 transformers model architectures are supported.
 
 a. Megatron-LM [BertModel](https://github.com/NVIDIA/Megatron-LM/blob/main/megatron/model/bert_model.py) : 
 🤗 transformers models with `megatron-bert` in config's model type, e.g., 
 [MegatronBERT](https://huggingface.co/docs/transformers/model_doc/megatron-bert)
     
 b. Megatron-LM [GPTModel](https://github.com/NVIDIA/Megatron-LM/blob/main/megatron/model/gpt_model.py) : 
 🤗 transformers models with `gpt2` in config's model type, e.g., 
 [OpenAI GPT2](https://huggingface.co/docs/transformers/model_doc/gpt2)
    
 c. Megatron-LM [T5Model](https://github.com/NVIDIA/Megatron-LM/blob/main/megatron/model/t5_model.py) : 
 🤗 transformers models with `t5` in  config's model type, e.g., 
 [T5](https://huggingface.co/docs/transformers/model_doc/t5) and 
 [MT5](https://huggingface.co/docs/transformers/model_doc/mt5)",H2,https://huggingface.co/docs/accelerate/source/usage_guides/megatron_lm#caveats,False,2770.0,343.0
1722,rendered properly in your Markdown viewer.,,H2,https://huggingface.co/docs/accelerate/source/usage_guides/tracking#rendered-properly-in-your-markdown-viewer,False,0.0,1.0
1723,Tracking,"There are a large number of experiment tracking API's available, however getting them all to work with in a multi-processing environment can oftentimes be complex.
 🤗 Accelerate provides a general tracking API that can be used to log useful items during your script through [`Accelerator.log`]",H1,https://huggingface.co/docs/accelerate/source/usage_guides/tracking#tracking,False,293.0,45.0
1724,Integrated Trackers,Currently `Accelerate` supports four trackers out-of-the-box:,H2,https://huggingface.co/docs/accelerate/source/usage_guides/tracking#integrated-trackers,False,61.0,6.0
1725,Implementing Custom Trackers,"To implement a new tracker to be used in `Accelerator`, a new one can be made through implementing the [`GeneralTracker`] class.
 Every tracker must implement three functions and have three properties:
   - `__init__`: 
     - Should store a `run_name` and initialize the tracker API of the integrated library. 
     - If a tracker stores their data locally (such as TensorBoard), a `logging_dir` parameter can be added.
   - `store_init_configuration`: 
     - Should take in a `values` dictionary and store them as a one-time experiment configuration
   - `log`: 
     - Should take in a `values` dictionary and a `step`, and should log them to the run
 
   - `name` (`str`):
     - A unique string name for the tracker, such as `""wandb""` for the wandb tracker. 
     - This will be used for interacting with this tracker specifically
   - `requires_logging_directory` (`bool`):
     - Whether a `logging_dir` is needed for this particular tracker and if it uses one.
   - `tracker`: 
     - This should be implemented as a `@property` function 
     - Should return the internal tracking mechanism the library uses, such as the `run` object for `wandb`.
 
 Each method should also utilize the [`state.PartialState`] class if the logger should only be executed on the main process for instance.
 
 A brief example can be seen below with an integration with Weights and Biases, containing only the relevant information and logging just on 
 the main process:
 ```python
 from accelerate.tracking import GeneralTracker, on_main_process
 from typing import Optional
 
 import wandb
 
 
 class MyCustomTracker(GeneralTracker):
     name = ""wandb""
     requires_logging_directory = False
 
     @on_main_process
     def __init__(self, run_name: str):
         self.run_name = run_name
         run = wandb.init(self.run_name)
 
     @property
     def tracker(self):
         return self.run.run
 
     @on_main_process
     def store_init_configuration(self, values: dict):
         wandb.config(values)
 
     @on_main_process
     def log(self, values: dict, step: Optional[int] = None):
         wandb.log(values, step=step)
 ```
 
 When you are ready to build your `Accelerator` object, pass in an **instance** of your tracker to [`Accelerator.log_with`] to have it automatically
 be used with the API:
 
 ```python
 tracker = MyCustomTracker(""some_run_name"")
 accelerator = Accelerator(log_with=tracker)
 ```
 
 These also can be mixed with existing trackers, including with `""all""`:
 
 ```python
 tracker = MyCustomTracker(""some_run_name"")
 accelerator = Accelerator(log_with=[tracker, ""all""])
 ```",H2,https://huggingface.co/docs/accelerate/source/usage_guides/tracking#implementing-custom-trackers,False,2603.0,481.0
1726,Accessing the internal tracker,"If some custom interactions with a tracker might be wanted directly, you can quickly access one using the 
 [`Accelerator.get_tracker`] method. Just pass in the string corresponding to a tracker's `.name` attribute 
 and it will return that tracker on the main process.
 
 This example shows doing so with wandb:
 
 ```python
 wandb_tracker = accelerator.get_tracker(""wandb"")
 ```
 
 From there you can interact with `wandb`'s `run` object like normal:
 
 ```python
 wandb_run.log_artifact(some_artifact_to_log)
 ```
 
 <Tip>
   Trackers built in Accelerate will automatically execute on the correct process, 
   so if a tracker is only meant to be ran on the main process it will do so 
   automatically.
 </Tip>
 
 If you want to truly remove Accelerate's wrapping entirely, you can 
 achieve the same outcome with:
 
 ```python
 wandb_tracker = accelerator.get_tracker(""wandb"", unwrap=True)
 with accelerator.on_main_process:
     wandb_tracker.log_artifact(some_artifact_to_log)
 ```",H2,https://huggingface.co/docs/accelerate/source/usage_guides/tracking#accessing-the-internal-tracker,False,987.0,146.0
1727,When a wrapper cannot work,"If a library has an API that does not follow a strict `.log` with an overall dictionary such as Neptune.AI, logging can be done manually under an `if accelerator.is_main_process` statement:
 ```diff
   from accelerate import Accelerator
 + import neptune.new as neptune
 
   accelerator = Accelerator()
 + run = neptune.init(...)
 
   my_model, my_optimizer, my_training_dataloader = accelerate.prepare(my_model, my_optimizer, my_training_dataloader)
   device = accelerator.device
   my_model.to(device)
 
   for iteration in config[""num_iterations""]:
       for batch in my_training_dataloader:
           my_optimizer.zero_grad()
           inputs, targets = batch
           inputs = inputs.to(device)
           targets = targets.to(device)
           outputs = my_model(inputs)
           loss = my_loss_function(outputs, targets)
           total_loss += loss
           accelerator.backward(loss)
           my_optimizer.step()
 +         if accelerator.is_main_process:
 +             run[""logs/training/batch/loss""].log(loss)
 ```",H2,https://huggingface.co/docs/accelerate/source/usage_guides/tracking#when-a-wrapper-cannot-work,False,1040.0,226.0
1728,rendered properly in your Markdown viewer.,,H2,https://huggingface.co/docs/accelerate/source/usage_guides/big_modeling#rendered-properly-in-your-markdown-viewer,False,0.0,1.0
1729,Handling big models for inference,"When loading a pre-trained model in PyTorch, the usual workflow looks like this:
 
 ```py
 import torch
 
 my_model = ModelClass(...)
 state_dict = torch.load(checkpoint_file)
 my_model.load_state_dict(state_dict)
 ```
 
 In plain English, those steps are:
 1. Create the model with randomly initialized weights
 2. Load the model weights (in a dictionary usually called a state dict) from the disk
 3. Load those weights inside the model
 
 While this works very well for regularly sized models, this workflow has some clear limitations when we deal with a huge model: in step 1, we load a full version of the model in RAM, and spend some time randomly initializing the weights (which will be discarded in step 3). In step 2, we load another full version of the model in RAM, with the pre-trained weights. If you're loading a model with 6 billion parameters, this means you will need 24GB of RAM for each copy of the model, so 48GB in total (half of it to load the model in FP16).
 
 <Tip warning={true}>
 
     This API is quite new and still in its experimental stage. While we strive to provide a stable API, it's possible some small parts of the public API will change in the future.
 
 </Tip>",H1,https://huggingface.co/docs/accelerate/source/usage_guides/big_modeling#handling-big-models-for-inference,False,1198.0,211.0
1730,How the Process Works: A Quick Overview,"<Youtube id=""MWCSGj9jEAo"" />",H2,https://huggingface.co/docs/accelerate/source/usage_guides/big_modeling#how-the-process-works-a-quick-overview,False,28.0,3.0
1731,How the Process Works: Working with Code,,H2,https://huggingface.co/docs/accelerate/source/usage_guides/big_modeling#how-the-process-works-working-with-code,False,0.0,1.0
1732,Instantiating an empty model,"The first tool 🤗 Accelerate introduces to help with big models is a context manager [`init_empty_weights`] that helps you initialize a model without using any RAM so that step 1 can be done on models of any size. Here is how it works:
 
 ```py
 from accelerate import init_empty_weights
 
 with init_empty_weights():
     my_model = ModelClass(...)
 ```
 
 For instance:
 
 ```py
 with init_empty_weights():
     model = nn.Sequential(*[nn.Linear(10000, 10000) for _ in range(1000)])
 ```
 
 initializes an empty model with a bit more than 100B parameters. Behind the scenes, this relies on the meta device introduced in PyTorch 1.9. During the initialization under the context manager, each time a parameter is created, it is instantly moved to that device.
 
 <Tip warning={true}>
 
     You can't move a model initialized like this on CPU or another device directly, since it doesn't have any data. It's also very likely that a forward pass with that empty model will fail, as not all operations are supported on the meta device.
 
 </Tip>",H3,https://huggingface.co/docs/accelerate/source/usage_guides/big_modeling#instantiating-an-empty-model,False,1042.0,179.0
1733,Sharded checkpoints,"It's possible your model is so big that even a single copy won't fit in RAM. That doesn't mean it can't be loaded: if you have one or several GPUs, this is more memory available to store your model. In this case, it's better if your checkpoint is split into several smaller files that we call checkpoint shards.
 
 🤗 Accelerate will handle sharded checkpoints as long as you follow the following format: your checkpoint should be in a folder, with several files containing the partial state dicts, and there should be an index in the JSON format that contains a dictionary mapping parameter names to the file containing their weights. You can easily shard your model with [`~Accelerator.save_model`]. For instance, we could have a folder containing:
 
 ```bash
 first_state_dict.bin
 index.json
 second_state_dict.bin
 ```
 
 with index.json being the following file:
 
 ```
 {
   ""linear1.weight"": ""first_state_dict.bin"",
   ""linear1.bias"": ""first_state_dict.bin"",
   ""linear2.weight"": ""second_state_dict.bin"",
   ""linear2.bias"": ""second_state_dict.bin""
 }
 ```
 
 and `first_state_dict.bin` containing the weights for `""linear1.weight""` and `""linear1.bias""`, `second_state_dict.bin` the ones for `""linear2.weight""` and `""linear2.bias""`",H3,https://huggingface.co/docs/accelerate/source/usage_guides/big_modeling#sharded-checkpoints,False,1237.0,178.0
1734,Loading weights,"The second tool 🤗 Accelerate introduces is a function [`load_checkpoint_and_dispatch`], that will allow you to load a checkpoint inside your empty model. This supports full checkpoints (a single file containing the whole state dict) as well as sharded checkpoints. It will also automatically dispatch those weights across the devices you have available (GPUs, CPU RAM), so if you are loading a sharded checkpoint, the maximum RAM usage will be the size of the biggest shard.
 
 If you want to use big model inference with 🤗 Transformers models, check out this [documentation](https://huggingface.co/docs/transformers/main/en/main_classes/model#large-model-loading).
 
 Here is how we can use this to load the [GPT2-1.5B](https://huggingface.co/marcsun13/gpt2-xl-linear-sharded) model.
 
 Let's download the sharded version of this model.
 
 ```bash
 pip install huggingface_hub
 ```
 
 ```py
 from huggingface_hub import snapshot_download
 checkpoint = ""marcsun13/gpt2-xl-linear-sharded""
 weights_location = snapshot_download(repo_id=checkpoint)
 ```
 
 In order to initialize the model, we will use the library minGPT. 
 
 ```bash
 git clone https://github.com/karpathy/minGPT.git
 pip install minGPT/
 ```
 
 ```py
 from accelerate import init_empty_weights
 from mingpt.model import GPT
 
 model_config = GPT.get_default_config()
 model_config.model_type = 'gpt2-xl'
 model_config.vocab_size = 50257
 model_config.block_size = 1024
 
 with init_empty_weights():
     model = GPT(model_config)
 ```
 
 Then, load the checkpoint we just downloaded with:
 
 ```py
 from accelerate import load_checkpoint_and_dispatch
 
 model = load_checkpoint_and_dispatch(
     model, checkpoint=weights_location, device_map=""auto"", no_split_module_classes=['Block']
 )
 ```",H3,https://huggingface.co/docs/accelerate/source/usage_guides/big_modeling#loading-weights,False,1759.0,219.0
1735,rendered properly in your Markdown viewer.,,H2,https://huggingface.co/docs/accelerate/source/usage_guides/training_zoo#rendered-properly-in-your-markdown-viewer,False,0.0,1.0
1736,Example Zoo,Below contains a non-exhuastive list of tutorials and scripts showcasing 🤗 Accelerate,H1,https://huggingface.co/docs/accelerate/source/usage_guides/training_zoo#example-zoo,False,85.0,12.0
1737,Official Accelerate Examples:,,H2,https://huggingface.co/docs/accelerate/source/usage_guides/training_zoo#official-accelerate-examples,False,0.0,1.0
1738,Basic Examples,These examples showcase the base features of Accelerate and are a great starting point,H3,https://huggingface.co/docs/accelerate/source/usage_guides/training_zoo#basic-examples,False,86.0,14.0
1739,Feature Specific Examples,These examples showcase specific features that the Accelerate framework offers,H3,https://huggingface.co/docs/accelerate/source/usage_guides/training_zoo#feature-specific-examples,False,78.0,10.0
1740,Full Examples,"These examples showcase every feature in Accelerate at once that was shown in ""Feature Specific Examples""",H3,https://huggingface.co/docs/accelerate/source/usage_guides/training_zoo#full-examples,False,105.0,16.0
1741,Integration Examples,"These are tutorials from libraries that integrate with 🤗 Accelerate: 
 
 > Don't find your integration here? Make a PR to include it!",H2,https://huggingface.co/docs/accelerate/source/usage_guides/training_zoo#integration-examples,False,133.0,24.0
1742,Catalyst,,H3,https://huggingface.co/docs/accelerate/source/usage_guides/training_zoo#catalyst,False,0.0,1.0
1743,DALLE2-pytorch,,H3,https://huggingface.co/docs/accelerate/source/usage_guides/training_zoo#dalle2-pytorch,False,0.0,1.0
1744,🤗 diffusers,,H3,https://huggingface.co/docs/accelerate/source/usage_guides/training_zoo#diffusers,False,0.0,1.0
1745,fastai,,H3,https://huggingface.co/docs/accelerate/source/usage_guides/training_zoo#fastai,False,0.0,1.0
1746,GradsFlow,,H3,https://huggingface.co/docs/accelerate/source/usage_guides/training_zoo#gradsflow,False,0.0,1.0
1747,imagen-pytorch,,H3,https://huggingface.co/docs/accelerate/source/usage_guides/training_zoo#imagen-pytorch,False,0.0,1.0
1748,Kornia,,H3,https://huggingface.co/docs/accelerate/source/usage_guides/training_zoo#kornia,False,0.0,1.0
1749,PyTorch Accelerated,,H3,https://huggingface.co/docs/accelerate/source/usage_guides/training_zoo#pytorch-accelerated,False,0.0,1.0
1750,PyTorch3D,,H3,https://huggingface.co/docs/accelerate/source/usage_guides/training_zoo#pytorch3d,False,0.0,1.0
1751,Stable-Dreamfusion,,H3,https://huggingface.co/docs/accelerate/source/usage_guides/training_zoo#stable-dreamfusion,False,0.0,1.0
1752,Tez,,H3,https://huggingface.co/docs/accelerate/source/usage_guides/training_zoo#tez,False,0.0,1.0
1753,trlx,,H3,https://huggingface.co/docs/accelerate/source/usage_guides/training_zoo#trlx,False,0.0,1.0
1754,Comfy-UI,,H3,https://huggingface.co/docs/accelerate/source/usage_guides/training_zoo#comfy-ui,False,0.0,1.0
1755,In Science,"Below contains a non-exhaustive list of papers utilizing 🤗 Accelerate. 
 
 > Don't find your paper here? Make a PR to include it!
 
 * Yuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Matiana, Joe Penna, Omer Levy: “Pick-a-Pic: An Open Dataset of User Preferences for Text-to-Image Generation”, 2023; [arXiv:2305.01569](http://arxiv.org/abs/2305.01569).
 * Lei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi Lan, Roy Ka-Wei Lee, Ee-Peng Lim: “Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models”, 2023; [arXiv:2305.04091](http://arxiv.org/abs/2305.04091).
 * Arthur Câmara, Claudia Hauff: “Moving Stuff Around: A study on efficiency of moving documents into memory for Neural IR models”, 2022; [arXiv:2205.08343](http://arxiv.org/abs/2205.08343).
 * Ying Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan Li, Max Ryabinin, Daniel Y. Fu, Zhiqiang Xie, Beidi Chen, Clark Barrett, Joseph E. Gonzalez, Percy Liang, Christopher Ré, Ion Stoica, Ce Zhang: “High-throughput Generative Inference of Large Language Models with a Single GPU”, 2023; [arXiv:2303.06865](http://arxiv.org/abs/2303.06865).
 * Peter Melchior, Yan Liang, ChangHoon Hahn, Andy Goulding: “Autoencoding Galaxy Spectra I: Architecture”, 2022; [arXiv:2211.07890](http://arxiv.org/abs/2211.07890).
 * Jiaao Chen, Aston Zhang, Mu Li, Alex Smola, Diyi Yang: “A Cheaper and Better Diffusion Language Model with Soft-Masked Noise”, 2023; [arXiv:2304.04746](http://arxiv.org/abs/2304.04746).
 * Ayaan Haque, Matthew Tancik, Alexei A. Efros, Aleksander Holynski, Angjoo Kanazawa: “Instruct-NeRF2NeRF: Editing 3D Scenes with Instructions”, 2023; [arXiv:2303.12789](http://arxiv.org/abs/2303.12789).
 * Luke Melas-Kyriazi, Christian Rupprecht, Iro Laina, Andrea Vedaldi: “RealFusion: 360° Reconstruction of Any Object from a Single Image”, 2023; [arXiv:2302.10663](http://arxiv.org/abs/2302.10663).
 * Xiaoshi Wu, Keqiang Sun, Feng Zhu, Rui Zhao, Hongsheng Li: “Better Aligning Text-to-Image Models with Human Preference”, 2023; [arXiv:2303.14420](http://arxiv.org/abs/2303.14420).
 * Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, Yueting Zhuang: “HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace”, 2023; [arXiv:2303.17580](http://arxiv.org/abs/2303.17580).
 * Yue Yang, Wenlin Yao, Hongming Zhang, Xiaoyang Wang, Dong Yu, Jianshu Chen: “Z-LaVI: Zero-Shot Language Solver Fueled by Visual Imagination”, 2022; [arXiv:2210.12261](http://arxiv.org/abs/2210.12261).
 * Sheng-Yen Chou, Pin-Yu Chen, Tsung-Yi Ho: “How to Backdoor Diffusion Models?”, 2022; [arXiv:2212.05400](http://arxiv.org/abs/2212.05400).
 * Junyoung Seo, Wooseok Jang, Min-Seop Kwak, Jaehoon Ko, Hyeonsu Kim, Junho Kim, Jin-Hwa Kim, Jiyoung Lee, Seungryong Kim: “Let 2D Diffusion Model Know 3D-Consistency for Robust Text-to-3D Generation”, 2023; [arXiv:2303.07937](http://arxiv.org/abs/2303.07937).
 * Or Patashnik, Daniel Garibi, Idan Azuri, Hadar Averbuch-Elor, Daniel Cohen-Or: “Localizing Object-level Shape Variations with Text-to-Image Diffusion Models”, 2023; [arXiv:2303.11306](http://arxiv.org/abs/2303.11306).
 * Dídac Surís, Sachit Menon, Carl Vondrick: “ViperGPT: Visual Inference via Python Execution for Reasoning”, 2023; [arXiv:2303.08128](http://arxiv.org/abs/2303.08128).
 * Chenyang Qi, Xiaodong Cun, Yong Zhang, Chenyang Lei, Xintao Wang, Ying Shan, Qifeng Chen: “FateZero: Fusing Attentions for Zero-shot Text-based Video Editing”, 2023; [arXiv:2303.09535](http://arxiv.org/abs/2303.09535).
 * Sean Welleck, Jiacheng Liu, Ximing Lu, Hannaneh Hajishirzi, Yejin Choi: “NaturalProver: Grounded Mathematical Proof Generation with Language Models”, 2022; [arXiv:2205.12910](http://arxiv.org/abs/2205.12910).
 * Elad Richardson, Gal Metzer, Yuval Alaluf, Raja Giryes, Daniel Cohen-Or: “TEXTure: Text-Guided Texturing of 3D Shapes”, 2023; [arXiv:2302.01721](http://arxiv.org/abs/2302.01721).
 * Puijin Cheng, Li Lin, Yijin Huang, Huaqing He, Wenhan Luo, Xiaoying Tang: “Learning Enhancement From Degradation: A Diffusion Model For Fundus Image Enhancement”, 2023; [arXiv:2303.04603](http://arxiv.org/abs/2303.04603).
 * Shun Shao, Yftah Ziser, Shay Cohen: “Erasure of Unaligned Attributes from Neural Representations”, 2023; [arXiv:2302.02997](http://arxiv.org/abs/2302.02997).
 * Seonghyeon Ye, Hyeonbin Hwang, Sohee Yang, Hyeongu Yun, Yireun Kim, Minjoon Seo: “In-Context Instruction Learning”, 2023; [arXiv:2302.14691](http://arxiv.org/abs/2302.14691).
 * Shikun Liu, Linxi Fan, Edward Johns, Zhiding Yu, Chaowei Xiao, Anima Anandkumar: “Prismer: A Vision-Language Model with An Ensemble of Experts”, 2023; [arXiv:2303.02506](http://arxiv.org/abs/2303.02506 ).
 * Haoyu Chen, Zhihua Wang, Yang Yang, Qilin Sun, Kede Ma: “Learning a Deep Color Difference Metric for Photographic Images”, 2023; [arXiv:2303.14964](http://arxiv.org/abs/2303.14964).
 * Van-Hoang Le, Hongyu Zhang: “Log Parsing with Prompt-based Few-shot Learning”, 2023; [arXiv:2302.07435](http://arxiv.org/abs/2302.07435).
 * Keito Kudo, Yoichi Aoki, Tatsuki Kuribayashi, Ana Brassard, Masashi Yoshikawa, Keisuke Sakaguchi, Kentaro Inui: “Do Deep Neural Networks Capture Compositionality in Arithmetic Reasoning?”, 2023; [arXiv:2302.07866](http://arxiv.org/abs/2302.07866).
 * Ruoyao Wang, Peter Jansen, Marc-Alexandre Côté, Prithviraj Ammanabrolu: “Behavior Cloned Transformers are Neurosymbolic Reasoners”, 2022; [arXiv:2210.07382](http://arxiv.org/abs/2210.07382).
 * Martin Wessel, Tomáš Horych, Terry Ruas, Akiko Aizawa, Bela Gipp, Timo Spinde: “Introducing MBIB -- the first Media Bias Identification Benchmark Task and Dataset Collection”, 2023; [arXiv:2304.13148](http://arxiv.org/abs/2304.13148 ). DOI: [https://dx.doi.org/10.1145/3539618.3591882 10.1145/3539618.3591882].
 * Hila Chefer, Yuval Alaluf, Yael Vinker, Lior Wolf, Daniel Cohen-Or: “Attend-and-Excite: Attention-Based Semantic Guidance for Text-to-Image Diffusion Models”, 2023; [arXiv:2301.13826](http://arxiv.org/abs/2301.13826).
 * Marcio Fonseca, Yftah Ziser, Shay B. Cohen: “Factorizing Content and Budget Decisions in Abstractive Summarization of Long Documents”, 2022; [arXiv:2205.12486](http://arxiv.org/abs/2205.12486).
 * Elad Richardson, Gal Metzer, Yuval Alaluf, Raja Giryes, Daniel Cohen-Or: “TEXTure: Text-Guided Texturing of 3D Shapes”, 2023; [arXiv:2302.01721](http://arxiv.org/abs/2302.01721).
 * Tianxing He, Jingyu Zhang, Tianle Wang, Sachin Kumar, Kyunghyun Cho, James Glass, Yulia Tsvetkov: “On the Blind Spots of Model-Based Evaluation Metrics for Text Generation”, 2022; [arXiv:2212.10020](http://arxiv.org/abs/2212.10020).
 * Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-Brown, Yoav Shoham: “In-Context Retrieval-Augmented Language Models”, 2023; [arXiv:2302.00083](http://arxiv.org/abs/2302.00083).
 * Dacheng Li, Rulin Shao, Hongyi Wang, Han Guo, Eric P. Xing, Hao Zhang: “MPCFormer: fast, performant and private Transformer inference with MPC”, 2022; [arXiv:2211.01452](http://arxiv.org/abs/2211.01452).
 * Baolin Peng, Michel Galley, Pengcheng He, Chris Brockett, Lars Liden, Elnaz Nouri, Zhou Yu, Bill Dolan, Jianfeng Gao: “GODEL: Large-Scale Pre-Training for Goal-Directed Dialog”, 2022; [arXiv:2206.11309](http://arxiv.org/abs/2206.11309).
 * Egil Rønningstad, Erik Velldal, Lilja Øvrelid: “Entity-Level Sentiment Analysis (ELSA): An exploratory task survey”, 2023, Proceedings of the 29th International Conference on Computational Linguistics, 2022, pages 6773-6783; [arXiv:2304.14241](http://arxiv.org/abs/2304.14241).
 * Charlie Snell, Ilya Kostrikov, Yi Su, Mengjiao Yang, Sergey Levine: “Offline RL for Natural Language Generation with Implicit Language Q Learning”, 2022; [arXiv:2206.11871](http://arxiv.org/abs/2206.11871).
 * Zhiruo Wang, Shuyan Zhou, Daniel Fried, Graham Neubig: “Execution-Based Evaluation for Open-Domain Code Generation”, 2022; [arXiv:2212.10481](http://arxiv.org/abs/2212.10481).
 * Minh-Long Luu, Zeyi Huang, Eric P. Xing, Yong Jae Lee, Haohan Wang: “Expeditious Saliency-guided Mix-up through Random Gradient Thresholding”, 2022; [arXiv:2212.04875](http://arxiv.org/abs/2212.04875).
 * Jun Hao Liew, Hanshu Yan, Daquan Zhou, Jiashi Feng: “MagicMix: Semantic Mixing with Diffusion Models”, 2022; [arXiv:2210.16056](http://arxiv.org/abs/2210.16056).
 * Yaqing Wang, Subhabrata Mukherjee, Xiaodong Liu, Jing Gao, Ahmed Hassan Awadallah, Jianfeng Gao: “LiST: Lite Prompted Self-training Makes Parameter-Efficient Few-shot Learners”, 2021; [arXiv:2110.06274](http://arxiv.org/abs/2110.06274).",H2,https://huggingface.co/docs/accelerate/source/usage_guides/training_zoo#in-science,False,8541.0,933.0
1756,rendered properly in your Markdown viewer.,,H2,https://huggingface.co/docs/accelerate/source/usage_guides/ipex#rendered-properly-in-your-markdown-viewer,False,0.0,1.0
1757,Intel® Extension for PyTorch,"[IPEX](https://github.com/intel/intel-extension-for-pytorch) is optimized for CPUs with AVX-512 or above, and functionally works for CPUs with only AVX2. So, it is expected to bring performance benefit for Intel CPU generations with AVX-512 or above while CPUs with only AVX2 (e.g., AMD CPUs or older Intel CPUs) might result in a better performance under IPEX, but not guaranteed. IPEX provides performance optimizations for CPU training with both Float32 and BFloat16. The usage of BFloat16 is the main focus of the following sections.
 
 Low precision data type BFloat16 has been natively supported on the 3rd Generation Xeon® Scalable Processors (aka Cooper Lake) with AVX512 instruction set and will be supported on the next generation of Intel® Xeon® Scalable Processors with Intel® Advanced Matrix Extensions (Intel® AMX) instruction set with further boosted performance. The Auto Mixed Precision for CPU backend has been enabled since PyTorch-1.10. At the same time, the support of Auto Mixed Precision with BFloat16 for CPU and BFloat16 optimization of operators has been massively enabled in Intel® Extension for PyTorch, and partially upstreamed to PyTorch master branch. Users can get better performance and user experience with IPEX Auto Mixed Precision.",H1,https://huggingface.co/docs/accelerate/source/usage_guides/ipex#intel-r-extension-for-pytorch,False,1267.0,190.0
1758,IPEX installation:,"IPEX release is following PyTorch, to install via pip:
 
 | PyTorch Version   | IPEX version   |
 | :---------------: | :----------:   |
 | 2.0               |  2.0.0         |
 | 1.13              |  1.13.0        |
 | 1.12              |  1.12.300      |
 | 1.11              |  1.11.200      |
 | 1.10              |  1.10.100      |
 
 ```
 pip install intel_extension_for_pytorch==<version_name> -f https://developer.intel.com/ipex-whl-stable-cpu
 ```
 
 Check more approaches for [IPEX installation](https://intel.github.io/intel-extension-for-pytorch/cpu/latest/tutorials/installation.html).",H2,https://huggingface.co/docs/accelerate/source/usage_guides/ipex#ipex-installation,False,598.0,169.0
1759,How It Works For Training optimization in CPU,"🤗 Accelerate has integrated [IPEX](https://github.com/intel/intel-extension-for-pytorch), all you need to do is enabling it through the config.
 
 **Scenario 1**: Acceleration of No distributed CPU training
 
 Run <u>accelerate config</u> on your machine:
 
 ```bash",H2,https://huggingface.co/docs/accelerate/source/usage_guides/ipex#how-it-works-for-training-optimization-in-cpu,False,266.0,34.0
1760,Related Resources,,H2,https://huggingface.co/docs/accelerate/source/usage_guides/ipex#related-resources,False,0.0,1.0
1761,rendered properly in your Markdown viewer.,,H2,https://huggingface.co/docs/accelerate/source/usage_guides/checkpoint#rendered-properly-in-your-markdown-viewer,False,0.0,1.0
1762,Checkpointing,"When training a PyTorch model with 🤗 Accelerate, you may often want to save and continue a state of training. Doing so requires",H1,https://huggingface.co/docs/accelerate/source/usage_guides/checkpoint#checkpointing,False,127.0,23.0
1763,Restoring the state of the DataLoader,"After resuming from a checkpoint, it may also be desirable to resume from a particular point in the active `DataLoader` if 
 the state was saved during the middle of an epoch. You can use [`~Accelerator.skip_first_batches`] to do so. 
 
 ```python
 from accelerate import Accelerator
 
 accelerator = Accelerator(project_dir=""my/save/path"")
 
 train_dataloader = accelerator.prepare(train_dataloader)
 accelerator.load_state(""my_state"")",H2,https://huggingface.co/docs/accelerate/source/usage_guides/checkpoint#restoring-the-state-of-the-dataloader,False,436.0,55.0
1764,rendered properly in your Markdown viewer.,,H2,https://huggingface.co/docs/accelerate/source/usage_guides/gradient_accumulation#rendered-properly-in-your-markdown-viewer,False,0.0,1.0
1765,Performing gradient accumulation with 🤗 Accelerate,"Gradient accumulation is a technique where you can train on bigger batch sizes than 
 your machine would normally be able to fit into memory. This is done by accumulating gradients over
 several batches, and only stepping the optimizer after a certain number of batches have been performed.
 
 While technically standard gradient accumulation code would work fine in a distributed setup, it is not the most efficient
 method for doing so and you may experience considerable slowdowns!
 
 In this tutorial you will see how to quickly setup gradient accumulation and perform it with the utilities provided in 🤗 Accelerate,
 which can total to adding just one new line of code!
 
 This example will use a very simplistic PyTorch training loop that performs gradient accumulation every two batches:
 
 ```python
 device = ""cuda""
 model.to(device)
 
 gradient_accumulation_steps = 2
 
 for index, batch in enumerate(training_dataloader):
     inputs, targets = batch
     inputs = inputs.to(device)
     targets = targets.to(device)
     outputs = model(inputs)
     loss = loss_function(outputs, targets)
     loss = loss / gradient_accumulation_steps
     loss.backward()
     if (index + 1) % gradient_accumulation_steps == 0:
         optimizer.step()
         scheduler.step()
         optimizer.zero_grad()
 ```",H1,https://huggingface.co/docs/accelerate/source/usage_guides/gradient_accumulation#performing-gradient-accumulation-with-accelerate,False,1312.0,237.0
1766,Converting it to 🤗 Accelerate,"First the code shown earlier will be converted to utilize 🤗 Accelerate without the special gradient accumulation helper:
 
 ```diff
 + from accelerate import Accelerator
 + accelerator = Accelerator()
 
 + model, optimizer, training_dataloader, scheduler = accelerator.prepare(
 +     model, optimizer, training_dataloader, scheduler
 + )
 
   for index, batch in enumerate(training_dataloader):",H2,https://huggingface.co/docs/accelerate/source/usage_guides/gradient_accumulation#converting-it-to-accelerate,False,395.0,56.0
1767,Letting 🤗 Accelerate handle gradient accumulation,"All that is left now is to let 🤗 Accelerate handle the gradient accumulation for us. To do so you should pass in a `gradient_accumulation_steps` parameter to [`Accelerator`], dictating the number 
 of steps to perform before each call to `step()` and how to automatically adjust the loss during the call to [`~Accelerator.backward`]:
 
 ```diff",H2,https://huggingface.co/docs/accelerate/source/usage_guides/gradient_accumulation#letting-accelerate-handle-gradient-accumulation,False,344.0,55.0
1768,The finished code,"Below is the finished implementation for performing gradient accumulation with 🤗 Accelerate
 
 ```python
 from accelerate import Accelerator
 accelerator = Accelerator(gradient_accumulation_steps=2)
 model, optimizer, training_dataloader, scheduler = accelerator.prepare(
     model, optimizer, training_dataloader, scheduler
 )
 for batch in training_dataloader:
     with accelerator.accumulate(model):
         inputs, targets = batch
         outputs = model(inputs)
         loss = loss_function(outputs, targets)
         accelerator.backward(loss)
         optimizer.step()
         scheduler.step()
         optimizer.zero_grad()
 ```
 
 <Tip warning={true}>
 
 It's important that **only one forward/backward** should be done inside the context manager `with accelerator.accumulate(model)`.
 
 </Tip>
 
 
 To learn more about what magic this wraps around, read the [Gradient Synchronization concept guide](../concept_guides/gradient_synchronization)",H2,https://huggingface.co/docs/accelerate/source/usage_guides/gradient_accumulation#the-finished-code,False,958.0,156.0
1769,Self-contained example,"Here is a self-contained example that you can run to see gradient accumulation in action with 🤗 Accelerate:
 
 ```python
 import torch
 import copy
 from accelerate import Accelerator
 from accelerate.utils import set_seed
 from torch.utils.data import TensorDataset, DataLoader",H2,https://huggingface.co/docs/accelerate/source/usage_guides/gradient_accumulation#self-contained-example,False,278.0,37.0
1770,rendered properly in your Markdown viewer.,,H2,https://huggingface.co/docs/accelerate/source/usage_guides/memory#rendered-properly-in-your-markdown-viewer,False,0.0,1.0
1771,Memory Utilities,"One of the most frustrating errors when it comes to running training scripts is hitting ""CUDA Out-of-Memory"", 
 as the entire script needs to be restarted, progress is lost, and typically a developer would want to simply
 start their script and let it run.
 
 `Accelerate` provides a utility heavily based on [toma](https://github.com/BlackHC/toma) to give this capability.",H1,https://huggingface.co/docs/accelerate/source/usage_guides/memory#memory-utilities,False,373.0,57.0
1772,find_executable_batch_size,"This algorithm operates with exponential decay, decreasing the batch size in half after each failed run on some 
 training script. To use it, restructure your training function to include an inner function that includes this wrapper, 
 and build your dataloaders inside it. At a minimum, this could look like 4 new lines of code. 
 > Note: The inner function *must* take in the batch size as the first parameter, but we do not pass one to it when called. The wrapper handles this for us
 
 It should also be noted that anything which will consume CUDA memory and passed to the `accelerator` **must** be declared inside the inner function,
 such as models and optimizers.
 
 ```diff
 def training_function(args):
     accelerator = Accelerator()
 
 +   @find_executable_batch_size(starting_batch_size=args.batch_size)
 +   def inner_training_loop(batch_size):
 +       nonlocal accelerator # Ensure they can be used in our context
 +       accelerator.free_memory() # Free all lingering references
         model = get_model()
         model.to(accelerator.device)
         optimizer = get_optimizer()
         train_dataloader, eval_dataloader = get_dataloaders(accelerator, batch_size)
         lr_scheduler = get_scheduler(
             optimizer, 
             num_training_steps=len(train_dataloader)*num_epochs
         )
         model, optimizer, train_dataloader, eval_dataloader, lr_scheduler = accelerator.prepare(
             model, optimizer, train_dataloader, eval_dataloader, lr_scheduler
         )
         train(model, optimizer, train_dataloader, lr_scheduler)
         validate(model, eval_dataloader)
 +   inner_training_loop()
 ```
 
 To find out more, check the documentation [here](../package_reference/utilities#accelerate.find_executable_batch_size).",H2,https://huggingface.co/docs/accelerate/source/usage_guides/memory#find-executable-batch-size,False,1776.0,338.0
1773,rendered properly in your Markdown viewer.,,H2,https://huggingface.co/docs/accelerate/source/usage_guides/deepspeed#rendered-properly-in-your-markdown-viewer,False,0.0,1.0
1774,DeepSpeed,"[DeepSpeed](https://github.com/microsoft/DeepSpeed) implements everything described in the [ZeRO paper](https://arxiv.org/abs/1910.02054). Currently, it provides full support for:
 
 1. Optimizer state partitioning (ZeRO stage 1)
 2. Gradient partitioning (ZeRO stage 2)
 3. Parameter partitioning (ZeRO stage 3)
 4. Custom mixed precision training handling
 5. A range of fast CUDA-extension-based optimizers
 6. ZeRO-Offload to CPU and Disk/NVMe
 
 ZeRO-Offload has its own dedicated paper: [ZeRO-Offload: Democratizing Billion-Scale Model Training](https://arxiv.org/abs/2101.06840). And NVMe-support is described in the paper [ZeRO-Infinity: Breaking the GPU
 Memory Wall for Extreme Scale Deep Learning](https://arxiv.org/abs/2104.07857).
 
 DeepSpeed ZeRO-2 is primarily used only for training, as its features are of no use to inference.
 
 DeepSpeed ZeRO-3 can be used for inference as well since it allows huge models to be loaded on multiple GPUs, which
 won't be possible on a single GPU.
 
 🤗 Accelerate integrates [DeepSpeed](https://github.com/microsoft/DeepSpeed) via 2 options:
 
 1. Integration of the DeepSpeed features via `deepspeed config file` specification in `accelerate config` . You just supply your custom config file or use our template. Most of
    this document is focused on this feature. This supports all the core features of DeepSpeed and gives user a lot of flexibility. 
    User may have to change a few lines of code depending on the config.
 2. Integration via `deepspeed_plugin`.This supports subset of the DeepSpeed features and uses default options for the rest of the configurations. 
    User need not change any code and is good for those who are fine with most of the default settings of DeepSpeed.",H1,https://huggingface.co/docs/accelerate/source/usage_guides/deepspeed#deepspeed,False,1744.0,256.0
1775,What is integrated?,"Training:
 
 1. DeepSpeed ZeRO training supports the full ZeRO stages 1, 2 and 3 as well as CPU/Disk offload of optimizer states, gradients and parameters. 
 Below is a short description of Data Parallelism using ZeRO - Zero Redundancy Optimizer along with diagram from this [blog post](https://www.microsoft.com/en-us/research/blog/zero-deepspeed-new-system-optimizations-enable-training-models-with-over-100-billion-parameters/)
 ![ZeRO Data Parallelism](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/parallelism-zero.png)
 
 (Source: [link](https://www.microsoft.com/en-us/research/blog/zero-deepspeed-new-system-optimizations-enable-training-models-with-over-100-billion-parameters/))
 
  a. **Stage 1** : Shards optimizer states across data parallel workers/GPUs
 
  b. **Stage 2** : Shards optimizer states + gradients across data parallel workers/GPUs
 
  c. **Stage 3**: Shards optimizer states + gradients + model parameters across data parallel workers/GPUs
 
  d. **Optimizer Offload**: Offloads the gradients + optimizer states to CPU/Disk building on top of ZERO Stage 2
 
  e. **Param Offload**: Offloads the model parameters to CPU/Disk building on top of ZERO Stage 3
 
 <u>Note</u>: With respect to Disk Offload, the disk should be an NVME for decent speed but it technically works on any Disk
 
 Inference:
 
 1. DeepSpeed ZeRO Inference supports ZeRO stage 3 with ZeRO-Infinity. It uses the same ZeRO protocol as training, but
    it doesn't use an optimizer and a lr scheduler and only stage 3 is relevant. For more details see:
    [deepspeed-zero-inference](#deepspeed-zero-inference).",H2,https://huggingface.co/docs/accelerate/source/usage_guides/deepspeed#what-is-integrated,False,1639.0,208.0
1776,How it works?,"**Pre-Requisites**: Install DeepSpeed version >=0.6.5. Please refer to the [DeepSpeed Installation details](https://github.com/microsoft/DeepSpeed#installation)
 for more information.
 
 We will first look at easy to use integration via `accelerate config`. 
 Followed by more flexible and feature rich `deepspeed config file` integration.",H2,https://huggingface.co/docs/accelerate/source/usage_guides/deepspeed#how-it-works,False,339.0,40.0
1777,Accelerate DeepSpeed Plugin,"On your machine(s) just run:
 
 ```bash
 accelerate config
 ```
 
 and answer the questions asked. It will ask whether you want to use a config file for DeepSpeed to which you should answer no. Then answer the following questions to generate a basic DeepSpeed config.
 This will generate a config file that will be used automatically to properly set the
 default options when doing
 
 ```bash
 accelerate launch my_script.py --args_to_my_script
 ```
 
 For instance, here is how you would run the NLP example `examples/nlp_example.py` (from the root of the repo) with DeepSpeed Plugin:
 
 **ZeRO Stage-2 DeepSpeed Plugin Example**
 ```bash
 compute_environment: LOCAL_MACHINE
 deepspeed_config:
  gradient_accumulation_steps: 1
  gradient_clipping: 1.0
  offload_optimizer_device: none
  offload_param_device: none
  zero3_init_flag: true
  zero_stage: 2
 distributed_type: DEEPSPEED
 fsdp_config: {}
 machine_rank: 0
 main_process_ip: null
 main_process_port: null
 main_training_function: main
 mixed_precision: fp16
 num_machines: 1
 num_processes: 2
 use_cpu: false
 ```
 
 ```bash
 accelerate launch examples/nlp_example.py --mixed_precision fp16
 ```
 
 **ZeRO Stage-3 with CPU Offload DeepSpeed Plugin Example**
 ```bash
 compute_environment: LOCAL_MACHINE
 deepspeed_config:
   gradient_accumulation_steps: 1
   gradient_clipping: 1.0
   offload_optimizer_device: cpu
   offload_param_device: cpu
   zero3_init_flag: true
   zero3_save_16bit_model: true
   zero_stage: 3
 distributed_type: DEEPSPEED
 fsdp_config: {}
 machine_rank: 0
 main_process_ip: null
 main_process_port: null
 main_training_function: main
 mixed_precision: fp16
 num_machines: 1
 num_processes: 2
 use_cpu: false
 ```
 
 ```bash
 accelerate launch examples/nlp_example.py --mixed_precision fp16
 ```
 
 Currently, `Accelerate` supports following config through the CLI:
 
 ```bash
 `zero_stage`: [0] Disabled, [1] optimizer state partitioning, [2] optimizer+gradient state partitioning and [3] optimizer+gradient+parameter partitioning
 `gradient_accumulation_steps`: Number of training steps to accumulate gradients before averaging and applying them.
 `gradient_clipping`: Enable gradient clipping with value.
 `offload_optimizer_device`: [none] Disable optimizer offloading, [cpu] offload optimizer to CPU, [nvme] offload optimizer to NVMe SSD. Only applicable with ZeRO >= Stage-2.
 `offload_param_device`: [none] Disable parameter offloading, [cpu] offload parameters to CPU, [nvme] offload parameters to NVMe SSD. Only applicable with ZeRO Stage-3.
 `zero3_init_flag`: Decides whether to enable `deepspeed.zero.Init` for constructing massive models. Only applicable with ZeRO Stage-3.
 `zero3_save_16bit_model`: Decides whether to save 16-bit model weights when using ZeRO Stage-3.
 `mixed_precision`: `no` for FP32 training, `fp16` for FP16 mixed-precision training and `bf16` for BF16 mixed-precision training. 
 ```
 To be able to tweak more options, you will need to use a DeepSpeed config file.",H3,https://huggingface.co/docs/accelerate/source/usage_guides/deepspeed#accelerate-deepspeed-plugin,False,2986.0,370.0
1778,DeepSpeed Config File,"On your machine(s) just run:
 
 ```bash
 accelerate config
 ```
 
 and answer the questions asked. It will ask whether you want to use a config file for deepspeed to which you answer yes 
 and provide the path to the deepspeed config file. 
 This will generate a config file that will be used automatically to properly set the
 default options when doing
 
 ```bash
 accelerate launch my_script.py --args_to_my_script
 ```
 
 For instance, here is how you would run the NLP example `examples/by_feature/deepspeed_with_config_support.py` (from the root of the repo) with DeepSpeed Config File:
 
 **ZeRO Stage-2 DeepSpeed Config File Example**
 ```bash
 compute_environment: LOCAL_MACHINE
 deepspeed_config:
  deepspeed_config_file: /home/ubuntu/accelerate/examples/configs/deepspeed_config_templates/zero_stage2_config.json
  zero3_init_flag: true
 distributed_type: DEEPSPEED
 fsdp_config: {}
 machine_rank: 0
 main_process_ip: null
 main_process_port: null
 main_training_function: main
 mixed_precision: fp16
 num_machines: 1
 num_processes: 2
 use_cpu: false
 ```
 
 with the contents of `zero_stage2_config.json` being:
 ```json
 {
     ""fp16"": {
         ""enabled"": true,
         ""loss_scale"": 0,
         ""loss_scale_window"": 1000,
         ""initial_scale_power"": 16,
         ""hysteresis"": 2,
         ""min_loss_scale"": 1
     },
     ""optimizer"": {
         ""type"": ""AdamW"",
         ""params"": {
             ""lr"": ""auto"",
             ""weight_decay"": ""auto"",
             ""torch_adam"": true,
             ""adam_w_mode"": true
         }
     },
     ""scheduler"": {
         ""type"": ""WarmupDecayLR"",
         ""params"": {
             ""warmup_min_lr"": ""auto"",
             ""warmup_max_lr"": ""auto"",
             ""warmup_num_steps"": ""auto"",
             ""total_num_steps"": ""auto""
         }
     },
     ""zero_optimization"": {
         ""stage"": 2,
         ""allgather_partitions"": true,
         ""allgather_bucket_size"": 2e8,
         ""overlap_comm"": true,
         ""reduce_scatter"": true,
         ""reduce_bucket_size"": ""auto"",
         ""contiguous_gradients"": true
     },
     ""gradient_accumulation_steps"": 1,
     ""gradient_clipping"": ""auto"",
     ""steps_per_print"": 2000,
     ""train_batch_size"": ""auto"",
     ""train_micro_batch_size_per_gpu"": ""auto"",
     ""wall_clock_breakdown"": false
 }
 ```
 
 ```bash",H3,https://huggingface.co/docs/accelerate/source/usage_guides/deepspeed#deepspeed-config-file,False,2317.0,525.0
1779,Saving and loading,"1. Saving and loading of models is unchanged for ZeRO Stage-1 and Stage-2.
 
 2. under ZeRO Stage-3, `state_dict` contains just the placeholders since the model weights are partitioned across multiple GPUs.
 ZeRO Stage-3 has 2 options:
 
    a. Saving the entire 16bit model weights to directly load later on using `model.load_state_dict(torch.load(pytorch_model.bin))`.
    For this, either set `zero_optimization.stage3_gather_16bit_weights_on_model_save` to True in DeepSpeed Config file or set
    `zero3_save_16bit_model` to True in DeepSpeed Plugin. 
    **Note that this option requires consolidation of the weights on one GPU it can be slow and memory demanding, so only use this feature when needed.**
    Below is the snippet from `examples/by_feature/deepspeed_with_config_support.py` showing this:
    ```python
    unwrapped_model = accelerator.unwrap_model(model)
 
    # New Code #
    # Saves the whole/unpartitioned fp16 model when in ZeRO Stage-3 to the output directory if
    # `stage3_gather_16bit_weights_on_model_save` is True in DeepSpeed Config file or
    # `zero3_save_16bit_model` is True in DeepSpeed Plugin.
    # For Zero Stages 1 and 2, models are saved as usual in the output directory.
    # The model name saved is `pytorch_model.bin`
    unwrapped_model.save_pretrained(
        args.output_dir,
        is_main_process=accelerator.is_main_process,
        save_function=accelerator.save,
        state_dict=accelerator.get_state_dict(model),
    )
    ```
 
    b. To get 32bit weights, first save the model using `model.save_checkpoint()`.
    Below is the snippet from `examples/by_feature/deepspeed_with_config_support.py` showing this:
    ```python
    success = model.save_checkpoint(PATH, ckpt_id, checkpoint_state_dict)
    status_msg = ""checkpointing: PATH={}, ckpt_id={}"".format(PATH, ckpt_id)
    if success:
        logging.info(f""Success {status_msg}"")
    else:
        logging.warning(f""Failure {status_msg}"")
    ``` 
    This will create ZeRO model and optimizer partitions along with `zero_to_fp32.py` script in checkpoint directory.
    You can use this script to do offline consolidation.  
    It requires no configuration files or GPUs. Here is an example of its usage:  
    ```bash
    $ cd /path/to/checkpoint_dir
    $ ./zero_to_fp32.py . pytorch_model.bin
    Processing zero checkpoint at global_step1
    Detected checkpoint of type zero stage 3, world_size: 2
    Saving fp32 state dict to pytorch_model.bin (total_numel=60506624)
    ```
    To get 32bit model for saving/inference, you can perform:
    ```python
    from deepspeed.utils.zero_to_fp32 import load_state_dict_from_zero_checkpoint
 
    unwrapped_model = accelerator.unwrap_model(model)
    fp32_model = load_state_dict_from_zero_checkpoint(unwrapped_model, checkpoint_dir)
    ```
    If you are only interested in the `state_dict`, you can do the following:
    ```python
    from deepspeed.utils.zero_to_fp32 import get_fp32_state_dict_from_zero_checkpoint
 
    state_dict = get_fp32_state_dict_from_zero_checkpoint(checkpoint_dir)
    ```
    Note that all these functions require ~2x memory (general RAM) of the size of the final checkpoint.",H2,https://huggingface.co/docs/accelerate/source/usage_guides/deepspeed#saving-and-loading,False,3180.0,532.0
1780,ZeRO Inference,"DeepSpeed ZeRO Inference supports ZeRO stage 3 with ZeRO-Infinity. 
 It uses the same ZeRO protocol as training, but it doesn't use an optimizer and a lr scheduler and only stage 3 is relevant.
 With accelerate integration, you just need to prepare the model and dataloader as shown below:
 
 ```python
 model, eval_dataloader = accelerator.prepare(model, eval_dataloader)
 ```",H2,https://huggingface.co/docs/accelerate/source/usage_guides/deepspeed#zero-inference,False,377.0,57.0
1781,Few caveats to be aware of,"1. Current integration doesn’t support Pipeline Parallelism of DeepSpeed.
 2. Current integration doesn’t support `mpu`, limiting the tensor parallelism which is supported in Megatron-LM. 
 3. Current integration doesn’t support multiple models.",H2,https://huggingface.co/docs/accelerate/source/usage_guides/deepspeed#few-caveats-to-be-aware-of,False,245.0,32.0
1782,DeepSpeed Resources,The documentation for the internals related to deepspeed can be found [here](../package_reference/deepspeed).,H2,https://huggingface.co/docs/accelerate/source/usage_guides/deepspeed#deepspeed-resources,False,109.0,12.0
1783,rendered properly in your Markdown viewer.,,H2,https://huggingface.co/docs/accelerate/source/usage_guides/fsdp#rendered-properly-in-your-markdown-viewer,False,0.0,1.0
1784,Fully Sharded Data Parallel,"To accelerate training huge models on larger batch sizes, we can use a fully sharded data parallel model.
 This type of data parallel paradigm enables fitting more data and larger models by sharding the optimizer states, gradients and parameters.
 To read more about it and the benefits, check out the [Fully Sharded Data Parallel blog](https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/).
 We have integrated the latest PyTorch's Fully Sharded Data Parallel (FSDP) training feature.
 All you need to do is enable it through the config.",H1,https://huggingface.co/docs/accelerate/source/usage_guides/fsdp#fully-sharded-data-parallel,False,563.0,79.0
1785,How it works out of the box,"On your machine(s) just run:
 
 ```bash
 accelerate config
 ```
 
 and answer the questions asked. This will generate a config file that will be used automatically to properly set the
 default options when doing
 
 ```bash
 accelerate launch my_script.py --args_to_my_script
 ```
 
 For instance, here is how you would run the NLP example (from the root of the repo) with FSDP enabled:
 
 ```bash
 compute_environment: LOCAL_MACHINE
 deepspeed_config: {}
 distributed_type: FSDP
 downcast_bf16: 'no'
 fsdp_config:
   fsdp_auto_wrap_policy: TRANSFORMER_BASED_WRAP
   fsdp_backward_prefetch_policy: BACKWARD_PRE
   fsdp_offload_params: false
   fsdp_sharding_strategy: 1
   fsdp_state_dict_type: FULL_STATE_DICT
   fsdp_transformer_layer_cls_to_wrap: GPT2Block
 machine_rank: 0
 main_process_ip: null
 main_process_port: null
 main_training_function: main
 mixed_precision: 'no'
 num_machines: 1
 num_processes: 2
 use_cpu: false
 ```
 
 ```bash
 accelerate launch examples/nlp_example.py
 ```
 
 Currently, `Accelerate` supports the following config through the CLI:
 
 ```bash
 `Sharding Strategy`: [1] FULL_SHARD (shards optimizer states, gradients and parameters), [2] SHARD_GRAD_OP (shards optimizer states and gradients), [3] NO_SHARD
 `Offload Params`: Decides Whether to offload parameters and gradients to CPU
 `Auto Wrap Policy`: [1] TRANSFORMER_BASED_WRAP, [2] SIZE_BASED_WRAP, [3] NO_WRAP [4] ""HYBRID_SHARD"" [5] ""HYBRID_SHARD_ZERO2""
 `Transformer Layer Class to Wrap`: When using `TRANSFORMER_BASED_WRAP`, user specifies comma-separated string of transformer layer class names (case-sensitive) to wrap ,e.g, 
 `BertLayer`, `GPTJBlock`, `T5Block`, `BertLayer,BertEmbeddings,BertSelfOutput`...
 `Min Num Params`: minimum number of parameters when using `SIZE_BASED_WRAP`
 `Backward Prefetch`: [1] BACKWARD_PRE, [2] BACKWARD_POST, [3] NO_PREFETCH
 `State Dict Type`: [1] FULL_STATE_DICT, [2] LOCAL_STATE_DICT, [3] SHARDED_STATE_DICT  
 `Use Orig Params`: If True, allows non-uniform `requires_grad` during init, which means support for interspersed frozen and trainable paramteres. 
 Useful in cases such as parameter-efficient fine-tuning. 
 Please refer this [blog](https://dev-discuss.pytorch.org/t/rethinking-pytorch-fully-sharded-data-parallel-fsdp-from-first-principles/1019)
 `Sync Module States`: If True, each individually wrapped FSDP unit will broadcast module parameters from rank 0
 `Forward Prefetch`: If True, then FSDP explicitly prefetches the next upcoming all-gather while executing in the forward pass
 ```
 
 For additional and more nuanced control, you can specify other FSDP parameters via `FullyShardedDataParallelPlugin`. 
 When creating `FullyShardedDataParallelPlugin` object, pass it the parameters that weren't part of the accelerate config or if you want to override them.
 The FSDP parameters will be picked based on the accelerate config file or launch command arguments and other parameters that you will pass directly through the `FullyShardedDataParallelPlugin` object will set/override that.
 
 Below is an example:
 
 ```py
 from accelerate import FullyShardedDataParallelPlugin
 from torch.distributed.fsdp.fully_sharded_data_parallel import FullOptimStateDictConfig, FullStateDictConfig
 
 fsdp_plugin = FullyShardedDataParallelPlugin(
     state_dict_config=FullStateDictConfig(offload_to_cpu=False, rank0_only=False),
     optim_state_dict_config=FullOptimStateDictConfig(offload_to_cpu=False, rank0_only=False),
 )
 
 accelerator = Accelerator(fsdp_plugin=fsdp_plugin)
 ```",H2,https://huggingface.co/docs/accelerate/source/usage_guides/fsdp#how-it-works-out-of-the-box,False,3522.0,406.0
1786,Saving and loading,"The new recommended way of checkpointing when using FSDP models is to use `SHARDED_STATE_DICT` as `StateDictType` when setting up the accelerate config.
 Below is the code snippet to save using `save_state` utility of accelerate.
 
 ```py
 accelerator.save_state(""ckpt"")
 ```
 
 Inspect the ckeckpoint folder to see model and optimizer as shards per process:
 ```
 ls ckpt",H2,https://huggingface.co/docs/accelerate/source/usage_guides/fsdp#saving-and-loading,False,372.0,55.0
1787,State Dict,"`accelerator.get_state_dict` will call the underlying `model.state_dict` implementation.  With a model wrapped by FSDP, the default behavior of `state_dict` is to gather all of the state in the rank 0 device.  This can cause CUDA out of memory errors if the parameters don't fit on a single GPU.
 
 To avoid this, PyTorch provides a context manager that adjusts the behavior of `state_dict`.  To offload some of the state dict onto CPU, you can use the following code:
 
 ```
 from torch.distributed.fsdp import FullyShardedDataParallel as FSDP, StateDictType, FullStateDictConfig
 
 full_state_dict_config = FullStateDictConfig(offload_to_cpu=True, rank0_only=True)
 with FSDP.state_dict_type(unwrapped_model, StateDictType.FULL_STATE_DICT, full_state_dict_config):
     state = accelerator.get_state_dict(unwrapped_model)
 ```
 
 You can then pass `state` into the `save_pretrained` method.  There are several modes for `StateDictType` and `FullStateDictConfig` that you can use to control the behavior of `state_dict`.  For more information, see the [PyTorch documentation](https://pytorch.org/docs/stable/fsdp.html).",H3,https://huggingface.co/docs/accelerate/source/usage_guides/fsdp#state-dict,False,1120.0,144.0
1788,A few caveats to be aware of,,H2,https://huggingface.co/docs/accelerate/source/usage_guides/fsdp#a-few-caveats-to-be-aware-of,False,0.0,1.0
1789,rendered properly in your Markdown viewer.,,H2,https://huggingface.co/docs/accelerate/source/usage_guides/quantization#rendered-properly-in-your-markdown-viewer,False,0.0,1.0
1790,Quantization,,H1,https://huggingface.co/docs/accelerate/source/usage_guides/quantization#quantization,False,0.0,1.0
1791,`bitsandbytes` Integration,"🤗 Accelerate brings `bitsandbytes` quantization to your model. You can now load any pytorch model in 8-bit or 4-bit with a few lines of code.
 
 If you want to use 🤗 Transformers models with `bitsandbytes`, you should follow this [documentation](https://huggingface.co/docs/transformers/main_classes/quantization). 
 
 To learn more about how the `bitsandbytes` quantization works, check out the blog posts on [8-bit quantization](https://huggingface.co/blog/hf-bitsandbytes-integration) and [4-bit quantization](https://huggingface.co/blog/4bit-transformers-bitsandbytes).",H2,https://huggingface.co/docs/accelerate/source/usage_guides/quantization#bitsandbytes-integration,False,573.0,63.0
1792,Pre-Requisites,You will need to install the following requirements:,H3,https://huggingface.co/docs/accelerate/source/usage_guides/quantization#pre-requisites,False,52.0,8.0
1793,How it works,"First, we need to initialize our model. To save memory, we can initialize an empty model using the context manager [`init_empty_weights`]. 
 
 Let's take the GPT2 model from minGPT library.
 ```py
 from accelerate import init_empty_weights
 from mingpt.model import GPT
 
 model_config = GPT.get_default_config()
 model_config.model_type = 'gpt2-xl'
 model_config.vocab_size = 50257
 model_config.block_size = 1024
 
 with init_empty_weights():
     empty_model = GPT(model_config)
 ```
 
 Then, we need to get the path to the weights of your model. The path can be the state_dict file (e.g. ""pytorch_model.bin"") or a folder containing the sharded checkpoints. 
 
 ```py
 from huggingface_hub import snapshot_download
 weights_location = snapshot_download(repo_id=""marcsun13/gpt2-xl-linear-sharded"")
 ```
 
 Finally, you need to set your quantization configuration with [`~utils.BnbQuantizationConfig`].
 
 Here's an example for 8-bit quantization:
 ```py
 from accelerate.utils import BnbQuantizationConfig
 bnb_quantization_config = BnbQuantizationConfig(load_in_8bit=True, llm_int8_threshold = 6)
 ```
 
 Here's an example for 4-bit quantization:
 ```py
 from accelerate.utils import BnbQuantizationConfig
 bnb_quantization_config = BnbQuantizationConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.bfloat16, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=""nf4"")
 ```
 
 To quantize your empty model with the selected configuration, you need to use [`~utils.load_and_quantize_model`]. 
 
 ```py
 from accelerate.utils import load_and_quantize_model
 quantized_model = load_and_quantize_model(empty_model, weights_location=weights_location, bnb_quantization_config=bnb_quantization_config, device_map = ""auto"")
 ```",H3,https://huggingface.co/docs/accelerate/source/usage_guides/quantization#how-it-works,False,1726.0,185.0
1794,Saving and loading 8-bit model,"You can save your 8-bit model with accelerate using [`~Accelerator.save_model`]. 
 
 ```py
 from accelerate import Accelerator
 accelerate = Accelerator()
 new_weights_location = ""path/to/save_directory""
 accelerate.save_model(quantized_model, new_weights_location)
 
 quantized_model_from_saved = load_and_quantize_model(empty_model, weights_location=new_weights_location, bnb_quantization_config=bnb_quantization_config, device_map = ""auto"")
 ```
 
 Note that 4-bit model serialization is currently not supported.",H3,https://huggingface.co/docs/accelerate/source/usage_guides/quantization#saving-and-loading-8-bit-model,False,515.0,45.0
1795,Offload modules to cpu and disk,"You can offload some modules to cpu/disk if you don't have enough space on the GPU to store the entire model on your GPUs.
 This uses big model inference under the hood. Check this [documentation](https://huggingface.co/docs/accelerate/usage_guides/big_modeling) for more details. 
 
 For 8-bit quantization, the selected modules will be converted to 8-bit precision. 
 
 For 4-bit quantization, the selected modules will be kept in `torch_dtype` that the user passed in `BnbQuantizationConfig`.  We will add support to convert these offloaded modules in 4-bit when 4-bit serialization will be possible. 
 
  You just need to pass a custom `device_map` in order to offload modules on cpu/disk. The offload modules will be dispatched on the GPU when needed. Here's an example :
 
 ```py
 device_map = {
     ""transformer.wte"": 0,
     ""transformer.wpe"": 0,
     ""transformer.drop"": 0,
     ""transformer.h"": ""cpu"",
     ""transformer.ln_f"": ""disk"",
     ""lm_head"": ""disk"",
 }
 ```",H3,https://huggingface.co/docs/accelerate/source/usage_guides/quantization#offload-modules-to-cpu-and-disk,False,977.0,165.0
1796,Fine-tune a quantized model,"It is not possible to perform pure 8bit or 4bit training on these models. However, you can train these models by leveraging parameter efficient fine tuning methods (PEFT) and train for example adapters on top of them. Please have a look at [peft](https://github.com/huggingface/peft) library for more details.
 
 Currently, you can't add adapters on top of any quantized model. However, with the official support of adapters with 🤗 Transformers models, you can fine-tune quantized models. If you want to finetune a 🤗 Transformers model , follow this [documentation](https://huggingface.co/docs/transformers/main_classes/quantization) instead. Check out this [demo](https://colab.research.google.com/drive/1VoYNfYDKcKRQRor98Zbf2-9VQTtGJ24k?usp=sharing) on how to fine-tune a 4-bit 🤗 Transformers model. 
 
 Note that you don’t need to pass `device_map` when loading the model for training. It will automatically load your model on your GPU. Please note that `device_map=auto` should be used for inference only.",H3,https://huggingface.co/docs/accelerate/source/usage_guides/quantization#fine-tune-a-quantized-model,False,1009.0,137.0
1797,Example demo - running GPT2 1.5b on a Google Colab,"Check out the Google Colab [demo](https://colab.research.google.com/drive/1T1pOgewAWVpR9gKpaEWw4orOrzPFb3yM?usp=sharing) for running quantized models on a GTP2 model. The GPT2-1.5B model checkpoint is in FP32 which uses 6GB of memory. After quantization, it uses 1.6GB with 8-bit modules and 1.2GB with 4-bit modules.",H3,https://huggingface.co/docs/accelerate/source/usage_guides/quantization#example-demo-running-gpt2-15b-on-a-google-colab,False,317.0,39.0
1798,rendered properly in your Markdown viewer.,,H2,https://huggingface.co/docs/accelerate/source/usage_guides/mps#rendered-properly-in-your-markdown-viewer,False,0.0,1.0
1799,Accelerated PyTorch Training on Mac,"With PyTorch v1.12 release, developers and researchers can take advantage of Apple silicon GPUs for significantly faster model training. 
 This unlocks the ability to perform machine learning workflows like prototyping and fine-tuning locally, right on Mac.
 Apple's Metal Performance Shaders (MPS) as a backend for PyTorch enables this and can be used via the new `""mps""` device. 
 This will map computational graphs and primitives on the MPS Graph framework and tuned kernels provided by MPS.
 For more information please refer official documents [Introducing Accelerated PyTorch Training on Mac](https://pytorch.org/blog/introducing-accelerated-pytorch-training-on-mac/)
 and [MPS BACKEND](https://pytorch.org/docs/stable/notes/mps.html).",H1,https://huggingface.co/docs/accelerate/source/usage_guides/mps#accelerated-pytorch-training-on-mac,False,741.0,93.0
1800,Benefits of Training and Inference using Apple Silicon Chips,"1. Enables users to train larger networks or batch sizes locally
 2. Reduces data retrieval latency and provides the GPU with direct access to the full memory store due to unified memory architecture. 
 Therefore, improving end-to-end performance.
 3. Reduces costs associated with cloud-based development or the need for additional local GPUs.
 
 **Pre-requisites**: To install torch with mps support, 
 please follow this nice medium article [GPU-Acceleration Comes to PyTorch on M1 Macs](https://medium.com/towards-data-science/gpu-acceleration-comes-to-pytorch-on-m1-macs-195c399efcc1).",H3,https://huggingface.co/docs/accelerate/source/usage_guides/mps#benefits-of-training-and-inference-using-apple-silicon-chips,False,590.0,74.0
1801,How it works out of the box,"It is enabled by default on MacOs machines with MPS enabled Apple Silicon GPUs.
 To disable it, pass `--cpu` flag to `accelerate launch` command or answer the corresponding question when answering the `accelerate config` questionnaire.
 
 You can directly run the following script to test it out on MPS enabled Apple Silicon machines:
 ```bash
 accelerate launch /examples/cv_example.py --data_dir images
 ```",H2,https://huggingface.co/docs/accelerate/source/usage_guides/mps#how-it-works-out-of-the-box,False,409.0,60.0
1802,A few caveats to be aware of,"1. We strongly recommend to install PyTorch >= 1.13 (nightly version at the time of writing) on your MacOS machine. 
 It has major fixes related to model correctness and performance improvements for transformer based models.
 Please refer to https://github.com/pytorch/pytorch/issues/82707 for more details.
 2. Distributed setups `gloo` and `nccl` are not working with `mps` device. 
 This means that currently only single GPU of `mps` device type can be used.
 
 Finally, please, remember that, 🤗 `Accelerate` only integrates MPS backend, therefore if you
 have any problems or questions with regards to MPS backend usage, please, file an issue with [PyTorch GitHub](https://github.com/pytorch/pytorch/issues).",H2,https://huggingface.co/docs/accelerate/source/usage_guides/mps#a-few-caveats-to-be-aware-of,False,712.0,102.0
1803,rendered properly in your Markdown viewer.,,H2,https://huggingface.co/docs/accelerate/source/usage_guides/local_sgd#rendered-properly-in-your-markdown-viewer,False,0.0,1.0
1804,Using Local SGD with 🤗 Accelerate,"Local SGD is a technique for distributed training where gradients are not synchronized every step. Thus, each process updates its own version of the model weights and after a given number of steps these weights are synchronized by averaging across all processes. This improves communication efficiency and can lead to substantial training speed up especially when a computer lacks a faster interconnect such as NVLink.
 Unlike gradient accumulation (where improving communication efficiency requires increasing the effective batch size), Local SGD does not require changing a batch size or a learning rate / schedule. However, if necessary, Local SGD can be combined with gradient accumulation as well.
 
 In this tutorial you will see how to quickly setup  Local SGD 🤗 Accelerate. Compared to a standard Accelerate setup, this requires only two extra lines of code.
 
 This example will use a very simplistic PyTorch training loop that performs gradient accumulation every two batches:
 
 ```python
 device = ""cuda""
 model.to(device)
 
 gradient_accumulation_steps = 2
 
 for index, batch in enumerate(training_dataloader):
     inputs, targets = batch
     inputs = inputs.to(device)
     targets = targets.to(device)
     outputs = model(inputs)
     loss = loss_function(outputs, targets)
     loss = loss / gradient_accumulation_steps
     loss.backward()
     if (index + 1) % gradient_accumulation_steps == 0:
         optimizer.step()
         scheduler.step()
         optimizer.zero_grad()
 ```",H1,https://huggingface.co/docs/accelerate/source/usage_guides/local_sgd#using-local-sgd-with-accelerate,False,1504.0,261.0
1805,Converting it to 🤗 Accelerate,"First the code shown earlier will be converted to use 🤗 Accelerate  with neither a LocalSGD or a gradient accumulation helper:
 
 ```diff
 + from accelerate import Accelerator
 + accelerator = Accelerator()
 
 + model, optimizer, training_dataloader, scheduler = accelerator.prepare(
 +     model, optimizer, training_dataloader, scheduler
 + )
 
   for index, batch in enumerate(training_dataloader):",H2,https://huggingface.co/docs/accelerate/source/usage_guides/local_sgd#converting-it-to-accelerate,False,401.0,60.0
1806,Letting 🤗 Accelerate handle model synchronization,"All that is left now is to let 🤗 Accelerate handle model parameter synchronization **and** the gradient accumulation for us. For simplicity let us assume we need to synchronize every 8 steps. This is
 achieved by adding one `with LocalSGD` statement and one call `local_sgd.step()` after every optimizer step:
 
 ```diff
 +local_sgd_steps=8
 
 +with LocalSGD(accelerator=accelerator, model=model, local_sgd_steps=8, enabled=True) as local_sgd:
     for batch in training_dataloader:
         with accelerator.accumulate(model):
             inputs, targets = batch
             outputs = model(inputs)
             loss = loss_function(outputs, targets)
             accelerator.backward(loss)
             optimizer.step()
             scheduler.step()
             optimizer.zero_grad()
 +           local_sgd.step()
 ```
 
 Under the hood, the Local SGD code **disables** automatic gradient synchornization (but accumulation still works as expected!). Instead it averages model parameters every `local_sgd_steps` steps (as well as in the end of the training loop).",H2,https://huggingface.co/docs/accelerate/source/usage_guides/local_sgd#letting-accelerate-handle-model-synchronization,False,1067.0,226.0
1807,Limitations,"The current implementation works only with basic multi-GPU (or multi-CPU) training without, e.g., [DeepSpeed.](https://github.com/microsoft/DeepSpeed).",H2,https://huggingface.co/docs/accelerate/source/usage_guides/local_sgd#limitations,False,151.0,14.0
1808,References,"Although we are not aware of the true origins of this simple approach, the idea of local SGD is quite old and goes
     back to at least:
 
     Zhang, J., De Sa, C., Mitliagkas, I., & Ré, C. (2016). [Parallel SGD: When does averaging help?. arXiv preprint
     arXiv:1606.07365.](https://arxiv.org/abs/1606.07365)
 
     We credit the term Local SGD to the following paper (but there might be earlier references we are not aware of).
 
     Stich, Sebastian Urban. [""Local SGD Converges Fast and Communicates Little."" ICLR 2019-International Conference on
     Learning Representations. No. CONF. 2019.](https://arxiv.org/abs/1805.09767)",H2,https://huggingface.co/docs/accelerate/source/usage_guides/local_sgd#references,False,638.0,114.0
1809,rendered properly in your Markdown viewer.,,H2,https://huggingface.co/docs/accelerate/source/concept_guides/gradient_synchronization#rendered-properly-in-your-markdown-viewer,False,0.0,1.0
1810,Gradient Synchronization,"PyTorch's distributed module operates by communicating back and forth between all of the GPUs in your system.
 This communication takes time, and ensuring all processes know the states of each other happens at particular triggerpoints
 when using the `ddp` module. 
 
 These triggerpoints are added to the PyTorch model, specifically their `forward()` and `backward()` methods. 
 This happens when the model is wrapped with `DistributedDataParallel`:
 ```python
 import torch.nn as nn
 from torch.nn.parallel import DistributedDataParallel
 
 model = nn.Linear(10, 10)
 ddp_model = DistributedDataParallel(model)
 ```
 In 🤗 Accelerate this conversion happens automatically when calling [`~Accelerator.prepare`] and passing in your model.
 
 ```diff
 + from accelerate import Accelerator
 + accelerator = Accelerator()",H1,https://huggingface.co/docs/accelerate/source/concept_guides/gradient_synchronization#gradient-synchronization,False,817.0,110.0
1811,The slowdown in gradient accumulation,"You now understand that PyTorch adds hooks to the `forward` and `backward` method of your PyTorch model when 
 training in a distributed setup. But how does this risk slowing down your code?
 
 In DDP (distributed data parallel), the specific order in which processes are performed and ran are expected
 at specific points and these must also occur at roughly the same time before moving on.
 
 The most direct example is when you update model parameters through
 `optimizer.step()`.
 Without gradient accumulation, all instances of the model need to have updated
 their gradients computed, collated, and updated before moving on to the next
 batch of data.
 When performing gradient accumulation, you accumulate `n` loss gradients and
 skip `optimizer.step()` until `n` batches have been reached. As all training
 processes only need to sychronize by the time `optimizer.step()` is called,
 without any modification to your training step, this neededless inter-process
 communication can cause a significant slowdown.
 
  How can you avoid this overhead?",H2,https://huggingface.co/docs/accelerate/source/concept_guides/gradient_synchronization#the-slowdown-in-gradient-accumulation,False,1055.0,163.0
1812,Solving the slowdown problem,"Since you are skipping model parameter updates when training on these batches, their gradients do not need to be synchronized until the point where `optimizer.step()` is actually called. 
 PyTorch cannot automagically tell when you need to do this, but they do provide a tool to help through the [`no_sync`](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel.no_sync) context manager
 that is added to your model after converting it to DDP.
 
 Under this context manager, PyTorch will skip synchronizing the gradients when
 `.backward()` is called, and the first call to `.backward()` outside this 
 context manager will trigger the synchronization. See an example below:
 ```python
 ddp_model, dataloader, optimizer = accelerator.prepare(model, dataloader, optimizer)
 
 for index, batch in enumerate(dataloader):
     inputs, targets = batch
     # Trigger gradient synchronization on the last batch
     if index != (len(dataloader) - 1):
         with ddp_model.no_sync():
             # Gradients only accumulate
             outputs = ddp_model(inputs)
             loss = loss_func(outputs)
             accelerator.backward(loss)
     else:
         # Gradients finally sync
         outputs = ddp_model(inputs)
         loss = loss_func(outputs)
         accelerator.backward(loss)
         optimizer.step()
 ```
 
 In 🤗 Accelerate to make this an API that can be called no matter the training device (though it may not do anything if you are not in a distributed system!),
 `ddp_model.no_sync` gets replaced with [`~Accelerator.no_sync`] and operates the same way:
 
 ```diff
   ddp_model, dataloader, optimizer = accelerator.prepare(model, dataloader, optimizer)
 
   for index, batch in enumerate(dataloader):
       inputs, targets = batch
       # Trigger gradient synchronization on the last batch",H2,https://huggingface.co/docs/accelerate/source/concept_guides/gradient_synchronization#solving-the-slowdown-problem,False,1892.0,353.0
1813,"Just how much of a slowdown is there, and easy mistakes you can make","To set up a realistic example, consider the following setup:
 
 * Two single-GPU T4 nodes and one node with two GPUs
 * Each GPU is a T4, and are hosted on GCP
 * The script used is a modification of the [NLP Example](https://github.com/muellerzr/timing_experiments/blob/main/baseline.py) script
 * Batch size per GPU is 16, and gradients are accumulated every 4 steps
 
 All scripts are available in [this repository](https://github.com/muellerzr/timing_experiments).
 
 If not careful about gradient synchronization and GPU communication, a *large* amount of time can be wasted 
 from when these GPUs communicate to each other during unnecessary periods.
 
 By how much?",H2,https://huggingface.co/docs/accelerate/source/concept_guides/gradient_synchronization#just-how-much-of-a-slowdown-is-there-and-easy-mistakes-you-can-make,False,672.0,101.0
1814,rendered properly in your Markdown viewer.,,H2,https://huggingface.co/docs/accelerate/source/concept_guides/deferring_execution#rendered-properly-in-your-markdown-viewer,False,0.0,1.0
1815,Deferring Executions,"When you run your usual script, instructions are executed in order. Using 🤗 Accelerate to deploy your script on several
 GPUs at the same time introduces a complication: while each process executes all instructions in order, some may be
 faster than others.
 
 You might need to wait for all processes to have reached a certain point before executing a given instruction. For
 instance, you shouldn't save a model before being sure every process is done with training, and you wouldn't want to 
 continue training before all the model weights have been loaded in. To do this, just write the following line in your code:
 
 ```
 accelerator.wait_for_everyone()
 ```
 
 This instruction will block all the processes that arrive first until all the other processes have reached that
 point (if you run your script on just one GPU or CPU, this won't do anything).
 
 A few example cases of when to use this utility are listed below:
 
 <Tip>
 
     Some of these are utilized with the [`~Accelerator.main_process_first`] context manager, which utilizes [`~Accelerator.wait_for_everyone`] to 
     run a particular set of code on the main process beforehand before triggering and launching the other processes
 
 </Tip>",H1,https://huggingface.co/docs/accelerate/source/concept_guides/deferring_execution#deferring-executions,False,1214.0,205.0
1816,Downloading a Dataset,"When downloading a dataset, you should download it first on the main process and then load the cached dataset afterward
 
 <Tip>
 
     `load_dataset` will perform a lock under the hood to stop multiple downloads from happening at once, but if you are downloading something 
     not using this library you should use this method.
     
 </Tip>
 
 ```python
 with accelerator.main_process_first():
     datasets = load_dataset(""glue"", ""mrpc"")
 ```
 
 Under the hood this is the same as calling: 
 
 ```python",H2,https://huggingface.co/docs/accelerate/source/concept_guides/deferring_execution#downloading-a-dataset,False,508.0,95.0
1817,Saving the `state_dict`,"When saving the `state_dict` of the model, since you would normally save one file on just the main process
 you should specify that:
 
 ```python
 if accelerator.is_main_process:
     model = accelerator.unwrap_model(model)
     torch.save(model.state_dict(), ""weights.pth"")
 ```",H2,https://huggingface.co/docs/accelerate/source/concept_guides/deferring_execution#saving-the-state-dict,False,279.0,41.0
1818,Loading in the `state_dict`,"When loading in the `state_dict` to a model, optimizer, or scheduler, you should wait 
 for all workers to have the weights loaded in before moving on to training
 
 ```python
 with accelerator.main_process_first():
     state = torch.load(""weights.pth"")
     model.load_state_dict(state)
 ```",H2,https://huggingface.co/docs/accelerate/source/concept_guides/deferring_execution#loading-in-the-state-dict,False,293.0,46.0
1819,Applying a multi-worker CPU operation,"Applying a `map()` operation on multiple workers, such as tokenizing should be done on the 
 main process first, and then propagated to each one. 
 
 ```python
 datasets = load_dataset(""glue"", ""mrpc"")
 
 with accelerator.main_process_first():
     tokenized_datasets = datasets.map(
         tokenize_function,
         batched=True,
         remove_columns=[""idx"", ""sentence1"", ""sentence2""],
     )
 ```",H2,https://huggingface.co/docs/accelerate/source/concept_guides/deferring_execution#applying-a-multi-worker-cpu-operation,False,404.0,77.0
1820,rendered properly in your Markdown viewer.,,H2,https://huggingface.co/docs/accelerate/source/concept_guides/performance#rendered-properly-in-your-markdown-viewer,False,0.0,1.0
1821,Comparing performance between different device setups,"Evaluating and comparing the performance from different setups can be quite tricky if you don't know what to look for.
 For example, you cannot run the same script with the same batch size across TPU, multi-GPU, and single-GPU with Accelerate 
 and expect your results to line up. 
 
 But why?
 
 There are three reasons for this that this tutorial will cover: 
 
 1. **Setting the right seeds**
 2. **Observed Batch Sizes**
 3. **Learning Rates**",H1,https://huggingface.co/docs/accelerate/source/concept_guides/performance#comparing-performance-between-different-device-setups,False,447.0,78.0
1822,Setting the Seed,"While this issue has not come up as much, make sure to use [`utils.set_seed`] to fully set the seed in all distributed cases so training will be reproducible:
 
 ```python
 from accelerate.utils import set_seed
 
 set_seed(42)
 ```
 
 Why is this important? Under the hood this will set **5** different seed settings:
 
 ```python
     random.seed(seed)
     np.random.seed(seed)
     torch.manual_seed(seed)
     torch.cuda.manual_seed_all(seed)
     # ^^ safe to call this function even if cuda is not available
     if is_tpu_available():
         xm.set_rng_state(seed)
 ```
 
 The random state, numpy's state, torch, torch's cuda state, and if TPUs are available torch_xla's cuda state.",H2,https://huggingface.co/docs/accelerate/source/concept_guides/performance#setting-the-seed,False,691.0,125.0
1823,Observed Batch Sizes,"When training with Accelerate, the batch size passed to the dataloader is the **batch size per GPU**. What this entails is 
 a batch size of 64 on two GPUs is truly a batch size of 128. As a result, when testing on a single GPU this needs to be accounted for,
 as well as similarly for TPUs. 
 
 The below table can be used as a quick reference to try out different batch sizes:
 
 <Tip>
 
 In this example, there are two GPUs for ""Multi-GPU"" and a TPU pod with 8 workers
 
 </Tip>
 
 | Single GPU Batch Size | Multi-GPU Equivalent Batch Size | TPU Equivalent Batch Size |
 |-----------------------|---------------------------------|---------------------------|
 | 256                   | 128                             | 32                        |
 | 128                   | 64                              | 16                        |
 | 64                    | 32                              | 8                         |
 | 32                    | 16                              | 4                         |",H2,https://huggingface.co/docs/accelerate/source/concept_guides/performance#observed-batch-sizes,False,1017.0,426.0
1824,Learning Rates,"As noted in multiple sources[[1](https://aws.amazon.com/blogs/machine-learning/scalable-multi-node-deep-learning-training-using-gpus-in-the-aws-cloud/)][[2](https://docs.nvidia.com/clara/tlt-mi_archive/clara-train-sdk-v2.0/nvmidl/appendix/training_with_multiple_gpus.html)], the learning rate should be scaled *linearly* based on the number of devices present. The below 
 snippet shows doing so with Accelerate:
 
 <Tip>
 
 Since users can have their own learning rate schedulers defined, we leave this up to the user to decide if they wish to scale their 
 learning rate or not.
  
 </Tip>
 
 ```python
 learning_rate = 1e-3
 accelerator = Accelerator()
 learning_rate *= accelerator.num_processes
 
 optimizer = AdamW(params=model.parameters(), lr=learning_rate)
 ```
 
 You will also find that `accelerate` will step the learning rate based on the number of processes being trained on. This is because 
 of the observed batch size noted earlier. So in the case of 2 GPUs, the learning rate will be stepped twice as often as a single GPU
 to account for the batch size being twice as large (if no changes to the batch size on the single GPU instance are made).",H2,https://huggingface.co/docs/accelerate/source/concept_guides/performance#learning-rates,False,1163.0,157.0
1825,Gradient Accumulation and Mixed Precision,"When using gradient accumulation and mixed precision, due to how gradient averaging works (accumulation) and the precision loss (mixed precision), 
 some degradation in performance is expected. This will be explicitly seen when comparing the batch-wise loss between different compute 
 setups. However, the overall loss, metric, and general performance at the end of training should be _roughly_ the same.",H2,https://huggingface.co/docs/accelerate/source/concept_guides/performance#gradient-accumulation-and-mixed-precision,False,405.0,60.0
1826,rendered properly in your Markdown viewer.,,H2,https://huggingface.co/docs/accelerate/source/concept_guides/training_tpu#rendered-properly-in-your-markdown-viewer,False,0.0,1.0
1827,Training on TPUs with 🤗 Accelerate,"Training on TPUs can be slightly different from training on multi-gpu, even with 🤗 Accelerate. This guide aims to show you 
 where you should be careful and why, as well as the best practices in general.",H1,https://huggingface.co/docs/accelerate/source/concept_guides/training_tpu#training-on-tpus-with-accelerate,False,203.0,37.0
1828,Training in a Notebook,"The main carepoint when training on TPUs comes from the [`notebook_launcher`]. As mentioned in the [notebook tutorial](../usage_guides/notebook), you need to 
 restructure your training code into a function that can get passed to the [`notebook_launcher`] function and be careful about not declaring any tensors on the GPU.
 
 While on a TPU that last part is not as important, a critical part to understand is that when you launch code from a notebook you do so through a process called **forking**. 
 When launching from the command-line, you perform **spawning**, where a python process is not currently running and you *spawn* a new process in. Since your Jupyter notebook is already 
 utilizing a python process, you need to *fork* a new process from it to launch your code. 
 
 Where this becomes important is in regard to declaring your model. On forked TPU processes, it is recommended that you instantiate your model *once* and pass this into your 
 training function. This is different than training on GPUs where you create `n` models that have their gradients synced and back-propagated at certain moments. Instead, one 
 model instance is shared between all the nodes and it is passed back and forth. This is important especially when training on low-resource TPUs such as those provided in Kaggle kernels or
 on Google Colaboratory. 
 
 Below is an example of a training function passed to the [`notebook_launcher`] if training on CPUs or GPUs:
 
 <Tip>
 
     This code snippet is based off the one from the `simple_nlp_example` notebook found [here](https://github.com/huggingface/notebooks/blob/main/examples/accelerate/simple_nlp_example.ipynb) with slight 
     modifications for the sake of simplicity
 
 </Tip>
 
 ```python
 def training_function():
     # Initialize accelerator
     accelerator = Accelerator()
     model = AutoModelForSequenceClassification.from_pretrained(""bert-base-cased"", num_labels=2)
     train_dataloader, eval_dataloader = create_dataloaders(
         train_batch_size=hyperparameters[""train_batch_size""], eval_batch_size=hyperparameters[""eval_batch_size""]
     )
 
     # Instantiate optimizer
     optimizer = AdamW(params=model.parameters(), lr=hyperparameters[""learning_rate""])
 
     # Prepare everything
     # There is no specific order to remember, we just need to unpack the objects in the same order we gave them to the
     # prepare method.
     model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(
         model, optimizer, train_dataloader, eval_dataloader
     )
 
     num_epochs = hyperparameters[""num_epochs""]
     # Now we train the model
     for epoch in range(num_epochs):
         model.train()
         for step, batch in enumerate(train_dataloader):
             outputs = model(**batch)
             loss = outputs.loss
             accelerator.backward(loss)
 
             optimizer.step()
             optimizer.zero_grad()
 ```
 
 ```python
 from accelerate import notebook_launcher
 
 notebook_launcher(training_function)
 ```
 
 <Tip>
 
     The `notebook_launcher` will default to 8 processes if 🤗 Accelerate has been configured for a TPU
 
 </Tip>
 
 If you use this example and declare the model *inside* the training loop, then on a low-resource system you will potentially see an error 
 like:
 
 ```
 ProcessExitedException: process 0 terminated with signal SIGSEGV
 ```
 
 This error is *extremely* cryptic but the basic explanation is you ran out of system RAM. You can avoid this entirely by reconfiguring the training function to 
 accept a single `model` argument, and declare it in an outside cell:
 
 ```python",H2,https://huggingface.co/docs/accelerate/source/concept_guides/training_tpu#training-in-a-notebook,False,3627.0,647.0
1829,Mixed Precision and Global Variables,"As mentioned in the [mixed precision tutorial](../usage_guides/mixed_precision), 🤗 Accelerate supports fp16 and bf16, both of which can be used on TPUs.
 That being said, ideally `bf16` should be utilized as it is extremely efficient to use.
 
 There are two ""layers"" when using `bf16` and 🤗 Accelerate on TPUs, at the base level and at the operation level. 
 
 At the base level, this is enabled when passing `mixed_precision=""bf16""` to `Accelerator`, such as:
 ```python
 accelerator = Accelerator(mixed_precision=""bf16"")
 ```
 By default, this will cast `torch.float` and `torch.double` to `bfloat16` on TPUs. 
 The specific configuration being set is an environmental variable of `XLA_USE_BF16` is set to `1`.
 
 There is a further configuration you can perform which is setting the `XLA_DOWNCAST_BF16` environmental variable. If set to `1`, then 
 `torch.float` is `bfloat16` and `torch.double` is `float32`.
 
 This is performed in the `Accelerator` object when passing `downcast_bf16=True`:
 ```python
 accelerator = Accelerator(mixed_precision=""bf16"", downcast_bf16=True)
 ```
 
 Using downcasting instead of bf16 everywhere is good for when you are trying to calculate metrics, log values, and more where raw bf16 tensors would be unusable.",H2,https://huggingface.co/docs/accelerate/source/concept_guides/training_tpu#mixed-precision-and-global-variables,False,1249.0,181.0
1830,Training Times on TPUs,"As you launch your script, you may notice that training seems exceptionally slow at first. This is because TPUs
 first run through a few batches of data to see how much memory to allocate before finally utilizing this configured 
 memory allocation extremely efficiently. 
 
 If you notice that your evaluation code to calculate the metrics of your model takes longer due to a larger batch size being used, 
 it is recommended to keep the batch size the same as the training data if it is too slow. Otherwise the memory will reallocate to this 
 new batch size after the first few iterations. 
 
 <Tip>
 
     Just because the memory is allocated does not mean it will be used or that the batch size will increase when going back to your training dataloader.
 
 </Tip>",H2,https://huggingface.co/docs/accelerate/source/concept_guides/training_tpu#training-times-on-tpus,False,768.0,143.0
